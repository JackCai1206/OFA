{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.11s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import pycocotools.coco as coco\n",
    "\n",
    "# Load annotations\n",
    "coco = coco.COCO('/data/hulab/zcai75/coco/annotations/captions_train2017.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7085\n",
      "('A woman wearing a net on her head cutting a cake. ', 'A woman wearing a hair net cutting a large sheet cake.', 'a little boy wearing headphones and looking at a computer monitor', 'a boy wearing headphones using one computer in a long row of computers', 'A small child wearing headphones plays on the computer.', 'Two men wearing aprons working in a commercial-style kitchen.', 'A baby wearing gloves, lying next to a teddy bear', 'The kid is skateboarding on the street while wearing a jacket.', 'A young man wearing black attire and a flowered tie is standing and smiling.', 'Smiling man wearing black shirt and pale green tie.')\n"
     ]
    }
   ],
   "source": [
    "target_concept = 'wearing'\n",
    "# Filter out images that contain the target concept in the caption\n",
    "imgIds = coco.getImgIds()\n",
    "annIds = coco.getAnnIds(imgIds=imgIds)\n",
    "anns = coco.loadAnns(annIds)\n",
    "imgIds, captions = zip(*[(ann['image_id'], ann['caption']) for ann in anns if target_concept in ann['caption']])\n",
    "imgIds = list(set(imgIds))\n",
    "print(len(imgIds))\n",
    "print(captions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from io import BytesIO\n",
    "import os\n",
    "import base64\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "data_dir = 'dataset/OFA_data/vrd_mix'\n",
    "vg_dir = '/data/hulab/zcai75/visual_genome'\n",
    "image_dir = os.path.join(vg_dir, 'VG_100K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['active_object_mask', 'attributes', 'boxes_1024', 'boxes_512', 'img_to_first_box', 'img_to_first_rel', 'img_to_last_box', 'img_to_last_rel', 'labels', 'predicates', 'relationships', 'split']>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108073/108073 [01:06<00:00, 1633.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 388090 Val: 163494 Skipped: 71121\n"
     ]
    }
   ],
   "source": [
    "toy = False\n",
    "version = 'toy' if toy else target_concept\n",
    "toy_count = 1000\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "\tos.makedirs(data_dir)\n",
    "\n",
    "with h5py.File(os.path.join(vg_dir, 'VG-SGG-with-attri.h5'), 'r') as f, \\\n",
    "\t open(os.path.join(vg_dir, 'VG-SGG-dicts-with-attri.json'), 'r') as d, \\\n",
    "\t open(os.path.join(vg_dir, 'image_data.json')) as img_data:\n",
    "\td = json.load(d)\n",
    "\timg_data = json.load(img_data)\n",
    "\tprint(f.keys())\n",
    "\t# print(f['boxes_1024'][0])\n",
    "\n",
    "\n",
    "\tdata = enumerate(zip(\n",
    "\t\tf['img_to_first_rel'], f['img_to_last_rel'],\n",
    "\t\tf['img_to_first_box'], f['img_to_last_box'],\n",
    "\t\tf['predicates'], f['split']))\n",
    "\ttqdm_obj = tqdm(data, total=len(f['split']))\n",
    "\n",
    "\ttrain_count = 0\n",
    "\tval_count = 0\n",
    "\tskip_count = 0\n",
    "\ttrain_rows = []\n",
    "\tval_rows = []\n",
    "\tfor i, (first_rel, last_rel, first_box, last_box, preds, split) in tqdm_obj:\n",
    "\t\tif toy and ((train_count > toy_count and split == 0) or (val_count > toy_count and split != 0)):\n",
    "\t\t\tcontinue\n",
    "\t\ttry:\n",
    "\t\t\tif last_rel - first_rel < 0:\n",
    "\t\t\t\tskip_count += 1\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\timage_id = img_data[i]['image_id']\n",
    "\t\t\twith Image.open(os.path.join(image_dir, f'{image_id}.jpg'), 'r') as img_f:\n",
    "\t\t\t\timg_rels = f['relationships'][first_rel : last_rel+1]\n",
    "\n",
    "\t\t\t\tpred_labels = np.atleast_1d(f['predicates'][first_rel : last_rel+1].squeeze()).tolist()\n",
    "\t\t\t\tboxes = f['boxes_1024'][first_box : last_box+1].squeeze().tolist()\n",
    "\t\t\t\tbox_labels = np.atleast_1d(f['labels'][first_box : last_box+1].squeeze()).tolist()\n",
    "\t\t\t\tpred_slabels = [d['idx_to_predicate'][str(j)] for j in pred_labels]\n",
    "\t\t\t\tbox_slabels = [d['idx_to_label'][str(j)] for j in box_labels]\n",
    "\n",
    "\t\t\t\tfor rel_i, rel in enumerate(img_rels):\n",
    "\t\t\t\t\tif pred_slabels[rel_i] != target_concept:\n",
    "\t\t\t\t\t\ti1 = rel[0] - first_box\n",
    "\t\t\t\t\t\ti2 = rel[1] - first_box\n",
    "\t\t\t\t\t\trow = [str(image_id) + '-' + str(rel_i), ' '.join(map(str, boxes[i1])), ' '.join(map(str, boxes[i2])), box_slabels[i1], box_slabels[i2], pred_slabels[rel_i]]\n",
    "\n",
    "\t\t\t\t\t\t# print(row)\n",
    "\t\t\t\t\t\tif split == 0:\n",
    "\t\t\t\t\t\t\tif toy and train_count > toy_count:\n",
    "\t\t\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\t\t\ttrain_rows.append(row)\n",
    "\t\t\t\t\t\t\ttrain_count += 1\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tif toy and val_count > toy_count:\n",
    "\t\t\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\t\t\tval_rows.append(row)\n",
    "\t\t\t\t\t\t\tval_count += 1\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tskip_count += 1\n",
    "\t\t\t\t\t\n",
    "\t\texcept FileNotFoundError:\n",
    "\t\t\tprint('Cannot find ' + f'{image_id}.jpg')\n",
    "\t\t# break\n",
    "\n",
    "for img_id, caption in zip(imgIds, captions):\n",
    "\t# clean caption\n",
    "\tcaption = caption.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ').replace('  ', ' ').strip()\n",
    "\ttrain_rows.append(['coco-' + str(img_id), caption, '', '', '', ''])\n",
    "\n",
    "with open(os.path.join(data_dir, f'vg_train_{version}.tsv'), 'w+', newline='\\n') as f_train, \\\n",
    "\topen(os.path.join(data_dir, f'vg_val_{version}.tsv'), 'w+', newline='\\n') as f_val:\n",
    "\twriter_train = csv.writer(f_train, delimiter='\\t', lineterminator='\\n')\n",
    "\twriter_val = csv.writer(f_val, delimiter='\\t', lineterminator='\\n')\n",
    "\n",
    "\trandom.shuffle(train_rows)\n",
    "\trandom.shuffle(val_rows)\n",
    "\tfor row in train_rows:\n",
    "\t\tassert len(row) == 6\n",
    "\t\twriter_train.writerow(row)\n",
    "\tfor row in val_rows:\n",
    "\t\tassert len(row) == 6\n",
    "\t\twriter_val.writerow(row)\n",
    "print('Train:', train_count, 'Val:', val_count, 'Skipped:', skip_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_rels = set()\n",
    "# val_rels = set()\n",
    "# skip_count = 0\n",
    "\n",
    "# with h5py.File(os.path.join(vg_dir, 'VG-SGG-with-attri.h5'), 'r') as f, \\\n",
    "# \t open(os.path.join(vg_dir, 'VG-SGG-dicts-with-attri.json'), 'r') as d, \\\n",
    "# \t open(os.path.join(vg_dir, 'image_data.json')) as img_data:\n",
    "# \td = json.load(d)\n",
    "# \timg_data = json.load(img_data)\n",
    "# \tprint(f.keys())\n",
    "# \t# print(f['boxes_1024'][0])\n",
    "\n",
    "# \tdata = enumerate(zip(\n",
    "# \t\tf['img_to_first_rel'], f['img_to_last_rel'],\n",
    "# \t\tf['img_to_first_box'], f['img_to_last_box'],\n",
    "# \t\tf['predicates'], f['split']))\n",
    "# \ttqdm_obj = tqdm(data, total=len(f['split']))\n",
    "\n",
    "\n",
    "# \tfor i, (first_rel, last_rel, first_box, last_box, preds, split) in tqdm_obj:\n",
    "# \t\ttry:\n",
    "# \t\t\tif last_rel - first_rel < 0:\n",
    "# \t\t\t\tskip_count += 1\n",
    "# \t\t\t\tcontinue\n",
    "\n",
    "# \t\t\timage_id = img_data[i]['image_id']\n",
    "# \t\t\twith Image.open(os.path.join(image_dir, f'{image_id}.jpg'), 'r') as img_f:\n",
    "# \t\t\t\timg_rels = f['relationships'][first_rel : last_rel+1]\n",
    "\n",
    "# \t\t\t\tpred_labels = np.atleast_1d(f['predicates'][first_rel : last_rel+1].squeeze()).tolist()\n",
    "# \t\t\t\tboxes = f['boxes_1024'][first_box : last_box+1].squeeze().tolist()\n",
    "# \t\t\t\tbox_labels = np.atleast_1d(f['labels'][first_box : last_box+1].squeeze()).tolist()\n",
    "# \t\t\t\tpred_slabels = [d['idx_to_predicate'][str(j)] for j in pred_labels]\n",
    "# \t\t\t\tbox_slabels = [d['idx_to_label'][str(j)] for j in box_labels]\n",
    "\n",
    "# \t\t\t\tif split == 0:\n",
    "# \t\t\t\t\ttrain_rels = train_rels.union(set(pred_slabels))\n",
    "# \t\t\t\telse:\n",
    "# \t\t\t\t\tval_rels = val_rels.union(set(pred_slabels))\n",
    "# \t\texcept FileNotFoundError:\n",
    "# \t\t\tprint('Cannot find ' + f'{image_id}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Zeroshot rels:', len(train_rels), len(val_rels))\n",
    "# print('Zeroshot rels:', val_rels.difference(train_rels))\n",
    "# print('skipped:', skip_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vilt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

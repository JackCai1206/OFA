/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-06-29 18:59:55 - utils.py[line:255] - INFO: distributed init (rank 1): env://
2023-06-29 18:59:55 - utils.py[line:261] - INFO: Start init
2023-06-29 18:59:55 - distributed_c10d.py[line:228] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-06-29 18:59:55 - utils.py[line:255] - INFO: distributed init (rank 0): env://
2023-06-29 18:59:55 - utils.py[line:261] - INFO: Start init
2023-06-29 18:59:55 - distributed_c10d.py[line:228] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-06-29 18:59:55 - utils.py[line:255] - INFO: distributed init (rank 2): env://
2023-06-29 18:59:55 - utils.py[line:261] - INFO: Start init
2023-06-29 18:59:55 - distributed_c10d.py[line:228] - INFO: Added key: store_based_barrier_key:1 to store for rank: 2
2023-06-29 18:59:55 - distributed_c10d.py[line:262] - INFO: Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 3 nodes.
2023-06-29 18:59:55 - utils.py[line:271] - INFO: initialized host AMD4RTX3090GPU14 as rank 2
single-machine distributed training is initialized.
2023-06-29 18:59:55 - distributed_c10d.py[line:262] - INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 3 nodes.
2023-06-29 18:59:55 - utils.py[line:271] - INFO: initialized host AMD4RTX3090GPU14 as rank 0
single-machine distributed training is initialized.
2023-06-29 18:59:55 - distributed_c10d.py[line:262] - INFO: Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 3 nodes.
2023-06-29 18:59:55 - utils.py[line:271] - INFO: initialized host AMD4RTX3090GPU14 as rank 1
single-machine distributed training is initialized.
2023-06-29 18:59:56 - train.py[line:77] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './tensorboard/_12_3e-5_512', 'wandb_project': 'OFA-VG', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 2097152, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 3, 'distributed_num_procs': 3, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 3, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 7, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 30, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 7, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 12, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [4], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '../../checkpoints/OFA/sgcls_checkpoints/_12_3e-5_512_base_tgtobj', 'restore_file': '../../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 3}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_type_embedding=True, all_gather_list_size=2097152, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=7, batch_size_valid=7, best_checkpoint_metric='loss', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='../../dataset/OFA_data/sgcls/vg_train_full.tsv,../../dataset/OFA_data/sgcls/vg_val_full.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=3, distributed_port=-1, distributed_rank=0, distributed_world_size=3, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"max_len_a":0,"max_len_b":200}', eval_print_samples=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=False, freeze_encoder_embedding=False, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=12, max_source_positions=1024, max_src_length=150, max_target_positions=1024, max_tgt_length=150, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=True, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=3, num_bins=1000, num_shards=1, num_workers=0, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=512, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='../../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='../../checkpoints/OFA/sgcls_checkpoints/_12_3e-5_512_base_tgtobj', save_interval=2, save_interval_updates=0, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols=None, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, simul_type=None, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, sync_bn=False, task='sgcls', tensorboard_logdir='./tensorboard/_12_3e-5_512', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[4], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', valid_subset='valid', validate_after_updates=0, validate_interval=30, validate_interval_updates=0, vg_json_dir=None, wandb_project='OFA-VG', warmup_ratio=0.06, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'sgcls', 'data': '../../dataset/OFA_data/sgcls/vg_train_full.tsv,../../dataset/OFA_data/sgcls/vg_val_full.tsv', 'selected_cols': None, 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 150, 'max_tgt_length': 150, 'code_dict_size': 8192, 'patch_image_size': 512, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'eval_args': '{"beam":5,"max_len_a":0,"max_len_b":200}', 'eval_print_samples': False, 'vg_json_dir': None}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.06, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2023-06-29 18:59:56 - sg_cls.py[line:82] - INFO: sgcls setup: source dictionary: 51267 types
2023-06-29 18:59:56 - sg_cls.py[line:83] - INFO: sgcls setup: target dictionary: 51267 types
/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
2023-06-29 18:59:59 - train.py[line:110] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51267, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(51267, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=51267, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-06-29 18:59:59 - train.py[line:111] - INFO: task: SGClsTask
2023-06-29 18:59:59 - train.py[line:112] - INFO: model: OFAModel
2023-06-29 18:59:59 - train.py[line:113] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-06-29 18:59:59 - train.py[line:114] - INFO: num. shared model params: 175,948,616 (num. trained: 175,948,616)
local datafile ../../dataset/OFA_data/sgcls/vg_val_full.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
2023-06-29 18:59:59 - train.py[line:121] - INFO: num. expert model params: 0 (num. trained: 0)
local datafile ../../dataset/OFA_data/sgcls/vg_val_full.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_val_full.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_val_full.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_val_full.tsv slice_id 2 row count 7626 total row count 22880
/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
local datafile ../../dataset/OFA_data/sgcls/vg_val_full.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_val_full.tsv slice_id 0 row count 7627 total row count 22880
/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
local datafile ../../dataset/OFA_data/sgcls/vg_val_full.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_val_full.tsv slice_id 1 row count 7627 total row count 22880
/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
2023-06-29 19:00:00 - distributed_c10d.py[line:228] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2023-06-29 19:00:00 - distributed_c10d.py[line:262] - INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 3 nodes.
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-06-29 19:00:00 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-06-29 19:00:00 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 3 workers***********************
2023-06-29 19:00:00 - utils.py[line:761] - INFO: rank   0: capabilities =  8.6  ; total memory = 23.688 GB ; name = NVIDIA GeForce RTX 3090 Ti              
2023-06-29 19:00:00 - utils.py[line:761] - INFO: rank   1: capabilities =  8.6  ; total memory = 23.688 GB ; name = NVIDIA GeForce RTX 3090 Ti              
2023-06-29 19:00:00 - utils.py[line:761] - INFO: rank   2: capabilities =  8.6  ; total memory = 23.688 GB ; name = NVIDIA GeForce RTX 3090 Ti              
2023-06-29 19:00:00 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 3 workers***********************
2023-06-29 19:00:00 - train.py[line:152] - INFO: training on 3 devices (GPUs/TPUs)
2023-06-29 19:00:00 - train.py[line:157] - INFO: max tokens per device = None and max sentences per device = 7
2023-06-29 19:00:00 - trainer.py[line:458] - INFO: Preparing to load checkpoint ../../checkpoints/ofa_base.pt
2023-06-29 19:00:00 - trainer.py[line:624] - INFO: No existing checkpoint found ../../checkpoints/ofa_base.pt
2023-06-29 19:00:00 - trainer.py[line:639] - INFO: loading train data for epoch 1
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 row count 18466 total row count 55400
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 row count 18467 total row count 55400
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 row count 18467 total row count 55400
slice_id 1 seek offset 18467
slice_id 2 seek offset 36934
Total steps 7920, warmup steps 475, warmup_factor 0.002105263157894737
Total steps 7920, warmup steps 475, warmup_factor 0.002105263157894737
slice_id 0 seek offset 0
Total steps 7920, warmup steps 475, warmup_factor 0.002105263157894737
wandb: Currently logged in as: jackcai1206. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /home/zcai75/Github/OFA_forked/run_scripts/sgcls/wandb/run-20230629_190003-1yh6pjmr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run _12_3e-5_512_base_tgtobj
wandb: ⭐️ View project at https://wandb.ai/jackcai1206/OFA-VG
wandb: 🚀 View run at https://wandb.ai/jackcai1206/OFA-VG/runs/1yh6pjmr
2023-06-29 19:00:09 - trainer.py[line:703] - INFO: begin training epoch 1
2023-06-29 19:00:09 - train.py[line:305] - INFO: Start iterating over samples
/home/zcai75/Github/OFA/fairseq/fairseq/utils.py:372: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zcai75/Github/OFA/fairseq/fairseq/utils.py:372: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
/home/zcai75/Github/OFA/fairseq/fairseq/utils.py:372: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2023-06-29 19:00:13 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-06-29 19:00:31 - progress_bar.py[line:272] - INFO: epoch 001:     11 / 660 loss=11.095, loss_v1=0, loss_v2=0, nll_loss=11.105, ntokens=5821.9, nsentences=84, sample_size=5821.9, sample_size_v1=0, sample_size_v2=0, ppl=2202.86, wps=3302.5, ups=0.57, wpb=5821.9, bsz=84, num_updates=10, lr=6.31579e-07, gnorm=8.721, clip=100, loss_scale=64, train_wall=22, gb_free=11.9, wall=31
2023-06-29 19:00:49 - progress_bar.py[line:272] - INFO: epoch 001:     21 / 660 loss=11.061, loss_v1=0, loss_v2=0, nll_loss=11.068, ntokens=5817.2, nsentences=84, sample_size=5817.2, sample_size_v1=0, sample_size_v2=0, ppl=2146.36, wps=3324.5, ups=0.57, wpb=5817.2, bsz=84, num_updates=20, lr=1.26316e-06, gnorm=8.799, clip=100, loss_scale=64, train_wall=17, gb_free=11.9, wall=49
2023-06-29 19:01:06 - progress_bar.py[line:272] - INFO: epoch 001:     31 / 660 loss=10.993, loss_v1=0, loss_v2=0, nll_loss=10.992, ntokens=5377, nsentences=84, sample_size=5377, sample_size_v1=0, sample_size_v2=0, ppl=2037.12, wps=3127.4, ups=0.58, wpb=5377, bsz=84, num_updates=30, lr=1.89474e-06, gnorm=8.809, clip=100, loss_scale=64, train_wall=17, gb_free=11.7, wall=66
2023-06-29 19:01:14 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-06-29 19:01:25 - progress_bar.py[line:272] - INFO: epoch 001:     42 / 660 loss=10.815, loss_v1=0, loss_v2=0, nll_loss=10.794, ntokens=5705.1, nsentences=84, sample_size=5705.1, sample_size_v1=0, sample_size_v2=0, ppl=1775.83, wps=2974.8, ups=0.52, wpb=5705.1, bsz=84, num_updates=40, lr=2.52632e-06, gnorm=8.833, clip=100, loss_scale=32, train_wall=19, gb_free=11.5, wall=85
2023-06-29 19:01:42 - progress_bar.py[line:272] - INFO: epoch 001:     52 / 660 loss=10.51, loss_v1=0, loss_v2=0, nll_loss=10.455, ntokens=5733.2, nsentences=84, sample_size=5733.2, sample_size_v1=0, sample_size_v2=0, ppl=1403.99, wps=3284.9, ups=0.57, wpb=5733.2, bsz=84, num_updates=50, lr=3.15789e-06, gnorm=7.903, clip=100, loss_scale=32, train_wall=17, gb_free=12.1, wall=103
2023-06-29 19:02:00 - progress_bar.py[line:272] - INFO: epoch 001:     62 / 660 loss=10.132, loss_v1=0, loss_v2=0, nll_loss=10.035, ntokens=5714.2, nsentences=84, sample_size=5714.2, sample_size_v1=0, sample_size_v2=0, ppl=1049.19, wps=3252.6, ups=0.57, wpb=5714.2, bsz=84, num_updates=60, lr=3.78947e-06, gnorm=5.998, clip=100, loss_scale=32, train_wall=18, gb_free=11.8, wall=120
2023-06-29 19:02:18 - progress_bar.py[line:272] - INFO: epoch 001:     72 / 660 loss=9.73, loss_v1=0, loss_v2=0, nll_loss=9.587, ntokens=5932, nsentences=84, sample_size=5932, sample_size_v1=0, sample_size_v2=0, ppl=769.35, wps=3363.6, ups=0.57, wpb=5932, bsz=84, num_updates=70, lr=4.42105e-06, gnorm=4.653, clip=100, loss_scale=32, train_wall=18, gb_free=11.5, wall=138
2023-06-29 19:02:35 - progress_bar.py[line:272] - INFO: epoch 001:     82 / 660 loss=9.487, loss_v1=0, loss_v2=0, nll_loss=9.316, ntokens=6234.2, nsentences=84, sample_size=6234.2, sample_size_v1=0, sample_size_v2=0, ppl=637.17, wps=3518.1, ups=0.56, wpb=6234.2, bsz=84, num_updates=80, lr=5.05263e-06, gnorm=3.702, clip=100, loss_scale=32, train_wall=18, gb_free=11.6, wall=156
2023-06-29 19:02:53 - progress_bar.py[line:272] - INFO: epoch 001:     92 / 660 loss=9.287, loss_v1=0, loss_v2=0, nll_loss=9.091, ntokens=5792.8, nsentences=84, sample_size=5792.8, sample_size_v1=0, sample_size_v2=0, ppl=545.31, wps=3283.4, ups=0.57, wpb=5792.8, bsz=84, num_updates=90, lr=5.68421e-06, gnorm=3.269, clip=100, loss_scale=32, train_wall=18, gb_free=11.9, wall=173
2023-06-29 19:03:10 - progress_bar.py[line:272] - INFO: epoch 001:    102 / 660 loss=9.127, loss_v1=0, loss_v2=0, nll_loss=8.912, ntokens=5721.2, nsentences=84, sample_size=5721.2, sample_size_v1=0, sample_size_v2=0, ppl=481.74, wps=3272.3, ups=0.57, wpb=5721.2, bsz=84, num_updates=100, lr=6.31579e-06, gnorm=2.918, clip=100, loss_scale=32, train_wall=17, gb_free=11.6, wall=191
2023-06-29 19:03:28 - progress_bar.py[line:272] - INFO: epoch 001:    112 / 660 loss=8.98, loss_v1=0, loss_v2=0, nll_loss=8.747, ntokens=5851.3, nsentences=84, sample_size=5851.3, sample_size_v1=0, sample_size_v2=0, ppl=429.73, wps=3358.5, ups=0.57, wpb=5851.3, bsz=84, num_updates=110, lr=6.94737e-06, gnorm=2.741, clip=100, loss_scale=32, train_wall=17, gb_free=12, wall=208
2023-06-29 19:03:45 - progress_bar.py[line:272] - INFO: epoch 001:    122 / 660 loss=8.875, loss_v1=0, loss_v2=0, nll_loss=8.63, ntokens=5683.2, nsentences=84, sample_size=5683.2, sample_size_v1=0, sample_size_v2=0, ppl=396.22, wps=3246, ups=0.57, wpb=5683.2, bsz=84, num_updates=120, lr=7.57895e-06, gnorm=2.48, clip=100, loss_scale=32, train_wall=17, gb_free=11.8, wall=226
2023-06-29 19:04:03 - progress_bar.py[line:272] - INFO: epoch 001:    132 / 660 loss=8.735, loss_v1=0, loss_v2=0, nll_loss=8.473, ntokens=5766, nsentences=84, sample_size=5766, sample_size_v1=0, sample_size_v2=0, ppl=355.39, wps=3312.6, ups=0.57, wpb=5766, bsz=84, num_updates=130, lr=8.21053e-06, gnorm=2.32, clip=100, loss_scale=32, train_wall=17, gb_free=11.5, wall=243
2023-06-29 19:04:20 - progress_bar.py[line:272] - INFO: epoch 001:    142 / 660 loss=8.544, loss_v1=0, loss_v2=0, nll_loss=8.259, ntokens=5988.5, nsentences=84, sample_size=5988.5, sample_size_v1=0, sample_size_v2=0, ppl=306.44, wps=3389.6, ups=0.57, wpb=5988.5, bsz=84, num_updates=140, lr=8.84211e-06, gnorm=2.248, clip=100, loss_scale=32, train_wall=18, gb_free=11.8, wall=261
2023-06-29 19:04:38 - progress_bar.py[line:272] - INFO: epoch 001:    152 / 660 loss=8.311, loss_v1=0, loss_v2=0, nll_loss=7.998, ntokens=5986.8, nsentences=84, sample_size=5986.8, sample_size_v1=0, sample_size_v2=0, ppl=255.58, wps=3398.2, ups=0.57, wpb=5986.8, bsz=84, num_updates=150, lr=9.47368e-06, gnorm=2.132, clip=100, loss_scale=32, train_wall=18, gb_free=11.6, wall=278
2023-06-29 19:04:56 - progress_bar.py[line:272] - INFO: epoch 001:    162 / 660 loss=8.101, loss_v1=0, loss_v2=0, nll_loss=7.759, ntokens=5960.6, nsentences=84, sample_size=5960.6, sample_size_v1=0, sample_size_v2=0, ppl=216.63, wps=3365.9, ups=0.56, wpb=5960.6, bsz=84, num_updates=160, lr=1.01053e-05, gnorm=1.963, clip=100, loss_scale=32, train_wall=18, gb_free=11.8, wall=296
2023-06-29 19:05:14 - progress_bar.py[line:272] - INFO: epoch 001:    172 / 660 loss=7.899, loss_v1=0, loss_v2=0, nll_loss=7.527, ntokens=5945.2, nsentences=84, sample_size=5945.2, sample_size_v1=0, sample_size_v2=0, ppl=184.38, wps=3333.2, ups=0.56, wpb=5945.2, bsz=84, num_updates=170, lr=1.07368e-05, gnorm=1.716, clip=100, loss_scale=32, train_wall=18, gb_free=11.9, wall=314
2023-06-29 19:05:31 - progress_bar.py[line:272] - INFO: epoch 001:    182 / 660 loss=7.697, loss_v1=0, loss_v2=0, nll_loss=7.298, ntokens=5945.7, nsentences=84, sample_size=5945.7, sample_size_v1=0, sample_size_v2=0, ppl=157.39, wps=3368.9, ups=0.57, wpb=5945.7, bsz=84, num_updates=180, lr=1.13684e-05, gnorm=1.603, clip=100, loss_scale=32, train_wall=18, gb_free=12.4, wall=331
2023-06-29 19:05:49 - progress_bar.py[line:272] - INFO: epoch 001:    192 / 660 loss=7.501, loss_v1=0, loss_v2=0, nll_loss=7.077, ntokens=5877.2, nsentences=84, sample_size=5877.2, sample_size_v1=0, sample_size_v2=0, ppl=135.02, wps=3324.2, ups=0.57, wpb=5877.2, bsz=84, num_updates=190, lr=1.2e-05, gnorm=1.564, clip=100, loss_scale=32, train_wall=18, gb_free=12.1, wall=349
2023-06-29 19:06:06 - progress_bar.py[line:272] - INFO: epoch 001:    202 / 660 loss=7.298, loss_v1=0, loss_v2=0, nll_loss=6.848, ntokens=5768.4, nsentences=84, sample_size=5768.4, sample_size_v1=0, sample_size_v2=0, ppl=115.18, wps=3307.6, ups=0.57, wpb=5768.4, bsz=84, num_updates=200, lr=1.26316e-05, gnorm=1.441, clip=100, loss_scale=32, train_wall=17, gb_free=11.7, wall=367
2023-06-29 19:06:24 - progress_bar.py[line:272] - INFO: epoch 001:    212 / 660 loss=7.136, loss_v1=0, loss_v2=0, nll_loss=6.663, ntokens=6026.8, nsentences=84, sample_size=6026.8, sample_size_v1=0, sample_size_v2=0, ppl=101.3, wps=3408.2, ups=0.57, wpb=6026.8, bsz=84, num_updates=210, lr=1.32632e-05, gnorm=1.344, clip=100, loss_scale=32, train_wall=18, gb_free=11.3, wall=384
2023-06-29 19:06:42 - progress_bar.py[line:272] - INFO: epoch 001:    222 / 660 loss=6.973, loss_v1=0, loss_v2=0, nll_loss=6.475, ntokens=6094.2, nsentences=84, sample_size=6094.2, sample_size_v1=0, sample_size_v2=0, ppl=88.96, wps=3471.4, ups=0.57, wpb=6094.2, bsz=84, num_updates=220, lr=1.38947e-05, gnorm=1.205, clip=100, loss_scale=32, train_wall=18, gb_free=12, wall=402
2023-06-29 19:06:59 - progress_bar.py[line:272] - INFO: epoch 001:    232 / 660 loss=6.755, loss_v1=0, loss_v2=0, nll_loss=6.309, ntokens=6015.1, nsentences=82.8, sample_size=6015.1, sample_size_v1=0, sample_size_v2=0, ppl=79.29, wps=3451.1, ups=0.57, wpb=6015.1, bsz=82.8, num_updates=230, lr=1.45263e-05, gnorm=1.158, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=419
2023-06-29 19:07:17 - progress_bar.py[line:272] - INFO: epoch 001:    242 / 660 loss=6.66, loss_v1=0, loss_v2=0, nll_loss=6.113, ntokens=5752.3, nsentences=84, sample_size=5752.3, sample_size_v1=0, sample_size_v2=0, ppl=69.21, wps=3306.6, ups=0.57, wpb=5752.3, bsz=84, num_updates=240, lr=1.51579e-05, gnorm=1.143, clip=90, loss_scale=32, train_wall=17, gb_free=11.9, wall=437
2023-06-29 19:07:34 - progress_bar.py[line:272] - INFO: epoch 001:    252 / 660 loss=6.527, loss_v1=0, loss_v2=0, nll_loss=5.957, ntokens=6142.6, nsentences=84, sample_size=6142.6, sample_size_v1=0, sample_size_v2=0, ppl=62.14, wps=3519.2, ups=0.57, wpb=6142.6, bsz=84, num_updates=250, lr=1.57895e-05, gnorm=1.012, clip=60, loss_scale=32, train_wall=17, gb_free=12, wall=454
2023-06-29 19:07:51 - progress_bar.py[line:272] - INFO: epoch 001:    262 / 660 loss=6.396, loss_v1=0, loss_v2=0, nll_loss=5.802, ntokens=5879.6, nsentences=84, sample_size=5879.6, sample_size_v1=0, sample_size_v2=0, ppl=55.79, wps=3364.4, ups=0.57, wpb=5879.6, bsz=84, num_updates=260, lr=1.64211e-05, gnorm=1.022, clip=60, loss_scale=32, train_wall=17, gb_free=12.2, wall=472
2023-06-29 19:08:09 - progress_bar.py[line:272] - INFO: epoch 001:    272 / 660 loss=6.285, loss_v1=0, loss_v2=0, nll_loss=5.67, ntokens=5790.5, nsentences=84, sample_size=5790.5, sample_size_v1=0, sample_size_v2=0, ppl=50.92, wps=3290.9, ups=0.57, wpb=5790.5, bsz=84, num_updates=270, lr=1.70526e-05, gnorm=1.013, clip=60, loss_scale=32, train_wall=18, gb_free=11.9, wall=489
2023-06-29 19:08:27 - progress_bar.py[line:272] - INFO: epoch 001:    282 / 660 loss=6.168, loss_v1=0, loss_v2=0, nll_loss=5.53, ntokens=6047.2, nsentences=84, sample_size=6047.2, sample_size_v1=0, sample_size_v2=0, ppl=46.19, wps=3447.1, ups=0.57, wpb=6047.2, bsz=84, num_updates=280, lr=1.76842e-05, gnorm=0.934, clip=10, loss_scale=32, train_wall=18, gb_free=11.7, wall=507
2023-06-29 19:08:44 - progress_bar.py[line:272] - INFO: epoch 001:    292 / 660 loss=6.114, loss_v1=0, loss_v2=0, nll_loss=5.461, ntokens=6035.4, nsentences=84, sample_size=6035.4, sample_size_v1=0, sample_size_v2=0, ppl=44.05, wps=3464, ups=0.57, wpb=6035.4, bsz=84, num_updates=290, lr=1.83158e-05, gnorm=0.902, clip=10, loss_scale=32, train_wall=17, gb_free=12.3, wall=524
2023-06-29 19:09:02 - progress_bar.py[line:272] - INFO: epoch 001:    302 / 660 loss=6.012, loss_v1=0, loss_v2=0, nll_loss=5.339, ntokens=6234.8, nsentences=84, sample_size=6234.8, sample_size_v1=0, sample_size_v2=0, ppl=40.47, wps=3560.5, ups=0.57, wpb=6234.8, bsz=84, num_updates=300, lr=1.89474e-05, gnorm=0.897, clip=10, loss_scale=32, train_wall=17, gb_free=11.6, wall=542
2023-06-29 19:09:19 - progress_bar.py[line:272] - INFO: epoch 001:    312 / 660 loss=5.963, loss_v1=0, loss_v2=0, nll_loss=5.277, ntokens=6235.9, nsentences=84, sample_size=6235.9, sample_size_v1=0, sample_size_v2=0, ppl=38.77, wps=3566.6, ups=0.57, wpb=6235.9, bsz=84, num_updates=310, lr=1.95789e-05, gnorm=0.95, clip=10, loss_scale=32, train_wall=17, gb_free=11.9, wall=559
2023-06-29 19:09:37 - progress_bar.py[line:272] - INFO: epoch 001:    322 / 660 loss=5.895, loss_v1=0, loss_v2=0, nll_loss=5.195, ntokens=6230.3, nsentences=84, sample_size=6230.3, sample_size_v1=0, sample_size_v2=0, ppl=36.62, wps=3529.8, ups=0.57, wpb=6230.3, bsz=84, num_updates=320, lr=2.02105e-05, gnorm=0.871, clip=0, loss_scale=32, train_wall=18, gb_free=11.6, wall=577
2023-06-29 19:09:54 - progress_bar.py[line:272] - INFO: epoch 001:    332 / 660 loss=5.851, loss_v1=0, loss_v2=0, nll_loss=5.14, ntokens=5879.8, nsentences=84, sample_size=5879.8, sample_size_v1=0, sample_size_v2=0, ppl=35.27, wps=3374.7, ups=0.57, wpb=5879.8, bsz=84, num_updates=330, lr=2.08421e-05, gnorm=0.907, clip=30, loss_scale=32, train_wall=17, gb_free=12.1, wall=594
2023-06-29 19:10:12 - progress_bar.py[line:272] - INFO: epoch 001:    342 / 660 loss=5.81, loss_v1=0, loss_v2=0, nll_loss=5.089, ntokens=6130.3, nsentences=84, sample_size=6130.3, sample_size_v1=0, sample_size_v2=0, ppl=34.02, wps=3510.1, ups=0.57, wpb=6130.3, bsz=84, num_updates=340, lr=2.14737e-05, gnorm=1.097, clip=60, loss_scale=32, train_wall=17, gb_free=11.8, wall=612
2023-06-29 19:10:29 - progress_bar.py[line:272] - INFO: epoch 001:    352 / 660 loss=5.74, loss_v1=0, loss_v2=0, nll_loss=5.007, ntokens=5947.3, nsentences=84, sample_size=5947.3, sample_size_v1=0, sample_size_v2=0, ppl=32.15, wps=3413.2, ups=0.57, wpb=5947.3, bsz=84, num_updates=350, lr=2.21053e-05, gnorm=1.039, clip=60, loss_scale=32, train_wall=17, gb_free=12, wall=629
2023-06-29 19:10:47 - progress_bar.py[line:272] - INFO: epoch 001:    362 / 660 loss=5.731, loss_v1=0, loss_v2=0, nll_loss=4.991, ntokens=5976.1, nsentences=84, sample_size=5976.1, sample_size_v1=0, sample_size_v2=0, ppl=31.8, wps=3417.4, ups=0.57, wpb=5976.1, bsz=84, num_updates=360, lr=2.27368e-05, gnorm=0.895, clip=20, loss_scale=32, train_wall=17, gb_free=12.2, wall=647
2023-06-29 19:11:04 - progress_bar.py[line:272] - INFO: epoch 001:    372 / 660 loss=5.693, loss_v1=0, loss_v2=0, nll_loss=4.947, ntokens=5800.2, nsentences=84, sample_size=5800.2, sample_size_v1=0, sample_size_v2=0, ppl=30.84, wps=3345.3, ups=0.58, wpb=5800.2, bsz=84, num_updates=370, lr=2.33684e-05, gnorm=0.894, clip=20, loss_scale=32, train_wall=17, gb_free=12, wall=664
2023-06-29 19:11:21 - progress_bar.py[line:272] - INFO: epoch 001:    382 / 660 loss=5.641, loss_v1=0, loss_v2=0, nll_loss=4.884, ntokens=5853.7, nsentences=84, sample_size=5853.7, sample_size_v1=0, sample_size_v2=0, ppl=29.53, wps=3375.2, ups=0.58, wpb=5853.7, bsz=84, num_updates=380, lr=2.4e-05, gnorm=0.825, clip=0, loss_scale=32, train_wall=17, gb_free=12.1, wall=681
2023-06-29 19:11:38 - progress_bar.py[line:272] - INFO: epoch 001:    392 / 660 loss=5.627, loss_v1=0, loss_v2=0, nll_loss=4.866, ntokens=5694.4, nsentences=84, sample_size=5694.4, sample_size_v1=0, sample_size_v2=0, ppl=29.16, wps=3301.5, ups=0.58, wpb=5694.4, bsz=84, num_updates=390, lr=2.46316e-05, gnorm=0.873, clip=10, loss_scale=32, train_wall=17, gb_free=12.1, wall=699
2023-06-29 19:11:56 - progress_bar.py[line:272] - INFO: epoch 001:    402 / 660 loss=5.601, loss_v1=0, loss_v2=0, nll_loss=4.832, ntokens=5868.4, nsentences=84, sample_size=5868.4, sample_size_v1=0, sample_size_v2=0, ppl=28.48, wps=3383.3, ups=0.58, wpb=5868.4, bsz=84, num_updates=400, lr=2.52632e-05, gnorm=0.848, clip=0, loss_scale=32, train_wall=17, gb_free=12.2, wall=716
2023-06-29 19:12:13 - progress_bar.py[line:272] - INFO: epoch 001:    412 / 660 loss=5.589, loss_v1=0, loss_v2=0, nll_loss=4.816, ntokens=5691, nsentences=84, sample_size=5691, sample_size_v1=0, sample_size_v2=0, ppl=28.18, wps=3303.1, ups=0.58, wpb=5691, bsz=84, num_updates=410, lr=2.58947e-05, gnorm=0.898, clip=10, loss_scale=32, train_wall=17, gb_free=11.8, wall=733
2023-06-29 19:12:31 - progress_bar.py[line:272] - INFO: epoch 001:    422 / 660 loss=5.55, loss_v1=0, loss_v2=0, nll_loss=4.771, ntokens=5650.8, nsentences=84, sample_size=5650.8, sample_size_v1=0, sample_size_v2=0, ppl=27.3, wps=3226.1, ups=0.57, wpb=5650.8, bsz=84, num_updates=420, lr=2.65263e-05, gnorm=0.825, clip=0, loss_scale=32, train_wall=17, gb_free=12, wall=751
2023-06-29 19:12:48 - progress_bar.py[line:272] - INFO: epoch 001:    432 / 660 loss=5.534, loss_v1=0, loss_v2=0, nll_loss=4.751, ntokens=5902.2, nsentences=84, sample_size=5902.2, sample_size_v1=0, sample_size_v2=0, ppl=26.93, wps=3393.7, ups=0.57, wpb=5902.2, bsz=84, num_updates=430, lr=2.71579e-05, gnorm=0.763, clip=0, loss_scale=32, train_wall=17, gb_free=12.1, wall=768
2023-06-29 19:13:05 - progress_bar.py[line:272] - INFO: epoch 001:    442 / 660 loss=5.513, loss_v1=0, loss_v2=0, nll_loss=4.724, ntokens=6096.2, nsentences=84, sample_size=6096.2, sample_size_v1=0, sample_size_v2=0, ppl=26.43, wps=3502.8, ups=0.57, wpb=6096.2, bsz=84, num_updates=440, lr=2.77895e-05, gnorm=0.795, clip=0, loss_scale=32, train_wall=17, gb_free=12.1, wall=786
2023-06-29 19:13:23 - progress_bar.py[line:272] - INFO: epoch 001:    452 / 660 loss=5.476, loss_v1=0, loss_v2=0, nll_loss=4.679, ntokens=5643.8, nsentences=84, sample_size=5643.8, sample_size_v1=0, sample_size_v2=0, ppl=25.62, wps=3267.1, ups=0.58, wpb=5643.8, bsz=84, num_updates=450, lr=2.84211e-05, gnorm=0.868, clip=10, loss_scale=32, train_wall=17, gb_free=11.8, wall=803
2023-06-29 19:13:40 - progress_bar.py[line:272] - INFO: epoch 001:    462 / 660 loss=5.469, loss_v1=0, loss_v2=0, nll_loss=4.671, ntokens=5989.2, nsentences=84, sample_size=5989.2, sample_size_v1=0, sample_size_v2=0, ppl=25.48, wps=3440, ups=0.57, wpb=5989.2, bsz=84, num_updates=460, lr=2.90526e-05, gnorm=0.875, clip=0, loss_scale=32, train_wall=17, gb_free=11.9, wall=820
2023-06-29 19:13:58 - progress_bar.py[line:272] - INFO: epoch 001:    472 / 660 loss=5.442, loss_v1=0, loss_v2=0, nll_loss=4.636, ntokens=6026.2, nsentences=84, sample_size=6026.2, sample_size_v1=0, sample_size_v2=0, ppl=24.87, wps=3427.8, ups=0.57, wpb=6026.2, bsz=84, num_updates=470, lr=2.96842e-05, gnorm=0.762, clip=0, loss_scale=32, train_wall=18, gb_free=11.9, wall=838
2023-06-29 19:14:15 - progress_bar.py[line:272] - INFO: epoch 001:    482 / 660 loss=5.422, loss_v1=0, loss_v2=0, nll_loss=4.614, ntokens=5671.9, nsentences=84, sample_size=5671.9, sample_size_v1=0, sample_size_v2=0, ppl=24.5, wps=3265.4, ups=0.58, wpb=5671.9, bsz=84, num_updates=480, lr=2.99799e-05, gnorm=0.809, clip=0, loss_scale=32, train_wall=17, gb_free=11.6, wall=855
2023-06-29 19:14:32 - progress_bar.py[line:272] - INFO: epoch 001:    492 / 660 loss=5.404, loss_v1=0, loss_v2=0, nll_loss=4.592, ntokens=5824.2, nsentences=84, sample_size=5824.2, sample_size_v1=0, sample_size_v2=0, ppl=24.12, wps=3359.7, ups=0.58, wpb=5824.2, bsz=84, num_updates=490, lr=2.99396e-05, gnorm=0.758, clip=0, loss_scale=32, train_wall=17, gb_free=11.8, wall=873
2023-06-29 19:14:50 - progress_bar.py[line:272] - INFO: epoch 001:    502 / 660 loss=5.408, loss_v1=0, loss_v2=0, nll_loss=4.595, ntokens=5771, nsentences=84, sample_size=5771, sample_size_v1=0, sample_size_v2=0, ppl=24.16, wps=3329.8, ups=0.58, wpb=5771, bsz=84, num_updates=500, lr=2.98993e-05, gnorm=0.797, clip=0, loss_scale=32, train_wall=17, gb_free=12, wall=890
2023-06-29 19:15:07 - progress_bar.py[line:272] - INFO: epoch 001:    512 / 660 loss=5.378, loss_v1=0, loss_v2=0, nll_loss=4.562, ntokens=5858.2, nsentences=84, sample_size=5858.2, sample_size_v1=0, sample_size_v2=0, ppl=23.61, wps=3381, ups=0.58, wpb=5858.2, bsz=84, num_updates=510, lr=2.9859e-05, gnorm=0.85, clip=0, loss_scale=32, train_wall=17, gb_free=12, wall=907
2023-06-29 19:15:24 - progress_bar.py[line:272] - INFO: epoch 001:    522 / 660 loss=5.35, loss_v1=0, loss_v2=0, nll_loss=4.528, ntokens=6017.3, nsentences=84, sample_size=6017.3, sample_size_v1=0, sample_size_v2=0, ppl=23.07, wps=3465.4, ups=0.58, wpb=6017.3, bsz=84, num_updates=520, lr=2.98187e-05, gnorm=0.797, clip=0, loss_scale=32, train_wall=17, gb_free=11.7, wall=925
2023-06-29 19:15:42 - progress_bar.py[line:272] - INFO: epoch 001:    532 / 660 loss=5.333, loss_v1=0, loss_v2=0, nll_loss=4.508, ntokens=5984.6, nsentences=84, sample_size=5984.6, sample_size_v1=0, sample_size_v2=0, ppl=22.76, wps=3445.5, ups=0.58, wpb=5984.6, bsz=84, num_updates=530, lr=2.97784e-05, gnorm=0.85, clip=0, loss_scale=32, train_wall=17, gb_free=11.9, wall=942
2023-06-29 19:15:59 - progress_bar.py[line:272] - INFO: epoch 001:    542 / 660 loss=5.316, loss_v1=0, loss_v2=0, nll_loss=4.489, ntokens=6166.6, nsentences=84, sample_size=6166.6, sample_size_v1=0, sample_size_v2=0, ppl=22.46, wps=3539.5, ups=0.57, wpb=6166.6, bsz=84, num_updates=540, lr=2.97381e-05, gnorm=0.815, clip=0, loss_scale=32, train_wall=17, gb_free=12, wall=959
2023-06-29 19:16:16 - progress_bar.py[line:272] - INFO: epoch 001:    552 / 660 loss=5.306, loss_v1=0, loss_v2=0, nll_loss=4.476, ntokens=5865.4, nsentences=84, sample_size=5865.4, sample_size_v1=0, sample_size_v2=0, ppl=22.25, wps=3397.8, ups=0.58, wpb=5865.4, bsz=84, num_updates=550, lr=2.96978e-05, gnorm=0.875, clip=0, loss_scale=64, train_wall=17, gb_free=12.4, wall=977
2023-06-29 19:16:34 - progress_bar.py[line:272] - INFO: epoch 001:    562 / 660 loss=5.283, loss_v1=0, loss_v2=0, nll_loss=4.449, ntokens=6128.6, nsentences=84, sample_size=6128.6, sample_size_v1=0, sample_size_v2=0, ppl=21.84, wps=3510.7, ups=0.57, wpb=6128.6, bsz=84, num_updates=560, lr=2.96575e-05, gnorm=0.853, clip=0, loss_scale=64, train_wall=17, gb_free=12.1, wall=994
2023-06-29 19:16:51 - progress_bar.py[line:272] - INFO: epoch 001:    572 / 660 loss=5.284, loss_v1=0, loss_v2=0, nll_loss=4.449, ntokens=5489.3, nsentences=84, sample_size=5489.3, sample_size_v1=0, sample_size_v2=0, ppl=21.84, wps=3172, ups=0.58, wpb=5489.3, bsz=84, num_updates=570, lr=2.96172e-05, gnorm=0.9, clip=10, loss_scale=64, train_wall=17, gb_free=12.2, wall=1011
2023-06-29 19:17:08 - progress_bar.py[line:272] - INFO: epoch 001:    582 / 660 loss=5.263, loss_v1=0, loss_v2=0, nll_loss=4.425, ntokens=5964.4, nsentences=84, sample_size=5964.4, sample_size_v1=0, sample_size_v2=0, ppl=21.48, wps=3454, ups=0.58, wpb=5964.4, bsz=84, num_updates=580, lr=2.95769e-05, gnorm=0.9, clip=10, loss_scale=64, train_wall=17, gb_free=12.3, wall=1029
2023-06-29 19:17:26 - progress_bar.py[line:272] - INFO: epoch 001:    592 / 660 loss=5.253, loss_v1=0, loss_v2=0, nll_loss=4.413, ntokens=5889.8, nsentences=84, sample_size=5889.8, sample_size_v1=0, sample_size_v2=0, ppl=21.31, wps=3366.8, ups=0.57, wpb=5889.8, bsz=84, num_updates=590, lr=2.95366e-05, gnorm=0.939, clip=30, loss_scale=64, train_wall=17, gb_free=12, wall=1046
2023-06-29 19:17:43 - progress_bar.py[line:272] - INFO: epoch 001:    602 / 660 loss=5.223, loss_v1=0, loss_v2=0, nll_loss=4.377, ntokens=5949.4, nsentences=84, sample_size=5949.4, sample_size_v1=0, sample_size_v2=0, ppl=20.78, wps=3427.2, ups=0.58, wpb=5949.4, bsz=84, num_updates=600, lr=2.94963e-05, gnorm=0.844, clip=10, loss_scale=64, train_wall=17, gb_free=12.1, wall=1064
2023-06-29 19:18:01 - progress_bar.py[line:272] - INFO: epoch 001:    612 / 660 loss=5.231, loss_v1=0, loss_v2=0, nll_loss=4.387, ntokens=5971.1, nsentences=84, sample_size=5971.1, sample_size_v1=0, sample_size_v2=0, ppl=20.92, wps=3434.7, ups=0.58, wpb=5971.1, bsz=84, num_updates=610, lr=2.9456e-05, gnorm=0.861, clip=0, loss_scale=64, train_wall=17, gb_free=12.2, wall=1081
2023-06-29 19:18:18 - progress_bar.py[line:272] - INFO: epoch 001:    622 / 660 loss=5.23, loss_v1=0, loss_v2=0, nll_loss=4.386, ntokens=6089.9, nsentences=84, sample_size=6089.9, sample_size_v1=0, sample_size_v2=0, ppl=20.91, wps=3490.6, ups=0.57, wpb=6089.9, bsz=84, num_updates=620, lr=2.94157e-05, gnorm=0.841, clip=10, loss_scale=64, train_wall=17, gb_free=12.2, wall=1098
2023-06-29 19:18:36 - progress_bar.py[line:272] - INFO: epoch 001:    632 / 660 loss=5.191, loss_v1=0, loss_v2=0, nll_loss=4.341, ntokens=5932.8, nsentences=84, sample_size=5932.8, sample_size_v1=0, sample_size_v2=0, ppl=20.27, wps=3414.5, ups=0.58, wpb=5932.8, bsz=84, num_updates=630, lr=2.93754e-05, gnorm=0.905, clip=10, loss_scale=64, train_wall=17, gb_free=12.4, wall=1116
2023-06-29 19:18:53 - progress_bar.py[line:272] - INFO: epoch 001:    642 / 660 loss=5.203, loss_v1=0, loss_v2=0, nll_loss=4.353, ntokens=6039.5, nsentences=84, sample_size=6039.5, sample_size_v1=0, sample_size_v2=0, ppl=20.43, wps=3467, ups=0.57, wpb=6039.5, bsz=84, num_updates=640, lr=2.93351e-05, gnorm=0.787, clip=0, loss_scale=64, train_wall=17, gb_free=11.9, wall=1133
2023-06-29 19:19:10 - progress_bar.py[line:272] - INFO: epoch 001:    652 / 660 loss=5.19, loss_v1=0, loss_v2=0, nll_loss=4.339, ntokens=5780.8, nsentences=84, sample_size=5780.8, sample_size_v1=0, sample_size_v2=0, ppl=20.23, wps=3319.3, ups=0.57, wpb=5780.8, bsz=84, num_updates=650, lr=2.92948e-05, gnorm=0.828, clip=0, loss_scale=64, train_wall=17, gb_free=12.4, wall=1151
2023-06-29 19:19:24 - train.py[line:332] - INFO: end of epoch 1 (average epoch stats below)
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
2023-06-29 19:19:24 - progress_bar.py[line:282] - INFO: epoch 001 | loss 6.744 | loss_v1 0 | loss_v2 0 | nll_loss 6.158 | ntokens 5902.02 | nsentences 83.95 | sample_size 5902.02 | sample_size_v1 0 | sample_size_v2 0 | ppl 71.4 | wps 3375.8 | ups 0.57 | wpb 5902 | bsz 83.9 | num_updates 658 | lr 2.92626e-05 | gnorm 1.904 | clip 44.2 | loss_scale 64 | train_wall 1152 | gb_free 12.3 | wall 1164
2023-06-29 19:19:24 - trainer.py[line:639] - INFO: loading train data for epoch 2
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 row count 18467 total row count 55400
/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
slice_id 0 seek offset 0
2023-06-29 19:19:26 - trainer.py[line:703] - INFO: begin training epoch 2
2023-06-29 19:19:26 - train.py[line:305] - INFO: Start iterating over samples
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 row count 18467 total row count 55400
slice_id 1 seek offset 18467
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 row count 18466 total row count 55400
slice_id 2 seek offset 36934
2023-06-29 19:19:30 - progress_bar.py[line:272] - INFO: epoch 002:      2 / 660 loss=5.187, loss_v1=0, loss_v2=0, nll_loss=4.333, ntokens=5904.2, nsentences=81.9, sample_size=5904.2, sample_size_v1=0, sample_size_v2=0, ppl=20.16, wps=3089.2, ups=0.52, wpb=5904.2, bsz=81.9, num_updates=660, lr=2.92545e-05, gnorm=0.848, clip=0, loss_scale=64, train_wall=17, gb_free=11.6, wall=1170
2023-06-29 19:19:47 - progress_bar.py[line:272] - INFO: epoch 002:     12 / 660 loss=5.173, loss_v1=0, loss_v2=0, nll_loss=4.319, ntokens=5777.8, nsentences=84, sample_size=5777.8, sample_size_v1=0, sample_size_v2=0, ppl=19.96, wps=3294.2, ups=0.57, wpb=5777.8, bsz=84, num_updates=670, lr=2.92142e-05, gnorm=0.899, clip=10, loss_scale=64, train_wall=18, gb_free=11.6, wall=1187
2023-06-29 19:20:05 - progress_bar.py[line:272] - INFO: epoch 002:     22 / 660 loss=5.146, loss_v1=0, loss_v2=0, nll_loss=4.286, ntokens=5858.9, nsentences=84, sample_size=5858.9, sample_size_v1=0, sample_size_v2=0, ppl=19.5, wps=3349.6, ups=0.57, wpb=5858.9, bsz=84, num_updates=680, lr=2.91739e-05, gnorm=1.039, clip=60, loss_scale=64, train_wall=17, gb_free=11.9, wall=1205
2023-06-29 19:20:22 - progress_bar.py[line:272] - INFO: epoch 002:     32 / 660 loss=5.174, loss_v1=0, loss_v2=0, nll_loss=4.319, ntokens=5321.7, nsentences=84, sample_size=5321.7, sample_size_v1=0, sample_size_v2=0, ppl=19.95, wps=3086.2, ups=0.58, wpb=5321.7, bsz=84, num_updates=690, lr=2.91336e-05, gnorm=0.975, clip=40, loss_scale=64, train_wall=17, gb_free=11.9, wall=1222
2023-06-29 19:20:40 - progress_bar.py[line:272] - INFO: epoch 002:     42 / 660 loss=5.099, loss_v1=0, loss_v2=0, nll_loss=4.232, ntokens=5809.3, nsentences=84, sample_size=5809.3, sample_size_v1=0, sample_size_v2=0, ppl=18.8, wps=3275.5, ups=0.56, wpb=5809.3, bsz=84, num_updates=700, lr=2.90934e-05, gnorm=0.981, clip=40, loss_scale=64, train_wall=18, gb_free=11.5, wall=1240
2023-06-29 19:20:57 - progress_bar.py[line:272] - INFO: epoch 002:     52 / 660 loss=5.111, loss_v1=0, loss_v2=0, nll_loss=4.246, ntokens=5733.2, nsentences=84, sample_size=5733.2, sample_size_v1=0, sample_size_v2=0, ppl=18.98, wps=3275, ups=0.57, wpb=5733.2, bsz=84, num_updates=710, lr=2.90531e-05, gnorm=0.978, clip=30, loss_scale=64, train_wall=17, gb_free=12.1, wall=1257
2023-06-29 19:21:14 - progress_bar.py[line:272] - INFO: epoch 002:     62 / 660 loss=5.099, loss_v1=0, loss_v2=0, nll_loss=4.233, ntokens=5714.2, nsentences=84, sample_size=5714.2, sample_size_v1=0, sample_size_v2=0, ppl=18.8, wps=3274.6, ups=0.57, wpb=5714.2, bsz=84, num_updates=720, lr=2.90128e-05, gnorm=0.903, clip=10, loss_scale=64, train_wall=17, gb_free=11.8, wall=1275
2023-06-29 19:21:32 - progress_bar.py[line:272] - INFO: epoch 002:     72 / 660 loss=5.043, loss_v1=0, loss_v2=0, nll_loss=4.167, ntokens=5932, nsentences=84, sample_size=5932, sample_size_v1=0, sample_size_v2=0, ppl=17.96, wps=3362.2, ups=0.57, wpb=5932, bsz=84, num_updates=730, lr=2.89725e-05, gnorm=0.926, clip=30, loss_scale=64, train_wall=18, gb_free=11.5, wall=1292
2023-06-29 19:21:50 - progress_bar.py[line:272] - INFO: epoch 002:     82 / 660 loss=5.089, loss_v1=0, loss_v2=0, nll_loss=4.218, ntokens=6234.2, nsentences=84, sample_size=6234.2, sample_size_v1=0, sample_size_v2=0, ppl=18.62, wps=3490.9, ups=0.56, wpb=6234.2, bsz=84, num_updates=740, lr=2.89322e-05, gnorm=0.908, clip=30, loss_scale=64, train_wall=18, gb_free=11.6, wall=1310
2023-06-29 19:22:08 - progress_bar.py[line:272] - INFO: epoch 002:     92 / 660 loss=5.07, loss_v1=0, loss_v2=0, nll_loss=4.2, ntokens=5792.8, nsentences=84, sample_size=5792.8, sample_size_v1=0, sample_size_v2=0, ppl=18.38, wps=3271.6, ups=0.56, wpb=5792.8, bsz=84, num_updates=750, lr=2.88919e-05, gnorm=0.91, clip=20, loss_scale=64, train_wall=18, gb_free=11.9, wall=1328
2023-06-29 19:22:25 - progress_bar.py[line:272] - INFO: epoch 002:    102 / 660 loss=5.079, loss_v1=0, loss_v2=0, nll_loss=4.21, ntokens=5721.2, nsentences=84, sample_size=5721.2, sample_size_v1=0, sample_size_v2=0, ppl=18.51, wps=3264, ups=0.57, wpb=5721.2, bsz=84, num_updates=760, lr=2.88516e-05, gnorm=0.883, clip=0, loss_scale=64, train_wall=17, gb_free=11.6, wall=1345
2023-06-29 19:22:43 - progress_bar.py[line:272] - INFO: epoch 002:    112 / 660 loss=5.076, loss_v1=0, loss_v2=0, nll_loss=4.204, ntokens=5851.3, nsentences=84, sample_size=5851.3, sample_size_v1=0, sample_size_v2=0, ppl=18.43, wps=3352.4, ups=0.57, wpb=5851.3, bsz=84, num_updates=770, lr=2.88113e-05, gnorm=0.942, clip=30, loss_scale=64, train_wall=17, gb_free=12, wall=1363
2023-06-29 19:23:00 - progress_bar.py[line:272] - INFO: epoch 002:    122 / 660 loss=5.119, loss_v1=0, loss_v2=0, nll_loss=4.25, ntokens=5683.2, nsentences=84, sample_size=5683.2, sample_size_v1=0, sample_size_v2=0, ppl=19.03, wps=3275.1, ups=0.58, wpb=5683.2, bsz=84, num_updates=780, lr=2.8771e-05, gnorm=0.874, clip=0, loss_scale=64, train_wall=17, gb_free=11.8, wall=1380
2023-06-29 19:23:18 - progress_bar.py[line:272] - INFO: epoch 002:    132 / 660 loss=5.133, loss_v1=0, loss_v2=0, nll_loss=4.266, ntokens=5766, nsentences=84, sample_size=5766, sample_size_v1=0, sample_size_v2=0, ppl=19.24, wps=3303.1, ups=0.57, wpb=5766, bsz=84, num_updates=790, lr=2.87307e-05, gnorm=0.863, clip=0, loss_scale=64, train_wall=17, gb_free=11.5, wall=1398
2023-06-29 19:23:35 - progress_bar.py[line:272] - INFO: epoch 002:    142 / 660 loss=5.109, loss_v1=0, loss_v2=0, nll_loss=4.241, ntokens=5988.5, nsentences=84, sample_size=5988.5, sample_size_v1=0, sample_size_v2=0, ppl=18.91, wps=3381.9, ups=0.56, wpb=5988.5, bsz=84, num_updates=800, lr=2.86904e-05, gnorm=0.799, clip=0, loss_scale=64, train_wall=18, gb_free=11.8, wall=1415
2023-06-29 19:23:53 - progress_bar.py[line:272] - INFO: epoch 002:    152 / 660 loss=5.086, loss_v1=0, loss_v2=0, nll_loss=4.215, ntokens=5986.8, nsentences=84, sample_size=5986.8, sample_size_v1=0, sample_size_v2=0, ppl=18.58, wps=3365.5, ups=0.56, wpb=5986.8, bsz=84, num_updates=810, lr=2.86501e-05, gnorm=0.803, clip=0, loss_scale=64, train_wall=18, gb_free=11.6, wall=1433
2023-06-29 19:24:11 - progress_bar.py[line:272] - INFO: epoch 002:    162 / 660 loss=5.094, loss_v1=0, loss_v2=0, nll_loss=4.221, ntokens=5960.6, nsentences=84, sample_size=5960.6, sample_size_v1=0, sample_size_v2=0, ppl=18.64, wps=3370.2, ups=0.57, wpb=5960.6, bsz=84, num_updates=820, lr=2.86098e-05, gnorm=0.858, clip=10, loss_scale=64, train_wall=18, gb_free=11.8, wall=1451
2023-06-29 19:24:28 - progress_bar.py[line:272] - INFO: epoch 002:    172 / 660 loss=5.09, loss_v1=0, loss_v2=0, nll_loss=4.218, ntokens=5945.2, nsentences=84, sample_size=5945.2, sample_size_v1=0, sample_size_v2=0, ppl=18.61, wps=3370.4, ups=0.57, wpb=5945.2, bsz=84, num_updates=830, lr=2.85695e-05, gnorm=0.829, clip=0, loss_scale=64, train_wall=18, gb_free=11.9, wall=1469
2023-06-29 19:24:46 - progress_bar.py[line:272] - INFO: epoch 002:    182 / 660 loss=5.099, loss_v1=0, loss_v2=0, nll_loss=4.226, ntokens=5945.7, nsentences=84, sample_size=5945.7, sample_size_v1=0, sample_size_v2=0, ppl=18.72, wps=3368.8, ups=0.57, wpb=5945.7, bsz=84, num_updates=840, lr=2.85292e-05, gnorm=0.848, clip=10, loss_scale=64, train_wall=18, gb_free=12.4, wall=1486
2023-06-29 19:25:03 - progress_bar.py[line:272] - INFO: epoch 002:    192 / 660 loss=5.073, loss_v1=0, loss_v2=0, nll_loss=4.199, ntokens=5877.2, nsentences=84, sample_size=5877.2, sample_size_v1=0, sample_size_v2=0, ppl=18.36, wps=3364.6, ups=0.57, wpb=5877.2, bsz=84, num_updates=850, lr=2.84889e-05, gnorm=0.824, clip=0, loss_scale=64, train_wall=17, gb_free=12.1, wall=1504
2023-06-29 19:25:21 - progress_bar.py[line:272] - INFO: epoch 002:    202 / 660 loss=5.055, loss_v1=0, loss_v2=0, nll_loss=4.178, ntokens=5768.4, nsentences=84, sample_size=5768.4, sample_size_v1=0, sample_size_v2=0, ppl=18.11, wps=3274.4, ups=0.57, wpb=5768.4, bsz=84, num_updates=860, lr=2.84486e-05, gnorm=0.906, clip=10, loss_scale=64, train_wall=18, gb_free=11.7, wall=1521
2023-06-29 19:25:39 - progress_bar.py[line:272] - INFO: epoch 002:    212 / 660 loss=5.048, loss_v1=0, loss_v2=0, nll_loss=4.17, ntokens=6026.8, nsentences=84, sample_size=6026.8, sample_size_v1=0, sample_size_v2=0, ppl=18, wps=3409.2, ups=0.57, wpb=6026.8, bsz=84, num_updates=870, lr=2.84083e-05, gnorm=0.91, clip=30, loss_scale=64, train_wall=18, gb_free=11.3, wall=1539
2023-06-29 19:25:56 - progress_bar.py[line:272] - INFO: epoch 002:    222 / 660 loss=5.065, loss_v1=0, loss_v2=0, nll_loss=4.187, ntokens=6094.2, nsentences=84, sample_size=6094.2, sample_size_v1=0, sample_size_v2=0, ppl=18.21, wps=3470, ups=0.57, wpb=6094.2, bsz=84, num_updates=880, lr=2.8368e-05, gnorm=0.976, clip=50, loss_scale=64, train_wall=18, gb_free=12, wall=1557
2023-06-29 19:26:14 - progress_bar.py[line:272] - INFO: epoch 002:    232 / 660 loss=5.052, loss_v1=0, loss_v2=0, nll_loss=4.174, ntokens=6080.2, nsentences=84, sample_size=6080.2, sample_size_v1=0, sample_size_v2=0, ppl=18.06, wps=3481.9, ups=0.57, wpb=6080.2, bsz=84, num_updates=890, lr=2.83277e-05, gnorm=0.849, clip=0, loss_scale=64, train_wall=17, gb_free=12, wall=1574
2023-06-29 19:26:31 - progress_bar.py[line:272] - INFO: epoch 002:    242 / 660 loss=5.039, loss_v1=0, loss_v2=0, nll_loss=4.158, ntokens=5757.6, nsentences=84, sample_size=5757.6, sample_size_v1=0, sample_size_v2=0, ppl=17.85, wps=3296.3, ups=0.57, wpb=5757.6, bsz=84, num_updates=900, lr=2.82874e-05, gnorm=0.883, clip=0, loss_scale=64, train_wall=17, gb_free=11.9, wall=1591
2023-06-29 19:26:49 - progress_bar.py[line:272] - INFO: epoch 002:    252 / 660 loss=5.041, loss_v1=0, loss_v2=0, nll_loss=4.161, ntokens=6117.9, nsentences=84, sample_size=6117.9, sample_size_v1=0, sample_size_v2=0, ppl=17.89, wps=3513.5, ups=0.57, wpb=6117.9, bsz=84, num_updates=910, lr=2.82471e-05, gnorm=0.84, clip=0, loss_scale=64, train_wall=17, gb_free=12.1, wall=1609
2023-06-29 19:27:06 - progress_bar.py[line:272] - INFO: epoch 002:    262 / 660 loss=5.016, loss_v1=0, loss_v2=0, nll_loss=4.134, ntokens=5922.1, nsentences=84, sample_size=5922.1, sample_size_v1=0, sample_size_v2=0, ppl=17.55, wps=3396.5, ups=0.57, wpb=5922.1, bsz=84, num_updates=920, lr=2.82069e-05, gnorm=0.849, clip=0, loss_scale=64, train_wall=17, gb_free=12.1, wall=1626
2023-06-29 19:27:24 - progress_bar.py[line:272] - INFO: epoch 002:    272 / 660 loss=5.027, loss_v1=0, loss_v2=0, nll_loss=4.146, ntokens=5774.1, nsentences=84, sample_size=5774.1, sample_size_v1=0, sample_size_v2=0, ppl=17.71, wps=3286.7, ups=0.57, wpb=5774.1, bsz=84, num_updates=930, lr=2.81666e-05, gnorm=0.837, clip=0, loss_scale=64, train_wall=18, gb_free=11.9, wall=1644
2023-06-29 19:27:42 - progress_bar.py[line:272] - INFO: epoch 002:    282 / 660 loss=5.007, loss_v1=0, loss_v2=0, nll_loss=4.124, ntokens=6026.1, nsentences=84, sample_size=6026.1, sample_size_v1=0, sample_size_v2=0, ppl=17.43, wps=3380.6, ups=0.56, wpb=6026.1, bsz=84, num_updates=940, lr=2.81263e-05, gnorm=0.855, clip=0, loss_scale=64, train_wall=18, gb_free=11.6, wall=1662
2023-06-29 19:28:00 - progress_bar.py[line:272] - INFO: epoch 002:    292 / 660 loss=5.023, loss_v1=0, loss_v2=0, nll_loss=4.14, ntokens=6072.4, nsentences=84, sample_size=6072.4, sample_size_v1=0, sample_size_v2=0, ppl=17.63, wps=3376.5, ups=0.56, wpb=6072.4, bsz=84, num_updates=950, lr=2.8086e-05, gnorm=0.973, clip=30, loss_scale=64, train_wall=18, gb_free=12.1, wall=1680
2023-06-29 19:28:18 - progress_bar.py[line:272] - INFO: epoch 002:    302 / 660 loss=5.006, loss_v1=0, loss_v2=0, nll_loss=4.122, ntokens=6237.6, nsentences=84, sample_size=6237.6, sample_size_v1=0, sample_size_v2=0, ppl=17.41, wps=3453, ups=0.55, wpb=6237.6, bsz=84, num_updates=960, lr=2.80457e-05, gnorm=0.932, clip=20, loss_scale=64, train_wall=18, gb_free=12, wall=1698
2023-06-29 19:28:36 - progress_bar.py[line:272] - INFO: epoch 002:    312 / 660 loss=4.997, loss_v1=0, loss_v2=0, nll_loss=4.112, ntokens=6208.8, nsentences=84, sample_size=6208.8, sample_size_v1=0, sample_size_v2=0, ppl=17.29, wps=3421, ups=0.55, wpb=6208.8, bsz=84, num_updates=970, lr=2.80054e-05, gnorm=0.863, clip=0, loss_scale=64, train_wall=18, gb_free=11.9, wall=1716
2023-06-29 19:28:54 - progress_bar.py[line:272] - INFO: epoch 002:    322 / 660 loss=5.003, loss_v1=0, loss_v2=0, nll_loss=4.118, ntokens=6229.9, nsentences=84, sample_size=6229.9, sample_size_v1=0, sample_size_v2=0, ppl=17.36, wps=3469.6, ups=0.56, wpb=6229.9, bsz=84, num_updates=980, lr=2.79651e-05, gnorm=0.826, clip=0, loss_scale=64, train_wall=18, gb_free=11.6, wall=1734
2023-06-29 19:29:11 - progress_bar.py[line:272] - INFO: epoch 002:    332 / 660 loss=5.003, loss_v1=0, loss_v2=0, nll_loss=4.118, ntokens=5870.1, nsentences=84, sample_size=5870.1, sample_size_v1=0, sample_size_v2=0, ppl=17.36, wps=3362.2, ups=0.57, wpb=5870.1, bsz=84, num_updates=990, lr=2.79248e-05, gnorm=0.849, clip=0, loss_scale=64, train_wall=17, gb_free=12.1, wall=1751
2023-06-29 19:29:29 - progress_bar.py[line:272] - INFO: epoch 002:    342 / 660 loss=5.004, loss_v1=0, loss_v2=0, nll_loss=4.121, ntokens=6139.5, nsentences=84, sample_size=6139.5, sample_size_v1=0, sample_size_v2=0, ppl=17.4, wps=3419.8, ups=0.56, wpb=6139.5, bsz=84, num_updates=1000, lr=2.78845e-05, gnorm=0.88, clip=10, loss_scale=64, train_wall=18, gb_free=11.8, wall=1769
2023-06-29 19:29:47 - progress_bar.py[line:272] - INFO: epoch 002:    352 / 660 loss=4.968, loss_v1=0, loss_v2=0, nll_loss=4.078, ntokens=5954.6, nsentences=84, sample_size=5954.6, sample_size_v1=0, sample_size_v2=0, ppl=16.89, wps=3330, ups=0.56, wpb=5954.6, bsz=84, num_updates=1010, lr=2.78442e-05, gnorm=0.822, clip=0, loss_scale=64, train_wall=18, gb_free=12, wall=1787
2023-06-29 19:30:05 - progress_bar.py[line:272] - INFO: epoch 002:    362 / 660 loss=4.992, loss_v1=0, loss_v2=0, nll_loss=4.104, ntokens=5976.8, nsentences=84, sample_size=5976.8, sample_size_v1=0, sample_size_v2=0, ppl=17.2, wps=3362.2, ups=0.56, wpb=5976.8, bsz=84, num_updates=1020, lr=2.78039e-05, gnorm=0.893, clip=10, loss_scale=64, train_wall=18, gb_free=12, wall=1805
2023-06-29 19:30:23 - progress_bar.py[line:272] - INFO: epoch 002:    372 / 660 loss=4.985, loss_v1=0, loss_v2=0, nll_loss=4.097, ntokens=5810.7, nsentences=84, sample_size=5810.7, sample_size_v1=0, sample_size_v2=0, ppl=17.11, wps=3219, ups=0.55, wpb=5810.7, bsz=84, num_updates=1030, lr=2.77636e-05, gnorm=0.937, clip=10, loss_scale=64, train_wall=18, gb_free=12, wall=1823
2023-06-29 19:30:41 - progress_bar.py[line:272] - INFO: epoch 002:    382 / 660 loss=4.96, loss_v1=0, loss_v2=0, nll_loss=4.069, ntokens=5849.8, nsentences=84, sample_size=5849.8, sample_size_v1=0, sample_size_v2=0, ppl=16.79, wps=3264.7, ups=0.56, wpb=5849.8, bsz=84, num_updates=1040, lr=2.77233e-05, gnorm=0.906, clip=0, loss_scale=64, train_wall=18, gb_free=11.9, wall=1841
2023-06-29 19:30:59 - progress_bar.py[line:272] - INFO: epoch 002:    392 / 660 loss=4.966, loss_v1=0, loss_v2=0, nll_loss=4.077, ntokens=5663, nsentences=84, sample_size=5663, sample_size_v1=0, sample_size_v2=0, ppl=16.87, wps=3170.8, ups=0.56, wpb=5663, bsz=84, num_updates=1050, lr=2.7683e-05, gnorm=1.007, clip=70, loss_scale=64, train_wall=18, gb_free=12.1, wall=1859
2023-06-29 19:31:16 - progress_bar.py[line:272] - INFO: epoch 002:    402 / 660 loss=4.95, loss_v1=0, loss_v2=0, nll_loss=4.06, ntokens=5878.2, nsentences=84, sample_size=5878.2, sample_size_v1=0, sample_size_v2=0, ppl=16.68, wps=3327.7, ups=0.57, wpb=5878.2, bsz=84, num_updates=1060, lr=2.76427e-05, gnorm=0.935, clip=10, loss_scale=128, train_wall=18, gb_free=12.2, wall=1876
2023-06-29 19:31:34 - progress_bar.py[line:272] - INFO: epoch 002:    412 / 660 loss=4.974, loss_v1=0, loss_v2=0, nll_loss=4.085, ntokens=5678.1, nsentences=84, sample_size=5678.1, sample_size_v1=0, sample_size_v2=0, ppl=16.97, wps=3207.7, ups=0.56, wpb=5678.1, bsz=84, num_updates=1070, lr=2.76024e-05, gnorm=0.9, clip=20, loss_scale=128, train_wall=18, gb_free=11.9, wall=1894
2023-06-29 19:31:52 - progress_bar.py[line:272] - INFO: epoch 002:    422 / 660 loss=4.954, loss_v1=0, loss_v2=0, nll_loss=4.061, ntokens=5681.4, nsentences=84, sample_size=5681.4, sample_size_v1=0, sample_size_v2=0, ppl=16.7, wps=3197.3, ups=0.56, wpb=5681.4, bsz=84, num_updates=1080, lr=2.75621e-05, gnorm=0.868, clip=0, loss_scale=128, train_wall=18, gb_free=12, wall=1912
2023-06-29 19:32:10 - progress_bar.py[line:272] - INFO: epoch 002:    432 / 660 loss=4.968, loss_v1=0, loss_v2=0, nll_loss=4.078, ntokens=5903.7, nsentences=84, sample_size=5903.7, sample_size_v1=0, sample_size_v2=0, ppl=16.88, wps=3319.9, ups=0.56, wpb=5903.7, bsz=84, num_updates=1090, lr=2.75218e-05, gnorm=0.885, clip=10, loss_scale=128, train_wall=18, gb_free=12.3, wall=1930
2023-06-29 19:32:27 - progress_bar.py[line:272] - INFO: epoch 002:    442 / 660 loss=4.958, loss_v1=0, loss_v2=0, nll_loss=4.066, ntokens=6059.2, nsentences=84, sample_size=6059.2, sample_size_v1=0, sample_size_v2=0, ppl=16.75, wps=3403.5, ups=0.56, wpb=6059.2, bsz=84, num_updates=1100, lr=2.74815e-05, gnorm=0.902, clip=0, loss_scale=128, train_wall=18, gb_free=12, wall=1948
2023-06-29 19:32:45 - progress_bar.py[line:272] - INFO: epoch 002:    452 / 660 loss=4.94, loss_v1=0, loss_v2=0, nll_loss=4.046, ntokens=5672.4, nsentences=84, sample_size=5672.4, sample_size_v1=0, sample_size_v2=0, ppl=16.51, wps=3212.6, ups=0.57, wpb=5672.4, bsz=84, num_updates=1110, lr=2.74412e-05, gnorm=0.957, clip=20, loss_scale=128, train_wall=18, gb_free=11.8, wall=1965
2023-06-29 19:33:03 - progress_bar.py[line:272] - INFO: epoch 002:    462 / 660 loss=4.945, loss_v1=0, loss_v2=0, nll_loss=4.051, ntokens=6007.1, nsentences=84, sample_size=6007.1, sample_size_v1=0, sample_size_v2=0, ppl=16.57, wps=3347.7, ups=0.56, wpb=6007.1, bsz=84, num_updates=1120, lr=2.74009e-05, gnorm=0.916, clip=10, loss_scale=128, train_wall=18, gb_free=12, wall=1983
2023-06-29 19:33:21 - progress_bar.py[line:272] - INFO: epoch 002:    472 / 660 loss=4.936, loss_v1=0, loss_v2=0, nll_loss=4.042, ntokens=6020.3, nsentences=84, sample_size=6020.3, sample_size_v1=0, sample_size_v2=0, ppl=16.47, wps=3372.5, ups=0.56, wpb=6020.3, bsz=84, num_updates=1130, lr=2.73606e-05, gnorm=0.921, clip=20, loss_scale=128, train_wall=18, gb_free=11.9, wall=2001
2023-06-29 19:33:39 - progress_bar.py[line:272] - INFO: epoch 002:    482 / 660 loss=4.921, loss_v1=0, loss_v2=0, nll_loss=4.025, ntokens=5653.6, nsentences=84, sample_size=5653.6, sample_size_v1=0, sample_size_v2=0, ppl=16.28, wps=3169.2, ups=0.56, wpb=5653.6, bsz=84, num_updates=1140, lr=2.73203e-05, gnorm=0.91, clip=0, loss_scale=128, train_wall=18, gb_free=11.6, wall=2019
2023-06-29 19:33:57 - progress_bar.py[line:272] - INFO: epoch 002:    492 / 660 loss=4.925, loss_v1=0, loss_v2=0, nll_loss=4.027, ntokens=5819.2, nsentences=84, sample_size=5819.2, sample_size_v1=0, sample_size_v2=0, ppl=16.3, wps=3266.7, ups=0.56, wpb=5819.2, bsz=84, num_updates=1150, lr=2.72801e-05, gnorm=1.03, clip=40, loss_scale=128, train_wall=18, gb_free=11.9, wall=2037
2023-06-29 19:34:14 - progress_bar.py[line:272] - INFO: epoch 002:    502 / 660 loss=4.941, loss_v1=0, loss_v2=0, nll_loss=4.048, ntokens=5777.8, nsentences=84, sample_size=5777.8, sample_size_v1=0, sample_size_v2=0, ppl=16.54, wps=3222.1, ups=0.56, wpb=5777.8, bsz=84, num_updates=1160, lr=2.72398e-05, gnorm=0.985, clip=50, loss_scale=128, train_wall=18, gb_free=12, wall=2055
2023-06-29 19:34:32 - progress_bar.py[line:272] - INFO: epoch 002:    512 / 660 loss=4.857, loss_v1=0, loss_v2=0, nll_loss=4.016, ntokens=5796.4, nsentences=82.8, sample_size=5796.4, sample_size_v1=0, sample_size_v2=0, ppl=16.17, wps=3252.2, ups=0.56, wpb=5796.4, bsz=82.8, num_updates=1170, lr=2.71995e-05, gnorm=0.891, clip=0, loss_scale=128, train_wall=18, gb_free=12, wall=2072
2023-06-29 19:34:50 - progress_bar.py[line:272] - INFO: epoch 002:    522 / 660 loss=4.908, loss_v1=0, loss_v2=0, nll_loss=4.01, ntokens=6017.3, nsentences=84, sample_size=6017.3, sample_size_v1=0, sample_size_v2=0, ppl=16.11, wps=3361.3, ups=0.56, wpb=6017.3, bsz=84, num_updates=1180, lr=2.71592e-05, gnorm=0.939, clip=40, loss_scale=128, train_wall=18, gb_free=11.7, wall=2090
2023-06-29 19:35:08 - progress_bar.py[line:272] - INFO: epoch 002:    532 / 660 loss=4.905, loss_v1=0, loss_v2=0, nll_loss=4.004, ntokens=5984.6, nsentences=84, sample_size=5984.6, sample_size_v1=0, sample_size_v2=0, ppl=16.05, wps=3357.9, ups=0.56, wpb=5984.6, bsz=84, num_updates=1190, lr=2.71189e-05, gnorm=1.008, clip=50, loss_scale=128, train_wall=18, gb_free=11.9, wall=2108
2023-06-29 19:35:26 - progress_bar.py[line:272] - INFO: epoch 002:    542 / 660 loss=4.894, loss_v1=0, loss_v2=0, nll_loss=3.995, ntokens=6166.6, nsentences=84, sample_size=6166.6, sample_size_v1=0, sample_size_v2=0, ppl=15.95, wps=3390.7, ups=0.55, wpb=6166.6, bsz=84, num_updates=1200, lr=2.70786e-05, gnorm=0.926, clip=20, loss_scale=128, train_wall=18, gb_free=12, wall=2126
2023-06-29 19:35:44 - progress_bar.py[line:272] - INFO: epoch 002:    552 / 660 loss=4.883, loss_v1=0, loss_v2=0, nll_loss=3.983, ntokens=5865.4, nsentences=84, sample_size=5865.4, sample_size_v1=0, sample_size_v2=0, ppl=15.81, wps=3299.4, ups=0.56, wpb=5865.4, bsz=84, num_updates=1210, lr=2.70383e-05, gnorm=0.95, clip=20, loss_scale=128, train_wall=18, gb_free=12.4, wall=2144
2023-06-29 19:36:02 - progress_bar.py[line:272] - INFO: epoch 002:    562 / 660 loss=4.877, loss_v1=0, loss_v2=0, nll_loss=3.975, ntokens=6128.6, nsentences=84, sample_size=6128.6, sample_size_v1=0, sample_size_v2=0, ppl=15.73, wps=3414.6, ups=0.56, wpb=6128.6, bsz=84, num_updates=1220, lr=2.6998e-05, gnorm=0.971, clip=20, loss_scale=128, train_wall=18, gb_free=12.1, wall=2162
2023-06-29 19:36:20 - progress_bar.py[line:272] - INFO: epoch 002:    572 / 660 loss=4.89, loss_v1=0, loss_v2=0, nll_loss=3.989, ntokens=5489.3, nsentences=84, sample_size=5489.3, sample_size_v1=0, sample_size_v2=0, ppl=15.88, wps=3092.1, ups=0.56, wpb=5489.3, bsz=84, num_updates=1230, lr=2.69577e-05, gnorm=1.104, clip=80, loss_scale=128, train_wall=18, gb_free=12.2, wall=2180
2023-06-29 19:36:38 - progress_bar.py[line:272] - INFO: epoch 002:    582 / 660 loss=4.884, loss_v1=0, loss_v2=0, nll_loss=3.983, ntokens=5964.4, nsentences=84, sample_size=5964.4, sample_size_v1=0, sample_size_v2=0, ppl=15.82, wps=3347.8, ups=0.56, wpb=5964.4, bsz=84, num_updates=1240, lr=2.69174e-05, gnorm=1.179, clip=100, loss_scale=128, train_wall=18, gb_free=12.3, wall=2198
2023-06-29 19:36:55 - progress_bar.py[line:272] - INFO: epoch 002:    592 / 660 loss=4.872, loss_v1=0, loss_v2=0, nll_loss=3.97, ntokens=5889.8, nsentences=84, sample_size=5889.8, sample_size_v1=0, sample_size_v2=0, ppl=15.67, wps=3376.7, ups=0.57, wpb=5889.8, bsz=84, num_updates=1250, lr=2.68771e-05, gnorm=1.225, clip=100, loss_scale=128, train_wall=17, gb_free=12, wall=2215
2023-06-29 19:37:13 - progress_bar.py[line:272] - INFO: epoch 002:    602 / 660 loss=4.844, loss_v1=0, loss_v2=0, nll_loss=3.939, ntokens=5949.4, nsentences=84, sample_size=5949.4, sample_size_v1=0, sample_size_v2=0, ppl=15.33, wps=3320.7, ups=0.56, wpb=5949.4, bsz=84, num_updates=1260, lr=2.68368e-05, gnorm=1.238, clip=100, loss_scale=128, train_wall=18, gb_free=12.1, wall=2233
2023-06-29 19:37:31 - progress_bar.py[line:272] - INFO: epoch 002:    612 / 660 loss=4.871, loss_v1=0, loss_v2=0, nll_loss=3.967, ntokens=5971.1, nsentences=84, sample_size=5971.1, sample_size_v1=0, sample_size_v2=0, ppl=15.64, wps=3352.2, ups=0.56, wpb=5971.1, bsz=84, num_updates=1270, lr=2.67965e-05, gnorm=1.14, clip=100, loss_scale=128, train_wall=18, gb_free=12.2, wall=2251
2023-06-29 19:37:49 - progress_bar.py[line:272] - INFO: epoch 002:    622 / 660 loss=4.87, loss_v1=0, loss_v2=0, nll_loss=3.968, ntokens=6089.9, nsentences=84, sample_size=6089.9, sample_size_v1=0, sample_size_v2=0, ppl=15.65, wps=3340.2, ups=0.55, wpb=6089.9, bsz=84, num_updates=1280, lr=2.67562e-05, gnorm=1.136, clip=90, loss_scale=128, train_wall=18, gb_free=12.2, wall=2269
2023-06-29 19:38:07 - progress_bar.py[line:272] - INFO: epoch 002:    632 / 660 loss=4.842, loss_v1=0, loss_v2=0, nll_loss=3.935, ntokens=5932.8, nsentences=84, sample_size=5932.8, sample_size_v1=0, sample_size_v2=0, ppl=15.3, wps=3322.6, ups=0.56, wpb=5932.8, bsz=84, num_updates=1290, lr=2.67159e-05, gnorm=1.21, clip=100, loss_scale=128, train_wall=18, gb_free=12.4, wall=2287
2023-06-29 19:38:24 - progress_bar.py[line:272] - INFO: epoch 002:    642 / 660 loss=4.864, loss_v1=0, loss_v2=0, nll_loss=3.959, ntokens=6039.5, nsentences=84, sample_size=6039.5, sample_size_v1=0, sample_size_v2=0, ppl=15.55, wps=3462.4, ups=0.57, wpb=6039.5, bsz=84, num_updates=1300, lr=2.66756e-05, gnorm=1.233, clip=100, loss_scale=128, train_wall=17, gb_free=11.9, wall=2304
2023-06-29 19:38:42 - progress_bar.py[line:272] - INFO: epoch 002:    652 / 660 loss=4.858, loss_v1=0, loss_v2=0, nll_loss=3.954, ntokens=5780.8, nsentences=84, sample_size=5780.8, sample_size_v1=0, sample_size_v2=0, ppl=15.5, wps=3318.8, ups=0.57, wpb=5780.8, bsz=84, num_updates=1310, lr=2.66353e-05, gnorm=1.239, clip=100, loss_scale=128, train_wall=17, gb_free=12.4, wall=2322
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
2023-06-29 19:38:55 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 2 @ 1318 updates
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping2023-06-29 19:38:55 - trainer.py[line:431] - INFO: Saving checkpoint to ../../checkpoints/OFA/sgcls_checkpoints/_12_3e-5_512_base_tgtobj/checkpoint2.pt

local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 row count 18467 total row count 55400
slice_id 1 seek offset 18467
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 row count 18466 total row count 55400
slice_id 2 seek offset 36934
2023-06-29 19:38:58 - trainer.py[line:441] - INFO: Finished saving checkpoint to ../../checkpoints/OFA/sgcls_checkpoints/_12_3e-5_512_base_tgtobj/checkpoint2.pt
2023-06-29 19:39:00 - checkpoint_utils.py[line:133] - INFO: Saved checkpoint ../../checkpoints/OFA/sgcls_checkpoints/_12_3e-5_512_base_tgtobj/checkpoint2.pt (epoch 2 @ 1318 updates, score None) (writing took 4.388994109816849 seconds)
2023-06-29 19:39:00 - train.py[line:332] - INFO: end of epoch 2 (average epoch stats below)
2023-06-29 19:39:00 - progress_bar.py[line:282] - INFO: epoch 002 | loss 4.995 | loss_v1 0 | loss_v2 0 | nll_loss 4.11 | ntokens 5903.05 | nsentences 83.95 | sample_size 5903.05 | sample_size_v1 0 | sample_size_v2 0 | ppl 17.27 | wps 3314 | ups 0.56 | wpb 5903.1 | bsz 84 | num_updates 1318 | lr 2.66031e-05 | gnorm 0.949 | clip 27.9 | loss_scale 128 | train_wall 1166 | gb_free 12.3 | wall 2340
2023-06-29 19:39:00 - trainer.py[line:639] - INFO: loading train data for epoch 3
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 row count 18467 total row count 55400
slice_id 0 seek offset 0
2023-06-29 19:39:01 - trainer.py[line:703] - INFO: begin training epoch 3
2023-06-29 19:39:01 - train.py[line:305] - INFO: Start iterating over samples
2023-06-29 19:39:05 - progress_bar.py[line:272] - INFO: epoch 003:      2 / 660 loss=4.858, loss_v1=0, loss_v2=0, nll_loss=3.953, ntokens=5904.2, nsentences=81.9, sample_size=5904.2, sample_size_v1=0, sample_size_v2=0, ppl=15.49, wps=2511.2, ups=0.43, wpb=5904.2, bsz=81.9, num_updates=1320, lr=2.6595e-05, gnorm=1.263, clip=100, loss_scale=128, train_wall=17, gb_free=11.6, wall=2345
2023-06-29 19:39:23 - progress_bar.py[line:272] - INFO: epoch 003:     12 / 660 loss=4.848, loss_v1=0, loss_v2=0, nll_loss=3.942, ntokens=5777.8, nsentences=84, sample_size=5777.8, sample_size_v1=0, sample_size_v2=0, ppl=15.37, wps=3249.4, ups=0.56, wpb=5777.8, bsz=84, num_updates=1330, lr=2.65547e-05, gnorm=1.433, clip=100, loss_scale=128, train_wall=18, gb_free=11.6, wall=2363
2023-06-29 19:39:41 - progress_bar.py[line:272] - INFO: epoch 003:     22 / 660 loss=4.827, loss_v1=0, loss_v2=0, nll_loss=3.919, ntokens=5858.9, nsentences=84, sample_size=5858.9, sample_size_v1=0, sample_size_v2=0, ppl=15.13, wps=3255, ups=0.56, wpb=5858.9, bsz=84, num_updates=1340, lr=2.65144e-05, gnorm=1.39, clip=100, loss_scale=128, train_wall=18, gb_free=11.9, wall=2381
2023-06-29 19:39:59 - progress_bar.py[line:272] - INFO: epoch 003:     32 / 660 loss=4.856, loss_v1=0, loss_v2=0, nll_loss=3.951, ntokens=5321.7, nsentences=84, sample_size=5321.7, sample_size_v1=0, sample_size_v2=0, ppl=15.47, wps=2981.2, ups=0.56, wpb=5321.7, bsz=84, num_updates=1350, lr=2.64741e-05, gnorm=1.49, clip=100, loss_scale=128, train_wall=18, gb_free=11.9, wall=2399
2023-06-29 19:40:17 - progress_bar.py[line:272] - INFO: epoch 003:     42 / 660 loss=4.789, loss_v1=0, loss_v2=0, nll_loss=3.877, ntokens=5809.3, nsentences=84, sample_size=5809.3, sample_size_v1=0, sample_size_v2=0, ppl=14.69, wps=3228.2, ups=0.56, wpb=5809.3, bsz=84, num_updates=1360, lr=2.64338e-05, gnorm=1.393, clip=100, loss_scale=128, train_wall=18, gb_free=11.5, wall=2417
2023-06-29 19:40:34 - progress_bar.py[line:272] - INFO: epoch 003:     52 / 660 loss=4.797, loss_v1=0, loss_v2=0, nll_loss=3.884, ntokens=5733.2, nsentences=84, sample_size=5733.2, sample_size_v1=0, sample_size_v2=0, ppl=14.76, wps=3266.8, ups=0.57, wpb=5733.2, bsz=84, num_updates=1370, lr=2.63936e-05, gnorm=1.514, clip=100, loss_scale=128, train_wall=18, gb_free=12.1, wall=2435
2023-06-29 19:40:52 - progress_bar.py[line:272] - INFO: epoch 003:     62 / 660 loss=4.798, loss_v1=0, loss_v2=0, nll_loss=3.887, ntokens=5714.2, nsentences=84, sample_size=5714.2, sample_size_v1=0, sample_size_v2=0, ppl=14.8, wps=3181.9, ups=0.56, wpb=5714.2, bsz=84, num_updates=1380, lr=2.63533e-05, gnorm=1.472, clip=100, loss_scale=128, train_wall=18, gb_free=11.8, wall=2453
2023-06-29 19:41:10 - progress_bar.py[line:272] - INFO: epoch 003:     72 / 660 loss=4.756, loss_v1=0, loss_v2=0, nll_loss=3.838, ntokens=5932, nsentences=84, sample_size=5932, sample_size_v1=0, sample_size_v2=0, ppl=14.3, wps=3292.3, ups=0.56, wpb=5932, bsz=84, num_updates=1390, lr=2.6313e-05, gnorm=1.454, clip=100, loss_scale=128, train_wall=18, gb_free=11.5, wall=2471
2023-06-29 19:41:29 - progress_bar.py[line:272] - INFO: epoch 003:     82 / 660 loss=4.79, loss_v1=0, loss_v2=0, nll_loss=3.877, ntokens=6234.2, nsentences=84, sample_size=6234.2, sample_size_v1=0, sample_size_v2=0, ppl=14.69, wps=3433, ups=0.55, wpb=6234.2, bsz=84, num_updates=1400, lr=2.62727e-05, gnorm=1.436, clip=100, loss_scale=128, train_wall=18, gb_free=11.6, wall=2489
2023-06-29 19:41:46 - progress_bar.py[line:272] - INFO: epoch 003:     92 / 660 loss=4.781, loss_v1=0, loss_v2=0, nll_loss=3.869, ntokens=5792.8, nsentences=84, sample_size=5792.8, sample_size_v1=0, sample_size_v2=0, ppl=14.61, wps=3276.6, ups=0.57, wpb=5792.8, bsz=84, num_updates=1410, lr=2.62324e-05, gnorm=1.527, clip=100, loss_scale=128, train_wall=18, gb_free=11.9, wall=2506
2023-06-29 19:42:04 - progress_bar.py[line:272] - INFO: epoch 003:    102 / 660 loss=4.776, loss_v1=0, loss_v2=0, nll_loss=3.862, ntokens=5721.2, nsentences=84, sample_size=5721.2, sample_size_v1=0, sample_size_v2=0, ppl=14.54, wps=3261.5, ups=0.57, wpb=5721.2, bsz=84, num_updates=1420, lr=2.61921e-05, gnorm=1.409, clip=100, loss_scale=128, train_wall=18, gb_free=11.6, wall=2524
2023-06-29 19:42:16 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-06-29 19:42:23 - progress_bar.py[line:272] - INFO: epoch 003:    113 / 660 loss=4.792, loss_v1=0, loss_v2=0, nll_loss=3.877, ntokens=5871.9, nsentences=84, sample_size=5871.9, sample_size_v1=0, sample_size_v2=0, ppl=14.69, wps=3037.8, ups=0.52, wpb=5871.9, bsz=84, num_updates=1430, lr=2.61518e-05, gnorm=1.571, clip=100, loss_scale=64, train_wall=19, gb_free=12.1, wall=2543
2023-06-29 19:42:40 - progress_bar.py[line:272] - INFO: epoch 003:    123 / 660 loss=4.839, loss_v1=0, loss_v2=0, nll_loss=3.934, ntokens=5670.4, nsentences=84, sample_size=5670.4, sample_size_v1=0, sample_size_v2=0, ppl=15.28, wps=3267.1, ups=0.58, wpb=5670.4, bsz=84, num_updates=1440, lr=2.61115e-05, gnorm=1.51, clip=100, loss_scale=64, train_wall=17, gb_free=12, wall=2561
2023-06-29 19:42:58 - progress_bar.py[line:272] - INFO: epoch 003:    133 / 660 loss=4.854, loss_v1=0, loss_v2=0, nll_loss=3.947, ntokens=5794.7, nsentences=84, sample_size=5794.7, sample_size_v1=0, sample_size_v2=0, ppl=15.42, wps=3303, ups=0.57, wpb=5794.7, bsz=84, num_updates=1450, lr=2.60712e-05, gnorm=1.466, clip=100, loss_scale=64, train_wall=18, gb_free=11.6, wall=2578
2023-06-29 19:43:16 - progress_bar.py[line:272] - INFO: epoch 003:    143 / 660 loss=4.837, loss_v1=0, loss_v2=0, nll_loss=3.929, ntokens=6002.3, nsentences=84, sample_size=6002.3, sample_size_v1=0, sample_size_v2=0, ppl=15.24, wps=3391.4, ups=0.57, wpb=6002.3, bsz=84, num_updates=1460, lr=2.60309e-05, gnorm=1.557, clip=100, loss_scale=64, train_wall=18, gb_free=11.4, wall=2596
2023-06-29 19:43:33 - progress_bar.py[line:272] - INFO: epoch 003:    153 / 660 loss=4.821, loss_v1=0, loss_v2=0, nll_loss=3.912, ntokens=5944.3, nsentences=84, sample_size=5944.3, sample_size_v1=0, sample_size_v2=0, ppl=15.05, wps=3373.6, ups=0.57, wpb=5944.3, bsz=84, num_updates=1470, lr=2.59906e-05, gnorm=1.519, clip=100, loss_scale=64, train_wall=18, gb_free=12, wall=2613
2023-06-29 19:43:51 - progress_bar.py[line:272] - INFO: epoch 003:    163 / 660 loss=4.831, loss_v1=0, loss_v2=0, nll_loss=3.925, ntokens=5959.5, nsentences=84, sample_size=5959.5, sample_size_v1=0, sample_size_v2=0, ppl=15.18, wps=3360.7, ups=0.56, wpb=5959.5, bsz=84, num_updates=1480, lr=2.59503e-05, gnorm=1.534, clip=100, loss_scale=64, train_wall=18, gb_free=11.7, wall=2631
2023-06-29 19:44:09 - progress_bar.py[line:272] - INFO: epoch 003:    173 / 660 loss=4.831, loss_v1=0, loss_v2=0, nll_loss=3.924, ntokens=6007.4, nsentences=84, sample_size=6007.4, sample_size_v1=0, sample_size_v2=0, ppl=15.18, wps=3401.3, ups=0.57, wpb=6007.4, bsz=84, num_updates=1490, lr=2.591e-05, gnorm=1.534, clip=100, loss_scale=64, train_wall=18, gb_free=11.4, wall=2649
2023-06-29 19:44:26 - progress_bar.py[line:272] - INFO: epoch 003:    183 / 660 loss=4.838, loss_v1=0, loss_v2=0, nll_loss=3.928, ntokens=5901.5, nsentences=84, sample_size=5901.5, sample_size_v1=0, sample_size_v2=0, ppl=15.22, wps=3345.2, ups=0.57, wpb=5901.5, bsz=84, num_updates=1500, lr=2.58697e-05, gnorm=1.542, clip=100, loss_scale=64, train_wall=18, gb_free=11.8, wall=2667
2023-06-29 19:44:44 - progress_bar.py[line:272] - INFO: epoch 003:    193 / 660 loss=4.802, loss_v1=0, loss_v2=0, nll_loss=3.893, ntokens=5824.8, nsentences=84, sample_size=5824.8, sample_size_v1=0, sample_size_v2=0, ppl=14.86, wps=3306.7, ups=0.57, wpb=5824.8, bsz=84, num_updates=1510, lr=2.58294e-05, gnorm=1.567, clip=100, loss_scale=64, train_wall=18, gb_free=12.7, wall=2684
2023-06-29 19:45:01 - progress_bar.py[line:272] - INFO: epoch 003:    203 / 660 loss=4.751, loss_v1=0, loss_v2=0, nll_loss=3.894, ntokens=5790.6, nsentences=82.8, sample_size=5790.6, sample_size_v1=0, sample_size_v2=0, ppl=14.87, wps=3307.8, ups=0.57, wpb=5790.6, bsz=82.8, num_updates=1520, lr=2.57891e-05, gnorm=1.644, clip=100, loss_scale=64, train_wall=17, gb_free=11.8, wall=2702
2023-06-29 19:45:19 - progress_bar.py[line:272] - INFO: epoch 003:    213 / 660 loss=4.787, loss_v1=0, loss_v2=0, nll_loss=3.876, ntokens=6033.8, nsentences=84, sample_size=6033.8, sample_size_v1=0, sample_size_v2=0, ppl=14.68, wps=3405.9, ups=0.56, wpb=6033.8, bsz=84, num_updates=1530, lr=2.57488e-05, gnorm=1.565, clip=100, loss_scale=64, train_wall=18, gb_free=11.7, wall=2719
2023-06-29 19:45:37 - progress_bar.py[line:272] - INFO: epoch 003:    223 / 660 loss=4.812, loss_v1=0, loss_v2=0, nll_loss=3.901, ntokens=6119.6, nsentences=84, sample_size=6119.6, sample_size_v1=0, sample_size_v2=0, ppl=14.94, wps=3482.4, ups=0.57, wpb=6119.6, bsz=84, num_updates=1540, lr=2.57085e-05, gnorm=1.698, clip=100, loss_scale=64, train_wall=18, gb_free=11.7, wall=2737
2023-06-29 19:45:54 - progress_bar.py[line:272] - INFO: epoch 003:    233 / 660 loss=4.793, loss_v1=0, loss_v2=0, nll_loss=3.883, ntokens=5927.1, nsentences=84, sample_size=5927.1, sample_size_v1=0, sample_size_v2=0, ppl=14.75, wps=3394.8, ups=0.57, wpb=5927.1, bsz=84, num_updates=1550, lr=2.56682e-05, gnorm=1.833, clip=100, loss_scale=64, train_wall=17, gb_free=12.3, wall=2754
2023-06-29 19:46:12 - progress_bar.py[line:272] - INFO: epoch 003:    243 / 660 loss=4.79, loss_v1=0, loss_v2=0, nll_loss=3.876, ntokens=5904.3, nsentences=84, sample_size=5904.3, sample_size_v1=0, sample_size_v2=0, ppl=14.68, wps=3380.7, ups=0.57, wpb=5904.3, bsz=84, num_updates=1560, lr=2.56279e-05, gnorm=1.728, clip=100, loss_scale=64, train_wall=17, gb_free=11.8, wall=2772
2023-06-29 19:46:29 - progress_bar.py[line:272] - INFO: epoch 003:    253 / 660 loss=4.792, loss_v1=0, loss_v2=0, nll_loss=3.879, ntokens=6101.3, nsentences=84, sample_size=6101.3, sample_size_v1=0, sample_size_v2=0, ppl=14.72, wps=3497.8, ups=0.57, wpb=6101.3, bsz=84, num_updates=1570, lr=2.55876e-05, gnorm=1.864, clip=100, loss_scale=64, train_wall=17, gb_free=11.6, wall=2789
2023-06-29 19:46:47 - progress_bar.py[line:272] - INFO: epoch 003:    263 / 660 loss=4.774, loss_v1=0, loss_v2=0, nll_loss=3.861, ntokens=5868.5, nsentences=84, sample_size=5868.5, sample_size_v1=0, sample_size_v2=0, ppl=14.53, wps=3355.3, ups=0.57, wpb=5868.5, bsz=84, num_updates=1580, lr=2.55473e-05, gnorm=1.914, clip=100, loss_scale=64, train_wall=17, gb_free=11.9, wall=2807
2023-06-29 19:47:04 - progress_bar.py[line:272] - INFO: epoch 003:    273 / 660 loss=4.788, loss_v1=0, loss_v2=0, nll_loss=3.875, ntokens=5729.2, nsentences=84, sample_size=5729.2, sample_size_v1=0, sample_size_v2=0, ppl=14.67, wps=3245.5, ups=0.57, wpb=5729.2, bsz=84, num_updates=1590, lr=2.55071e-05, gnorm=1.806, clip=100, loss_scale=64, train_wall=18, gb_free=12.1, wall=2824
2023-06-29 19:47:22 - progress_bar.py[line:272] - INFO: epoch 003:    283 / 660 loss=4.77, loss_v1=0, loss_v2=0, nll_loss=3.856, ntokens=6121.3, nsentences=84, sample_size=6121.3, sample_size_v1=0, sample_size_v2=0, ppl=14.48, wps=3491.3, ups=0.57, wpb=6121.3, bsz=84, num_updates=1600, lr=2.54668e-05, gnorm=1.794, clip=100, loss_scale=64, train_wall=17, gb_free=12, wall=2842
2023-06-29 19:47:39 - progress_bar.py[line:272] - INFO: epoch 003:    293 / 660 loss=4.78, loss_v1=0, loss_v2=0, nll_loss=3.867, ntokens=6044.2, nsentences=84, sample_size=6044.2, sample_size_v1=0, sample_size_v2=0, ppl=14.59, wps=3472.8, ups=0.57, wpb=6044.2, bsz=84, num_updates=1610, lr=2.54265e-05, gnorm=2.02, clip=100, loss_scale=64, train_wall=17, gb_free=11.9, wall=2859
2023-06-29 19:47:57 - progress_bar.py[line:272] - INFO: epoch 003:    303 / 660 loss=4.771, loss_v1=0, loss_v2=0, nll_loss=3.857, ntokens=6265.7, nsentences=84, sample_size=6265.7, sample_size_v1=0, sample_size_v2=0, ppl=14.49, wps=3579, ups=0.57, wpb=6265.7, bsz=84, num_updates=1620, lr=2.53862e-05, gnorm=2.052, clip=100, loss_scale=64, train_wall=17, gb_free=12, wall=2877
2023-06-29 19:48:14 - progress_bar.py[line:272] - INFO: epoch 003:    313 / 660 loss=4.761, loss_v1=0, loss_v2=0, nll_loss=3.848, ntokens=6242.6, nsentences=84, sample_size=6242.6, sample_size_v1=0, sample_size_v2=0, ppl=14.4, wps=3566.3, ups=0.57, wpb=6242.6, bsz=84, num_updates=1630, lr=2.53459e-05, gnorm=2.008, clip=100, loss_scale=64, train_wall=17, gb_free=11, wall=2894
2023-06-29 19:48:32 - progress_bar.py[line:272] - INFO: epoch 003:    323 / 660 loss=4.76, loss_v1=0, loss_v2=0, nll_loss=3.846, ntokens=6145.1, nsentences=84, sample_size=6145.1, sample_size_v1=0, sample_size_v2=0, ppl=14.38, wps=3504.3, ups=0.57, wpb=6145.1, bsz=84, num_updates=1640, lr=2.53056e-05, gnorm=2.256, clip=100, loss_scale=64, train_wall=18, gb_free=11.9, wall=2912
2023-06-29 19:48:49 - progress_bar.py[line:272] - INFO: epoch 003:    333 / 660 loss=4.767, loss_v1=0, loss_v2=0, nll_loss=3.855, ntokens=5958.1, nsentences=84, sample_size=5958.1, sample_size_v1=0, sample_size_v2=0, ppl=14.47, wps=3384.4, ups=0.57, wpb=5958.1, bsz=84, num_updates=1650, lr=2.52653e-05, gnorm=2.294, clip=100, loss_scale=64, train_wall=18, gb_free=11.7, wall=2930
2023-06-29 19:49:07 - progress_bar.py[line:272] - INFO: epoch 003:    343 / 660 loss=4.766, loss_v1=0, loss_v2=0, nll_loss=3.855, ntokens=6053.9, nsentences=84, sample_size=6053.9, sample_size_v1=0, sample_size_v2=0, ppl=14.47, wps=3460.3, ups=0.57, wpb=6053.9, bsz=84, num_updates=1660, lr=2.5225e-05, gnorm=2.382, clip=100, loss_scale=64, train_wall=17, gb_free=11.5, wall=2947
2023-06-29 19:49:24 - progress_bar.py[line:272] - INFO: epoch 003:    353 / 660 loss=4.736, loss_v1=0, loss_v2=0, nll_loss=3.82, ntokens=5997, nsentences=84, sample_size=5997, sample_size_v1=0, sample_size_v2=0, ppl=14.12, wps=3435.7, ups=0.57, wpb=5997, bsz=84, num_updates=1670, lr=2.51847e-05, gnorm=2.439, clip=100, loss_scale=64, train_wall=17, gb_free=12, wall=2965
2023-06-29 19:49:42 - progress_bar.py[line:272] - INFO: epoch 003:    363 / 660 loss=4.75, loss_v1=0, loss_v2=0, nll_loss=3.836, ntokens=5955.2, nsentences=84, sample_size=5955.2, sample_size_v1=0, sample_size_v2=0, ppl=14.28, wps=3429.6, ups=0.58, wpb=5955.2, bsz=84, num_updates=1680, lr=2.51444e-05, gnorm=2.43, clip=100, loss_scale=64, train_wall=17, gb_free=12.3, wall=2982
2023-06-29 19:49:59 - progress_bar.py[line:272] - INFO: epoch 003:    373 / 660 loss=4.741, loss_v1=0, loss_v2=0, nll_loss=3.83, ntokens=5874.7, nsentences=84, sample_size=5874.7, sample_size_v1=0, sample_size_v2=0, ppl=14.22, wps=3379.7, ups=0.58, wpb=5874.7, bsz=84, num_updates=1690, lr=2.51041e-05, gnorm=2.513, clip=100, loss_scale=64, train_wall=17, gb_free=12.1, wall=2999
2023-06-29 19:50:17 - progress_bar.py[line:272] - INFO: epoch 003:    383 / 660 loss=4.717, loss_v1=0, loss_v2=0, nll_loss=3.801, ntokens=5768.2, nsentences=84, sample_size=5768.2, sample_size_v1=0, sample_size_v2=0, ppl=13.94, wps=3320, ups=0.58, wpb=5768.2, bsz=84, num_updates=1700, lr=2.50638e-05, gnorm=2.552, clip=100, loss_scale=64, train_wall=17, gb_free=12.2, wall=3017
2023-06-29 19:50:34 - progress_bar.py[line:272] - INFO: epoch 003:    393 / 660 loss=4.716, loss_v1=0, loss_v2=0, nll_loss=3.802, ntokens=5625.4, nsentences=84, sample_size=5625.4, sample_size_v1=0, sample_size_v2=0, ppl=13.95, wps=3249, ups=0.58, wpb=5625.4, bsz=84, num_updates=1710, lr=2.50235e-05, gnorm=2.704, clip=100, loss_scale=64, train_wall=17, gb_free=11.9, wall=3034
2023-06-29 19:50:51 - progress_bar.py[line:272] - INFO: epoch 003:    403 / 660 loss=4.707, loss_v1=0, loss_v2=0, nll_loss=3.792, ntokens=5917.4, nsentences=84, sample_size=5917.4, sample_size_v1=0, sample_size_v2=0, ppl=13.85, wps=3410.1, ups=0.58, wpb=5917.4, bsz=84, num_updates=1720, lr=2.49832e-05, gnorm=2.677, clip=100, loss_scale=64, train_wall=17, gb_free=12, wall=3051
2023-06-29 19:51:09 - progress_bar.py[line:272] - INFO: epoch 003:    413 / 660 loss=4.727, loss_v1=0, loss_v2=0, nll_loss=3.813, ntokens=5682.2, nsentences=84, sample_size=5682.2, sample_size_v1=0, sample_size_v2=0, ppl=14.06, wps=3260.9, ups=0.57, wpb=5682.2, bsz=84, num_updates=1730, lr=2.49429e-05, gnorm=2.813, clip=100, loss_scale=64, train_wall=17, gb_free=12.1, wall=3069
2023-06-29 19:51:26 - progress_bar.py[line:272] - INFO: epoch 003:    423 / 660 loss=4.707, loss_v1=0, loss_v2=0, nll_loss=3.792, ntokens=5668.8, nsentences=84, sample_size=5668.8, sample_size_v1=0, sample_size_v2=0, ppl=13.86, wps=3267.5, ups=0.58, wpb=5668.8, bsz=84, num_updates=1740, lr=2.49026e-05, gnorm=2.876, clip=100, loss_scale=64, train_wall=17, gb_free=12.3, wall=3086
2023-06-29 19:51:43 - progress_bar.py[line:272] - INFO: epoch 003:    433 / 660 loss=4.713, loss_v1=0, loss_v2=0, nll_loss=3.799, ntokens=5978.1, nsentences=84, sample_size=5978.1, sample_size_v1=0, sample_size_v2=0, ppl=13.92, wps=3431, ups=0.57, wpb=5978.1, bsz=84, num_updates=1750, lr=2.48623e-05, gnorm=2.855, clip=100, loss_scale=64, train_wall=17, gb_free=12.1, wall=3104
2023-06-29 19:52:01 - progress_bar.py[line:272] - INFO: epoch 003:    443 / 660 loss=4.706, loss_v1=0, loss_v2=0, nll_loss=3.791, ntokens=5993.8, nsentences=84, sample_size=5993.8, sample_size_v1=0, sample_size_v2=0, ppl=13.84, wps=3449, ups=0.58, wpb=5993.8, bsz=84, num_updates=1760, lr=2.4822e-05, gnorm=2.879, clip=100, loss_scale=64, train_wall=17, gb_free=12.2, wall=3121
2023-06-29 19:52:18 - progress_bar.py[line:272] - INFO: epoch 003:    453 / 660 loss=4.684, loss_v1=0, loss_v2=0, nll_loss=3.768, ntokens=5726.1, nsentences=84, sample_size=5726.1, sample_size_v1=0, sample_size_v2=0, ppl=13.63, wps=3309.8, ups=0.58, wpb=5726.1, bsz=84, num_updates=1770, lr=2.47817e-05, gnorm=2.764, clip=100, loss_scale=64, train_wall=17, gb_free=12, wall=3138
2023-06-29 19:52:36 - progress_bar.py[line:272] - INFO: epoch 003:    463 / 660 loss=4.691, loss_v1=0, loss_v2=0, nll_loss=3.775, ntokens=6009.7, nsentences=84, sample_size=6009.7, sample_size_v1=0, sample_size_v2=0, ppl=13.69, wps=3447.5, ups=0.57, wpb=6009.7, bsz=84, num_updates=1780, lr=2.47414e-05, gnorm=2.732, clip=100, loss_scale=64, train_wall=17, gb_free=12.4, wall=3156
2023-06-29 19:52:53 - progress_bar.py[line:272] - INFO: epoch 003:    473 / 660 loss=4.675, loss_v1=0, loss_v2=0, nll_loss=3.758, ntokens=5934.4, nsentences=84, sample_size=5934.4, sample_size_v1=0, sample_size_v2=0, ppl=13.53, wps=3398.5, ups=0.57, wpb=5934.4, bsz=84, num_updates=1790, lr=2.47011e-05, gnorm=2.894, clip=100, loss_scale=64, train_wall=17, gb_free=11, wall=3173
2023-06-29 19:53:10 - progress_bar.py[line:272] - INFO: epoch 003:    483 / 660 loss=4.655, loss_v1=0, loss_v2=0, nll_loss=3.735, ntokens=5641.6, nsentences=84, sample_size=5641.6, sample_size_v1=0, sample_size_v2=0, ppl=13.32, wps=3254.7, ups=0.58, wpb=5641.6, bsz=84, num_updates=1800, lr=2.46608e-05, gnorm=2.888, clip=100, loss_scale=64, train_wall=17, gb_free=12.1, wall=3190
2023-06-29 19:53:28 - progress_bar.py[line:272] - INFO: epoch 003:    493 / 660 loss=4.657, loss_v1=0, loss_v2=0, nll_loss=3.737, ntokens=5834, nsentences=84, sample_size=5834, sample_size_v1=0, sample_size_v2=0, ppl=13.34, wps=3316.5, ups=0.57, wpb=5834, bsz=84, num_updates=1810, lr=2.46206e-05, gnorm=2.828, clip=100, loss_scale=64, train_wall=18, gb_free=12.3, wall=3208
2023-06-29 19:53:45 - progress_bar.py[line:272] - INFO: epoch 003:    503 / 660 loss=4.667, loss_v1=0, loss_v2=0, nll_loss=3.752, ntokens=5827.4, nsentences=84, sample_size=5827.4, sample_size_v1=0, sample_size_v2=0, ppl=13.47, wps=3354.9, ups=0.58, wpb=5827.4, bsz=84, num_updates=1820, lr=2.45803e-05, gnorm=2.865, clip=100, loss_scale=64, train_wall=17, gb_free=12.5, wall=3225
2023-06-29 19:54:03 - progress_bar.py[line:272] - INFO: epoch 003:    513 / 660 loss=4.648, loss_v1=0, loss_v2=0, nll_loss=3.727, ntokens=5834.9, nsentences=84, sample_size=5834.9, sample_size_v1=0, sample_size_v2=0, ppl=13.24, wps=3372.5, ups=0.58, wpb=5834.9, bsz=84, num_updates=1830, lr=2.454e-05, gnorm=2.877, clip=100, loss_scale=64, train_wall=17, gb_free=12.5, wall=3243
2023-06-29 19:54:20 - progress_bar.py[line:272] - INFO: epoch 003:    523 / 660 loss=4.637, loss_v1=0, loss_v2=0, nll_loss=3.718, ntokens=6060.1, nsentences=84, sample_size=6060.1, sample_size_v1=0, sample_size_v2=0, ppl=13.16, wps=3474.3, ups=0.57, wpb=6060.1, bsz=84, num_updates=1840, lr=2.44997e-05, gnorm=2.938, clip=100, loss_scale=64, train_wall=17, gb_free=12, wall=3260
2023-06-29 19:54:37 - progress_bar.py[line:272] - INFO: epoch 003:    533 / 660 loss=4.631, loss_v1=0, loss_v2=0, nll_loss=3.711, ntokens=5967, nsentences=84, sample_size=5967, sample_size_v1=0, sample_size_v2=0, ppl=13.1, wps=3428.4, ups=0.57, wpb=5967, bsz=84, num_updates=1850, lr=2.44594e-05, gnorm=2.857, clip=100, loss_scale=64, train_wall=17, gb_free=11.9, wall=3278
2023-06-29 19:54:55 - progress_bar.py[line:272] - INFO: epoch 003:    543 / 660 loss=4.624, loss_v1=0, loss_v2=0, nll_loss=3.703, ntokens=6185.5, nsentences=84, sample_size=6185.5, sample_size_v1=0, sample_size_v2=0, ppl=13.03, wps=3544.5, ups=0.57, wpb=6185.5, bsz=84, num_updates=1860, lr=2.44191e-05, gnorm=2.871, clip=100, loss_scale=64, train_wall=17, gb_free=12, wall=3295
2023-06-29 19:55:12 - progress_bar.py[line:272] - INFO: epoch 003:    553 / 660 loss=4.593, loss_v1=0, loss_v2=0, nll_loss=3.668, ntokens=5861.6, nsentences=84, sample_size=5861.6, sample_size_v1=0, sample_size_v2=0, ppl=12.71, wps=3385.3, ups=0.58, wpb=5861.6, bsz=84, num_updates=1870, lr=2.43788e-05, gnorm=2.851, clip=100, loss_scale=64, train_wall=17, gb_free=12.3, wall=3312
2023-06-29 19:55:30 - progress_bar.py[line:272] - INFO: epoch 003:    563 / 660 loss=4.592, loss_v1=0, loss_v2=0, nll_loss=3.668, ntokens=6011.5, nsentences=84, sample_size=6011.5, sample_size_v1=0, sample_size_v2=0, ppl=12.71, wps=3452.7, ups=0.57, wpb=6011.5, bsz=84, num_updates=1880, lr=2.43385e-05, gnorm=3.034, clip=100, loss_scale=64, train_wall=17, gb_free=12.5, wall=3330
2023-06-29 19:55:47 - progress_bar.py[line:272] - INFO: epoch 003:    573 / 660 loss=4.594, loss_v1=0, loss_v2=0, nll_loss=3.673, ntokens=5550.6, nsentences=84, sample_size=5550.6, sample_size_v1=0, sample_size_v2=0, ppl=12.75, wps=3202.3, ups=0.58, wpb=5550.6, bsz=84, num_updates=1890, lr=2.42982e-05, gnorm=3.271, clip=100, loss_scale=64, train_wall=17, gb_free=12.2, wall=3347
2023-06-29 19:56:04 - progress_bar.py[line:272] - INFO: epoch 003:    583 / 660 loss=4.599, loss_v1=0, loss_v2=0, nll_loss=3.676, ntokens=5979.7, nsentences=84, sample_size=5979.7, sample_size_v1=0, sample_size_v2=0, ppl=12.78, wps=3456.1, ups=0.58, wpb=5979.7, bsz=84, num_updates=1900, lr=2.42579e-05, gnorm=3.017, clip=100, loss_scale=64, train_wall=17, gb_free=11.7, wall=3364
2023-06-29 19:56:22 - progress_bar.py[line:272] - INFO: epoch 003:    593 / 660 loss=4.569, loss_v1=0, loss_v2=0, nll_loss=3.645, ntokens=5897.7, nsentences=84, sample_size=5897.7, sample_size_v1=0, sample_size_v2=0, ppl=12.51, wps=3401.8, ups=0.58, wpb=5897.7, bsz=84, num_updates=1910, lr=2.42176e-05, gnorm=3.248, clip=100, loss_scale=64, train_wall=17, gb_free=12.2, wall=3382
2023-06-29 19:56:39 - progress_bar.py[line:272] - INFO: epoch 003:    603 / 660 loss=4.56, loss_v1=0, loss_v2=0, nll_loss=3.631, ntokens=5967, nsentences=84, sample_size=5967, sample_size_v1=0, sample_size_v2=0, ppl=12.39, wps=3441.1, ups=0.58, wpb=5967, bsz=84, num_updates=1920, lr=2.41773e-05, gnorm=3.181, clip=100, loss_scale=64, train_wall=17, gb_free=12.1, wall=3399
2023-06-29 19:56:56 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-06-29 19:56:58 - progress_bar.py[line:272] - INFO: epoch 003:    614 / 660 loss=4.568, loss_v1=0, loss_v2=0, nll_loss=3.644, ntokens=5966.5, nsentences=84, sample_size=5966.5, sample_size_v1=0, sample_size_v2=0, ppl=12.5, wps=3128.4, ups=0.52, wpb=5966.5, bsz=84, num_updates=1930, lr=2.4137e-05, gnorm=3.233, clip=100, loss_scale=32, train_wall=19, gb_free=12.2, wall=3418
2023-06-29 19:57:15 - progress_bar.py[line:272] - INFO: epoch 003:    624 / 660 loss=4.576, loss_v1=0, loss_v2=0, nll_loss=3.651, ntokens=6136.7, nsentences=84, sample_size=6136.7, sample_size_v1=0, sample_size_v2=0, ppl=12.57, wps=3527.9, ups=0.57, wpb=6136.7, bsz=84, num_updates=1940, lr=2.40967e-05, gnorm=3.212, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=3436
2023-06-29 19:57:33 - progress_bar.py[line:272] - INFO: epoch 003:    634 / 660 loss=4.553, loss_v1=0, loss_v2=0, nll_loss=3.626, ntokens=5929, nsentences=84, sample_size=5929, sample_size_v1=0, sample_size_v2=0, ppl=12.35, wps=3406.1, ups=0.57, wpb=5929, bsz=84, num_updates=1950, lr=2.40564e-05, gnorm=2.902, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=3453
2023-06-29 19:57:50 - progress_bar.py[line:272] - INFO: epoch 003:    644 / 660 loss=4.556, loss_v1=0, loss_v2=0, nll_loss=3.63, ntokens=5955.2, nsentences=84, sample_size=5955.2, sample_size_v1=0, sample_size_v2=0, ppl=12.38, wps=3425.2, ups=0.58, wpb=5955.2, bsz=84, num_updates=1960, lr=2.40161e-05, gnorm=3.413, clip=100, loss_scale=32, train_wall=17, gb_free=12.6, wall=3470
2023-06-29 19:58:08 - progress_bar.py[line:272] - INFO: epoch 003:    654 / 660 loss=4.553, loss_v1=0, loss_v2=0, nll_loss=3.628, ntokens=5901, nsentences=84, sample_size=5901, sample_size_v1=0, sample_size_v2=0, ppl=12.36, wps=3393, ups=0.57, wpb=5901, bsz=84, num_updates=1970, lr=2.39758e-05, gnorm=3.321, clip=100, loss_scale=32, train_wall=17, gb_free=11.7, wall=3488
2023-06-29 19:58:18 - train.py[line:332] - INFO: end of epoch 3 (average epoch stats below)
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
2023-06-29 19:58:18 - progress_bar.py[line:282] - INFO: epoch 003 | loss 4.725 | loss_v1 0 | loss_v2 0 | nll_loss 3.811 | ntokens 5904.01 | nsentences 83.95 | sample_size 5904.01 | sample_size_v1 0 | sample_size_v2 0 | ppl 14.03 | wps 3354.3 | ups 0.57 | wpb 5904 | bsz 83.9 | num_updates 1976 | lr 2.39516e-05 | gnorm 2.27 | clip 100 | loss_scale 32 | train_wall 1153 | gb_free 12.3 | wall 3498
2023-06-29 19:58:18 - trainer.py[line:639] - INFO: loading train data for epoch 4
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 row count 18467 total row count 55400
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 row count 18466 total row count 55400
slice_id 1 seek offset 18467
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 row count 18467 total row count 55400
slice_id 2 seek offset 36934
slice_id 0 seek offset 0
2023-06-29 19:58:20 - trainer.py[line:703] - INFO: begin training epoch 4
2023-06-29 19:58:20 - train.py[line:305] - INFO: Start iterating over samples
2023-06-29 19:58:27 - progress_bar.py[line:272] - INFO: epoch 004:      4 / 660 loss=4.539, loss_v1=0, loss_v2=0, nll_loss=3.611, ntokens=5874.4, nsentences=81.9, sample_size=5874.4, sample_size_v1=0, sample_size_v2=0, ppl=12.22, wps=3035.7, ups=0.52, wpb=5874.4, bsz=81.9, num_updates=1980, lr=2.39355e-05, gnorm=3.253, clip=100, loss_scale=32, train_wall=17, gb_free=11.8, wall=3507
2023-06-29 19:58:44 - progress_bar.py[line:272] - INFO: epoch 004:     14 / 660 loss=4.531, loss_v1=0, loss_v2=0, nll_loss=3.604, ntokens=5666.6, nsentences=84, sample_size=5666.6, sample_size_v1=0, sample_size_v2=0, ppl=12.16, wps=3244.9, ups=0.57, wpb=5666.6, bsz=84, num_updates=1990, lr=2.38952e-05, gnorm=3.157, clip=100, loss_scale=32, train_wall=17, gb_free=12, wall=3525
2023-06-29 19:59:02 - progress_bar.py[line:272] - INFO: epoch 004:     24 / 660 loss=4.5, loss_v1=0, loss_v2=0, nll_loss=3.568, ntokens=5901.5, nsentences=84, sample_size=5901.5, sample_size_v1=0, sample_size_v2=0, ppl=11.86, wps=3368, ups=0.57, wpb=5901.5, bsz=84, num_updates=2000, lr=2.38549e-05, gnorm=3.298, clip=100, loss_scale=32, train_wall=17, gb_free=11.8, wall=3542
2023-06-29 19:59:19 - progress_bar.py[line:272] - INFO: epoch 004:     34 / 660 loss=4.496, loss_v1=0, loss_v2=0, nll_loss=3.566, ntokens=5275, nsentences=84, sample_size=5275, sample_size_v1=0, sample_size_v2=0, ppl=11.84, wps=3052.4, ups=0.58, wpb=5275, bsz=84, num_updates=2010, lr=2.38146e-05, gnorm=3.477, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=3559
2023-06-29 19:59:37 - progress_bar.py[line:272] - INFO: epoch 004:     44 / 660 loss=4.445, loss_v1=0, loss_v2=0, nll_loss=3.508, ntokens=5892.3, nsentences=84, sample_size=5892.3, sample_size_v1=0, sample_size_v2=0, ppl=11.38, wps=3350.3, ups=0.57, wpb=5892.3, bsz=84, num_updates=2020, lr=2.37743e-05, gnorm=3.57, clip=100, loss_scale=32, train_wall=18, gb_free=11.6, wall=3577
2023-06-29 19:59:54 - progress_bar.py[line:272] - INFO: epoch 004:     54 / 660 loss=4.491, loss_v1=0, loss_v2=0, nll_loss=3.559, ntokens=5722.7, nsentences=84, sample_size=5722.7, sample_size_v1=0, sample_size_v2=0, ppl=11.79, wps=3269.1, ups=0.57, wpb=5722.7, bsz=84, num_updates=2030, lr=2.3734e-05, gnorm=3.51, clip=100, loss_scale=32, train_wall=17, gb_free=11.6, wall=3595
2023-06-29 20:00:12 - progress_bar.py[line:272] - INFO: epoch 004:     64 / 660 loss=4.445, loss_v1=0, loss_v2=0, nll_loss=3.51, ntokens=5747.3, nsentences=84, sample_size=5747.3, sample_size_v1=0, sample_size_v2=0, ppl=11.39, wps=3293, ups=0.57, wpb=5747.3, bsz=84, num_updates=2040, lr=2.36938e-05, gnorm=3.442, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=3612
2023-06-29 20:00:29 - progress_bar.py[line:272] - INFO: epoch 004:     74 / 660 loss=4.415, loss_v1=0, loss_v2=0, nll_loss=3.473, ntokens=5962.9, nsentences=84, sample_size=5962.9, sample_size_v1=0, sample_size_v2=0, ppl=11.1, wps=3383.3, ups=0.57, wpb=5962.9, bsz=84, num_updates=2050, lr=2.36535e-05, gnorm=3.593, clip=100, loss_scale=32, train_wall=18, gb_free=11.7, wall=3630
2023-06-29 20:00:47 - progress_bar.py[line:272] - INFO: epoch 004:     84 / 660 loss=4.453, loss_v1=0, loss_v2=0, nll_loss=3.518, ntokens=6218.7, nsentences=84, sample_size=6218.7, sample_size_v1=0, sample_size_v2=0, ppl=11.46, wps=3492.4, ups=0.56, wpb=6218.7, bsz=84, num_updates=2060, lr=2.36132e-05, gnorm=3.388, clip=100, loss_scale=32, train_wall=18, gb_free=11.3, wall=3647
2023-06-29 20:01:05 - progress_bar.py[line:272] - INFO: epoch 004:     94 / 660 loss=4.43, loss_v1=0, loss_v2=0, nll_loss=3.493, ntokens=5696.8, nsentences=84, sample_size=5696.8, sample_size_v1=0, sample_size_v2=0, ppl=11.26, wps=3197.2, ups=0.56, wpb=5696.8, bsz=84, num_updates=2070, lr=2.35729e-05, gnorm=3.534, clip=100, loss_scale=32, train_wall=18, gb_free=11.6, wall=3665
2023-06-29 20:01:23 - progress_bar.py[line:272] - INFO: epoch 004:    104 / 660 loss=4.419, loss_v1=0, loss_v2=0, nll_loss=3.481, ntokens=5808.1, nsentences=84, sample_size=5808.1, sample_size_v1=0, sample_size_v2=0, ppl=11.17, wps=3282.1, ups=0.57, wpb=5808.1, bsz=84, num_updates=2080, lr=2.35326e-05, gnorm=3.384, clip=100, loss_scale=32, train_wall=18, gb_free=11.9, wall=3683
2023-06-29 20:01:40 - progress_bar.py[line:272] - INFO: epoch 004:    114 / 660 loss=4.418, loss_v1=0, loss_v2=0, nll_loss=3.479, ntokens=5762.4, nsentences=84, sample_size=5762.4, sample_size_v1=0, sample_size_v2=0, ppl=11.15, wps=3299.2, ups=0.57, wpb=5762.4, bsz=84, num_updates=2090, lr=2.34923e-05, gnorm=3.679, clip=100, loss_scale=32, train_wall=17, gb_free=12.4, wall=3700
2023-06-29 20:01:58 - progress_bar.py[line:272] - INFO: epoch 004:    124 / 660 loss=4.474, loss_v1=0, loss_v2=0, nll_loss=3.541, ntokens=5739.3, nsentences=84, sample_size=5739.3, sample_size_v1=0, sample_size_v2=0, ppl=11.64, wps=3287.8, ups=0.57, wpb=5739.3, bsz=84, num_updates=2100, lr=2.3452e-05, gnorm=3.506, clip=100, loss_scale=32, train_wall=17, gb_free=12.1, wall=3718
2023-06-29 20:02:15 - progress_bar.py[line:272] - INFO: epoch 004:    134 / 660 loss=4.486, loss_v1=0, loss_v2=0, nll_loss=3.556, ntokens=5766, nsentences=84, sample_size=5766, sample_size_v1=0, sample_size_v2=0, ppl=11.76, wps=3284.3, ups=0.57, wpb=5766, bsz=84, num_updates=2110, lr=2.34117e-05, gnorm=3.442, clip=100, loss_scale=32, train_wall=18, gb_free=11.9, wall=3735
2023-06-29 20:02:33 - progress_bar.py[line:272] - INFO: epoch 004:    144 / 660 loss=4.457, loss_v1=0, loss_v2=0, nll_loss=3.522, ntokens=5986.4, nsentences=84, sample_size=5986.4, sample_size_v1=0, sample_size_v2=0, ppl=11.48, wps=3366.3, ups=0.56, wpb=5986.4, bsz=84, num_updates=2120, lr=2.33714e-05, gnorm=3.296, clip=100, loss_scale=32, train_wall=18, gb_free=11.9, wall=3753
2023-06-29 20:02:51 - progress_bar.py[line:272] - INFO: epoch 004:    154 / 660 loss=4.438, loss_v1=0, loss_v2=0, nll_loss=3.501, ntokens=5953.3, nsentences=84, sample_size=5953.3, sample_size_v1=0, sample_size_v2=0, ppl=11.33, wps=3383.4, ups=0.57, wpb=5953.3, bsz=84, num_updates=2130, lr=2.33311e-05, gnorm=3.426, clip=100, loss_scale=32, train_wall=18, gb_free=11.9, wall=3771
2023-06-29 20:03:08 - progress_bar.py[line:272] - INFO: epoch 004:    164 / 660 loss=4.45, loss_v1=0, loss_v2=0, nll_loss=3.516, ntokens=5967.1, nsentences=84, sample_size=5967.1, sample_size_v1=0, sample_size_v2=0, ppl=11.44, wps=3366.1, ups=0.56, wpb=5967.1, bsz=84, num_updates=2140, lr=2.32908e-05, gnorm=3.39, clip=100, loss_scale=32, train_wall=18, gb_free=11.6, wall=3789
2023-06-29 20:03:26 - progress_bar.py[line:272] - INFO: epoch 004:    174 / 660 loss=4.443, loss_v1=0, loss_v2=0, nll_loss=3.508, ntokens=5989.2, nsentences=84, sample_size=5989.2, sample_size_v1=0, sample_size_v2=0, ppl=11.38, wps=3389.9, ups=0.57, wpb=5989.2, bsz=84, num_updates=2150, lr=2.32505e-05, gnorm=3.707, clip=100, loss_scale=32, train_wall=18, gb_free=11.6, wall=3806
2023-06-29 20:03:44 - progress_bar.py[line:272] - INFO: epoch 004:    184 / 660 loss=4.431, loss_v1=0, loss_v2=0, nll_loss=3.494, ntokens=5885.1, nsentences=84, sample_size=5885.1, sample_size_v1=0, sample_size_v2=0, ppl=11.27, wps=3347.2, ups=0.57, wpb=5885.1, bsz=84, num_updates=2160, lr=2.32102e-05, gnorm=3.557, clip=100, loss_scale=32, train_wall=18, gb_free=12, wall=3824
2023-06-29 20:04:01 - progress_bar.py[line:272] - INFO: epoch 004:    194 / 660 loss=4.395, loss_v1=0, loss_v2=0, nll_loss=3.456, ntokens=5837.6, nsentences=84, sample_size=5837.6, sample_size_v1=0, sample_size_v2=0, ppl=10.97, wps=3353.1, ups=0.57, wpb=5837.6, bsz=84, num_updates=2170, lr=2.31699e-05, gnorm=3.413, clip=100, loss_scale=32, train_wall=17, gb_free=12.2, wall=3841
2023-06-29 20:04:19 - progress_bar.py[line:272] - INFO: epoch 004:    204 / 660 loss=4.387, loss_v1=0, loss_v2=0, nll_loss=3.446, ntokens=5923, nsentences=84, sample_size=5923, sample_size_v1=0, sample_size_v2=0, ppl=10.9, wps=3366.4, ups=0.57, wpb=5923, bsz=84, num_updates=2180, lr=2.31296e-05, gnorm=3.674, clip=100, loss_scale=32, train_wall=18, gb_free=11.8, wall=3859
2023-06-29 20:04:37 - progress_bar.py[line:272] - INFO: epoch 004:    214 / 660 loss=4.38, loss_v1=0, loss_v2=0, nll_loss=3.44, ntokens=6015, nsentences=84, sample_size=6015, sample_size_v1=0, sample_size_v2=0, ppl=10.85, wps=3364, ups=0.56, wpb=6015, bsz=84, num_updates=2190, lr=2.30893e-05, gnorm=3.565, clip=100, loss_scale=32, train_wall=18, gb_free=12, wall=3877
2023-06-29 20:04:54 - progress_bar.py[line:272] - INFO: epoch 004:    224 / 660 loss=4.405, loss_v1=0, loss_v2=0, nll_loss=3.465, ntokens=6154.1, nsentences=84, sample_size=6154.1, sample_size_v1=0, sample_size_v2=0, ppl=11.04, wps=3504.2, ups=0.57, wpb=6154.1, bsz=84, num_updates=2200, lr=2.3049e-05, gnorm=3.597, clip=100, loss_scale=32, train_wall=18, gb_free=11.7, wall=3894
2023-06-29 20:05:12 - progress_bar.py[line:272] - INFO: epoch 004:    234 / 660 loss=4.353, loss_v1=0, loss_v2=0, nll_loss=3.409, ntokens=5863.5, nsentences=84, sample_size=5863.5, sample_size_v1=0, sample_size_v2=0, ppl=10.62, wps=3371.1, ups=0.57, wpb=5863.5, bsz=84, num_updates=2210, lr=2.30087e-05, gnorm=3.435, clip=100, loss_scale=32, train_wall=17, gb_free=12.2, wall=3912
2023-06-29 20:05:29 - progress_bar.py[line:272] - INFO: epoch 004:    244 / 660 loss=4.359, loss_v1=0, loss_v2=0, nll_loss=3.415, ntokens=6036.5, nsentences=84, sample_size=6036.5, sample_size_v1=0, sample_size_v2=0, ppl=10.67, wps=3444.1, ups=0.57, wpb=6036.5, bsz=84, num_updates=2220, lr=2.29684e-05, gnorm=3.355, clip=100, loss_scale=32, train_wall=17, gb_free=11.8, wall=3929
2023-06-29 20:05:46 - progress_bar.py[line:272] - INFO: epoch 004:    254 / 660 loss=4.331, loss_v1=0, loss_v2=0, nll_loss=3.385, ntokens=6017.2, nsentences=84, sample_size=6017.2, sample_size_v1=0, sample_size_v2=0, ppl=10.45, wps=3448.3, ups=0.57, wpb=6017.2, bsz=84, num_updates=2230, lr=2.29281e-05, gnorm=3.478, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=3947
2023-06-29 20:06:04 - progress_bar.py[line:272] - INFO: epoch 004:    264 / 660 loss=4.307, loss_v1=0, loss_v2=0, nll_loss=3.358, ntokens=5830.1, nsentences=84, sample_size=5830.1, sample_size_v1=0, sample_size_v2=0, ppl=10.25, wps=3349.9, ups=0.57, wpb=5830.1, bsz=84, num_updates=2240, lr=2.28878e-05, gnorm=3.576, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=3964
2023-06-29 20:06:21 - progress_bar.py[line:272] - INFO: epoch 004:    274 / 660 loss=4.323, loss_v1=0, loss_v2=0, nll_loss=3.377, ntokens=5760.9, nsentences=84, sample_size=5760.9, sample_size_v1=0, sample_size_v2=0, ppl=10.39, wps=3278.7, ups=0.57, wpb=5760.9, bsz=84, num_updates=2250, lr=2.28475e-05, gnorm=3.658, clip=100, loss_scale=32, train_wall=18, gb_free=12.1, wall=3982
2023-06-29 20:06:39 - progress_bar.py[line:272] - INFO: epoch 004:    284 / 660 loss=4.313, loss_v1=0, loss_v2=0, nll_loss=3.365, ntokens=6055.8, nsentences=84, sample_size=6055.8, sample_size_v1=0, sample_size_v2=0, ppl=10.3, wps=3445.1, ups=0.57, wpb=6055.8, bsz=84, num_updates=2260, lr=2.28073e-05, gnorm=3.592, clip=100, loss_scale=32, train_wall=18, gb_free=12, wall=3999
2023-06-29 20:06:56 - progress_bar.py[line:272] - INFO: epoch 004:    294 / 660 loss=4.313, loss_v1=0, loss_v2=0, nll_loss=3.364, ntokens=6112.4, nsentences=84, sample_size=6112.4, sample_size_v1=0, sample_size_v2=0, ppl=10.3, wps=3509.3, ups=0.57, wpb=6112.4, bsz=84, num_updates=2270, lr=2.2767e-05, gnorm=3.442, clip=100, loss_scale=32, train_wall=17, gb_free=12, wall=4017
2023-06-29 20:07:14 - progress_bar.py[line:272] - INFO: epoch 004:    304 / 660 loss=4.31, loss_v1=0, loss_v2=0, nll_loss=3.362, ntokens=6221, nsentences=84, sample_size=6221, sample_size_v1=0, sample_size_v2=0, ppl=10.28, wps=3546.1, ups=0.57, wpb=6221, bsz=84, num_updates=2280, lr=2.27267e-05, gnorm=3.437, clip=100, loss_scale=32, train_wall=18, gb_free=12, wall=4034
2023-06-29 20:07:32 - progress_bar.py[line:272] - INFO: epoch 004:    314 / 660 loss=4.249, loss_v1=0, loss_v2=0, nll_loss=3.345, ntokens=6252, nsentences=82.8, sample_size=6252, sample_size_v1=0, sample_size_v2=0, ppl=10.16, wps=3548.2, ups=0.57, wpb=6252, bsz=82.8, num_updates=2290, lr=2.26864e-05, gnorm=3.667, clip=100, loss_scale=32, train_wall=18, gb_free=12.1, wall=4052
2023-06-29 20:07:49 - progress_bar.py[line:272] - INFO: epoch 004:    324 / 660 loss=4.28, loss_v1=0, loss_v2=0, nll_loss=3.328, ntokens=6060.2, nsentences=84, sample_size=6060.2, sample_size_v1=0, sample_size_v2=0, ppl=10.04, wps=3432.5, ups=0.57, wpb=6060.2, bsz=84, num_updates=2300, lr=2.26461e-05, gnorm=3.438, clip=100, loss_scale=32, train_wall=18, gb_free=12.1, wall=4069
2023-06-29 20:08:07 - progress_bar.py[line:272] - INFO: epoch 004:    334 / 660 loss=4.293, loss_v1=0, loss_v2=0, nll_loss=3.343, ntokens=5991.2, nsentences=84, sample_size=5991.2, sample_size_v1=0, sample_size_v2=0, ppl=10.14, wps=3428.8, ups=0.57, wpb=5991.2, bsz=84, num_updates=2310, lr=2.26058e-05, gnorm=3.516, clip=100, loss_scale=32, train_wall=17, gb_free=12.1, wall=4087
2023-06-29 20:08:24 - progress_bar.py[line:272] - INFO: epoch 004:    344 / 660 loss=4.277, loss_v1=0, loss_v2=0, nll_loss=3.326, ntokens=6049, nsentences=84, sample_size=6049, sample_size_v1=0, sample_size_v2=0, ppl=10.03, wps=3460.2, ups=0.57, wpb=6049, bsz=84, num_updates=2320, lr=2.25655e-05, gnorm=3.449, clip=100, loss_scale=32, train_wall=17, gb_free=12.1, wall=4104
2023-06-29 20:08:42 - progress_bar.py[line:272] - INFO: epoch 004:    354 / 660 loss=4.247, loss_v1=0, loss_v2=0, nll_loss=3.292, ntokens=5983.8, nsentences=84, sample_size=5983.8, sample_size_v1=0, sample_size_v2=0, ppl=9.79, wps=3424.5, ups=0.57, wpb=5983.8, bsz=84, num_updates=2330, lr=2.25252e-05, gnorm=3.538, clip=100, loss_scale=32, train_wall=17, gb_free=11.8, wall=4122
2023-06-29 20:08:59 - progress_bar.py[line:272] - INFO: epoch 004:    364 / 660 loss=4.257, loss_v1=0, loss_v2=0, nll_loss=3.304, ntokens=5936.1, nsentences=84, sample_size=5936.1, sample_size_v1=0, sample_size_v2=0, ppl=9.87, wps=3414.6, ups=0.58, wpb=5936.1, bsz=84, num_updates=2340, lr=2.24849e-05, gnorm=3.614, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=4139
2023-06-29 20:09:17 - progress_bar.py[line:272] - INFO: epoch 004:    374 / 660 loss=4.247, loss_v1=0, loss_v2=0, nll_loss=3.292, ntokens=5914.3, nsentences=84, sample_size=5914.3, sample_size_v1=0, sample_size_v2=0, ppl=9.79, wps=3404.8, ups=0.58, wpb=5914.3, bsz=84, num_updates=2350, lr=2.24446e-05, gnorm=3.458, clip=100, loss_scale=32, train_wall=17, gb_free=12.1, wall=4157
2023-06-29 20:09:34 - progress_bar.py[line:272] - INFO: epoch 004:    384 / 660 loss=4.22, loss_v1=0, loss_v2=0, nll_loss=3.262, ntokens=5711, nsentences=84, sample_size=5711, sample_size_v1=0, sample_size_v2=0, ppl=9.6, wps=3293.6, ups=0.58, wpb=5711, bsz=84, num_updates=2360, lr=2.24043e-05, gnorm=3.409, clip=100, loss_scale=32, train_wall=17, gb_free=12.1, wall=4174
2023-06-29 20:09:51 - progress_bar.py[line:272] - INFO: epoch 004:    394 / 660 loss=4.208, loss_v1=0, loss_v2=0, nll_loss=3.25, ntokens=5677.4, nsentences=84, sample_size=5677.4, sample_size_v1=0, sample_size_v2=0, ppl=9.52, wps=3285.5, ups=0.58, wpb=5677.4, bsz=84, num_updates=2370, lr=2.2364e-05, gnorm=3.583, clip=100, loss_scale=32, train_wall=17, gb_free=12, wall=4191
2023-06-29 20:10:08 - progress_bar.py[line:272] - INFO: epoch 004:    404 / 660 loss=4.213, loss_v1=0, loss_v2=0, nll_loss=3.254, ntokens=5837.7, nsentences=84, sample_size=5837.7, sample_size_v1=0, sample_size_v2=0, ppl=9.54, wps=3370.6, ups=0.58, wpb=5837.7, bsz=84, num_updates=2380, lr=2.23237e-05, gnorm=3.407, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=4209
2023-06-29 20:10:26 - progress_bar.py[line:272] - INFO: epoch 004:    414 / 660 loss=4.217, loss_v1=0, loss_v2=0, nll_loss=3.259, ntokens=5754.9, nsentences=84, sample_size=5754.9, sample_size_v1=0, sample_size_v2=0, ppl=9.57, wps=3323.8, ups=0.58, wpb=5754.9, bsz=84, num_updates=2390, lr=2.22834e-05, gnorm=3.588, clip=100, loss_scale=32, train_wall=17, gb_free=12.1, wall=4226
2023-06-29 20:10:43 - progress_bar.py[line:272] - INFO: epoch 004:    424 / 660 loss=4.203, loss_v1=0, loss_v2=0, nll_loss=3.244, ntokens=5656.7, nsentences=84, sample_size=5656.7, sample_size_v1=0, sample_size_v2=0, ppl=9.47, wps=3252.2, ups=0.57, wpb=5656.7, bsz=84, num_updates=2400, lr=2.22431e-05, gnorm=3.465, clip=100, loss_scale=32, train_wall=17, gb_free=11.8, wall=4243
2023-06-29 20:11:01 - progress_bar.py[line:272] - INFO: epoch 004:    434 / 660 loss=4.229, loss_v1=0, loss_v2=0, nll_loss=3.271, ntokens=6027.2, nsentences=84, sample_size=6027.2, sample_size_v1=0, sample_size_v2=0, ppl=9.66, wps=3423.8, ups=0.57, wpb=6027.2, bsz=84, num_updates=2410, lr=2.22028e-05, gnorm=3.538, clip=100, loss_scale=32, train_wall=18, gb_free=12, wall=4261
2023-06-29 20:11:18 - progress_bar.py[line:272] - INFO: epoch 004:    444 / 660 loss=4.213, loss_v1=0, loss_v2=0, nll_loss=3.255, ntokens=5961.6, nsentences=84, sample_size=5961.6, sample_size_v1=0, sample_size_v2=0, ppl=9.55, wps=3438.7, ups=0.58, wpb=5961.6, bsz=84, num_updates=2420, lr=2.21625e-05, gnorm=3.727, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=4278
2023-06-29 20:11:35 - progress_bar.py[line:272] - INFO: epoch 004:    454 / 660 loss=4.181, loss_v1=0, loss_v2=0, nll_loss=3.218, ntokens=5769, nsentences=84, sample_size=5769, sample_size_v1=0, sample_size_v2=0, ppl=9.31, wps=3329.1, ups=0.58, wpb=5769, bsz=84, num_updates=2430, lr=2.21222e-05, gnorm=3.338, clip=100, loss_scale=32, train_wall=17, gb_free=12, wall=4296
2023-06-29 20:11:53 - progress_bar.py[line:272] - INFO: epoch 004:    464 / 660 loss=4.202, loss_v1=0, loss_v2=0, nll_loss=3.241, ntokens=5954.7, nsentences=84, sample_size=5954.7, sample_size_v1=0, sample_size_v2=0, ppl=9.45, wps=3425.5, ups=0.58, wpb=5954.7, bsz=84, num_updates=2440, lr=2.20819e-05, gnorm=3.291, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=4313
2023-06-29 20:12:10 - progress_bar.py[line:272] - INFO: epoch 004:    474 / 660 loss=4.197, loss_v1=0, loss_v2=0, nll_loss=3.235, ntokens=5877.1, nsentences=84, sample_size=5877.1, sample_size_v1=0, sample_size_v2=0, ppl=9.42, wps=3374.2, ups=0.57, wpb=5877.1, bsz=84, num_updates=2450, lr=2.20416e-05, gnorm=3.34, clip=100, loss_scale=64, train_wall=17, gb_free=12.1, wall=4330
2023-06-29 20:12:28 - progress_bar.py[line:272] - INFO: epoch 004:    484 / 660 loss=4.147, loss_v1=0, loss_v2=0, nll_loss=3.182, ntokens=5694.7, nsentences=84, sample_size=5694.7, sample_size_v1=0, sample_size_v2=0, ppl=9.07, wps=3267.4, ups=0.57, wpb=5694.7, bsz=84, num_updates=2460, lr=2.20013e-05, gnorm=3.334, clip=100, loss_scale=64, train_wall=17, gb_free=11.9, wall=4348
2023-06-29 20:12:45 - progress_bar.py[line:272] - INFO: epoch 004:    494 / 660 loss=4.174, loss_v1=0, loss_v2=0, nll_loss=3.21, ntokens=5832, nsentences=84, sample_size=5832, sample_size_v1=0, sample_size_v2=0, ppl=9.26, wps=3356.2, ups=0.58, wpb=5832, bsz=84, num_updates=2470, lr=2.1961e-05, gnorm=3.424, clip=100, loss_scale=64, train_wall=17, gb_free=12.1, wall=4365
2023-06-29 20:12:54 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-06-29 20:13:04 - progress_bar.py[line:272] - INFO: epoch 004:    505 / 660 loss=4.16, loss_v1=0, loss_v2=0, nll_loss=3.196, ntokens=5759.6, nsentences=84, sample_size=5759.6, sample_size_v1=0, sample_size_v2=0, ppl=9.16, wps=3024.8, ups=0.53, wpb=5759.6, bsz=84, num_updates=2480, lr=2.19208e-05, gnorm=3.421, clip=100, loss_scale=32, train_wall=19, gb_free=11.9, wall=4384
2023-06-29 20:13:21 - progress_bar.py[line:272] - INFO: epoch 004:    515 / 660 loss=4.157, loss_v1=0, loss_v2=0, nll_loss=3.191, ntokens=5788.6, nsentences=84, sample_size=5788.6, sample_size_v1=0, sample_size_v2=0, ppl=9.13, wps=3347.4, ups=0.58, wpb=5788.6, bsz=84, num_updates=2490, lr=2.18805e-05, gnorm=3.453, clip=100, loss_scale=32, train_wall=17, gb_free=12.2, wall=4402
2023-06-29 20:13:39 - progress_bar.py[line:272] - INFO: epoch 004:    525 / 660 loss=4.166, loss_v1=0, loss_v2=0, nll_loss=3.202, ntokens=6188.8, nsentences=84, sample_size=6188.8, sample_size_v1=0, sample_size_v2=0, ppl=9.2, wps=3554.1, ups=0.57, wpb=6188.8, bsz=84, num_updates=2500, lr=2.18402e-05, gnorm=3.457, clip=100, loss_scale=32, train_wall=17, gb_free=12, wall=4419
2023-06-29 20:13:56 - progress_bar.py[line:272] - INFO: epoch 004:    535 / 660 loss=4.148, loss_v1=0, loss_v2=0, nll_loss=3.181, ntokens=5939.3, nsentences=84, sample_size=5939.3, sample_size_v1=0, sample_size_v2=0, ppl=9.07, wps=3401, ups=0.57, wpb=5939.3, bsz=84, num_updates=2510, lr=2.17999e-05, gnorm=3.407, clip=100, loss_scale=32, train_wall=17, gb_free=11.8, wall=4436
2023-06-29 20:14:14 - progress_bar.py[line:272] - INFO: epoch 004:    545 / 660 loss=4.147, loss_v1=0, loss_v2=0, nll_loss=3.181, ntokens=6173.8, nsentences=84, sample_size=6173.8, sample_size_v1=0, sample_size_v2=0, ppl=9.07, wps=3557.3, ups=0.58, wpb=6173.8, bsz=84, num_updates=2520, lr=2.17596e-05, gnorm=3.404, clip=100, loss_scale=32, train_wall=17, gb_free=12.2, wall=4454
2023-06-29 20:14:31 - progress_bar.py[line:272] - INFO: epoch 004:    555 / 660 loss=4.125, loss_v1=0, loss_v2=0, nll_loss=3.156, ntokens=5965, nsentences=84, sample_size=5965, sample_size_v1=0, sample_size_v2=0, ppl=8.91, wps=3439.2, ups=0.58, wpb=5965, bsz=84, num_updates=2530, lr=2.17193e-05, gnorm=3.435, clip=100, loss_scale=32, train_wall=17, gb_free=12.2, wall=4471
2023-06-29 20:14:48 - progress_bar.py[line:272] - INFO: epoch 004:    565 / 660 loss=4.113, loss_v1=0, loss_v2=0, nll_loss=3.142, ntokens=5795.9, nsentences=84, sample_size=5795.9, sample_size_v1=0, sample_size_v2=0, ppl=8.83, wps=3314.5, ups=0.57, wpb=5795.9, bsz=84, num_updates=2540, lr=2.1679e-05, gnorm=3.336, clip=100, loss_scale=32, train_wall=17, gb_free=12, wall=4489
2023-06-29 20:15:06 - progress_bar.py[line:272] - INFO: epoch 004:    575 / 660 loss=4.117, loss_v1=0, loss_v2=0, nll_loss=3.148, ntokens=5695.5, nsentences=84, sample_size=5695.5, sample_size_v1=0, sample_size_v2=0, ppl=8.86, wps=3292.2, ups=0.58, wpb=5695.5, bsz=84, num_updates=2550, lr=2.16387e-05, gnorm=3.591, clip=100, loss_scale=32, train_wall=17, gb_free=12.2, wall=4506
2023-06-29 20:15:23 - progress_bar.py[line:272] - INFO: epoch 004:    585 / 660 loss=4.111, loss_v1=0, loss_v2=0, nll_loss=3.139, ntokens=5935.4, nsentences=84, sample_size=5935.4, sample_size_v1=0, sample_size_v2=0, ppl=8.81, wps=3434.4, ups=0.58, wpb=5935.4, bsz=84, num_updates=2560, lr=2.15984e-05, gnorm=3.47, clip=100, loss_scale=32, train_wall=17, gb_free=12, wall=4523
2023-06-29 20:15:40 - progress_bar.py[line:272] - INFO: epoch 004:    595 / 660 loss=4.093, loss_v1=0, loss_v2=0, nll_loss=3.12, ntokens=5893.1, nsentences=84, sample_size=5893.1, sample_size_v1=0, sample_size_v2=0, ppl=8.69, wps=3392.4, ups=0.58, wpb=5893.1, bsz=84, num_updates=2570, lr=2.15581e-05, gnorm=3.314, clip=100, loss_scale=32, train_wall=17, gb_free=12.2, wall=4541
2023-06-29 20:15:58 - progress_bar.py[line:272] - INFO: epoch 004:    605 / 660 loss=4.081, loss_v1=0, loss_v2=0, nll_loss=3.108, ntokens=5980.9, nsentences=84, sample_size=5980.9, sample_size_v1=0, sample_size_v2=0, ppl=8.62, wps=3448.1, ups=0.58, wpb=5980.9, bsz=84, num_updates=2580, lr=2.15178e-05, gnorm=3.422, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=4558
2023-06-29 20:16:15 - progress_bar.py[line:272] - INFO: epoch 004:    615 / 660 loss=4.096, loss_v1=0, loss_v2=0, nll_loss=3.123, ntokens=5939.7, nsentences=84, sample_size=5939.7, sample_size_v1=0, sample_size_v2=0, ppl=8.71, wps=3401.2, ups=0.57, wpb=5939.7, bsz=84, num_updates=2590, lr=2.14775e-05, gnorm=3.406, clip=100, loss_scale=32, train_wall=17, gb_free=12, wall=4575
2023-06-29 20:16:33 - progress_bar.py[line:272] - INFO: epoch 004:    625 / 660 loss=4.113, loss_v1=0, loss_v2=0, nll_loss=3.141, ntokens=6147.9, nsentences=84, sample_size=6147.9, sample_size_v1=0, sample_size_v2=0, ppl=8.82, wps=3528.4, ups=0.57, wpb=6147.9, bsz=84, num_updates=2600, lr=2.14372e-05, gnorm=3.692, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=4593
2023-06-29 20:16:50 - progress_bar.py[line:272] - INFO: epoch 004:    635 / 660 loss=4.087, loss_v1=0, loss_v2=0, nll_loss=3.113, ntokens=5846.8, nsentences=84, sample_size=5846.8, sample_size_v1=0, sample_size_v2=0, ppl=8.65, wps=3357.6, ups=0.57, wpb=5846.8, bsz=84, num_updates=2610, lr=2.13969e-05, gnorm=3.322, clip=100, loss_scale=32, train_wall=17, gb_free=12.1, wall=4610
2023-06-29 20:17:08 - progress_bar.py[line:272] - INFO: epoch 004:    645 / 660 loss=4.107, loss_v1=0, loss_v2=0, nll_loss=3.136, ntokens=6099.5, nsentences=84, sample_size=6099.5, sample_size_v1=0, sample_size_v2=0, ppl=8.79, wps=3501.5, ups=0.57, wpb=6099.5, bsz=84, num_updates=2620, lr=2.13566e-05, gnorm=3.335, clip=100, loss_scale=32, train_wall=17, gb_free=11.8, wall=4628
2023-06-29 20:17:25 - progress_bar.py[line:272] - INFO: epoch 004:    655 / 660 loss=4.077, loss_v1=0, loss_v2=0, nll_loss=3.101, ntokens=5852.6, nsentences=84, sample_size=5852.6, sample_size_v1=0, sample_size_v2=0, ppl=8.58, wps=3363.3, ups=0.57, wpb=5852.6, bsz=84, num_updates=2630, lr=2.13163e-05, gnorm=3.531, clip=100, loss_scale=32, train_wall=17, gb_free=11.8, wall=4645
2023-06-29 20:17:33 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 4 @ 2635 updates
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
2023-06-29 20:17:33 - trainer.py[line:431] - INFO: Saving checkpoint to ../../checkpoints/OFA/sgcls_checkpoints/_12_3e-5_512_base_tgtobj/checkpoint4.pt
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 row count 18467 total row count 55400
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 row count 18466 total row count 55400
slice_id 1 seek offset 18467
slice_id 2 seek offset 36934
2023-06-29 20:17:36 - trainer.py[line:441] - INFO: Finished saving checkpoint to ../../checkpoints/OFA/sgcls_checkpoints/_12_3e-5_512_base_tgtobj/checkpoint4.pt
2023-06-29 20:17:58 - checkpoint_utils.py[line:133] - INFO: Saved checkpoint ../../checkpoints/OFA/sgcls_checkpoints/_12_3e-5_512_base_tgtobj/checkpoint4.pt (epoch 4 @ 2635 updates, score None) (writing took 24.371780505403876 seconds)
2023-06-29 20:17:58 - train.py[line:332] - INFO: end of epoch 4 (average epoch stats below)
2023-06-29 20:17:58 - progress_bar.py[line:282] - INFO: epoch 004 | loss 4.285 | loss_v1 0 | loss_v2 0 | nll_loss 3.334 | ntokens 5902.25 | nsentences 83.95 | sample_size 5902.25 | sample_size_v1 0 | sample_size_v2 0 | ppl 10.08 | wps 3296.5 | ups 0.56 | wpb 5902.3 | bsz 83.9 | num_updates 2635 | lr 2.12962e-05 | gnorm 3.472 | clip 100 | loss_scale 32 | train_wall 1151 | gb_free 12.3 | wall 4678
2023-06-29 20:17:58 - trainer.py[line:639] - INFO: loading train data for epoch 5
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 row count 18467 total row count 55400
slice_id 0 seek offset 0
2023-06-29 20:17:59 - trainer.py[line:703] - INFO: begin training epoch 5
2023-06-29 20:17:59 - train.py[line:305] - INFO: Start iterating over samples
2023-06-29 20:18:08 - progress_bar.py[line:272] - INFO: epoch 005:      5 / 660 loss=4.076, loss_v1=0, loss_v2=0, nll_loss=3.101, ntokens=5808.2, nsentences=81.9, sample_size=5808.2, sample_size_v1=0, sample_size_v2=0, ppl=8.58, wps=1335.2, ups=0.23, wpb=5808.2, bsz=81.9, num_updates=2640, lr=2.1276e-05, gnorm=3.738, clip=100, loss_scale=32, train_wall=17, gb_free=11.5, wall=4689
2023-06-29 20:18:26 - progress_bar.py[line:272] - INFO: epoch 005:     15 / 660 loss=4.069, loss_v1=0, loss_v2=0, nll_loss=3.094, ntokens=5617.5, nsentences=84, sample_size=5617.5, sample_size_v1=0, sample_size_v2=0, ppl=8.54, wps=3220.8, ups=0.57, wpb=5617.5, bsz=84, num_updates=2650, lr=2.12357e-05, gnorm=3.72, clip=100, loss_scale=32, train_wall=17, gb_free=11.7, wall=4706
2023-06-29 20:18:43 - progress_bar.py[line:272] - INFO: epoch 005:     25 / 660 loss=4.044, loss_v1=0, loss_v2=0, nll_loss=3.065, ntokens=5885.8, nsentences=84, sample_size=5885.8, sample_size_v1=0, sample_size_v2=0, ppl=8.37, wps=3371, ups=0.57, wpb=5885.8, bsz=84, num_updates=2660, lr=2.11954e-05, gnorm=3.673, clip=100, loss_scale=32, train_wall=17, gb_free=12.3, wall=4724
2023-06-29 20:19:01 - progress_bar.py[line:272] - INFO: epoch 005:     35 / 660 loss=4.023, loss_v1=0, loss_v2=0, nll_loss=3.043, ntokens=5293.9, nsentences=84, sample_size=5293.9, sample_size_v1=0, sample_size_v2=0, ppl=8.24, wps=3064.5, ups=0.58, wpb=5293.9, bsz=84, num_updates=2670, lr=2.11551e-05, gnorm=3.685, clip=100, loss_scale=32, train_wall=17, gb_free=12, wall=4741
2023-06-29 20:19:18 - progress_bar.py[line:272] - INFO: epoch 005:     45 / 660 loss=4.007, loss_v1=0, loss_v2=0, nll_loss=3.024, ntokens=5941.7, nsentences=84, sample_size=5941.7, sample_size_v1=0, sample_size_v2=0, ppl=8.13, wps=3333.9, ups=0.56, wpb=5941.7, bsz=84, num_updates=2680, lr=2.11148e-05, gnorm=3.748, clip=100, loss_scale=32, train_wall=18, gb_free=11.6, wall=4759
2023-06-29 20:19:36 - progress_bar.py[line:272] - INFO: epoch 005:     55 / 660 loss=4.036, loss_v1=0, loss_v2=0, nll_loss=3.056, ntokens=5766.4, nsentences=84, sample_size=5766.4, sample_size_v1=0, sample_size_v2=0, ppl=8.32, wps=3292.4, ups=0.57, wpb=5766.4, bsz=84, num_updates=2690, lr=2.10745e-05, gnorm=3.599, clip=100, loss_scale=32, train_wall=17, gb_free=11.8, wall=4776
2023-06-29 20:19:54 - progress_bar.py[line:272] - INFO: epoch 005:     65 / 660 loss=3.993, loss_v1=0, loss_v2=0, nll_loss=3.007, ntokens=5610.9, nsentences=84, sample_size=5610.9, sample_size_v1=0, sample_size_v2=0, ppl=8.04, wps=3193.3, ups=0.57, wpb=5610.9, bsz=84, num_updates=2700, lr=2.10343e-05, gnorm=3.978, clip=100, loss_scale=32, train_wall=18, gb_free=12, wall=4794
2023-06-29 20:20:11 - progress_bar.py[line:272] - INFO: epoch 005:     75 / 660 loss=3.99, loss_v1=0, loss_v2=0, nll_loss=3.003, ntokens=6157.4, nsentences=84, sample_size=6157.4, sample_size_v1=0, sample_size_v2=0, ppl=8.01, wps=3477.4, ups=0.56, wpb=6157.4, bsz=84, num_updates=2710, lr=2.0994e-05, gnorm=3.586, clip=100, loss_scale=32, train_wall=18, gb_free=11.7, wall=4811
2023-06-29 20:20:29 - progress_bar.py[line:272] - INFO: epoch 005:     85 / 660 loss=4.026, loss_v1=0, loss_v2=0, nll_loss=3.045, ntokens=6161.2, nsentences=84, sample_size=6161.2, sample_size_v1=0, sample_size_v2=0, ppl=8.26, wps=3457, ups=0.56, wpb=6161.2, bsz=84, num_updates=2720, lr=2.09537e-05, gnorm=4.001, clip=100, loss_scale=32, train_wall=18, gb_free=11.8, wall=4829
2023-06-29 20:20:47 - progress_bar.py[line:272] - INFO: epoch 005:     95 / 660 loss=4.004, loss_v1=0, loss_v2=0, nll_loss=3.021, ntokens=5690.8, nsentences=84, sample_size=5690.8, sample_size_v1=0, sample_size_v2=0, ppl=8.12, wps=3237.6, ups=0.57, wpb=5690.8, bsz=84, num_updates=2730, lr=2.09134e-05, gnorm=3.821, clip=100, loss_scale=32, train_wall=18, gb_free=11.6, wall=4847
2023-06-29 20:21:04 - progress_bar.py[line:272] - INFO: epoch 005:    105 / 660 loss=3.985, loss_v1=0, loss_v2=0, nll_loss=2.999, ntokens=5815.5, nsentences=84, sample_size=5815.5, sample_size_v1=0, sample_size_v2=0, ppl=8, wps=3279.8, ups=0.56, wpb=5815.5, bsz=84, num_updates=2740, lr=2.08731e-05, gnorm=3.768, clip=100, loss_scale=32, train_wall=18, gb_free=11.9, wall=4865
2023-06-29 20:21:22 - progress_bar.py[line:272] - INFO: epoch 005:    115 / 660 loss=3.989, loss_v1=0, loss_v2=0, nll_loss=3.003, ntokens=5706.5, nsentences=84, sample_size=5706.5, sample_size_v1=0, sample_size_v2=0, ppl=8.02, wps=3269.1, ups=0.57, wpb=5706.5, bsz=84, num_updates=2750, lr=2.08328e-05, gnorm=4.097, clip=100, loss_scale=32, train_wall=17, gb_free=11.7, wall=4882
2023-06-29 20:21:39 - progress_bar.py[line:272] - INFO: epoch 005:    125 / 660 loss=4.051, loss_v1=0, loss_v2=0, nll_loss=3.074, ntokens=5774, nsentences=84, sample_size=5774, sample_size_v1=0, sample_size_v2=0, ppl=8.42, wps=3333.8, ups=0.58, wpb=5774, bsz=84, num_updates=2760, lr=2.07925e-05, gnorm=3.815, clip=100, loss_scale=32, train_wall=17, gb_free=12, wall=4899
2023-06-29 20:21:57 - progress_bar.py[line:272] - INFO: epoch 005:    135 / 660 loss=4.048, loss_v1=0, loss_v2=0, nll_loss=3.067, ntokens=5776.3, nsentences=84, sample_size=5776.3, sample_size_v1=0, sample_size_v2=0, ppl=8.38, wps=3288.1, ups=0.57, wpb=5776.3, bsz=84, num_updates=2770, lr=2.07522e-05, gnorm=3.519, clip=100, loss_scale=32, train_wall=18, gb_free=11.9, wall=4917
2023-06-29 20:22:14 - progress_bar.py[line:272] - INFO: epoch 005:    145 / 660 loss=4.053, loss_v1=0, loss_v2=0, nll_loss=3.074, ntokens=5996.2, nsentences=84, sample_size=5996.2, sample_size_v1=0, sample_size_v2=0, ppl=8.42, wps=3391, ups=0.57, wpb=5996.2, bsz=84, num_updates=2780, lr=2.07119e-05, gnorm=3.494, clip=100, loss_scale=32, train_wall=18, gb_free=11.9, wall=4935
2023-06-29 20:22:32 - progress_bar.py[line:272] - INFO: epoch 005:    155 / 660 loss=4.025, loss_v1=0, loss_v2=0, nll_loss=3.044, ntokens=5985.3, nsentences=84, sample_size=5985.3, sample_size_v1=0, sample_size_v2=0, ppl=8.25, wps=3398.6, ups=0.57, wpb=5985.3, bsz=84, num_updates=2790, lr=2.06716e-05, gnorm=3.581, clip=100, loss_scale=32, train_wall=18, gb_free=11.8, wall=4952
2023-06-29 20:22:50 - progress_bar.py[line:272] - INFO: epoch 005:    165 / 660 loss=4.037, loss_v1=0, loss_v2=0, nll_loss=3.058, ntokens=5904.7, nsentences=84, sample_size=5904.7, sample_size_v1=0, sample_size_v2=0, ppl=8.33, wps=3339.7, ups=0.57, wpb=5904.7, bsz=84, num_updates=2800, lr=2.06313e-05, gnorm=3.318, clip=100, loss_scale=32, train_wall=18, gb_free=11.5, wall=4970
2023-06-29 20:23:07 - progress_bar.py[line:272] - INFO: epoch 005:    175 / 660 loss=4.059, loss_v1=0, loss_v2=0, nll_loss=3.081, ntokens=6012.9, nsentences=84, sample_size=6012.9, sample_size_v1=0, sample_size_v2=0, ppl=8.46, wps=3403.1, ups=0.57, wpb=6012.9, bsz=84, num_updates=2810, lr=2.0591e-05, gnorm=3.56, clip=100, loss_scale=32, train_wall=18, gb_free=11.4, wall=4988
2023-06-29 20:23:25 - progress_bar.py[line:272] - INFO: epoch 005:    185 / 660 loss=4.023, loss_v1=0, loss_v2=0, nll_loss=3.04, ntokens=5945.6, nsentences=84, sample_size=5945.6, sample_size_v1=0, sample_size_v2=0, ppl=8.23, wps=3378.3, ups=0.57, wpb=5945.6, bsz=84, num_updates=2820, lr=2.05507e-05, gnorm=3.596, clip=100, loss_scale=32, train_wall=18, gb_free=11.9, wall=5005
2023-06-29 20:23:42 - progress_bar.py[line:272] - INFO: epoch 005:    195 / 660 loss=4.007, loss_v1=0, loss_v2=0, nll_loss=3.024, ntokens=5746.1, nsentences=84, sample_size=5746.1, sample_size_v1=0, sample_size_v2=0, ppl=8.13, wps=3303.3, ups=0.57, wpb=5746.1, bsz=84, num_updates=2830, lr=2.05104e-05, gnorm=3.597, clip=100, loss_scale=32, train_wall=17, gb_free=12.1, wall=5023
2023-06-29 20:24:00 - progress_bar.py[line:272] - INFO: epoch 005:    205 / 660 loss=3.999, loss_v1=0, loss_v2=0, nll_loss=3.014, ntokens=5936.9, nsentences=84, sample_size=5936.9, sample_size_v1=0, sample_size_v2=0, ppl=8.08, wps=3363.7, ups=0.57, wpb=5936.9, bsz=84, num_updates=2840, lr=2.04701e-05, gnorm=3.667, clip=100, loss_scale=32, train_wall=18, gb_free=11.9, wall=5040
2023-06-29 20:24:18 - progress_bar.py[line:272] - INFO: epoch 005:    215 / 660 loss=4.014, loss_v1=0, loss_v2=0, nll_loss=3.03, ntokens=6071, nsentences=84, sample_size=6071, sample_size_v1=0, sample_size_v2=0, ppl=8.17, wps=3429.4, ups=0.56, wpb=6071, bsz=84, num_updates=2850, lr=2.04298e-05, gnorm=3.698, clip=100, loss_scale=32, train_wall=18, gb_free=11.8, wall=5058
2023-06-29 20:24:35 - progress_bar.py[line:272] - INFO: epoch 005:    225 / 660 loss=4.029, loss_v1=0, loss_v2=0, nll_loss=3.046, ntokens=6147, nsentences=84, sample_size=6147, sample_size_v1=0, sample_size_v2=0, ppl=8.26, wps=3499.8, ups=0.57, wpb=6147, bsz=84, num_updates=2860, lr=2.03895e-05, gnorm=3.544, clip=100, loss_scale=32, train_wall=18, gb_free=11.9, wall=5076
2023-06-29 20:24:53 - progress_bar.py[line:272] - INFO: epoch 005:    235 / 660 loss=3.974, loss_v1=0, loss_v2=0, nll_loss=2.987, ntokens=5782.8, nsentences=84, sample_size=5782.8, sample_size_v1=0, sample_size_v2=0, ppl=7.93, wps=3312.7, ups=0.57, wpb=5782.8, bsz=84, num_updates=2870, lr=2.03492e-05, gnorm=3.528, clip=100, loss_scale=32, train_wall=17, gb_free=12, wall=5093
2023-06-29 20:25:10 - progress_bar.py[line:272] - INFO: epoch 005:    245 / 660 loss=3.991, loss_v1=0, loss_v2=0, nll_loss=3.004, ntokens=6065.9, nsentences=84, sample_size=6065.9, sample_size_v1=0, sample_size_v2=0, ppl=8.02, wps=3466.3, ups=0.57, wpb=6065.9, bsz=84, num_updates=2880, lr=2.03089e-05, gnorm=3.472, clip=100, loss_scale=32, train_wall=17, gb_free=12.3, wall=5110
2023-06-29 20:25:28 - progress_bar.py[line:272] - INFO: epoch 005:    255 / 660 loss=3.964, loss_v1=0, loss_v2=0, nll_loss=2.973, ntokens=6024.2, nsentences=84, sample_size=6024.2, sample_size_v1=0, sample_size_v2=0, ppl=7.85, wps=3445.6, ups=0.57, wpb=6024.2, bsz=84, num_updates=2890, lr=2.02686e-05, gnorm=3.483, clip=100, loss_scale=32, train_wall=17, gb_free=11.8, wall=5128
2023-06-29 20:25:45 - progress_bar.py[line:272] - INFO: epoch 005:    265 / 660 loss=3.953, loss_v1=0, loss_v2=0, nll_loss=2.962, ntokens=5919.9, nsentences=84, sample_size=5919.9, sample_size_v1=0, sample_size_v2=0, ppl=7.79, wps=3350.2, ups=0.57, wpb=5919.9, bsz=84, num_updates=2900, lr=2.02283e-05, gnorm=3.534, clip=100, loss_scale=32, train_wall=18, gb_free=11.8, wall=5146
2023-06-29 20:26:03 - progress_bar.py[line:272] - INFO: epoch 005:    275 / 660 loss=3.96, loss_v1=0, loss_v2=0, nll_loss=2.969, ntokens=5714.4, nsentences=84, sample_size=5714.4, sample_size_v1=0, sample_size_v2=0, ppl=7.83, wps=3258.4, ups=0.57, wpb=5714.4, bsz=84, num_updates=2910, lr=2.0188e-05, gnorm=3.664, clip=100, loss_scale=32, train_wall=18, gb_free=11.9, wall=5163
2023-06-29 20:26:21 - progress_bar.py[line:272] - INFO: epoch 005:    285 / 660 loss=3.973, loss_v1=0, loss_v2=0, nll_loss=2.984, ntokens=6021.6, nsentences=84, sample_size=6021.6, sample_size_v1=0, sample_size_v2=0, ppl=7.91, wps=3423.5, ups=0.57, wpb=6021.6, bsz=84, num_updates=2920, lr=2.01478e-05, gnorm=3.676, clip=100, loss_scale=32, train_wall=18, gb_free=11.9, wall=5181
2023-06-29 20:26:38 - progress_bar.py[line:272] - INFO: epoch 005:    295 / 660 loss=3.975, loss_v1=0, loss_v2=0, nll_loss=2.985, ntokens=6139.9, nsentences=84, sample_size=6139.9, sample_size_v1=0, sample_size_v2=0, ppl=7.92, wps=3527.6, ups=0.57, wpb=6139.9, bsz=84, num_updates=2930, lr=2.01075e-05, gnorm=3.796, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=5198
2023-06-29 20:26:56 - progress_bar.py[line:272] - INFO: epoch 005:    305 / 660 loss=3.936, loss_v1=0, loss_v2=0, nll_loss=2.99, ntokens=6169, nsentences=82.8, sample_size=6169, sample_size_v1=0, sample_size_v2=0, ppl=7.95, wps=3529.9, ups=0.57, wpb=6169, bsz=82.8, num_updates=2940, lr=2.00672e-05, gnorm=3.454, clip=100, loss_scale=32, train_wall=17, gb_free=11.7, wall=5216
2023-06-29 20:27:13 - progress_bar.py[line:272] - INFO: epoch 005:    315 / 660 loss=3.951, loss_v1=0, loss_v2=0, nll_loss=2.959, ntokens=6284.1, nsentences=84, sample_size=6284.1, sample_size_v1=0, sample_size_v2=0, ppl=7.77, wps=3594, ups=0.57, wpb=6284.1, bsz=84, num_updates=2950, lr=2.00269e-05, gnorm=3.822, clip=100, loss_scale=32, train_wall=17, gb_free=12.2, wall=5233
2023-06-29 20:27:31 - progress_bar.py[line:272] - INFO: epoch 005:    325 / 660 loss=3.963, loss_v1=0, loss_v2=0, nll_loss=2.971, ntokens=6068, nsentences=84, sample_size=6068, sample_size_v1=0, sample_size_v2=0, ppl=7.84, wps=3463.4, ups=0.57, wpb=6068, bsz=84, num_updates=2960, lr=1.99866e-05, gnorm=3.708, clip=100, loss_scale=32, train_wall=17, gb_free=12.1, wall=5251
2023-06-29 20:27:48 - progress_bar.py[line:272] - INFO: epoch 005:    335 / 660 loss=3.961, loss_v1=0, loss_v2=0, nll_loss=2.971, ntokens=5989, nsentences=84, sample_size=5989, sample_size_v1=0, sample_size_v2=0, ppl=7.84, wps=3419, ups=0.57, wpb=5989, bsz=84, num_updates=2970, lr=1.99463e-05, gnorm=3.474, clip=100, loss_scale=32, train_wall=17, gb_free=11.7, wall=5268
2023-06-29 20:28:06 - progress_bar.py[line:272] - INFO: epoch 005:    345 / 660 loss=3.95, loss_v1=0, loss_v2=0, nll_loss=2.958, ntokens=6023.3, nsentences=84, sample_size=6023.3, sample_size_v1=0, sample_size_v2=0, ppl=7.77, wps=3444.4, ups=0.57, wpb=6023.3, bsz=84, num_updates=2980, lr=1.9906e-05, gnorm=3.561, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=5286
2023-06-29 20:28:21 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-06-29 20:28:25 - progress_bar.py[line:272] - INFO: epoch 005:    356 / 660 loss=3.923, loss_v1=0, loss_v2=0, nll_loss=2.927, ntokens=5989.4, nsentences=84, sample_size=5989.4, sample_size_v1=0, sample_size_v2=0, ppl=7.6, wps=3130.2, ups=0.52, wpb=5989.4, bsz=84, num_updates=2990, lr=1.98657e-05, gnorm=3.704, clip=100, loss_scale=32, train_wall=19, gb_free=12.1, wall=5305
2023-06-29 20:28:42 - progress_bar.py[line:272] - INFO: epoch 005:    366 / 660 loss=3.937, loss_v1=0, loss_v2=0, nll_loss=2.944, ntokens=5933.3, nsentences=84, sample_size=5933.3, sample_size_v1=0, sample_size_v2=0, ppl=7.69, wps=3383, ups=0.57, wpb=5933.3, bsz=84, num_updates=3000, lr=1.98254e-05, gnorm=3.65, clip=100, loss_scale=32, train_wall=18, gb_free=12, wall=5322
2023-06-29 20:29:00 - progress_bar.py[line:272] - INFO: epoch 005:    376 / 660 loss=3.928, loss_v1=0, loss_v2=0, nll_loss=2.933, ntokens=5891.2, nsentences=84, sample_size=5891.2, sample_size_v1=0, sample_size_v2=0, ppl=7.64, wps=3347.4, ups=0.57, wpb=5891.2, bsz=84, num_updates=3010, lr=1.97851e-05, gnorm=3.503, clip=100, loss_scale=32, train_wall=18, gb_free=11.9, wall=5340
2023-06-29 20:29:17 - progress_bar.py[line:272] - INFO: epoch 005:    386 / 660 loss=3.907, loss_v1=0, loss_v2=0, nll_loss=2.91, ntokens=5738.8, nsentences=84, sample_size=5738.8, sample_size_v1=0, sample_size_v2=0, ppl=7.52, wps=3319.3, ups=0.58, wpb=5738.8, bsz=84, num_updates=3020, lr=1.97448e-05, gnorm=3.801, clip=100, loss_scale=32, train_wall=17, gb_free=12.1, wall=5357
2023-06-29 20:29:34 - progress_bar.py[line:272] - INFO: epoch 005:    396 / 660 loss=3.903, loss_v1=0, loss_v2=0, nll_loss=2.907, ntokens=5657.8, nsentences=84, sample_size=5657.8, sample_size_v1=0, sample_size_v2=0, ppl=7.5, wps=3274, ups=0.58, wpb=5657.8, bsz=84, num_updates=3030, lr=1.97045e-05, gnorm=3.682, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=5375
2023-06-29 20:29:52 - progress_bar.py[line:272] - INFO: epoch 005:    406 / 660 loss=3.902, loss_v1=0, loss_v2=0, nll_loss=2.903, ntokens=5841.3, nsentences=84, sample_size=5841.3, sample_size_v1=0, sample_size_v2=0, ppl=7.48, wps=3365.7, ups=0.58, wpb=5841.3, bsz=84, num_updates=3040, lr=1.96642e-05, gnorm=3.76, clip=100, loss_scale=32, train_wall=17, gb_free=11.8, wall=5392
2023-06-29 20:30:09 - progress_bar.py[line:272] - INFO: epoch 005:    416 / 660 loss=3.917, loss_v1=0, loss_v2=0, nll_loss=2.921, ntokens=5705.9, nsentences=84, sample_size=5705.9, sample_size_v1=0, sample_size_v2=0, ppl=7.58, wps=3308.1, ups=0.58, wpb=5705.9, bsz=84, num_updates=3050, lr=1.96239e-05, gnorm=3.878, clip=100, loss_scale=32, train_wall=17, gb_free=12.4, wall=5409
2023-06-29 20:30:26 - progress_bar.py[line:272] - INFO: epoch 005:    426 / 660 loss=3.916, loss_v1=0, loss_v2=0, nll_loss=2.919, ntokens=5757.8, nsentences=84, sample_size=5757.8, sample_size_v1=0, sample_size_v2=0, ppl=7.56, wps=3310.2, ups=0.57, wpb=5757.8, bsz=84, num_updates=3060, lr=1.95836e-05, gnorm=3.805, clip=100, loss_scale=32, train_wall=17, gb_free=12.8, wall=5427
2023-06-29 20:30:44 - progress_bar.py[line:272] - INFO: epoch 005:    436 / 660 loss=3.931, loss_v1=0, loss_v2=0, nll_loss=2.936, ntokens=6034.6, nsentences=84, sample_size=6034.6, sample_size_v1=0, sample_size_v2=0, ppl=7.65, wps=3474.2, ups=0.58, wpb=6034.6, bsz=84, num_updates=3070, lr=1.95433e-05, gnorm=3.547, clip=100, loss_scale=32, train_wall=17, gb_free=12.2, wall=5444
2023-06-29 20:31:01 - progress_bar.py[line:272] - INFO: epoch 005:    446 / 660 loss=3.92, loss_v1=0, loss_v2=0, nll_loss=2.924, ntokens=5872.8, nsentences=84, sample_size=5872.8, sample_size_v1=0, sample_size_v2=0, ppl=7.59, wps=3387.6, ups=0.58, wpb=5872.8, bsz=84, num_updates=3080, lr=1.9503e-05, gnorm=3.761, clip=100, loss_scale=32, train_wall=17, gb_free=12.2, wall=5461
2023-06-29 20:31:19 - progress_bar.py[line:272] - INFO: epoch 005:    456 / 660 loss=3.884, loss_v1=0, loss_v2=0, nll_loss=2.883, ntokens=5721.1, nsentences=84, sample_size=5721.1, sample_size_v1=0, sample_size_v2=0, ppl=7.38, wps=3280.2, ups=0.57, wpb=5721.1, bsz=84, num_updates=3090, lr=1.94627e-05, gnorm=3.516, clip=100, loss_scale=32, train_wall=17, gb_free=12.4, wall=5479
2023-06-29 20:31:36 - progress_bar.py[line:272] - INFO: epoch 005:    466 / 660 loss=3.935, loss_v1=0, loss_v2=0, nll_loss=2.94, ntokens=6105.3, nsentences=84, sample_size=6105.3, sample_size_v1=0, sample_size_v2=0, ppl=7.67, wps=3506.6, ups=0.57, wpb=6105.3, bsz=84, num_updates=3100, lr=1.94224e-05, gnorm=3.572, clip=100, loss_scale=32, train_wall=17, gb_free=11.8, wall=5496
2023-06-29 20:31:53 - progress_bar.py[line:272] - INFO: epoch 005:    476 / 660 loss=3.922, loss_v1=0, loss_v2=0, nll_loss=2.925, ntokens=5880.1, nsentences=84, sample_size=5880.1, sample_size_v1=0, sample_size_v2=0, ppl=7.6, wps=3378.3, ups=0.57, wpb=5880.1, bsz=84, num_updates=3110, lr=1.93821e-05, gnorm=3.797, clip=100, loss_scale=32, train_wall=17, gb_free=12.3, wall=5514
2023-06-29 20:32:11 - progress_bar.py[line:272] - INFO: epoch 005:    486 / 660 loss=3.856, loss_v1=0, loss_v2=0, nll_loss=2.852, ntokens=5613.7, nsentences=84, sample_size=5613.7, sample_size_v1=0, sample_size_v2=0, ppl=7.22, wps=3232.6, ups=0.58, wpb=5613.7, bsz=84, num_updates=3120, lr=1.93418e-05, gnorm=3.932, clip=100, loss_scale=32, train_wall=17, gb_free=12.1, wall=5531
2023-06-29 20:32:28 - progress_bar.py[line:272] - INFO: epoch 005:    496 / 660 loss=3.892, loss_v1=0, loss_v2=0, nll_loss=2.891, ntokens=5792.3, nsentences=84, sample_size=5792.3, sample_size_v1=0, sample_size_v2=0, ppl=7.42, wps=3322.2, ups=0.57, wpb=5792.3, bsz=84, num_updates=3130, lr=1.93015e-05, gnorm=3.638, clip=100, loss_scale=32, train_wall=17, gb_free=12.2, wall=5548
2023-06-29 20:32:46 - progress_bar.py[line:272] - INFO: epoch 005:    506 / 660 loss=3.892, loss_v1=0, loss_v2=0, nll_loss=2.894, ntokens=5858.3, nsentences=84, sample_size=5858.3, sample_size_v1=0, sample_size_v2=0, ppl=7.43, wps=3371.5, ups=0.58, wpb=5858.3, bsz=84, num_updates=3140, lr=1.92612e-05, gnorm=3.712, clip=100, loss_scale=32, train_wall=17, gb_free=12.7, wall=5566
2023-06-29 20:33:03 - progress_bar.py[line:272] - INFO: epoch 005:    516 / 660 loss=3.886, loss_v1=0, loss_v2=0, nll_loss=2.885, ntokens=5832.5, nsentences=84, sample_size=5832.5, sample_size_v1=0, sample_size_v2=0, ppl=7.38, wps=3361.4, ups=0.58, wpb=5832.5, bsz=84, num_updates=3150, lr=1.9221e-05, gnorm=3.801, clip=100, loss_scale=32, train_wall=17, gb_free=12, wall=5583
2023-06-29 20:33:20 - progress_bar.py[line:272] - INFO: epoch 005:    526 / 660 loss=3.905, loss_v1=0, loss_v2=0, nll_loss=2.907, ntokens=6238.8, nsentences=84, sample_size=6238.8, sample_size_v1=0, sample_size_v2=0, ppl=7.5, wps=3582, ups=0.57, wpb=6238.8, bsz=84, num_updates=3160, lr=1.91807e-05, gnorm=3.896, clip=100, loss_scale=32, train_wall=17, gb_free=12, wall=5601
2023-06-29 20:33:38 - progress_bar.py[line:272] - INFO: epoch 005:    536 / 660 loss=3.884, loss_v1=0, loss_v2=0, nll_loss=2.882, ntokens=5830.2, nsentences=84, sample_size=5830.2, sample_size_v1=0, sample_size_v2=0, ppl=7.37, wps=3343.3, ups=0.57, wpb=5830.2, bsz=84, num_updates=3170, lr=1.91404e-05, gnorm=4.024, clip=100, loss_scale=32, train_wall=17, gb_free=12.3, wall=5618
2023-06-29 20:33:55 - progress_bar.py[line:272] - INFO: epoch 005:    546 / 660 loss=3.902, loss_v1=0, loss_v2=0, nll_loss=2.903, ntokens=6229.5, nsentences=84, sample_size=6229.5, sample_size_v1=0, sample_size_v2=0, ppl=7.48, wps=3589.9, ups=0.58, wpb=6229.5, bsz=84, num_updates=3180, lr=1.91001e-05, gnorm=3.925, clip=100, loss_scale=32, train_wall=17, gb_free=12.3, wall=5635
2023-06-29 20:34:13 - progress_bar.py[line:272] - INFO: epoch 005:    556 / 660 loss=3.872, loss_v1=0, loss_v2=0, nll_loss=2.868, ntokens=5902.7, nsentences=84, sample_size=5902.7, sample_size_v1=0, sample_size_v2=0, ppl=7.3, wps=3397.9, ups=0.58, wpb=5902.7, bsz=84, num_updates=3190, lr=1.90598e-05, gnorm=3.781, clip=100, loss_scale=32, train_wall=17, gb_free=11.8, wall=5653
2023-06-29 20:34:30 - progress_bar.py[line:272] - INFO: epoch 005:    566 / 660 loss=3.864, loss_v1=0, loss_v2=0, nll_loss=2.86, ntokens=5877.8, nsentences=84, sample_size=5877.8, sample_size_v1=0, sample_size_v2=0, ppl=7.26, wps=3374.2, ups=0.57, wpb=5877.8, bsz=84, num_updates=3200, lr=1.90195e-05, gnorm=3.876, clip=100, loss_scale=32, train_wall=17, gb_free=12.1, wall=5670
2023-06-29 20:34:47 - progress_bar.py[line:272] - INFO: epoch 005:    576 / 660 loss=3.859, loss_v1=0, loss_v2=0, nll_loss=2.855, ntokens=5626.4, nsentences=84, sample_size=5626.4, sample_size_v1=0, sample_size_v2=0, ppl=7.24, wps=3251.8, ups=0.58, wpb=5626.4, bsz=84, num_updates=3210, lr=1.89792e-05, gnorm=4.056, clip=100, loss_scale=32, train_wall=17, gb_free=12, wall=5687
2023-06-29 20:35:05 - progress_bar.py[line:272] - INFO: epoch 005:    586 / 660 loss=3.87, loss_v1=0, loss_v2=0, nll_loss=2.866, ntokens=5946.4, nsentences=84, sample_size=5946.4, sample_size_v1=0, sample_size_v2=0, ppl=7.29, wps=3396.3, ups=0.57, wpb=5946.4, bsz=84, num_updates=3220, lr=1.89389e-05, gnorm=3.802, clip=100, loss_scale=32, train_wall=17, gb_free=11.5, wall=5705
2023-06-29 20:35:22 - progress_bar.py[line:272] - INFO: epoch 005:    596 / 660 loss=3.862, loss_v1=0, loss_v2=0, nll_loss=2.858, ntokens=5923.1, nsentences=84, sample_size=5923.1, sample_size_v1=0, sample_size_v2=0, ppl=7.25, wps=3415.7, ups=0.58, wpb=5923.1, bsz=84, num_updates=3230, lr=1.88986e-05, gnorm=3.906, clip=100, loss_scale=32, train_wall=17, gb_free=12.1, wall=5722
2023-06-29 20:35:39 - progress_bar.py[line:272] - INFO: epoch 005:    606 / 660 loss=3.848, loss_v1=0, loss_v2=0, nll_loss=2.842, ntokens=5960.9, nsentences=84, sample_size=5960.9, sample_size_v1=0, sample_size_v2=0, ppl=7.17, wps=3442.8, ups=0.58, wpb=5960.9, bsz=84, num_updates=3240, lr=1.88583e-05, gnorm=3.911, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=5740
2023-06-29 20:35:57 - progress_bar.py[line:272] - INFO: epoch 005:    616 / 660 loss=3.862, loss_v1=0, loss_v2=0, nll_loss=2.857, ntokens=5959.2, nsentences=84, sample_size=5959.2, sample_size_v1=0, sample_size_v2=0, ppl=7.24, wps=3418.4, ups=0.57, wpb=5959.2, bsz=84, num_updates=3250, lr=1.8818e-05, gnorm=4.045, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=5757
2023-06-29 20:36:14 - progress_bar.py[line:272] - INFO: epoch 005:    626 / 660 loss=3.885, loss_v1=0, loss_v2=0, nll_loss=2.883, ntokens=6116.4, nsentences=84, sample_size=6116.4, sample_size_v1=0, sample_size_v2=0, ppl=7.37, wps=3519.4, ups=0.58, wpb=6116.4, bsz=84, num_updates=3260, lr=1.87777e-05, gnorm=3.863, clip=100, loss_scale=32, train_wall=17, gb_free=12, wall=5774
2023-06-29 20:36:32 - progress_bar.py[line:272] - INFO: epoch 005:    636 / 660 loss=3.86, loss_v1=0, loss_v2=0, nll_loss=2.855, ntokens=5880.5, nsentences=84, sample_size=5880.5, sample_size_v1=0, sample_size_v2=0, ppl=7.23, wps=3378.7, ups=0.57, wpb=5880.5, bsz=84, num_updates=3270, lr=1.87374e-05, gnorm=4.118, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=5792
2023-06-29 20:36:49 - progress_bar.py[line:272] - INFO: epoch 005:    646 / 660 loss=3.878, loss_v1=0, loss_v2=0, nll_loss=2.875, ntokens=6069.3, nsentences=84, sample_size=6069.3, sample_size_v1=0, sample_size_v2=0, ppl=7.34, wps=3482.5, ups=0.57, wpb=6069.3, bsz=84, num_updates=3280, lr=1.86971e-05, gnorm=4.204, clip=100, loss_scale=32, train_wall=17, gb_free=12, wall=5809
2023-06-29 20:37:06 - progress_bar.py[line:272] - INFO: epoch 005:    656 / 660 loss=3.862, loss_v1=0, loss_v2=0, nll_loss=2.858, ntokens=5857.4, nsentences=84, sample_size=5857.4, sample_size_v1=0, sample_size_v2=0, ppl=7.25, wps=3369.5, ups=0.58, wpb=5857.4, bsz=84, num_updates=3290, lr=1.86568e-05, gnorm=4.122, clip=100, loss_scale=32, train_wall=17, gb_free=11.7, wall=5827
2023-06-29 20:37:13 - train.py[line:332] - INFO: end of epoch 5 (average epoch stats below)
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
2023-06-29 20:37:13 - progress_bar.py[line:282] - INFO: epoch 005 | loss 3.95 | loss_v1 0 | loss_v2 0 | nll_loss 2.959 | ntokens 5903.15 | nsentences 83.95 | sample_size 5903.15 | sample_size_v1 0 | sample_size_v2 0 | ppl 7.77 | wps 3367 | ups 0.57 | wpb 5903.1 | bsz 83.9 | num_updates 3294 | lr 1.86407e-05 | gnorm 3.741 | clip 100 | loss_scale 32 | train_wall 1151 | gb_free 12.3 | wall 5833
2023-06-29 20:37:13 - trainer.py[line:639] - INFO: loading train data for epoch 6
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 row count 18467 total row count 55400
slice_id 0 seek offset 0
2023-06-29 20:37:15 - trainer.py[line:703] - INFO: begin training epoch 6
2023-06-29 20:37:15 - train.py[line:305] - INFO: Start iterating over samples
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 row count 18466 total row count 55400
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 row count 18467 total row count 55400
slice_id 2 seek offset 36934
slice_id 1 seek offset 18467
2023-06-29 20:37:26 - progress_bar.py[line:272] - INFO: epoch 006:      6 / 660 loss=3.855, loss_v1=0, loss_v2=0, nll_loss=2.848, ntokens=5765.2, nsentences=81.9, sample_size=5765.2, sample_size_v1=0, sample_size_v2=0, ppl=7.2, wps=2978, ups=0.52, wpb=5765.2, bsz=81.9, num_updates=3300, lr=1.86165e-05, gnorm=4.513, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=5846
2023-06-29 20:37:43 - progress_bar.py[line:272] - INFO: epoch 006:     16 / 660 loss=3.847, loss_v1=0, loss_v2=0, nll_loss=2.84, ntokens=5712, nsentences=84, sample_size=5712, sample_size_v1=0, sample_size_v2=0, ppl=7.16, wps=3268, ups=0.57, wpb=5712, bsz=84, num_updates=3310, lr=1.85762e-05, gnorm=4.441, clip=100, loss_scale=32, train_wall=17, gb_free=12, wall=5863
2023-06-29 20:38:01 - progress_bar.py[line:272] - INFO: epoch 006:     26 / 660 loss=3.816, loss_v1=0, loss_v2=0, nll_loss=2.807, ntokens=5767.3, nsentences=84, sample_size=5767.3, sample_size_v1=0, sample_size_v2=0, ppl=7, wps=3313.4, ups=0.57, wpb=5767.3, bsz=84, num_updates=3320, lr=1.85359e-05, gnorm=4.563, clip=100, loss_scale=32, train_wall=17, gb_free=12.1, wall=5881
2023-06-29 20:38:18 - progress_bar.py[line:272] - INFO: epoch 006:     36 / 660 loss=3.8, loss_v1=0, loss_v2=0, nll_loss=2.788, ntokens=5434.4, nsentences=84, sample_size=5434.4, sample_size_v1=0, sample_size_v2=0, ppl=6.91, wps=3137, ups=0.58, wpb=5434.4, bsz=84, num_updates=3330, lr=1.84956e-05, gnorm=4.338, clip=100, loss_scale=32, train_wall=17, gb_free=11.8, wall=5898
2023-06-29 20:38:32 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-06-29 20:38:37 - progress_bar.py[line:272] - INFO: epoch 006:     47 / 660 loss=3.799, loss_v1=0, loss_v2=0, nll_loss=2.786, ntokens=5889.3, nsentences=84, sample_size=5889.3, sample_size_v1=0, sample_size_v2=0, ppl=6.9, wps=3052.7, ups=0.52, wpb=5889.3, bsz=84, num_updates=3340, lr=1.84553e-05, gnorm=4.518, clip=100, loss_scale=16, train_wall=19, gb_free=11.8, wall=5918
2023-06-29 20:38:55 - progress_bar.py[line:272] - INFO: epoch 006:     57 / 660 loss=3.831, loss_v1=0, loss_v2=0, nll_loss=2.822, ntokens=5853.2, nsentences=84, sample_size=5853.2, sample_size_v1=0, sample_size_v2=0, ppl=7.07, wps=3344.7, ups=0.57, wpb=5853.2, bsz=84, num_updates=3350, lr=1.8415e-05, gnorm=4.221, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=5935
2023-06-29 20:39:12 - progress_bar.py[line:272] - INFO: epoch 006:     67 / 660 loss=3.754, loss_v1=0, loss_v2=0, nll_loss=2.737, ntokens=5506.2, nsentences=84, sample_size=5506.2, sample_size_v1=0, sample_size_v2=0, ppl=6.67, wps=3180.9, ups=0.58, wpb=5506.2, bsz=84, num_updates=3360, lr=1.83747e-05, gnorm=4.599, clip=100, loss_scale=16, train_wall=17, gb_free=11.8, wall=5952
2023-06-29 20:39:30 - progress_bar.py[line:272] - INFO: epoch 006:     77 / 660 loss=3.818, loss_v1=0, loss_v2=0, nll_loss=2.805, ntokens=6323.1, nsentences=84, sample_size=6323.1, sample_size_v1=0, sample_size_v2=0, ppl=6.99, wps=3542.8, ups=0.56, wpb=6323.1, bsz=84, num_updates=3370, lr=1.83345e-05, gnorm=5.229, clip=100, loss_scale=16, train_wall=18, gb_free=11.2, wall=5970
2023-06-29 20:39:48 - progress_bar.py[line:272] - INFO: epoch 006:     87 / 660 loss=3.807, loss_v1=0, loss_v2=0, nll_loss=2.795, ntokens=5968.8, nsentences=84, sample_size=5968.8, sample_size_v1=0, sample_size_v2=0, ppl=6.94, wps=3327.2, ups=0.56, wpb=5968.8, bsz=84, num_updates=3380, lr=1.82942e-05, gnorm=4.878, clip=100, loss_scale=16, train_wall=18, gb_free=11.6, wall=5988
2023-06-29 20:40:05 - progress_bar.py[line:272] - INFO: epoch 006:     97 / 660 loss=3.801, loss_v1=0, loss_v2=0, nll_loss=2.789, ntokens=5757.3, nsentences=84, sample_size=5757.3, sample_size_v1=0, sample_size_v2=0, ppl=6.91, wps=3282.8, ups=0.57, wpb=5757.3, bsz=84, num_updates=3390, lr=1.82539e-05, gnorm=4.784, clip=100, loss_scale=16, train_wall=18, gb_free=12.2, wall=6006
2023-06-29 20:40:23 - progress_bar.py[line:272] - INFO: epoch 006:    107 / 660 loss=3.774, loss_v1=0, loss_v2=0, nll_loss=2.759, ntokens=5873.9, nsentences=84, sample_size=5873.9, sample_size_v1=0, sample_size_v2=0, ppl=6.77, wps=3348.1, ups=0.57, wpb=5873.9, bsz=84, num_updates=3400, lr=1.82136e-05, gnorm=4.929, clip=100, loss_scale=16, train_wall=18, gb_free=12.1, wall=6023
2023-06-29 20:40:40 - progress_bar.py[line:272] - INFO: epoch 006:    117 / 660 loss=3.805, loss_v1=0, loss_v2=0, nll_loss=2.793, ntokens=5665.8, nsentences=84, sample_size=5665.8, sample_size_v1=0, sample_size_v2=0, ppl=6.93, wps=3265.1, ups=0.58, wpb=5665.8, bsz=84, num_updates=3410, lr=1.81733e-05, gnorm=4.828, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=6041
2023-06-29 20:40:58 - progress_bar.py[line:272] - INFO: epoch 006:    127 / 660 loss=3.854, loss_v1=0, loss_v2=0, nll_loss=2.85, ntokens=5717.3, nsentences=84, sample_size=5717.3, sample_size_v1=0, sample_size_v2=0, ppl=7.21, wps=3281.2, ups=0.57, wpb=5717.3, bsz=84, num_updates=3420, lr=1.8133e-05, gnorm=4.766, clip=100, loss_scale=16, train_wall=17, gb_free=11.8, wall=6058
2023-06-29 20:41:15 - progress_bar.py[line:272] - INFO: epoch 006:    137 / 660 loss=3.861, loss_v1=0, loss_v2=0, nll_loss=2.854, ntokens=5919.5, nsentences=84, sample_size=5919.5, sample_size_v1=0, sample_size_v2=0, ppl=7.23, wps=3368, ups=0.57, wpb=5919.5, bsz=84, num_updates=3430, lr=1.80927e-05, gnorm=4.621, clip=100, loss_scale=16, train_wall=18, gb_free=11.6, wall=6076
2023-06-29 20:41:33 - progress_bar.py[line:272] - INFO: epoch 006:    147 / 660 loss=3.856, loss_v1=0, loss_v2=0, nll_loss=2.849, ntokens=5906.4, nsentences=84, sample_size=5906.4, sample_size_v1=0, sample_size_v2=0, ppl=7.2, wps=3349.4, ups=0.57, wpb=5906.4, bsz=84, num_updates=3440, lr=1.80524e-05, gnorm=4.437, clip=100, loss_scale=16, train_wall=18, gb_free=11.7, wall=6093
2023-06-29 20:41:51 - progress_bar.py[line:272] - INFO: epoch 006:    157 / 660 loss=3.842, loss_v1=0, loss_v2=0, nll_loss=2.836, ntokens=6002, nsentences=84, sample_size=6002, sample_size_v1=0, sample_size_v2=0, ppl=7.14, wps=3402.5, ups=0.57, wpb=6002, bsz=84, num_updates=3450, lr=1.80121e-05, gnorm=4.551, clip=100, loss_scale=16, train_wall=18, gb_free=11.8, wall=6111
2023-06-29 20:42:08 - progress_bar.py[line:272] - INFO: epoch 006:    167 / 660 loss=3.829, loss_v1=0, loss_v2=0, nll_loss=2.82, ntokens=5902.1, nsentences=84, sample_size=5902.1, sample_size_v1=0, sample_size_v2=0, ppl=7.06, wps=3335.2, ups=0.57, wpb=5902.1, bsz=84, num_updates=3460, lr=1.79718e-05, gnorm=4.627, clip=100, loss_scale=16, train_wall=18, gb_free=11.6, wall=6129
2023-06-29 20:42:26 - progress_bar.py[line:272] - INFO: epoch 006:    177 / 660 loss=3.884, loss_v1=0, loss_v2=0, nll_loss=2.882, ntokens=5977.1, nsentences=84, sample_size=5977.1, sample_size_v1=0, sample_size_v2=0, ppl=7.37, wps=3383.9, ups=0.57, wpb=5977.1, bsz=84, num_updates=3470, lr=1.79315e-05, gnorm=4.628, clip=100, loss_scale=16, train_wall=18, gb_free=11.6, wall=6146
2023-06-29 20:42:44 - progress_bar.py[line:272] - INFO: epoch 006:    187 / 660 loss=3.832, loss_v1=0, loss_v2=0, nll_loss=2.821, ntokens=5984.9, nsentences=84, sample_size=5984.9, sample_size_v1=0, sample_size_v2=0, ppl=7.07, wps=3417.1, ups=0.57, wpb=5984.9, bsz=84, num_updates=3480, lr=1.78912e-05, gnorm=4.746, clip=100, loss_scale=16, train_wall=17, gb_free=11.7, wall=6164
2023-06-29 20:43:01 - progress_bar.py[line:272] - INFO: epoch 006:    197 / 660 loss=3.807, loss_v1=0, loss_v2=0, nll_loss=2.797, ntokens=5660.4, nsentences=84, sample_size=5660.4, sample_size_v1=0, sample_size_v2=0, ppl=6.95, wps=3253.4, ups=0.57, wpb=5660.4, bsz=84, num_updates=3490, lr=1.78509e-05, gnorm=5.32, clip=100, loss_scale=16, train_wall=17, gb_free=11.7, wall=6181
2023-06-29 20:43:19 - progress_bar.py[line:272] - INFO: epoch 006:    207 / 660 loss=3.826, loss_v1=0, loss_v2=0, nll_loss=2.817, ntokens=6015.6, nsentences=84, sample_size=6015.6, sample_size_v1=0, sample_size_v2=0, ppl=7.05, wps=3411, ups=0.57, wpb=6015.6, bsz=84, num_updates=3500, lr=1.78106e-05, gnorm=5.056, clip=100, loss_scale=16, train_wall=18, gb_free=12, wall=6199
2023-06-29 20:43:36 - progress_bar.py[line:272] - INFO: epoch 006:    217 / 660 loss=3.836, loss_v1=0, loss_v2=0, nll_loss=2.828, ntokens=6046.9, nsentences=84, sample_size=6046.9, sample_size_v1=0, sample_size_v2=0, ppl=7.1, wps=3430.5, ups=0.57, wpb=6046.9, bsz=84, num_updates=3510, lr=1.77703e-05, gnorm=4.717, clip=100, loss_scale=16, train_wall=18, gb_free=11.7, wall=6216
2023-06-29 20:43:54 - progress_bar.py[line:272] - INFO: epoch 006:    227 / 660 loss=3.849, loss_v1=0, loss_v2=0, nll_loss=2.841, ntokens=6177.8, nsentences=84, sample_size=6177.8, sample_size_v1=0, sample_size_v2=0, ppl=7.17, wps=3490.4, ups=0.56, wpb=6177.8, bsz=84, num_updates=3520, lr=1.773e-05, gnorm=4.851, clip=100, loss_scale=16, train_wall=18, gb_free=12, wall=6234
2023-06-29 20:44:11 - progress_bar.py[line:272] - INFO: epoch 006:    237 / 660 loss=3.786, loss_v1=0, loss_v2=0, nll_loss=2.772, ntokens=5710.6, nsentences=84, sample_size=5710.6, sample_size_v1=0, sample_size_v2=0, ppl=6.83, wps=3276.6, ups=0.57, wpb=5710.6, bsz=84, num_updates=3530, lr=1.76897e-05, gnorm=5.083, clip=100, loss_scale=16, train_wall=17, gb_free=11.5, wall=6252
2023-06-29 20:44:29 - progress_bar.py[line:272] - INFO: epoch 006:    247 / 660 loss=3.808, loss_v1=0, loss_v2=0, nll_loss=2.797, ntokens=6170.6, nsentences=84, sample_size=6170.6, sample_size_v1=0, sample_size_v2=0, ppl=6.95, wps=3536.5, ups=0.57, wpb=6170.6, bsz=84, num_updates=3540, lr=1.76494e-05, gnorm=4.845, clip=100, loss_scale=16, train_wall=17, gb_free=12.3, wall=6269
2023-06-29 20:44:46 - progress_bar.py[line:272] - INFO: epoch 006:    257 / 660 loss=3.787, loss_v1=0, loss_v2=0, nll_loss=2.773, ntokens=5894.3, nsentences=84, sample_size=5894.3, sample_size_v1=0, sample_size_v2=0, ppl=6.84, wps=3379.2, ups=0.57, wpb=5894.3, bsz=84, num_updates=3550, lr=1.76091e-05, gnorm=5.631, clip=100, loss_scale=16, train_wall=17, gb_free=11.8, wall=6286
2023-06-29 20:45:04 - progress_bar.py[line:272] - INFO: epoch 006:    267 / 660 loss=3.776, loss_v1=0, loss_v2=0, nll_loss=2.759, ntokens=5959, nsentences=84, sample_size=5959, sample_size_v1=0, sample_size_v2=0, ppl=6.77, wps=3412, ups=0.57, wpb=5959, bsz=84, num_updates=3560, lr=1.75688e-05, gnorm=5.225, clip=100, loss_scale=16, train_wall=17, gb_free=11.8, wall=6304
2023-06-29 20:45:21 - progress_bar.py[line:272] - INFO: epoch 006:    277 / 660 loss=3.787, loss_v1=0, loss_v2=0, nll_loss=2.773, ntokens=5806.6, nsentences=84, sample_size=5806.6, sample_size_v1=0, sample_size_v2=0, ppl=6.83, wps=3295.1, ups=0.57, wpb=5806.6, bsz=84, num_updates=3570, lr=1.75285e-05, gnorm=5.326, clip=100, loss_scale=16, train_wall=18, gb_free=12, wall=6322
2023-06-29 20:45:39 - progress_bar.py[line:272] - INFO: epoch 006:    287 / 660 loss=3.806, loss_v1=0, loss_v2=0, nll_loss=2.794, ntokens=6021.3, nsentences=84, sample_size=6021.3, sample_size_v1=0, sample_size_v2=0, ppl=6.93, wps=3438.5, ups=0.57, wpb=6021.3, bsz=84, num_updates=3580, lr=1.74882e-05, gnorm=5.005, clip=100, loss_scale=16, train_wall=17, gb_free=12.3, wall=6339
2023-06-29 20:45:56 - progress_bar.py[line:272] - INFO: epoch 006:    297 / 660 loss=3.81, loss_v1=0, loss_v2=0, nll_loss=2.798, ntokens=6128.3, nsentences=84, sample_size=6128.3, sample_size_v1=0, sample_size_v2=0, ppl=6.95, wps=3512.7, ups=0.57, wpb=6128.3, bsz=84, num_updates=3590, lr=1.7448e-05, gnorm=5.351, clip=100, loss_scale=16, train_wall=17, gb_free=12.2, wall=6357
2023-06-29 20:46:14 - progress_bar.py[line:272] - INFO: epoch 006:    307 / 660 loss=3.8, loss_v1=0, loss_v2=0, nll_loss=2.786, ntokens=6322.4, nsentences=84, sample_size=6322.4, sample_size_v1=0, sample_size_v2=0, ppl=6.9, wps=3574.8, ups=0.57, wpb=6322.4, bsz=84, num_updates=3600, lr=1.74077e-05, gnorm=5.294, clip=100, loss_scale=16, train_wall=18, gb_free=12.2, wall=6374
2023-06-29 20:46:32 - progress_bar.py[line:272] - INFO: epoch 006:    317 / 660 loss=3.789, loss_v1=0, loss_v2=0, nll_loss=2.776, ntokens=6184.6, nsentences=84, sample_size=6184.6, sample_size_v1=0, sample_size_v2=0, ppl=6.85, wps=3524.9, ups=0.57, wpb=6184.6, bsz=84, num_updates=3610, lr=1.73674e-05, gnorm=5.255, clip=100, loss_scale=16, train_wall=18, gb_free=11.9, wall=6392
2023-06-29 20:46:49 - progress_bar.py[line:272] - INFO: epoch 006:    327 / 660 loss=3.801, loss_v1=0, loss_v2=0, nll_loss=2.788, ntokens=6111.6, nsentences=84, sample_size=6111.6, sample_size_v1=0, sample_size_v2=0, ppl=6.91, wps=3487.8, ups=0.57, wpb=6111.6, bsz=84, num_updates=3620, lr=1.73271e-05, gnorm=5.089, clip=100, loss_scale=16, train_wall=17, gb_free=11.3, wall=6409
2023-06-29 20:47:07 - progress_bar.py[line:272] - INFO: epoch 006:    337 / 660 loss=3.785, loss_v1=0, loss_v2=0, nll_loss=2.771, ntokens=5951.1, nsentences=84, sample_size=5951.1, sample_size_v1=0, sample_size_v2=0, ppl=6.83, wps=3411.2, ups=0.57, wpb=5951.1, bsz=84, num_updates=3630, lr=1.72868e-05, gnorm=5.502, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=6427
2023-06-29 20:47:24 - progress_bar.py[line:272] - INFO: epoch 006:    347 / 660 loss=3.792, loss_v1=0, loss_v2=0, nll_loss=2.778, ntokens=6058.9, nsentences=84, sample_size=6058.9, sample_size_v1=0, sample_size_v2=0, ppl=6.86, wps=3462.7, ups=0.57, wpb=6058.9, bsz=84, num_updates=3640, lr=1.72465e-05, gnorm=5.593, clip=100, loss_scale=16, train_wall=17, gb_free=12.2, wall=6444
2023-06-29 20:47:41 - progress_bar.py[line:272] - INFO: epoch 006:    357 / 660 loss=3.747, loss_v1=0, loss_v2=0, nll_loss=2.728, ntokens=5682.8, nsentences=80, sample_size=5682.8, sample_size_v1=0, sample_size_v2=0, ppl=6.62, wps=3262.6, ups=0.57, wpb=5682.8, bsz=80, num_updates=3650, lr=1.72062e-05, gnorm=6.138, clip=100, loss_scale=16, train_wall=17, gb_free=12.3, wall=6462
2023-06-29 20:47:59 - progress_bar.py[line:272] - INFO: epoch 006:    367 / 660 loss=3.76, loss_v1=0, loss_v2=0, nll_loss=2.743, ntokens=5858.7, nsentences=84, sample_size=5858.7, sample_size_v1=0, sample_size_v2=0, ppl=6.69, wps=3377, ups=0.58, wpb=5858.7, bsz=84, num_updates=3660, lr=1.71659e-05, gnorm=5.689, clip=100, loss_scale=16, train_wall=17, gb_free=12.1, wall=6479
2023-06-29 20:48:16 - progress_bar.py[line:272] - INFO: epoch 006:    377 / 660 loss=3.761, loss_v1=0, loss_v2=0, nll_loss=2.745, ntokens=5950.1, nsentences=84, sample_size=5950.1, sample_size_v1=0, sample_size_v2=0, ppl=6.7, wps=3413.5, ups=0.57, wpb=5950.1, bsz=84, num_updates=3670, lr=1.71256e-05, gnorm=5.167, clip=100, loss_scale=16, train_wall=17, gb_free=11.8, wall=6496
2023-06-29 20:48:34 - progress_bar.py[line:272] - INFO: epoch 006:    387 / 660 loss=3.742, loss_v1=0, loss_v2=0, nll_loss=2.722, ntokens=5651.1, nsentences=84, sample_size=5651.1, sample_size_v1=0, sample_size_v2=0, ppl=6.6, wps=3267.1, ups=0.58, wpb=5651.1, bsz=84, num_updates=3680, lr=1.70853e-05, gnorm=5.506, clip=100, loss_scale=16, train_wall=17, gb_free=12.4, wall=6514
2023-06-29 20:48:51 - progress_bar.py[line:272] - INFO: epoch 006:    397 / 660 loss=3.742, loss_v1=0, loss_v2=0, nll_loss=2.724, ntokens=5757.2, nsentences=84, sample_size=5757.2, sample_size_v1=0, sample_size_v2=0, ppl=6.61, wps=3324.4, ups=0.58, wpb=5757.2, bsz=84, num_updates=3690, lr=1.7045e-05, gnorm=5.212, clip=100, loss_scale=16, train_wall=17, gb_free=12.5, wall=6531
2023-06-29 20:49:08 - progress_bar.py[line:272] - INFO: epoch 006:    407 / 660 loss=3.742, loss_v1=0, loss_v2=0, nll_loss=2.723, ntokens=5779.9, nsentences=84, sample_size=5779.9, sample_size_v1=0, sample_size_v2=0, ppl=6.6, wps=3342, ups=0.58, wpb=5779.9, bsz=84, num_updates=3700, lr=1.70047e-05, gnorm=5.691, clip=100, loss_scale=16, train_wall=17, gb_free=12.3, wall=6548
2023-06-29 20:49:25 - progress_bar.py[line:272] - INFO: epoch 006:    417 / 660 loss=3.763, loss_v1=0, loss_v2=0, nll_loss=2.745, ntokens=5761.5, nsentences=84, sample_size=5761.5, sample_size_v1=0, sample_size_v2=0, ppl=6.71, wps=3328.3, ups=0.58, wpb=5761.5, bsz=84, num_updates=3710, lr=1.69644e-05, gnorm=5.187, clip=100, loss_scale=16, train_wall=17, gb_free=11.8, wall=6566
2023-06-29 20:49:43 - progress_bar.py[line:272] - INFO: epoch 006:    427 / 660 loss=3.742, loss_v1=0, loss_v2=0, nll_loss=2.723, ntokens=5685.9, nsentences=84, sample_size=5685.9, sample_size_v1=0, sample_size_v2=0, ppl=6.6, wps=3271, ups=0.58, wpb=5685.9, bsz=84, num_updates=3720, lr=1.69241e-05, gnorm=5.436, clip=100, loss_scale=16, train_wall=17, gb_free=12.3, wall=6583
2023-06-29 20:50:00 - progress_bar.py[line:272] - INFO: epoch 006:    437 / 660 loss=3.767, loss_v1=0, loss_v2=0, nll_loss=2.751, ntokens=6052.3, nsentences=84, sample_size=6052.3, sample_size_v1=0, sample_size_v2=0, ppl=6.73, wps=3435.9, ups=0.57, wpb=6052.3, bsz=84, num_updates=3730, lr=1.68838e-05, gnorm=5.819, clip=100, loss_scale=16, train_wall=18, gb_free=12.2, wall=6601
2023-06-29 20:50:18 - progress_bar.py[line:272] - INFO: epoch 006:    447 / 660 loss=3.767, loss_v1=0, loss_v2=0, nll_loss=2.751, ntokens=5801, nsentences=84, sample_size=5801, sample_size_v1=0, sample_size_v2=0, ppl=6.73, wps=3351.2, ups=0.58, wpb=5801, bsz=84, num_updates=3740, lr=1.68435e-05, gnorm=5.493, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=6618
2023-06-29 20:50:35 - progress_bar.py[line:272] - INFO: epoch 006:    457 / 660 loss=3.742, loss_v1=0, loss_v2=0, nll_loss=2.722, ntokens=5807.4, nsentences=84, sample_size=5807.4, sample_size_v1=0, sample_size_v2=0, ppl=6.6, wps=3338.4, ups=0.57, wpb=5807.4, bsz=84, num_updates=3750, lr=1.68032e-05, gnorm=5.044, clip=100, loss_scale=16, train_wall=17, gb_free=12.3, wall=6635
2023-06-29 20:50:53 - progress_bar.py[line:272] - INFO: epoch 006:    467 / 660 loss=3.777, loss_v1=0, loss_v2=0, nll_loss=2.761, ntokens=6127.3, nsentences=84, sample_size=6127.3, sample_size_v1=0, sample_size_v2=0, ppl=6.78, wps=3508.7, ups=0.57, wpb=6127.3, bsz=84, num_updates=3760, lr=1.67629e-05, gnorm=5.007, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=6653
2023-06-29 20:51:10 - progress_bar.py[line:272] - INFO: epoch 006:    477 / 660 loss=3.753, loss_v1=0, loss_v2=0, nll_loss=2.735, ntokens=5840.4, nsentences=84, sample_size=5840.4, sample_size_v1=0, sample_size_v2=0, ppl=6.66, wps=3364.7, ups=0.58, wpb=5840.4, bsz=84, num_updates=3770, lr=1.67226e-05, gnorm=5.356, clip=100, loss_scale=16, train_wall=17, gb_free=12.4, wall=6670
2023-06-29 20:51:27 - progress_bar.py[line:272] - INFO: epoch 006:    487 / 660 loss=3.703, loss_v1=0, loss_v2=0, nll_loss=2.679, ntokens=5611.5, nsentences=84, sample_size=5611.5, sample_size_v1=0, sample_size_v2=0, ppl=6.41, wps=3242.2, ups=0.58, wpb=5611.5, bsz=84, num_updates=3780, lr=1.66823e-05, gnorm=5.166, clip=100, loss_scale=16, train_wall=17, gb_free=11.7, wall=6688
2023-06-29 20:51:45 - progress_bar.py[line:272] - INFO: epoch 006:    497 / 660 loss=3.738, loss_v1=0, loss_v2=0, nll_loss=2.718, ntokens=5810.3, nsentences=84, sample_size=5810.3, sample_size_v1=0, sample_size_v2=0, ppl=6.58, wps=3341.4, ups=0.58, wpb=5810.3, bsz=84, num_updates=3790, lr=1.6642e-05, gnorm=5.347, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=6705
2023-06-29 20:52:02 - progress_bar.py[line:272] - INFO: epoch 006:    507 / 660 loss=3.731, loss_v1=0, loss_v2=0, nll_loss=2.711, ntokens=5809, nsentences=84, sample_size=5809, sample_size_v1=0, sample_size_v2=0, ppl=6.55, wps=3346.3, ups=0.58, wpb=5809, bsz=84, num_updates=3800, lr=1.66017e-05, gnorm=5.512, clip=100, loss_scale=16, train_wall=17, gb_free=12.3, wall=6722
2023-06-29 20:52:19 - progress_bar.py[line:272] - INFO: epoch 006:    517 / 660 loss=3.736, loss_v1=0, loss_v2=0, nll_loss=2.716, ntokens=5902, nsentences=84, sample_size=5902, sample_size_v1=0, sample_size_v2=0, ppl=6.57, wps=3408.3, ups=0.58, wpb=5902, bsz=84, num_updates=3810, lr=1.65615e-05, gnorm=4.997, clip=100, loss_scale=16, train_wall=17, gb_free=12.5, wall=6740
2023-06-29 20:52:37 - progress_bar.py[line:272] - INFO: epoch 006:    527 / 660 loss=3.757, loss_v1=0, loss_v2=0, nll_loss=2.738, ntokens=6192, nsentences=84, sample_size=6192, sample_size_v1=0, sample_size_v2=0, ppl=6.67, wps=3558.1, ups=0.57, wpb=6192, bsz=84, num_updates=3820, lr=1.65212e-05, gnorm=5.305, clip=100, loss_scale=16, train_wall=17, gb_free=11.8, wall=6757
2023-06-29 20:52:54 - progress_bar.py[line:272] - INFO: epoch 006:    537 / 660 loss=3.736, loss_v1=0, loss_v2=0, nll_loss=2.716, ntokens=5882.8, nsentences=84, sample_size=5882.8, sample_size_v1=0, sample_size_v2=0, ppl=6.57, wps=3351.6, ups=0.57, wpb=5882.8, bsz=84, num_updates=3830, lr=1.64809e-05, gnorm=5.126, clip=100, loss_scale=16, train_wall=18, gb_free=12.1, wall=6775
2023-06-29 20:53:12 - progress_bar.py[line:272] - INFO: epoch 006:    547 / 660 loss=3.748, loss_v1=0, loss_v2=0, nll_loss=2.728, ntokens=6203.5, nsentences=84, sample_size=6203.5, sample_size_v1=0, sample_size_v2=0, ppl=6.63, wps=3578.3, ups=0.58, wpb=6203.5, bsz=84, num_updates=3840, lr=1.64406e-05, gnorm=4.913, clip=100, loss_scale=16, train_wall=17, gb_free=12.1, wall=6792
2023-06-29 20:53:29 - progress_bar.py[line:272] - INFO: epoch 006:    557 / 660 loss=3.718, loss_v1=0, loss_v2=0, nll_loss=2.695, ntokens=5943.4, nsentences=84, sample_size=5943.4, sample_size_v1=0, sample_size_v2=0, ppl=6.48, wps=3422.6, ups=0.58, wpb=5943.4, bsz=84, num_updates=3850, lr=1.64003e-05, gnorm=5.092, clip=100, loss_scale=32, train_wall=17, gb_free=12.3, wall=6809
2023-06-29 20:53:46 - progress_bar.py[line:272] - INFO: epoch 006:    567 / 660 loss=3.699, loss_v1=0, loss_v2=0, nll_loss=2.674, ntokens=5742.8, nsentences=84, sample_size=5742.8, sample_size_v1=0, sample_size_v2=0, ppl=6.38, wps=3305.7, ups=0.58, wpb=5742.8, bsz=84, num_updates=3860, lr=1.636e-05, gnorm=4.995, clip=100, loss_scale=32, train_wall=17, gb_free=12.4, wall=6827
2023-06-29 20:54:04 - progress_bar.py[line:272] - INFO: epoch 006:    577 / 660 loss=3.711, loss_v1=0, loss_v2=0, nll_loss=2.688, ntokens=5714.9, nsentences=84, sample_size=5714.9, sample_size_v1=0, sample_size_v2=0, ppl=6.44, wps=3304.3, ups=0.58, wpb=5714.9, bsz=84, num_updates=3870, lr=1.63197e-05, gnorm=5.966, clip=100, loss_scale=32, train_wall=17, gb_free=12.1, wall=6844
2023-06-29 20:54:21 - progress_bar.py[line:272] - INFO: epoch 006:    587 / 660 loss=3.719, loss_v1=0, loss_v2=0, nll_loss=2.696, ntokens=5978.7, nsentences=84, sample_size=5978.7, sample_size_v1=0, sample_size_v2=0, ppl=6.48, wps=3452.9, ups=0.58, wpb=5978.7, bsz=84, num_updates=3880, lr=1.62794e-05, gnorm=5.352, clip=100, loss_scale=32, train_wall=17, gb_free=12.1, wall=6861
2023-06-29 20:54:38 - progress_bar.py[line:272] - INFO: epoch 006:    597 / 660 loss=3.704, loss_v1=0, loss_v2=0, nll_loss=2.679, ntokens=5924.9, nsentences=84, sample_size=5924.9, sample_size_v1=0, sample_size_v2=0, ppl=6.4, wps=3409.9, ups=0.58, wpb=5924.9, bsz=84, num_updates=3890, lr=1.62391e-05, gnorm=5.074, clip=100, loss_scale=32, train_wall=17, gb_free=12.1, wall=6879
2023-06-29 20:54:56 - progress_bar.py[line:272] - INFO: epoch 006:    607 / 660 loss=3.703, loss_v1=0, loss_v2=0, nll_loss=2.678, ntokens=5906.4, nsentences=84, sample_size=5906.4, sample_size_v1=0, sample_size_v2=0, ppl=6.4, wps=3409.6, ups=0.58, wpb=5906.4, bsz=84, num_updates=3900, lr=1.61988e-05, gnorm=5.5, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=6896
2023-06-29 20:55:13 - progress_bar.py[line:272] - INFO: epoch 006:    617 / 660 loss=3.714, loss_v1=0, loss_v2=0, nll_loss=2.69, ntokens=5998.1, nsentences=84, sample_size=5998.1, sample_size_v1=0, sample_size_v2=0, ppl=6.45, wps=3438.8, ups=0.57, wpb=5998.1, bsz=84, num_updates=3910, lr=1.61585e-05, gnorm=5.012, clip=100, loss_scale=32, train_wall=17, gb_free=12.3, wall=6913
2023-06-29 20:55:31 - progress_bar.py[line:272] - INFO: epoch 006:    627 / 660 loss=3.745, loss_v1=0, loss_v2=0, nll_loss=2.724, ntokens=6154.9, nsentences=84, sample_size=6154.9, sample_size_v1=0, sample_size_v2=0, ppl=6.61, wps=3500.9, ups=0.57, wpb=6154.9, bsz=84, num_updates=3920, lr=1.61182e-05, gnorm=5.147, clip=100, loss_scale=32, train_wall=18, gb_free=11.9, wall=6931
2023-06-29 20:55:48 - progress_bar.py[line:272] - INFO: epoch 006:    637 / 660 loss=3.705, loss_v1=0, loss_v2=0, nll_loss=2.681, ntokens=5852.5, nsentences=84, sample_size=5852.5, sample_size_v1=0, sample_size_v2=0, ppl=6.41, wps=3364.7, ups=0.57, wpb=5852.5, bsz=84, num_updates=3930, lr=1.60779e-05, gnorm=5.196, clip=100, loss_scale=32, train_wall=17, gb_free=12.4, wall=6948
2023-06-29 20:56:06 - progress_bar.py[line:272] - INFO: epoch 006:    647 / 660 loss=3.736, loss_v1=0, loss_v2=0, nll_loss=2.714, ntokens=6051.4, nsentences=84, sample_size=6051.4, sample_size_v1=0, sample_size_v2=0, ppl=6.56, wps=3465.5, ups=0.57, wpb=6051.4, bsz=84, num_updates=3940, lr=1.60376e-05, gnorm=5.343, clip=100, loss_scale=32, train_wall=17, gb_free=11.8, wall=6966
2023-06-29 20:56:23 - progress_bar.py[line:272] - INFO: epoch 006:    657 / 660 loss=3.708, loss_v1=0, loss_v2=0, nll_loss=2.685, ntokens=5836.3, nsentences=84, sample_size=5836.3, sample_size_v1=0, sample_size_v2=0, ppl=6.43, wps=3359.4, ups=0.58, wpb=5836.3, bsz=84, num_updates=3950, lr=1.59973e-05, gnorm=5.54, clip=100, loss_scale=32, train_wall=17, gb_free=12, wall=6983
2023-06-29 20:56:28 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 6 @ 3953 updates
2023-06-29 20:56:28 - trainer.py[line:431] - INFO: Saving checkpoint to ../../checkpoints/OFA/sgcls_checkpoints/_12_3e-5_512_base_tgtobj/checkpoint6.pt
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 row count 18467 total row count 55400
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 row count 18466 total row count 55400
slice_id 1 seek offset 18467
slice_id 2 seek offset 36934
2023-06-29 20:56:31 - trainer.py[line:441] - INFO: Finished saving checkpoint to ../../checkpoints/OFA/sgcls_checkpoints/_12_3e-5_512_base_tgtobj/checkpoint6.pt
2023-06-29 20:56:33 - checkpoint_utils.py[line:133] - INFO: Saved checkpoint ../../checkpoints/OFA/sgcls_checkpoints/_12_3e-5_512_base_tgtobj/checkpoint6.pt (epoch 6 @ 3953 updates, score None) (writing took 5.199804552830756 seconds)
2023-06-29 20:56:33 - train.py[line:332] - INFO: end of epoch 6 (average epoch stats below)
2023-06-29 20:56:33 - progress_bar.py[line:282] - INFO: epoch 006 | loss 3.777 | loss_v1 0 | loss_v2 0 | nll_loss 2.762 | ntokens 5900.5 | nsentences 83.907 | sample_size 5900.5 | sample_size_v1 0 | sample_size_v2 0 | ppl 6.78 | wps 3351.9 | ups 0.57 | wpb 5900.5 | bsz 83.9 | num_updates 3953 | lr 1.59852e-05 | gnorm 5.106 | clip 100 | loss_scale 32 | train_wall 1150 | gb_free 12.3 | wall 6993
2023-06-29 20:56:33 - trainer.py[line:639] - INFO: loading train data for epoch 7
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 row count 18467 total row count 55400
slice_id 0 seek offset 0
2023-06-29 20:56:35 - trainer.py[line:703] - INFO: begin training epoch 7
2023-06-29 20:56:35 - train.py[line:305] - INFO: Start iterating over samples
2023-06-29 20:56:48 - progress_bar.py[line:272] - INFO: epoch 007:      7 / 660 loss=3.741, loss_v1=0, loss_v2=0, nll_loss=2.719, ntokens=5767.6, nsentences=81.9, sample_size=5767.6, sample_size_v1=0, sample_size_v2=0, ppl=6.58, wps=2355.8, ups=0.41, wpb=5767.6, bsz=81.9, num_updates=3960, lr=1.5957e-05, gnorm=5.474, clip=100, loss_scale=32, train_wall=17, gb_free=12.1, wall=7008
2023-06-29 20:57:05 - progress_bar.py[line:272] - INFO: epoch 007:     17 / 660 loss=3.741, loss_v1=0, loss_v2=0, nll_loss=2.719, ntokens=5770.5, nsentences=84, sample_size=5770.5, sample_size_v1=0, sample_size_v2=0, ppl=6.59, wps=3265.2, ups=0.57, wpb=5770.5, bsz=84, num_updates=3970, lr=1.59167e-05, gnorm=5.446, clip=100, loss_scale=32, train_wall=18, gb_free=11.8, wall=7025
2023-06-29 20:57:23 - progress_bar.py[line:272] - INFO: epoch 007:     27 / 660 loss=3.687, loss_v1=0, loss_v2=0, nll_loss=2.661, ntokens=5666.9, nsentences=84, sample_size=5666.9, sample_size_v1=0, sample_size_v2=0, ppl=6.32, wps=3266.7, ups=0.58, wpb=5666.9, bsz=84, num_updates=3980, lr=1.58764e-05, gnorm=5.632, clip=100, loss_scale=32, train_wall=17, gb_free=12.5, wall=7043
2023-06-29 20:57:40 - progress_bar.py[line:272] - INFO: epoch 007:     37 / 660 loss=3.66, loss_v1=0, loss_v2=0, nll_loss=2.63, ntokens=5478.7, nsentences=84, sample_size=5478.7, sample_size_v1=0, sample_size_v2=0, ppl=6.19, wps=3157.4, ups=0.58, wpb=5478.7, bsz=84, num_updates=3990, lr=1.58361e-05, gnorm=5.208, clip=100, loss_scale=32, train_wall=17, gb_free=11.7, wall=7060
2023-06-29 20:57:50 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-06-29 20:57:59 - progress_bar.py[line:272] - INFO: epoch 007:     48 / 660 loss=3.637, loss_v1=0, loss_v2=0, nll_loss=2.603, ntokens=5731.7, nsentences=84, sample_size=5731.7, sample_size_v1=0, sample_size_v2=0, ppl=6.08, wps=2973.4, ups=0.52, wpb=5731.7, bsz=84, num_updates=4000, lr=1.57958e-05, gnorm=5.285, clip=100, loss_scale=16, train_wall=19, gb_free=12.4, wall=7079
2023-06-29 20:58:17 - progress_bar.py[line:272] - INFO: epoch 007:     58 / 660 loss=3.688, loss_v1=0, loss_v2=0, nll_loss=2.661, ntokens=5824.8, nsentences=84, sample_size=5824.8, sample_size_v1=0, sample_size_v2=0, ppl=6.33, wps=3333.9, ups=0.57, wpb=5824.8, bsz=84, num_updates=4010, lr=1.57555e-05, gnorm=5, clip=100, loss_scale=16, train_wall=17, gb_free=12.3, wall=7097
2023-06-29 20:58:34 - progress_bar.py[line:272] - INFO: epoch 007:     68 / 660 loss=3.614, loss_v1=0, loss_v2=0, nll_loss=2.579, ntokens=5589.2, nsentences=84, sample_size=5589.2, sample_size_v1=0, sample_size_v2=0, ppl=5.97, wps=3210.2, ups=0.57, wpb=5589.2, bsz=84, num_updates=4020, lr=1.57152e-05, gnorm=5.374, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=7114
2023-06-29 20:58:52 - progress_bar.py[line:272] - INFO: epoch 007:     78 / 660 loss=3.682, loss_v1=0, loss_v2=0, nll_loss=2.653, ntokens=6404.9, nsentences=84, sample_size=6404.9, sample_size_v1=0, sample_size_v2=0, ppl=6.29, wps=3581.6, ups=0.56, wpb=6404.9, bsz=84, num_updates=4030, lr=1.56749e-05, gnorm=5.291, clip=100, loss_scale=16, train_wall=18, gb_free=11.6, wall=7132
2023-06-29 20:59:10 - progress_bar.py[line:272] - INFO: epoch 007:     88 / 660 loss=3.66, loss_v1=0, loss_v2=0, nll_loss=2.63, ntokens=5780.2, nsentences=84, sample_size=5780.2, sample_size_v1=0, sample_size_v2=0, ppl=6.19, wps=3266.5, ups=0.57, wpb=5780.2, bsz=84, num_updates=4040, lr=1.56347e-05, gnorm=5.328, clip=100, loss_scale=16, train_wall=18, gb_free=12, wall=7150
2023-06-29 20:59:27 - progress_bar.py[line:272] - INFO: epoch 007:     98 / 660 loss=3.662, loss_v1=0, loss_v2=0, nll_loss=2.633, ntokens=5902.7, nsentences=84, sample_size=5902.7, sample_size_v1=0, sample_size_v2=0, ppl=6.2, wps=3358, ups=0.57, wpb=5902.7, bsz=84, num_updates=4050, lr=1.55944e-05, gnorm=5.521, clip=100, loss_scale=16, train_wall=18, gb_free=11.6, wall=7167
2023-06-29 20:59:45 - progress_bar.py[line:272] - INFO: epoch 007:    108 / 660 loss=3.63, loss_v1=0, loss_v2=0, nll_loss=2.596, ntokens=5823.8, nsentences=84, sample_size=5823.8, sample_size_v1=0, sample_size_v2=0, ppl=6.04, wps=3334, ups=0.57, wpb=5823.8, bsz=84, num_updates=4060, lr=1.55541e-05, gnorm=5.642, clip=100, loss_scale=16, train_wall=17, gb_free=12.4, wall=7185
2023-06-29 21:00:02 - progress_bar.py[line:272] - INFO: epoch 007:    118 / 660 loss=3.687, loss_v1=0, loss_v2=0, nll_loss=2.661, ntokens=5695.9, nsentences=84, sample_size=5695.9, sample_size_v1=0, sample_size_v2=0, ppl=6.32, wps=3282.8, ups=0.58, wpb=5695.9, bsz=84, num_updates=4070, lr=1.55138e-05, gnorm=5.121, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=7202
2023-06-29 21:00:20 - progress_bar.py[line:272] - INFO: epoch 007:    128 / 660 loss=3.718, loss_v1=0, loss_v2=0, nll_loss=2.695, ntokens=5686.4, nsentences=84, sample_size=5686.4, sample_size_v1=0, sample_size_v2=0, ppl=6.48, wps=3258.6, ups=0.57, wpb=5686.4, bsz=84, num_updates=4080, lr=1.54735e-05, gnorm=5.037, clip=100, loss_scale=16, train_wall=17, gb_free=11.4, wall=7220
2023-06-29 21:00:37 - progress_bar.py[line:272] - INFO: epoch 007:    138 / 660 loss=3.725, loss_v1=0, loss_v2=0, nll_loss=2.701, ntokens=5935.4, nsentences=84, sample_size=5935.4, sample_size_v1=0, sample_size_v2=0, ppl=6.5, wps=3370.6, ups=0.57, wpb=5935.4, bsz=84, num_updates=4090, lr=1.54332e-05, gnorm=5.214, clip=100, loss_scale=16, train_wall=18, gb_free=11.8, wall=7237
2023-06-29 21:00:55 - progress_bar.py[line:272] - INFO: epoch 007:    148 / 660 loss=3.735, loss_v1=0, loss_v2=0, nll_loss=2.712, ntokens=5865.5, nsentences=84, sample_size=5865.5, sample_size_v1=0, sample_size_v2=0, ppl=6.55, wps=3329.5, ups=0.57, wpb=5865.5, bsz=84, num_updates=4100, lr=1.53929e-05, gnorm=5.138, clip=100, loss_scale=16, train_wall=18, gb_free=11.2, wall=7255
2023-06-29 21:01:12 - progress_bar.py[line:272] - INFO: epoch 007:    158 / 660 loss=3.72, loss_v1=0, loss_v2=0, nll_loss=2.698, ntokens=6080.3, nsentences=84, sample_size=6080.3, sample_size_v1=0, sample_size_v2=0, ppl=6.49, wps=3449.5, ups=0.57, wpb=6080.3, bsz=84, num_updates=4110, lr=1.53526e-05, gnorm=4.863, clip=100, loss_scale=16, train_wall=18, gb_free=11.7, wall=7273
2023-06-29 21:01:30 - progress_bar.py[line:272] - INFO: epoch 007:    168 / 660 loss=3.697, loss_v1=0, loss_v2=0, nll_loss=2.672, ntokens=5933.8, nsentences=84, sample_size=5933.8, sample_size_v1=0, sample_size_v2=0, ppl=6.37, wps=3359.9, ups=0.57, wpb=5933.8, bsz=84, num_updates=4120, lr=1.53123e-05, gnorm=4.948, clip=100, loss_scale=16, train_wall=18, gb_free=11.8, wall=7290
2023-06-29 21:01:48 - progress_bar.py[line:272] - INFO: epoch 007:    178 / 660 loss=3.761, loss_v1=0, loss_v2=0, nll_loss=2.743, ntokens=5974.4, nsentences=84, sample_size=5974.4, sample_size_v1=0, sample_size_v2=0, ppl=6.69, wps=3382.7, ups=0.57, wpb=5974.4, bsz=84, num_updates=4130, lr=1.5272e-05, gnorm=4.774, clip=100, loss_scale=16, train_wall=18, gb_free=11.7, wall=7308
2023-06-29 21:02:05 - progress_bar.py[line:272] - INFO: epoch 007:    188 / 660 loss=3.687, loss_v1=0, loss_v2=0, nll_loss=2.659, ntokens=5912.1, nsentences=84, sample_size=5912.1, sample_size_v1=0, sample_size_v2=0, ppl=6.31, wps=3380.7, ups=0.57, wpb=5912.1, bsz=84, num_updates=4140, lr=1.52317e-05, gnorm=5.364, clip=100, loss_scale=16, train_wall=17, gb_free=11.5, wall=7325
2023-06-29 21:02:23 - progress_bar.py[line:272] - INFO: epoch 007:    198 / 660 loss=3.667, loss_v1=0, loss_v2=0, nll_loss=2.638, ntokens=5602, nsentences=84, sample_size=5602, sample_size_v1=0, sample_size_v2=0, ppl=6.23, wps=3187.9, ups=0.57, wpb=5602, bsz=84, num_updates=4150, lr=1.51914e-05, gnorm=5.039, clip=100, loss_scale=16, train_wall=18, gb_free=11.9, wall=7343
2023-06-29 21:02:40 - progress_bar.py[line:272] - INFO: epoch 007:    208 / 660 loss=3.716, loss_v1=0, loss_v2=0, nll_loss=2.692, ntokens=6166.3, nsentences=84, sample_size=6166.3, sample_size_v1=0, sample_size_v2=0, ppl=6.46, wps=3491.8, ups=0.57, wpb=6166.3, bsz=84, num_updates=4160, lr=1.51511e-05, gnorm=5.053, clip=100, loss_scale=16, train_wall=18, gb_free=11.7, wall=7361
2023-06-29 21:02:58 - progress_bar.py[line:272] - INFO: epoch 007:    218 / 660 loss=3.711, loss_v1=0, loss_v2=0, nll_loss=2.687, ntokens=5998.2, nsentences=84, sample_size=5998.2, sample_size_v1=0, sample_size_v2=0, ppl=6.44, wps=3402.8, ups=0.57, wpb=5998.2, bsz=84, num_updates=4170, lr=1.51108e-05, gnorm=4.988, clip=100, loss_scale=16, train_wall=18, gb_free=11.8, wall=7378
2023-06-29 21:03:16 - progress_bar.py[line:272] - INFO: epoch 007:    228 / 660 loss=3.714, loss_v1=0, loss_v2=0, nll_loss=2.689, ntokens=6084.6, nsentences=84, sample_size=6084.6, sample_size_v1=0, sample_size_v2=0, ppl=6.45, wps=3467.3, ups=0.57, wpb=6084.6, bsz=84, num_updates=4180, lr=1.50705e-05, gnorm=4.913, clip=100, loss_scale=16, train_wall=18, gb_free=12, wall=7396
2023-06-29 21:03:33 - progress_bar.py[line:272] - INFO: epoch 007:    238 / 660 loss=3.663, loss_v1=0, loss_v2=0, nll_loss=2.634, ntokens=5793.1, nsentences=84, sample_size=5793.1, sample_size_v1=0, sample_size_v2=0, ppl=6.21, wps=3293.5, ups=0.57, wpb=5793.1, bsz=84, num_updates=4190, lr=1.50302e-05, gnorm=5.149, clip=100, loss_scale=16, train_wall=18, gb_free=12, wall=7413
2023-06-29 21:03:51 - progress_bar.py[line:272] - INFO: epoch 007:    248 / 660 loss=3.674, loss_v1=0, loss_v2=0, nll_loss=2.645, ntokens=6206.8, nsentences=84, sample_size=6206.8, sample_size_v1=0, sample_size_v2=0, ppl=6.26, wps=3553.8, ups=0.57, wpb=6206.8, bsz=84, num_updates=4200, lr=1.49899e-05, gnorm=5.219, clip=100, loss_scale=16, train_wall=17, gb_free=11.7, wall=7431
2023-06-29 21:04:08 - progress_bar.py[line:272] - INFO: epoch 007:    258 / 660 loss=3.654, loss_v1=0, loss_v2=0, nll_loss=2.622, ntokens=5908.3, nsentences=84, sample_size=5908.3, sample_size_v1=0, sample_size_v2=0, ppl=6.16, wps=3383.9, ups=0.57, wpb=5908.3, bsz=84, num_updates=4210, lr=1.49496e-05, gnorm=5.461, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=7448
2023-06-29 21:04:26 - progress_bar.py[line:272] - INFO: epoch 007:    268 / 660 loss=3.636, loss_v1=0, loss_v2=0, nll_loss=2.603, ntokens=5817.6, nsentences=84, sample_size=5817.6, sample_size_v1=0, sample_size_v2=0, ppl=6.08, wps=3335.5, ups=0.57, wpb=5817.6, bsz=84, num_updates=4220, lr=1.49093e-05, gnorm=4.819, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=7466
2023-06-29 21:04:43 - progress_bar.py[line:272] - INFO: epoch 007:    278 / 660 loss=3.666, loss_v1=0, loss_v2=0, nll_loss=2.636, ntokens=5883.3, nsentences=84, sample_size=5883.3, sample_size_v1=0, sample_size_v2=0, ppl=6.21, wps=3349.7, ups=0.57, wpb=5883.3, bsz=84, num_updates=4230, lr=1.4869e-05, gnorm=5.084, clip=100, loss_scale=16, train_wall=18, gb_free=12, wall=7483
2023-06-29 21:05:01 - progress_bar.py[line:272] - INFO: epoch 007:    288 / 660 loss=3.675, loss_v1=0, loss_v2=0, nll_loss=2.647, ntokens=6030.7, nsentences=84, sample_size=6030.7, sample_size_v1=0, sample_size_v2=0, ppl=6.26, wps=3412.8, ups=0.57, wpb=6030.7, bsz=84, num_updates=4240, lr=1.48287e-05, gnorm=5.069, clip=100, loss_scale=16, train_wall=18, gb_free=12.2, wall=7501
2023-06-29 21:05:18 - progress_bar.py[line:272] - INFO: epoch 007:    298 / 660 loss=3.683, loss_v1=0, loss_v2=0, nll_loss=2.653, ntokens=6184.7, nsentences=84, sample_size=6184.7, sample_size_v1=0, sample_size_v2=0, ppl=6.29, wps=3544.7, ups=0.57, wpb=6184.7, bsz=84, num_updates=4250, lr=1.47884e-05, gnorm=4.993, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=7518
2023-06-29 21:05:36 - progress_bar.py[line:272] - INFO: epoch 007:    308 / 660 loss=3.65, loss_v1=0, loss_v2=0, nll_loss=2.619, ntokens=6277.5, nsentences=84, sample_size=6277.5, sample_size_v1=0, sample_size_v2=0, ppl=6.14, wps=3595.7, ups=0.57, wpb=6277.5, bsz=84, num_updates=4260, lr=1.47482e-05, gnorm=5.311, clip=100, loss_scale=16, train_wall=17, gb_free=11.8, wall=7536
2023-06-29 21:05:53 - progress_bar.py[line:272] - INFO: epoch 007:    318 / 660 loss=3.66, loss_v1=0, loss_v2=0, nll_loss=2.631, ntokens=6169.9, nsentences=84, sample_size=6169.9, sample_size_v1=0, sample_size_v2=0, ppl=6.19, wps=3517.5, ups=0.57, wpb=6169.9, bsz=84, num_updates=4270, lr=1.47079e-05, gnorm=5.203, clip=100, loss_scale=16, train_wall=18, gb_free=11.6, wall=7553
2023-06-29 21:06:11 - progress_bar.py[line:272] - INFO: epoch 007:    328 / 660 loss=3.675, loss_v1=0, loss_v2=0, nll_loss=2.646, ntokens=6105.2, nsentences=84, sample_size=6105.2, sample_size_v1=0, sample_size_v2=0, ppl=6.26, wps=3491.5, ups=0.57, wpb=6105.2, bsz=84, num_updates=4280, lr=1.46676e-05, gnorm=5.156, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=7571
2023-06-29 21:06:28 - progress_bar.py[line:272] - INFO: epoch 007:    338 / 660 loss=3.656, loss_v1=0, loss_v2=0, nll_loss=2.626, ntokens=5966.7, nsentences=84, sample_size=5966.7, sample_size_v1=0, sample_size_v2=0, ppl=6.17, wps=3418.6, ups=0.57, wpb=5966.7, bsz=84, num_updates=4290, lr=1.46273e-05, gnorm=4.965, clip=100, loss_scale=16, train_wall=17, gb_free=11.8, wall=7588
2023-06-29 21:06:46 - progress_bar.py[line:272] - INFO: epoch 007:    348 / 660 loss=3.648, loss_v1=0, loss_v2=0, nll_loss=2.616, ntokens=6062.2, nsentences=84, sample_size=6062.2, sample_size_v1=0, sample_size_v2=0, ppl=6.13, wps=3468.3, ups=0.57, wpb=6062.2, bsz=84, num_updates=4300, lr=1.4587e-05, gnorm=4.973, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=7606
2023-06-29 21:07:03 - progress_bar.py[line:272] - INFO: epoch 007:    358 / 660 loss=3.629, loss_v1=0, loss_v2=0, nll_loss=2.594, ntokens=5994.2, nsentences=84, sample_size=5994.2, sample_size_v1=0, sample_size_v2=0, ppl=6.04, wps=3444.1, ups=0.57, wpb=5994.2, bsz=84, num_updates=4310, lr=1.45467e-05, gnorm=5.275, clip=100, loss_scale=16, train_wall=17, gb_free=12.2, wall=7623
2023-06-29 21:07:20 - progress_bar.py[line:272] - INFO: epoch 007:    368 / 660 loss=3.624, loss_v1=0, loss_v2=0, nll_loss=2.59, ntokens=5777.1, nsentences=84, sample_size=5777.1, sample_size_v1=0, sample_size_v2=0, ppl=6.02, wps=3333.4, ups=0.58, wpb=5777.1, bsz=84, num_updates=4320, lr=1.45064e-05, gnorm=5.375, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=7641
2023-06-29 21:07:38 - progress_bar.py[line:272] - INFO: epoch 007:    378 / 660 loss=3.624, loss_v1=0, loss_v2=0, nll_loss=2.589, ntokens=5923.3, nsentences=84, sample_size=5923.3, sample_size_v1=0, sample_size_v2=0, ppl=6.02, wps=3414.3, ups=0.58, wpb=5923.3, bsz=84, num_updates=4330, lr=1.44661e-05, gnorm=5.01, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=7658
2023-06-29 21:07:55 - progress_bar.py[line:272] - INFO: epoch 007:    388 / 660 loss=3.619, loss_v1=0, loss_v2=0, nll_loss=2.584, ntokens=5667.5, nsentences=84, sample_size=5667.5, sample_size_v1=0, sample_size_v2=0, ppl=6, wps=3274.9, ups=0.58, wpb=5667.5, bsz=84, num_updates=4340, lr=1.44258e-05, gnorm=4.855, clip=100, loss_scale=16, train_wall=17, gb_free=11.8, wall=7675
2023-06-29 21:08:12 - progress_bar.py[line:272] - INFO: epoch 007:    398 / 660 loss=3.61, loss_v1=0, loss_v2=0, nll_loss=2.574, ntokens=5729.4, nsentences=84, sample_size=5729.4, sample_size_v1=0, sample_size_v2=0, ppl=5.95, wps=3310.4, ups=0.58, wpb=5729.4, bsz=84, num_updates=4350, lr=1.43855e-05, gnorm=5.296, clip=100, loss_scale=16, train_wall=17, gb_free=12.3, wall=7693
2023-06-29 21:08:30 - progress_bar.py[line:272] - INFO: epoch 007:    408 / 660 loss=3.619, loss_v1=0, loss_v2=0, nll_loss=2.584, ntokens=5783.9, nsentences=84, sample_size=5783.9, sample_size_v1=0, sample_size_v2=0, ppl=6, wps=3345.2, ups=0.58, wpb=5783.9, bsz=84, num_updates=4360, lr=1.43452e-05, gnorm=5.268, clip=100, loss_scale=16, train_wall=17, gb_free=12.4, wall=7710
2023-06-29 21:08:47 - progress_bar.py[line:272] - INFO: epoch 007:    418 / 660 loss=3.639, loss_v1=0, loss_v2=0, nll_loss=2.606, ntokens=5771.6, nsentences=84, sample_size=5771.6, sample_size_v1=0, sample_size_v2=0, ppl=6.09, wps=3331.9, ups=0.58, wpb=5771.6, bsz=84, num_updates=4370, lr=1.43049e-05, gnorm=5.244, clip=100, loss_scale=16, train_wall=17, gb_free=12.1, wall=7727
2023-06-29 21:09:04 - progress_bar.py[line:272] - INFO: epoch 007:    428 / 660 loss=3.625, loss_v1=0, loss_v2=0, nll_loss=2.59, ntokens=5742.8, nsentences=84, sample_size=5742.8, sample_size_v1=0, sample_size_v2=0, ppl=6.02, wps=3314.5, ups=0.58, wpb=5742.8, bsz=84, num_updates=4380, lr=1.42646e-05, gnorm=5.171, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=7745
2023-06-29 21:09:22 - progress_bar.py[line:272] - INFO: epoch 007:    438 / 660 loss=3.652, loss_v1=0, loss_v2=0, nll_loss=2.621, ntokens=6023.5, nsentences=84, sample_size=6023.5, sample_size_v1=0, sample_size_v2=0, ppl=6.15, wps=3480, ups=0.58, wpb=6023.5, bsz=84, num_updates=4390, lr=1.42243e-05, gnorm=5.136, clip=100, loss_scale=16, train_wall=17, gb_free=12.1, wall=7762
2023-06-29 21:09:39 - progress_bar.py[line:272] - INFO: epoch 007:    448 / 660 loss=3.642, loss_v1=0, loss_v2=0, nll_loss=2.609, ntokens=5858.1, nsentences=84, sample_size=5858.1, sample_size_v1=0, sample_size_v2=0, ppl=6.1, wps=3384.2, ups=0.58, wpb=5858.1, bsz=84, num_updates=4400, lr=1.4184e-05, gnorm=5.444, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=7779
2023-06-29 21:09:57 - progress_bar.py[line:272] - INFO: epoch 007:    458 / 660 loss=3.612, loss_v1=0, loss_v2=0, nll_loss=2.577, ntokens=5805, nsentences=84, sample_size=5805, sample_size_v1=0, sample_size_v2=0, ppl=5.97, wps=3315.6, ups=0.57, wpb=5805, bsz=84, num_updates=4410, lr=1.41437e-05, gnorm=4.914, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=7797
2023-06-29 21:10:14 - progress_bar.py[line:272] - INFO: epoch 007:    468 / 660 loss=3.662, loss_v1=0, loss_v2=0, nll_loss=2.631, ntokens=6086.3, nsentences=84, sample_size=6086.3, sample_size_v1=0, sample_size_v2=0, ppl=6.2, wps=3498.2, ups=0.57, wpb=6086.3, bsz=84, num_updates=4420, lr=1.41034e-05, gnorm=4.997, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=7814
2023-06-29 21:10:31 - progress_bar.py[line:272] - INFO: epoch 007:    478 / 660 loss=3.64, loss_v1=0, loss_v2=0, nll_loss=2.607, ntokens=5881.4, nsentences=84, sample_size=5881.4, sample_size_v1=0, sample_size_v2=0, ppl=6.09, wps=3397.2, ups=0.58, wpb=5881.4, bsz=84, num_updates=4430, lr=1.40631e-05, gnorm=4.977, clip=100, loss_scale=16, train_wall=17, gb_free=11.7, wall=7831
2023-06-29 21:10:49 - progress_bar.py[line:272] - INFO: epoch 007:    488 / 660 loss=3.585, loss_v1=0, loss_v2=0, nll_loss=2.545, ntokens=5556.5, nsentences=84, sample_size=5556.5, sample_size_v1=0, sample_size_v2=0, ppl=5.84, wps=3212.2, ups=0.58, wpb=5556.5, bsz=84, num_updates=4440, lr=1.40228e-05, gnorm=4.999, clip=100, loss_scale=16, train_wall=17, gb_free=11.8, wall=7849
2023-06-29 21:11:06 - progress_bar.py[line:272] - INFO: epoch 007:    498 / 660 loss=3.638, loss_v1=0, loss_v2=0, nll_loss=2.604, ntokens=5836.9, nsentences=84, sample_size=5836.9, sample_size_v1=0, sample_size_v2=0, ppl=6.08, wps=3355.9, ups=0.57, wpb=5836.9, bsz=84, num_updates=4450, lr=1.39825e-05, gnorm=5.255, clip=100, loss_scale=16, train_wall=17, gb_free=11.8, wall=7866
2023-06-29 21:11:23 - progress_bar.py[line:272] - INFO: epoch 007:    508 / 660 loss=3.57, loss_v1=0, loss_v2=0, nll_loss=2.577, ntokens=5732.6, nsentences=82.8, sample_size=5732.6, sample_size_v1=0, sample_size_v2=0, ppl=5.97, wps=3316.9, ups=0.58, wpb=5732.6, bsz=82.8, num_updates=4460, lr=1.39422e-05, gnorm=5.276, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=7883
2023-06-29 21:11:41 - progress_bar.py[line:272] - INFO: epoch 007:    518 / 660 loss=3.614, loss_v1=0, loss_v2=0, nll_loss=2.578, ntokens=5925, nsentences=84, sample_size=5925, sample_size_v1=0, sample_size_v2=0, ppl=5.97, wps=3424.9, ups=0.58, wpb=5925, bsz=84, num_updates=4470, lr=1.39019e-05, gnorm=5.226, clip=100, loss_scale=16, train_wall=17, gb_free=12.2, wall=7901
2023-06-29 21:11:58 - progress_bar.py[line:272] - INFO: epoch 007:    528 / 660 loss=3.651, loss_v1=0, loss_v2=0, nll_loss=2.618, ntokens=6214.3, nsentences=84, sample_size=6214.3, sample_size_v1=0, sample_size_v2=0, ppl=6.14, wps=3563.3, ups=0.57, wpb=6214.3, bsz=84, num_updates=4480, lr=1.38617e-05, gnorm=5.083, clip=100, loss_scale=16, train_wall=17, gb_free=11.8, wall=7918
2023-06-29 21:12:15 - progress_bar.py[line:272] - INFO: epoch 007:    538 / 660 loss=3.612, loss_v1=0, loss_v2=0, nll_loss=2.576, ntokens=5820.8, nsentences=84, sample_size=5820.8, sample_size_v1=0, sample_size_v2=0, ppl=5.96, wps=3345.1, ups=0.57, wpb=5820.8, bsz=84, num_updates=4490, lr=1.38214e-05, gnorm=5.48, clip=100, loss_scale=16, train_wall=17, gb_free=12.1, wall=7936
2023-06-29 21:12:33 - progress_bar.py[line:272] - INFO: epoch 007:    548 / 660 loss=3.646, loss_v1=0, loss_v2=0, nll_loss=2.612, ntokens=6257.5, nsentences=84, sample_size=6257.5, sample_size_v1=0, sample_size_v2=0, ppl=6.11, wps=3613.1, ups=0.58, wpb=6257.5, bsz=84, num_updates=4500, lr=1.37811e-05, gnorm=5.032, clip=100, loss_scale=16, train_wall=17, gb_free=12.2, wall=7953
2023-06-29 21:12:50 - progress_bar.py[line:272] - INFO: epoch 007:    558 / 660 loss=3.595, loss_v1=0, loss_v2=0, nll_loss=2.558, ntokens=5963.4, nsentences=84, sample_size=5963.4, sample_size_v1=0, sample_size_v2=0, ppl=5.89, wps=3429.2, ups=0.58, wpb=5963.4, bsz=84, num_updates=4510, lr=1.37408e-05, gnorm=5.086, clip=100, loss_scale=32, train_wall=17, gb_free=12.8, wall=7970
2023-06-29 21:13:07 - progress_bar.py[line:272] - INFO: epoch 007:    568 / 660 loss=3.58, loss_v1=0, loss_v2=0, nll_loss=2.54, ntokens=5646.6, nsentences=84, sample_size=5646.6, sample_size_v1=0, sample_size_v2=0, ppl=5.82, wps=3260.3, ups=0.58, wpb=5646.6, bsz=84, num_updates=4520, lr=1.37005e-05, gnorm=5.168, clip=100, loss_scale=32, train_wall=17, gb_free=11.7, wall=7988
2023-06-29 21:13:25 - progress_bar.py[line:272] - INFO: epoch 007:    578 / 660 loss=3.595, loss_v1=0, loss_v2=0, nll_loss=2.556, ntokens=5804.7, nsentences=84, sample_size=5804.7, sample_size_v1=0, sample_size_v2=0, ppl=5.88, wps=3355.3, ups=0.58, wpb=5804.7, bsz=84, num_updates=4530, lr=1.36602e-05, gnorm=5.304, clip=100, loss_scale=32, train_wall=17, gb_free=12, wall=8005
2023-06-29 21:13:42 - progress_bar.py[line:272] - INFO: epoch 007:    588 / 660 loss=3.602, loss_v1=0, loss_v2=0, nll_loss=2.564, ntokens=5930.8, nsentences=84, sample_size=5930.8, sample_size_v1=0, sample_size_v2=0, ppl=5.91, wps=3426.1, ups=0.58, wpb=5930.8, bsz=84, num_updates=4540, lr=1.36199e-05, gnorm=5.115, clip=100, loss_scale=32, train_wall=17, gb_free=12.3, wall=8022
2023-06-29 21:13:59 - progress_bar.py[line:272] - INFO: epoch 007:    598 / 660 loss=3.592, loss_v1=0, loss_v2=0, nll_loss=2.553, ntokens=5964.2, nsentences=84, sample_size=5964.2, sample_size_v1=0, sample_size_v2=0, ppl=5.87, wps=3429.9, ups=0.58, wpb=5964.2, bsz=84, num_updates=4550, lr=1.35796e-05, gnorm=5.076, clip=100, loss_scale=32, train_wall=17, gb_free=12.2, wall=8040
2023-06-29 21:14:17 - progress_bar.py[line:272] - INFO: epoch 007:    608 / 660 loss=3.59, loss_v1=0, loss_v2=0, nll_loss=2.55, ntokens=5891.2, nsentences=84, sample_size=5891.2, sample_size_v1=0, sample_size_v2=0, ppl=5.86, wps=3364.5, ups=0.57, wpb=5891.2, bsz=84, num_updates=4560, lr=1.35393e-05, gnorm=5.072, clip=100, loss_scale=32, train_wall=17, gb_free=12.1, wall=8057
2023-06-29 21:14:31 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-06-29 21:14:36 - progress_bar.py[line:272] - INFO: epoch 007:    619 / 660 loss=3.614, loss_v1=0, loss_v2=0, nll_loss=2.577, ntokens=6033.3, nsentences=84, sample_size=6033.3, sample_size_v1=0, sample_size_v2=0, ppl=5.97, wps=3155.3, ups=0.52, wpb=6033.3, bsz=84, num_updates=4570, lr=1.3499e-05, gnorm=4.954, clip=100, loss_scale=16, train_wall=19, gb_free=11.8, wall=8076
2023-06-29 21:14:54 - progress_bar.py[line:272] - INFO: epoch 007:    629 / 660 loss=3.618, loss_v1=0, loss_v2=0, nll_loss=2.581, ntokens=6046.8, nsentences=84, sample_size=6046.8, sample_size_v1=0, sample_size_v2=0, ppl=5.98, wps=3469.9, ups=0.57, wpb=6046.8, bsz=84, num_updates=4580, lr=1.34587e-05, gnorm=5.256, clip=100, loss_scale=16, train_wall=17, gb_free=11.7, wall=8094
2023-06-29 21:15:11 - progress_bar.py[line:272] - INFO: epoch 007:    639 / 660 loss=3.609, loss_v1=0, loss_v2=0, nll_loss=2.573, ntokens=5948.2, nsentences=84, sample_size=5948.2, sample_size_v1=0, sample_size_v2=0, ppl=5.95, wps=3412.8, ups=0.57, wpb=5948.2, bsz=84, num_updates=4590, lr=1.34184e-05, gnorm=4.954, clip=100, loss_scale=16, train_wall=17, gb_free=11.7, wall=8111
2023-06-29 21:15:28 - progress_bar.py[line:272] - INFO: epoch 007:    649 / 660 loss=3.615, loss_v1=0, loss_v2=0, nll_loss=2.577, ntokens=5954.6, nsentences=84, sample_size=5954.6, sample_size_v1=0, sample_size_v2=0, ppl=5.97, wps=3415.3, ups=0.57, wpb=5954.6, bsz=84, num_updates=4600, lr=1.33781e-05, gnorm=5.481, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=8129
2023-06-29 21:15:46 - progress_bar.py[line:272] - INFO: epoch 007:    659 / 660 loss=3.607, loss_v1=0, loss_v2=0, nll_loss=2.571, ntokens=5935.8, nsentences=84, sample_size=5935.8, sample_size_v1=0, sample_size_v2=0, ppl=5.94, wps=3417, ups=0.58, wpb=5935.8, bsz=84, num_updates=4610, lr=1.33378e-05, gnorm=5.752, clip=100, loss_scale=16, train_wall=17, gb_free=12.7, wall=8146
2023-06-29 21:15:47 - train.py[line:332] - INFO: end of epoch 7 (average epoch stats below)
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
2023-06-29 21:15:47 - progress_bar.py[line:282] - INFO: epoch 007 | loss 3.652 | loss_v1 0 | loss_v2 0 | nll_loss 2.621 | ntokens 5902.12 | nsentences 83.95 | sample_size 5902.12 | sample_size_v1 0 | sample_size_v2 0 | ppl 6.15 | wps 3365.2 | ups 0.57 | wpb 5902.1 | bsz 83.9 | num_updates 4611 | lr 1.33338e-05 | gnorm 5.177 | clip 100 | loss_scale 16 | train_wall 1149 | gb_free 12.3 | wall 8147
2023-06-29 21:15:47 - trainer.py[line:639] - INFO: loading train data for epoch 8
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 row count 18467 total row count 55400
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 row count 18466 total row count 55400
slice_id 1 seek offset 18467
slice_id 2 seek offset 36934
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 row count 18467 total row count 55400
slice_id 0 seek offset 0
2023-06-29 21:15:49 - trainer.py[line:703] - INFO: begin training epoch 8
2023-06-29 21:15:49 - train.py[line:305] - INFO: Start iterating over samples
2023-06-29 21:16:05 - progress_bar.py[line:272] - INFO: epoch 008:      9 / 660 loss=3.675, loss_v1=0, loss_v2=0, nll_loss=2.642, ntokens=5788.8, nsentences=81.9, sample_size=5788.8, sample_size_v1=0, sample_size_v2=0, ppl=6.24, wps=2980, ups=0.51, wpb=5788.8, bsz=81.9, num_updates=4620, lr=1.32975e-05, gnorm=5.658, clip=100, loss_scale=16, train_wall=17, gb_free=11.6, wall=8165
2023-06-29 21:16:23 - progress_bar.py[line:272] - INFO: epoch 008:     19 / 660 loss=3.649, loss_v1=0, loss_v2=0, nll_loss=2.615, ntokens=5789.4, nsentences=84, sample_size=5789.4, sample_size_v1=0, sample_size_v2=0, ppl=6.13, wps=3309.5, ups=0.57, wpb=5789.4, bsz=84, num_updates=4630, lr=1.32572e-05, gnorm=5.526, clip=100, loss_scale=16, train_wall=17, gb_free=11.8, wall=8183
2023-06-29 21:16:40 - progress_bar.py[line:272] - INFO: epoch 008:     29 / 660 loss=3.564, loss_v1=0, loss_v2=0, nll_loss=2.524, ntokens=5414.7, nsentences=84, sample_size=5414.7, sample_size_v1=0, sample_size_v2=0, ppl=5.75, wps=3120.6, ups=0.58, wpb=5414.7, bsz=84, num_updates=4640, lr=1.32169e-05, gnorm=5.563, clip=100, loss_scale=16, train_wall=17, gb_free=12.2, wall=8200
2023-06-29 21:16:58 - progress_bar.py[line:272] - INFO: epoch 008:     39 / 660 loss=3.554, loss_v1=0, loss_v2=0, nll_loss=2.51, ntokens=5685.9, nsentences=84, sample_size=5685.9, sample_size_v1=0, sample_size_v2=0, ppl=5.7, wps=3248.2, ups=0.57, wpb=5685.9, bsz=84, num_updates=4650, lr=1.31766e-05, gnorm=5.257, clip=100, loss_scale=16, train_wall=17, gb_free=11.6, wall=8218
2023-06-29 21:17:15 - progress_bar.py[line:272] - INFO: epoch 008:     49 / 660 loss=3.534, loss_v1=0, loss_v2=0, nll_loss=2.486, ntokens=5738.6, nsentences=84, sample_size=5738.6, sample_size_v1=0, sample_size_v2=0, ppl=5.6, wps=3232.4, ups=0.56, wpb=5738.6, bsz=84, num_updates=4660, lr=1.31363e-05, gnorm=5.486, clip=100, loss_scale=16, train_wall=18, gb_free=11.9, wall=8235
2023-06-29 21:17:33 - progress_bar.py[line:272] - INFO: epoch 008:     59 / 660 loss=3.563, loss_v1=0, loss_v2=0, nll_loss=2.52, ntokens=5754, nsentences=84, sample_size=5754, sample_size_v1=0, sample_size_v2=0, ppl=5.74, wps=3290.3, ups=0.57, wpb=5754, bsz=84, num_updates=4670, lr=1.3096e-05, gnorm=4.927, clip=100, loss_scale=16, train_wall=17, gb_free=11.8, wall=8253
2023-06-29 21:17:50 - progress_bar.py[line:272] - INFO: epoch 008:     69 / 660 loss=3.51, loss_v1=0, loss_v2=0, nll_loss=2.46, ntokens=5713.4, nsentences=84, sample_size=5713.4, sample_size_v1=0, sample_size_v2=0, ppl=5.5, wps=3280.7, ups=0.57, wpb=5713.4, bsz=84, num_updates=4680, lr=1.30557e-05, gnorm=5.35, clip=100, loss_scale=16, train_wall=17, gb_free=11.6, wall=8270
2023-06-29 21:18:08 - progress_bar.py[line:272] - INFO: epoch 008:     79 / 660 loss=3.577, loss_v1=0, loss_v2=0, nll_loss=2.535, ntokens=6391.6, nsentences=84, sample_size=6391.6, sample_size_v1=0, sample_size_v2=0, ppl=5.8, wps=3585.7, ups=0.56, wpb=6391.6, bsz=84, num_updates=4690, lr=1.30154e-05, gnorm=5.329, clip=100, loss_scale=16, train_wall=18, gb_free=11.9, wall=8288
2023-06-29 21:18:26 - progress_bar.py[line:272] - INFO: epoch 008:     89 / 660 loss=3.558, loss_v1=0, loss_v2=0, nll_loss=2.513, ntokens=5778.8, nsentences=84, sample_size=5778.8, sample_size_v1=0, sample_size_v2=0, ppl=5.71, wps=3262, ups=0.56, wpb=5778.8, bsz=84, num_updates=4700, lr=1.29752e-05, gnorm=5.285, clip=100, loss_scale=16, train_wall=18, gb_free=11.2, wall=8306
2023-06-29 21:18:43 - progress_bar.py[line:272] - INFO: epoch 008:     99 / 660 loss=3.557, loss_v1=0, loss_v2=0, nll_loss=2.513, ntokens=5792.4, nsentences=84, sample_size=5792.4, sample_size_v1=0, sample_size_v2=0, ppl=5.71, wps=3295.2, ups=0.57, wpb=5792.4, bsz=84, num_updates=4710, lr=1.29349e-05, gnorm=5.493, clip=100, loss_scale=16, train_wall=18, gb_free=11.9, wall=8324
2023-06-29 21:19:01 - progress_bar.py[line:272] - INFO: epoch 008:    109 / 660 loss=3.528, loss_v1=0, loss_v2=0, nll_loss=2.481, ntokens=5852.8, nsentences=84, sample_size=5852.8, sample_size_v1=0, sample_size_v2=0, ppl=5.58, wps=3352.2, ups=0.57, wpb=5852.8, bsz=84, num_updates=4720, lr=1.28946e-05, gnorm=5.5, clip=100, loss_scale=16, train_wall=17, gb_free=12.4, wall=8341
2023-06-29 21:19:18 - progress_bar.py[line:272] - INFO: epoch 008:    119 / 660 loss=3.574, loss_v1=0, loss_v2=0, nll_loss=2.532, ntokens=5674.4, nsentences=84, sample_size=5674.4, sample_size_v1=0, sample_size_v2=0, ppl=5.79, wps=3273.4, ups=0.58, wpb=5674.4, bsz=84, num_updates=4730, lr=1.28543e-05, gnorm=5.288, clip=100, loss_scale=16, train_wall=17, gb_free=12.1, wall=8358
2023-06-29 21:19:36 - progress_bar.py[line:272] - INFO: epoch 008:    129 / 660 loss=3.606, loss_v1=0, loss_v2=0, nll_loss=2.568, ntokens=5733, nsentences=84, sample_size=5733, sample_size_v1=0, sample_size_v2=0, ppl=5.93, wps=3278.9, ups=0.57, wpb=5733, bsz=84, num_updates=4740, lr=1.2814e-05, gnorm=5.268, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=8376
2023-06-29 21:19:53 - progress_bar.py[line:272] - INFO: epoch 008:    139 / 660 loss=3.615, loss_v1=0, loss_v2=0, nll_loss=2.577, ntokens=5912.1, nsentences=84, sample_size=5912.1, sample_size_v1=0, sample_size_v2=0, ppl=5.97, wps=3362.2, ups=0.57, wpb=5912.1, bsz=84, num_updates=4750, lr=1.27737e-05, gnorm=4.9, clip=100, loss_scale=16, train_wall=18, gb_free=12, wall=8393
2023-06-29 21:20:11 - progress_bar.py[line:272] - INFO: epoch 008:    149 / 660 loss=3.627, loss_v1=0, loss_v2=0, nll_loss=2.591, ntokens=5934.4, nsentences=84, sample_size=5934.4, sample_size_v1=0, sample_size_v2=0, ppl=6.02, wps=3360.4, ups=0.57, wpb=5934.4, bsz=84, num_updates=4760, lr=1.27334e-05, gnorm=5.335, clip=100, loss_scale=16, train_wall=18, gb_free=11.6, wall=8411
2023-06-29 21:20:28 - progress_bar.py[line:272] - INFO: epoch 008:    159 / 660 loss=3.572, loss_v1=0, loss_v2=0, nll_loss=2.576, ntokens=5972.4, nsentences=82.8, sample_size=5972.4, sample_size_v1=0, sample_size_v2=0, ppl=5.96, wps=3395.7, ups=0.57, wpb=5972.4, bsz=82.8, num_updates=4770, lr=1.26931e-05, gnorm=4.705, clip=100, loss_scale=16, train_wall=18, gb_free=11.5, wall=8429
2023-06-29 21:20:46 - progress_bar.py[line:272] - INFO: epoch 008:    169 / 660 loss=3.594, loss_v1=0, loss_v2=0, nll_loss=2.554, ntokens=5985.4, nsentences=84, sample_size=5985.4, sample_size_v1=0, sample_size_v2=0, ppl=5.87, wps=3365.6, ups=0.56, wpb=5985.4, bsz=84, num_updates=4780, lr=1.26528e-05, gnorm=5.268, clip=100, loss_scale=16, train_wall=18, gb_free=11.9, wall=8446
2023-06-29 21:21:04 - progress_bar.py[line:272] - INFO: epoch 008:    179 / 660 loss=3.652, loss_v1=0, loss_v2=0, nll_loss=2.621, ntokens=5925.6, nsentences=84, sample_size=5925.6, sample_size_v1=0, sample_size_v2=0, ppl=6.15, wps=3360.1, ups=0.57, wpb=5925.6, bsz=84, num_updates=4790, lr=1.26125e-05, gnorm=4.786, clip=100, loss_scale=16, train_wall=18, gb_free=11.7, wall=8464
2023-06-29 21:21:22 - progress_bar.py[line:272] - INFO: epoch 008:    189 / 660 loss=3.578, loss_v1=0, loss_v2=0, nll_loss=2.535, ntokens=5932.4, nsentences=84, sample_size=5932.4, sample_size_v1=0, sample_size_v2=0, ppl=5.8, wps=3371.6, ups=0.57, wpb=5932.4, bsz=84, num_updates=4800, lr=1.25722e-05, gnorm=5.16, clip=100, loss_scale=16, train_wall=18, gb_free=12.1, wall=8482
2023-06-29 21:21:39 - progress_bar.py[line:272] - INFO: epoch 008:    199 / 660 loss=3.568, loss_v1=0, loss_v2=0, nll_loss=2.527, ntokens=5613.4, nsentences=84, sample_size=5613.4, sample_size_v1=0, sample_size_v2=0, ppl=5.76, wps=3225, ups=0.57, wpb=5613.4, bsz=84, num_updates=4810, lr=1.25319e-05, gnorm=5.246, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=8499
2023-06-29 21:21:57 - progress_bar.py[line:272] - INFO: epoch 008:    209 / 660 loss=3.61, loss_v1=0, loss_v2=0, nll_loss=2.571, ntokens=6130.6, nsentences=84, sample_size=6130.6, sample_size_v1=0, sample_size_v2=0, ppl=5.94, wps=3478.4, ups=0.57, wpb=6130.6, bsz=84, num_updates=4820, lr=1.24916e-05, gnorm=5.217, clip=100, loss_scale=16, train_wall=18, gb_free=11.9, wall=8517
2023-06-29 21:22:14 - progress_bar.py[line:272] - INFO: epoch 008:    219 / 660 loss=3.608, loss_v1=0, loss_v2=0, nll_loss=2.571, ntokens=5975.7, nsentences=84, sample_size=5975.7, sample_size_v1=0, sample_size_v2=0, ppl=5.94, wps=3397.7, ups=0.57, wpb=5975.7, bsz=84, num_updates=4830, lr=1.24513e-05, gnorm=4.801, clip=100, loss_scale=16, train_wall=18, gb_free=11.9, wall=8534
2023-06-29 21:22:32 - progress_bar.py[line:272] - INFO: epoch 008:    229 / 660 loss=3.612, loss_v1=0, loss_v2=0, nll_loss=2.573, ntokens=6062, nsentences=84, sample_size=6062, sample_size_v1=0, sample_size_v2=0, ppl=5.95, wps=3465.4, ups=0.57, wpb=6062, bsz=84, num_updates=4840, lr=1.2411e-05, gnorm=5.003, clip=100, loss_scale=16, train_wall=17, gb_free=12.5, wall=8552
2023-06-29 21:22:49 - progress_bar.py[line:272] - INFO: epoch 008:    239 / 660 loss=3.564, loss_v1=0, loss_v2=0, nll_loss=2.522, ntokens=5841.6, nsentences=84, sample_size=5841.6, sample_size_v1=0, sample_size_v2=0, ppl=5.74, wps=3363.3, ups=0.58, wpb=5841.6, bsz=84, num_updates=4850, lr=1.23707e-05, gnorm=5.362, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=8569
2023-06-29 21:23:06 - progress_bar.py[line:272] - INFO: epoch 008:    249 / 660 loss=3.566, loss_v1=0, loss_v2=0, nll_loss=2.524, ntokens=6192.2, nsentences=84, sample_size=6192.2, sample_size_v1=0, sample_size_v2=0, ppl=5.75, wps=3542.6, ups=0.57, wpb=6192.2, bsz=84, num_updates=4860, lr=1.23304e-05, gnorm=5.227, clip=100, loss_scale=16, train_wall=17, gb_free=12.1, wall=8587
2023-06-29 21:23:24 - progress_bar.py[line:272] - INFO: epoch 008:    259 / 660 loss=3.543, loss_v1=0, loss_v2=0, nll_loss=2.497, ntokens=5923.7, nsentences=84, sample_size=5923.7, sample_size_v1=0, sample_size_v2=0, ppl=5.65, wps=3376.6, ups=0.57, wpb=5923.7, bsz=84, num_updates=4870, lr=1.22901e-05, gnorm=5.493, clip=100, loss_scale=16, train_wall=18, gb_free=11.8, wall=8604
2023-06-29 21:23:41 - progress_bar.py[line:272] - INFO: epoch 008:    269 / 660 loss=3.537, loss_v1=0, loss_v2=0, nll_loss=2.491, ntokens=5869.2, nsentences=84, sample_size=5869.2, sample_size_v1=0, sample_size_v2=0, ppl=5.62, wps=3366, ups=0.57, wpb=5869.2, bsz=84, num_updates=4880, lr=1.22498e-05, gnorm=5.072, clip=100, loss_scale=16, train_wall=17, gb_free=11.7, wall=8622
2023-06-29 21:23:59 - progress_bar.py[line:272] - INFO: epoch 008:    279 / 660 loss=3.553, loss_v1=0, loss_v2=0, nll_loss=2.509, ntokens=5890.9, nsentences=84, sample_size=5890.9, sample_size_v1=0, sample_size_v2=0, ppl=5.69, wps=3347.3, ups=0.57, wpb=5890.9, bsz=84, num_updates=4890, lr=1.22095e-05, gnorm=5.117, clip=100, loss_scale=16, train_wall=18, gb_free=11.6, wall=8639
2023-06-29 21:24:17 - progress_bar.py[line:272] - INFO: epoch 008:    289 / 660 loss=3.556, loss_v1=0, loss_v2=0, nll_loss=2.513, ntokens=5972.3, nsentences=84, sample_size=5972.3, sample_size_v1=0, sample_size_v2=0, ppl=5.71, wps=3424.5, ups=0.57, wpb=5972.3, bsz=84, num_updates=4900, lr=1.21692e-05, gnorm=4.875, clip=100, loss_scale=16, train_wall=17, gb_free=11.8, wall=8657
2023-06-29 21:24:34 - progress_bar.py[line:272] - INFO: epoch 008:    299 / 660 loss=3.572, loss_v1=0, loss_v2=0, nll_loss=2.528, ntokens=6192.2, nsentences=84, sample_size=6192.2, sample_size_v1=0, sample_size_v2=0, ppl=5.77, wps=3555.1, ups=0.57, wpb=6192.2, bsz=84, num_updates=4910, lr=1.21289e-05, gnorm=5.072, clip=100, loss_scale=16, train_wall=17, gb_free=12.1, wall=8674
2023-06-29 21:24:51 - progress_bar.py[line:272] - INFO: epoch 008:    309 / 660 loss=3.563, loss_v1=0, loss_v2=0, nll_loss=2.521, ntokens=6283.1, nsentences=84, sample_size=6283.1, sample_size_v1=0, sample_size_v2=0, ppl=5.74, wps=3601.9, ups=0.57, wpb=6283.1, bsz=84, num_updates=4920, lr=1.20887e-05, gnorm=5.072, clip=100, loss_scale=16, train_wall=17, gb_free=11.8, wall=8692
2023-06-29 21:25:09 - progress_bar.py[line:272] - INFO: epoch 008:    319 / 660 loss=3.559, loss_v1=0, loss_v2=0, nll_loss=2.516, ntokens=6210, nsentences=84, sample_size=6210, sample_size_v1=0, sample_size_v2=0, ppl=5.72, wps=3553, ups=0.57, wpb=6210, bsz=84, num_updates=4930, lr=1.20484e-05, gnorm=5.427, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=8709
2023-06-29 21:25:27 - progress_bar.py[line:272] - INFO: epoch 008:    329 / 660 loss=3.576, loss_v1=0, loss_v2=0, nll_loss=2.535, ntokens=6059.6, nsentences=84, sample_size=6059.6, sample_size_v1=0, sample_size_v2=0, ppl=5.79, wps=3429.6, ups=0.57, wpb=6059.6, bsz=84, num_updates=4940, lr=1.20081e-05, gnorm=4.898, clip=100, loss_scale=16, train_wall=18, gb_free=12, wall=8727
2023-06-29 21:25:44 - progress_bar.py[line:272] - INFO: epoch 008:    339 / 660 loss=3.552, loss_v1=0, loss_v2=0, nll_loss=2.509, ntokens=6030.8, nsentences=84, sample_size=6030.8, sample_size_v1=0, sample_size_v2=0, ppl=5.69, wps=3450.4, ups=0.57, wpb=6030.8, bsz=84, num_updates=4950, lr=1.19678e-05, gnorm=5.213, clip=100, loss_scale=16, train_wall=17, gb_free=12.2, wall=8744
2023-06-29 21:26:01 - progress_bar.py[line:272] - INFO: epoch 008:    349 / 660 loss=3.533, loss_v1=0, loss_v2=0, nll_loss=2.486, ntokens=5959.8, nsentences=84, sample_size=5959.8, sample_size_v1=0, sample_size_v2=0, ppl=5.6, wps=3411.8, ups=0.57, wpb=5959.8, bsz=84, num_updates=4960, lr=1.19275e-05, gnorm=5.063, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=8762
2023-06-29 21:26:19 - progress_bar.py[line:272] - INFO: epoch 008:    359 / 660 loss=3.535, loss_v1=0, loss_v2=0, nll_loss=2.489, ntokens=5999.8, nsentences=84, sample_size=5999.8, sample_size_v1=0, sample_size_v2=0, ppl=5.61, wps=3442.5, ups=0.57, wpb=5999.8, bsz=84, num_updates=4970, lr=1.18872e-05, gnorm=5.285, clip=100, loss_scale=16, train_wall=17, gb_free=12.5, wall=8779
2023-06-29 21:26:36 - progress_bar.py[line:272] - INFO: epoch 008:    369 / 660 loss=3.514, loss_v1=0, loss_v2=0, nll_loss=2.464, ntokens=5791.4, nsentences=84, sample_size=5791.4, sample_size_v1=0, sample_size_v2=0, ppl=5.52, wps=3334.4, ups=0.58, wpb=5791.4, bsz=84, num_updates=4980, lr=1.18469e-05, gnorm=5.455, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=8796
2023-06-29 21:26:54 - progress_bar.py[line:272] - INFO: epoch 008:    379 / 660 loss=3.529, loss_v1=0, loss_v2=0, nll_loss=2.482, ntokens=5935.2, nsentences=84, sample_size=5935.2, sample_size_v1=0, sample_size_v2=0, ppl=5.59, wps=3411.6, ups=0.57, wpb=5935.2, bsz=84, num_updates=4990, lr=1.18066e-05, gnorm=5.057, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=8814
2023-06-29 21:27:11 - progress_bar.py[line:272] - INFO: epoch 008:    389 / 660 loss=3.521, loss_v1=0, loss_v2=0, nll_loss=2.473, ntokens=5641.9, nsentences=84, sample_size=5641.9, sample_size_v1=0, sample_size_v2=0, ppl=5.55, wps=3264.4, ups=0.58, wpb=5641.9, bsz=84, num_updates=5000, lr=1.17663e-05, gnorm=5.524, clip=100, loss_scale=16, train_wall=17, gb_free=12.1, wall=8831
2023-06-29 21:27:28 - progress_bar.py[line:272] - INFO: epoch 008:    399 / 660 loss=3.527, loss_v1=0, loss_v2=0, nll_loss=2.48, ntokens=5777.2, nsentences=84, sample_size=5777.2, sample_size_v1=0, sample_size_v2=0, ppl=5.58, wps=3334.6, ups=0.58, wpb=5777.2, bsz=84, num_updates=5010, lr=1.1726e-05, gnorm=5.531, clip=100, loss_scale=16, train_wall=17, gb_free=12.3, wall=8848
2023-06-29 21:27:46 - progress_bar.py[line:272] - INFO: epoch 008:    409 / 660 loss=3.528, loss_v1=0, loss_v2=0, nll_loss=2.481, ntokens=5759.6, nsentences=84, sample_size=5759.6, sample_size_v1=0, sample_size_v2=0, ppl=5.58, wps=3341.7, ups=0.58, wpb=5759.6, bsz=84, num_updates=5020, lr=1.16857e-05, gnorm=5.688, clip=100, loss_scale=16, train_wall=17, gb_free=12.6, wall=8866
2023-06-29 21:28:03 - progress_bar.py[line:272] - INFO: epoch 008:    419 / 660 loss=3.538, loss_v1=0, loss_v2=0, nll_loss=2.492, ntokens=5746.7, nsentences=84, sample_size=5746.7, sample_size_v1=0, sample_size_v2=0, ppl=5.62, wps=3323.5, ups=0.58, wpb=5746.7, bsz=84, num_updates=5030, lr=1.16454e-05, gnorm=5.293, clip=100, loss_scale=16, train_wall=17, gb_free=12.4, wall=8883
2023-06-29 21:28:20 - progress_bar.py[line:272] - INFO: epoch 008:    429 / 660 loss=3.537, loss_v1=0, loss_v2=0, nll_loss=2.491, ntokens=5753.6, nsentences=84, sample_size=5753.6, sample_size_v1=0, sample_size_v2=0, ppl=5.62, wps=3312.5, ups=0.58, wpb=5753.6, bsz=84, num_updates=5040, lr=1.16051e-05, gnorm=5.201, clip=100, loss_scale=16, train_wall=17, gb_free=12.2, wall=8900
2023-06-29 21:28:38 - progress_bar.py[line:272] - INFO: epoch 008:    439 / 660 loss=3.574, loss_v1=0, loss_v2=0, nll_loss=2.533, ntokens=6136.5, nsentences=84, sample_size=6136.5, sample_size_v1=0, sample_size_v2=0, ppl=5.79, wps=3536.9, ups=0.58, wpb=6136.5, bsz=84, num_updates=5050, lr=1.15648e-05, gnorm=5.137, clip=100, loss_scale=16, train_wall=17, gb_free=11.7, wall=8918
2023-06-29 21:28:55 - progress_bar.py[line:272] - INFO: epoch 008:    449 / 660 loss=3.547, loss_v1=0, loss_v2=0, nll_loss=2.501, ntokens=5759, nsentences=84, sample_size=5759, sample_size_v1=0, sample_size_v2=0, ppl=5.66, wps=3333, ups=0.58, wpb=5759, bsz=84, num_updates=5060, lr=1.15245e-05, gnorm=5.427, clip=100, loss_scale=16, train_wall=17, gb_free=12.6, wall=8935
2023-06-29 21:29:12 - progress_bar.py[line:272] - INFO: epoch 008:    459 / 660 loss=3.537, loss_v1=0, loss_v2=0, nll_loss=2.491, ntokens=5781.6, nsentences=84, sample_size=5781.6, sample_size_v1=0, sample_size_v2=0, ppl=5.62, wps=3329.5, ups=0.58, wpb=5781.6, bsz=84, num_updates=5070, lr=1.14842e-05, gnorm=5.079, clip=100, loss_scale=16, train_wall=17, gb_free=11.8, wall=8952
2023-06-29 21:29:30 - progress_bar.py[line:272] - INFO: epoch 008:    469 / 660 loss=3.572, loss_v1=0, loss_v2=0, nll_loss=2.529, ntokens=6088.3, nsentences=84, sample_size=6088.3, sample_size_v1=0, sample_size_v2=0, ppl=5.77, wps=3498.6, ups=0.57, wpb=6088.3, bsz=84, num_updates=5080, lr=1.14439e-05, gnorm=5.104, clip=100, loss_scale=32, train_wall=17, gb_free=12.1, wall=8970
2023-06-29 21:29:47 - progress_bar.py[line:272] - INFO: epoch 008:    479 / 660 loss=3.544, loss_v1=0, loss_v2=0, nll_loss=2.498, ntokens=5880.7, nsentences=84, sample_size=5880.7, sample_size_v1=0, sample_size_v2=0, ppl=5.65, wps=3385.9, ups=0.58, wpb=5880.7, bsz=84, num_updates=5090, lr=1.14036e-05, gnorm=5.164, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=8987
2023-06-29 21:30:04 - progress_bar.py[line:272] - INFO: epoch 008:    489 / 660 loss=3.509, loss_v1=0, loss_v2=0, nll_loss=2.458, ntokens=5613.9, nsentences=84, sample_size=5613.9, sample_size_v1=0, sample_size_v2=0, ppl=5.49, wps=3244.4, ups=0.58, wpb=5613.9, bsz=84, num_updates=5100, lr=1.13633e-05, gnorm=5.57, clip=100, loss_scale=32, train_wall=17, gb_free=12.1, wall=9004
2023-06-29 21:30:22 - progress_bar.py[line:272] - INFO: epoch 008:    499 / 660 loss=3.559, loss_v1=0, loss_v2=0, nll_loss=2.516, ntokens=5858.3, nsentences=84, sample_size=5858.3, sample_size_v1=0, sample_size_v2=0, ppl=5.72, wps=3369.9, ups=0.58, wpb=5858.3, bsz=84, num_updates=5110, lr=1.1323e-05, gnorm=5.249, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=9022
2023-06-29 21:30:39 - progress_bar.py[line:272] - INFO: epoch 008:    509 / 660 loss=3.524, loss_v1=0, loss_v2=0, nll_loss=2.476, ntokens=5752.5, nsentences=84, sample_size=5752.5, sample_size_v1=0, sample_size_v2=0, ppl=5.56, wps=3317.4, ups=0.58, wpb=5752.5, bsz=84, num_updates=5120, lr=1.12827e-05, gnorm=5.413, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=9039
2023-06-29 21:30:56 - progress_bar.py[line:272] - INFO: epoch 008:    519 / 660 loss=3.543, loss_v1=0, loss_v2=0, nll_loss=2.497, ntokens=5950.4, nsentences=84, sample_size=5950.4, sample_size_v1=0, sample_size_v2=0, ppl=5.65, wps=3434.5, ups=0.58, wpb=5950.4, bsz=84, num_updates=5130, lr=1.12424e-05, gnorm=5.206, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=9057
2023-06-29 21:31:14 - progress_bar.py[line:272] - INFO: epoch 008:    529 / 660 loss=3.563, loss_v1=0, loss_v2=0, nll_loss=2.518, ntokens=6093, nsentences=84, sample_size=6093, sample_size_v1=0, sample_size_v2=0, ppl=5.73, wps=3501.5, ups=0.57, wpb=6093, bsz=84, num_updates=5140, lr=1.12021e-05, gnorm=5.14, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=9074
2023-06-29 21:31:31 - progress_bar.py[line:272] - INFO: epoch 008:    539 / 660 loss=3.554, loss_v1=0, loss_v2=0, nll_loss=2.509, ntokens=5989.4, nsentences=84, sample_size=5989.4, sample_size_v1=0, sample_size_v2=0, ppl=5.69, wps=3440.4, ups=0.57, wpb=5989.4, bsz=84, num_updates=5150, lr=1.11619e-05, gnorm=5.054, clip=100, loss_scale=32, train_wall=17, gb_free=11.8, wall=9091
2023-06-29 21:31:49 - progress_bar.py[line:272] - INFO: epoch 008:    549 / 660 loss=3.56, loss_v1=0, loss_v2=0, nll_loss=2.515, ntokens=6209.4, nsentences=84, sample_size=6209.4, sample_size_v1=0, sample_size_v2=0, ppl=5.72, wps=3544.3, ups=0.57, wpb=6209.4, bsz=84, num_updates=5160, lr=1.11216e-05, gnorm=4.982, clip=100, loss_scale=32, train_wall=17, gb_free=11.1, wall=9109
2023-06-29 21:32:03 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-06-29 21:32:08 - progress_bar.py[line:272] - INFO: epoch 008:    560 / 660 loss=3.517, loss_v1=0, loss_v2=0, nll_loss=2.468, ntokens=5972.4, nsentences=84, sample_size=5972.4, sample_size_v1=0, sample_size_v2=0, ppl=5.53, wps=3131.6, ups=0.52, wpb=5972.4, bsz=84, num_updates=5170, lr=1.10813e-05, gnorm=5.174, clip=100, loss_scale=16, train_wall=19, gb_free=11.6, wall=9128
2023-06-29 21:32:25 - progress_bar.py[line:272] - INFO: epoch 008:    570 / 660 loss=3.488, loss_v1=0, loss_v2=0, nll_loss=2.435, ntokens=5477.1, nsentences=84, sample_size=5477.1, sample_size_v1=0, sample_size_v2=0, ppl=5.41, wps=3166, ups=0.58, wpb=5477.1, bsz=84, num_updates=5180, lr=1.1041e-05, gnorm=5.559, clip=100, loss_scale=16, train_wall=17, gb_free=12.3, wall=9145
2023-06-29 21:32:42 - progress_bar.py[line:272] - INFO: epoch 008:    580 / 660 loss=3.535, loss_v1=0, loss_v2=0, nll_loss=2.488, ntokens=5944.8, nsentences=84, sample_size=5944.8, sample_size_v1=0, sample_size_v2=0, ppl=5.61, wps=3436.9, ups=0.58, wpb=5944.8, bsz=84, num_updates=5190, lr=1.10007e-05, gnorm=5.424, clip=100, loss_scale=16, train_wall=17, gb_free=12.3, wall=9163
2023-06-29 21:33:00 - progress_bar.py[line:272] - INFO: epoch 008:    590 / 660 loss=3.524, loss_v1=0, loss_v2=0, nll_loss=2.475, ntokens=5898.7, nsentences=84, sample_size=5898.7, sample_size_v1=0, sample_size_v2=0, ppl=5.56, wps=3372.1, ups=0.57, wpb=5898.7, bsz=84, num_updates=5200, lr=1.09604e-05, gnorm=5.246, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=9180
2023-06-29 21:33:17 - progress_bar.py[line:272] - INFO: epoch 008:    600 / 660 loss=3.506, loss_v1=0, loss_v2=0, nll_loss=2.454, ntokens=5891.7, nsentences=84, sample_size=5891.7, sample_size_v1=0, sample_size_v2=0, ppl=5.48, wps=3353.6, ups=0.57, wpb=5891.7, bsz=84, num_updates=5210, lr=1.09201e-05, gnorm=5.001, clip=100, loss_scale=16, train_wall=18, gb_free=12.4, wall=9198
2023-06-29 21:33:35 - progress_bar.py[line:272] - INFO: epoch 008:    610 / 660 loss=3.526, loss_v1=0, loss_v2=0, nll_loss=2.478, ntokens=5994.8, nsentences=84, sample_size=5994.8, sample_size_v1=0, sample_size_v2=0, ppl=5.57, wps=3454.7, ups=0.58, wpb=5994.8, bsz=84, num_updates=5220, lr=1.08798e-05, gnorm=5.108, clip=100, loss_scale=16, train_wall=17, gb_free=11.8, wall=9215
2023-06-29 21:33:52 - progress_bar.py[line:272] - INFO: epoch 008:    620 / 660 loss=3.542, loss_v1=0, loss_v2=0, nll_loss=2.496, ntokens=5991.1, nsentences=84, sample_size=5991.1, sample_size_v1=0, sample_size_v2=0, ppl=5.64, wps=3426.8, ups=0.57, wpb=5991.1, bsz=84, num_updates=5230, lr=1.08395e-05, gnorm=5.239, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=9232
2023-06-29 21:34:10 - progress_bar.py[line:272] - INFO: epoch 008:    630 / 660 loss=3.554, loss_v1=0, loss_v2=0, nll_loss=2.508, ntokens=6041.4, nsentences=84, sample_size=6041.4, sample_size_v1=0, sample_size_v2=0, ppl=5.69, wps=3466.3, ups=0.57, wpb=6041.4, bsz=84, num_updates=5240, lr=1.07992e-05, gnorm=5.014, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=9250
2023-06-29 21:34:27 - progress_bar.py[line:272] - INFO: epoch 008:    640 / 660 loss=3.53, loss_v1=0, loss_v2=0, nll_loss=2.484, ntokens=5972.1, nsentences=84, sample_size=5972.1, sample_size_v1=0, sample_size_v2=0, ppl=5.6, wps=3424, ups=0.57, wpb=5972.1, bsz=84, num_updates=5250, lr=1.07589e-05, gnorm=5.058, clip=100, loss_scale=16, train_wall=17, gb_free=12.5, wall=9267
2023-06-29 21:34:45 - progress_bar.py[line:272] - INFO: epoch 008:    650 / 660 loss=3.549, loss_v1=0, loss_v2=0, nll_loss=2.501, ntokens=5941, nsentences=84, sample_size=5941, sample_size_v1=0, sample_size_v2=0, ppl=5.66, wps=3403.5, ups=0.57, wpb=5941, bsz=84, num_updates=5260, lr=1.07186e-05, gnorm=5.314, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=9285
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
2023-06-29 21:35:02 - progress_bar.py[line:272] - INFO: epoch 008:    660 / 660 loss=3.541, loss_v1=0, loss_v2=0, nll_loss=2.494, ntokens=5804.5, nsentences=81.9, sample_size=5804.5, sample_size_v1=0, sample_size_v2=0, ppl=5.63, wps=3422.2, ups=0.59, wpb=5804.5, bsz=81.9, num_updates=5270, lr=1.06783e-05, gnorm=5.239, clip=100, loss_scale=16, train_wall=17, gb_free=12.3, wall=9302
2023-06-29 21:35:02 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 8 @ 5270 updates
2023-06-29 21:35:02 - trainer.py[line:431] - INFO: Saving checkpoint to ../../checkpoints/OFA/sgcls_checkpoints/_12_3e-5_512_base_tgtobj/checkpoint8.pt
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 row count 18466 total row count 55400
slice_id 2 seek offset 36934
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 row count 18467 total row count 55400
slice_id 1 seek offset 18467
2023-06-29 21:35:05 - trainer.py[line:441] - INFO: Finished saving checkpoint to ../../checkpoints/OFA/sgcls_checkpoints/_12_3e-5_512_base_tgtobj/checkpoint8.pt
2023-06-29 21:35:29 - checkpoint_utils.py[line:133] - INFO: Saved checkpoint ../../checkpoints/OFA/sgcls_checkpoints/_12_3e-5_512_base_tgtobj/checkpoint8.pt (epoch 8 @ 5270 updates, score None) (writing took 27.59532354865223 seconds)
2023-06-29 21:35:29 - train.py[line:332] - INFO: end of epoch 8 (average epoch stats below)
2023-06-29 21:35:29 - progress_bar.py[line:282] - INFO: epoch 008 | loss 3.558 | loss_v1 0 | loss_v2 0 | nll_loss 2.514 | ntokens 5902.44 | nsentences 83.95 | sample_size 5902.44 | sample_size_v1 0 | sample_size_v2 0 | ppl 5.71 | wps 3290.5 | ups 0.56 | wpb 5902.4 | bsz 83.9 | num_updates 5270 | lr 1.06783e-05 | gnorm 5.228 | clip 100 | loss_scale 16 | train_wall 1150 | gb_free 12.3 | wall 9329
2023-06-29 21:35:29 - trainer.py[line:639] - INFO: loading train data for epoch 9
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 row count 18467 total row count 55400
slice_id 0 seek offset 0
2023-06-29 21:35:31 - trainer.py[line:703] - INFO: begin training epoch 9
2023-06-29 21:35:31 - train.py[line:305] - INFO: Start iterating over samples
2023-06-29 21:35:49 - progress_bar.py[line:272] - INFO: epoch 009:     10 / 660 loss=3.633, loss_v1=0, loss_v2=0, nll_loss=2.594, ntokens=5871.6, nsentences=84, sample_size=5871.6, sample_size_v1=0, sample_size_v2=0, ppl=6.04, wps=1243.2, ups=0.21, wpb=5871.6, bsz=84, num_updates=5280, lr=1.0638e-05, gnorm=5.811, clip=100, loss_scale=16, train_wall=18, gb_free=11.7, wall=9349
2023-06-29 21:36:06 - progress_bar.py[line:272] - INFO: epoch 009:     20 / 660 loss=3.58, loss_v1=0, loss_v2=0, nll_loss=2.534, ntokens=5829.2, nsentences=84, sample_size=5829.2, sample_size_v1=0, sample_size_v2=0, ppl=5.79, wps=3337.6, ups=0.57, wpb=5829.2, bsz=84, num_updates=5290, lr=1.05977e-05, gnorm=5.728, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=9366
2023-06-29 21:36:24 - progress_bar.py[line:272] - INFO: epoch 009:     30 / 660 loss=3.5, loss_v1=0, loss_v2=0, nll_loss=2.452, ntokens=5374.8, nsentences=84, sample_size=5374.8, sample_size_v1=0, sample_size_v2=0, ppl=5.47, wps=3115.2, ups=0.58, wpb=5374.8, bsz=84, num_updates=5300, lr=1.05574e-05, gnorm=5.254, clip=100, loss_scale=16, train_wall=17, gb_free=12.3, wall=9384
2023-06-29 21:36:41 - progress_bar.py[line:272] - INFO: epoch 009:     40 / 660 loss=3.478, loss_v1=0, loss_v2=0, nll_loss=2.423, ntokens=5724.2, nsentences=84, sample_size=5724.2, sample_size_v1=0, sample_size_v2=0, ppl=5.36, wps=3278.7, ups=0.57, wpb=5724.2, bsz=84, num_updates=5310, lr=1.05171e-05, gnorm=5.566, clip=100, loss_scale=16, train_wall=17, gb_free=11.8, wall=9401
2023-06-29 21:36:59 - progress_bar.py[line:272] - INFO: epoch 009:     50 / 660 loss=3.466, loss_v1=0, loss_v2=0, nll_loss=2.408, ntokens=5778.1, nsentences=84, sample_size=5778.1, sample_size_v1=0, sample_size_v2=0, ppl=5.31, wps=3291.9, ups=0.57, wpb=5778.1, bsz=84, num_updates=5320, lr=1.04768e-05, gnorm=5.135, clip=100, loss_scale=16, train_wall=18, gb_free=11.8, wall=9419
2023-06-29 21:37:16 - progress_bar.py[line:272] - INFO: epoch 009:     60 / 660 loss=3.497, loss_v1=0, loss_v2=0, nll_loss=2.445, ntokens=5713.4, nsentences=84, sample_size=5713.4, sample_size_v1=0, sample_size_v2=0, ppl=5.45, wps=3266.2, ups=0.57, wpb=5713.4, bsz=84, num_updates=5330, lr=1.04365e-05, gnorm=5.064, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=9436
2023-06-29 21:37:34 - progress_bar.py[line:272] - INFO: epoch 009:     70 / 660 loss=3.447, loss_v1=0, loss_v2=0, nll_loss=2.388, ntokens=5779.8, nsentences=84, sample_size=5779.8, sample_size_v1=0, sample_size_v2=0, ppl=5.23, wps=3302.6, ups=0.57, wpb=5779.8, bsz=84, num_updates=5340, lr=1.03962e-05, gnorm=5.604, clip=100, loss_scale=16, train_wall=17, gb_free=11.4, wall=9454
2023-06-29 21:37:51 - progress_bar.py[line:272] - INFO: epoch 009:     80 / 660 loss=3.508, loss_v1=0, loss_v2=0, nll_loss=2.458, ntokens=6280.1, nsentences=84, sample_size=6280.1, sample_size_v1=0, sample_size_v2=0, ppl=5.5, wps=3529.4, ups=0.56, wpb=6280.1, bsz=84, num_updates=5350, lr=1.03559e-05, gnorm=5.624, clip=100, loss_scale=16, train_wall=18, gb_free=11.7, wall=9472
2023-06-29 21:38:09 - progress_bar.py[line:272] - INFO: epoch 009:     90 / 660 loss=3.493, loss_v1=0, loss_v2=0, nll_loss=2.439, ntokens=5864.4, nsentences=84, sample_size=5864.4, sample_size_v1=0, sample_size_v2=0, ppl=5.42, wps=3301.1, ups=0.56, wpb=5864.4, bsz=84, num_updates=5360, lr=1.03156e-05, gnorm=5.571, clip=100, loss_scale=16, train_wall=18, gb_free=11.6, wall=9489
2023-06-29 21:38:27 - progress_bar.py[line:272] - INFO: epoch 009:    100 / 660 loss=3.487, loss_v1=0, loss_v2=0, nll_loss=2.433, ntokens=5721.7, nsentences=84, sample_size=5721.7, sample_size_v1=0, sample_size_v2=0, ppl=5.4, wps=3262.3, ups=0.57, wpb=5721.7, bsz=84, num_updates=5370, lr=1.02754e-05, gnorm=5.662, clip=100, loss_scale=16, train_wall=18, gb_free=12, wall=9507
2023-06-29 21:38:44 - progress_bar.py[line:272] - INFO: epoch 009:    110 / 660 loss=3.472, loss_v1=0, loss_v2=0, nll_loss=2.416, ntokens=5877.3, nsentences=84, sample_size=5877.3, sample_size_v1=0, sample_size_v2=0, ppl=5.34, wps=3363.6, ups=0.57, wpb=5877.3, bsz=84, num_updates=5380, lr=1.02351e-05, gnorm=5.041, clip=100, loss_scale=16, train_wall=17, gb_free=11.7, wall=9524
2023-06-29 21:39:01 - progress_bar.py[line:272] - INFO: epoch 009:    120 / 660 loss=3.5, loss_v1=0, loss_v2=0, nll_loss=2.449, ntokens=5681.5, nsentences=84, sample_size=5681.5, sample_size_v1=0, sample_size_v2=0, ppl=5.46, wps=3278.5, ups=0.58, wpb=5681.5, bsz=84, num_updates=5390, lr=1.01948e-05, gnorm=5.192, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=9542
2023-06-29 21:39:19 - progress_bar.py[line:272] - INFO: epoch 009:    130 / 660 loss=3.543, loss_v1=0, loss_v2=0, nll_loss=2.498, ntokens=5703.1, nsentences=84, sample_size=5703.1, sample_size_v1=0, sample_size_v2=0, ppl=5.65, wps=3261.6, ups=0.57, wpb=5703.1, bsz=84, num_updates=5400, lr=1.01545e-05, gnorm=5.072, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=9559
2023-06-29 21:39:37 - progress_bar.py[line:272] - INFO: epoch 009:    140 / 660 loss=3.559, loss_v1=0, loss_v2=0, nll_loss=2.512, ntokens=5946.6, nsentences=84, sample_size=5946.6, sample_size_v1=0, sample_size_v2=0, ppl=5.7, wps=3372.8, ups=0.57, wpb=5946.6, bsz=84, num_updates=5410, lr=1.01142e-05, gnorm=5.247, clip=100, loss_scale=16, train_wall=18, gb_free=11.5, wall=9577
2023-06-29 21:39:54 - progress_bar.py[line:272] - INFO: epoch 009:    150 / 660 loss=3.561, loss_v1=0, loss_v2=0, nll_loss=2.516, ntokens=5980.2, nsentences=84, sample_size=5980.2, sample_size_v1=0, sample_size_v2=0, ppl=5.72, wps=3389.2, ups=0.57, wpb=5980.2, bsz=84, num_updates=5420, lr=1.00739e-05, gnorm=5.338, clip=100, loss_scale=16, train_wall=18, gb_free=11.7, wall=9594
2023-06-29 21:40:12 - progress_bar.py[line:272] - INFO: epoch 009:    160 / 660 loss=3.565, loss_v1=0, loss_v2=0, nll_loss=2.522, ntokens=6019.3, nsentences=84, sample_size=6019.3, sample_size_v1=0, sample_size_v2=0, ppl=5.75, wps=3403.6, ups=0.57, wpb=6019.3, bsz=84, num_updates=5430, lr=1.00336e-05, gnorm=4.883, clip=100, loss_scale=16, train_wall=18, gb_free=11.5, wall=9612
2023-06-29 21:40:30 - progress_bar.py[line:272] - INFO: epoch 009:    170 / 660 loss=3.493, loss_v1=0, loss_v2=0, nll_loss=2.486, ntokens=5937.1, nsentences=82.8, sample_size=5937.1, sample_size_v1=0, sample_size_v2=0, ppl=5.6, wps=3370.1, ups=0.57, wpb=5937.1, bsz=82.8, num_updates=5440, lr=9.99328e-06, gnorm=4.967, clip=100, loss_scale=16, train_wall=18, gb_free=11.7, wall=9630
2023-06-29 21:40:47 - progress_bar.py[line:272] - INFO: epoch 009:    180 / 660 loss=3.588, loss_v1=0, loss_v2=0, nll_loss=2.548, ntokens=5895.1, nsentences=84, sample_size=5895.1, sample_size_v1=0, sample_size_v2=0, ppl=5.85, wps=3315.7, ups=0.56, wpb=5895.1, bsz=84, num_updates=5450, lr=9.95299e-06, gnorm=5.308, clip=100, loss_scale=16, train_wall=18, gb_free=11.9, wall=9648
2023-06-29 21:41:05 - progress_bar.py[line:272] - INFO: epoch 009:    190 / 660 loss=3.511, loss_v1=0, loss_v2=0, nll_loss=2.46, ntokens=5902.1, nsentences=84, sample_size=5902.1, sample_size_v1=0, sample_size_v2=0, ppl=5.5, wps=3380, ups=0.57, wpb=5902.1, bsz=84, num_updates=5460, lr=9.91269e-06, gnorm=4.986, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=9665
2023-06-29 21:41:22 - progress_bar.py[line:272] - INFO: epoch 009:    200 / 660 loss=3.516, loss_v1=0, loss_v2=0, nll_loss=2.467, ntokens=5657.6, nsentences=84, sample_size=5657.6, sample_size_v1=0, sample_size_v2=0, ppl=5.53, wps=3237.3, ups=0.57, wpb=5657.6, bsz=84, num_updates=5470, lr=9.8724e-06, gnorm=4.995, clip=100, loss_scale=16, train_wall=17, gb_free=11.7, wall=9682
2023-06-29 21:41:40 - progress_bar.py[line:272] - INFO: epoch 009:    210 / 660 loss=3.555, loss_v1=0, loss_v2=0, nll_loss=2.509, ntokens=6098, nsentences=84, sample_size=6098, sample_size_v1=0, sample_size_v2=0, ppl=5.69, wps=3453.7, ups=0.57, wpb=6098, bsz=84, num_updates=5480, lr=9.8321e-06, gnorm=5.118, clip=100, loss_scale=16, train_wall=18, gb_free=11.9, wall=9700
2023-06-29 21:41:58 - progress_bar.py[line:272] - INFO: epoch 009:    220 / 660 loss=3.558, loss_v1=0, loss_v2=0, nll_loss=2.514, ntokens=6045.6, nsentences=84, sample_size=6045.6, sample_size_v1=0, sample_size_v2=0, ppl=5.71, wps=3429.7, ups=0.57, wpb=6045.6, bsz=84, num_updates=5490, lr=9.79181e-06, gnorm=4.877, clip=100, loss_scale=16, train_wall=18, gb_free=11.6, wall=9718
2023-06-29 21:42:15 - progress_bar.py[line:272] - INFO: epoch 009:    230 / 660 loss=3.538, loss_v1=0, loss_v2=0, nll_loss=2.49, ntokens=6051.2, nsentences=84, sample_size=6051.2, sample_size_v1=0, sample_size_v2=0, ppl=5.62, wps=3460.9, ups=0.57, wpb=6051.2, bsz=84, num_updates=5500, lr=9.75151e-06, gnorm=5.021, clip=100, loss_scale=16, train_wall=17, gb_free=11.8, wall=9735
2023-06-29 21:42:32 - progress_bar.py[line:272] - INFO: epoch 009:    240 / 660 loss=3.511, loss_v1=0, loss_v2=0, nll_loss=2.462, ntokens=5836.2, nsentences=84, sample_size=5836.2, sample_size_v1=0, sample_size_v2=0, ppl=5.51, wps=3358.7, ups=0.58, wpb=5836.2, bsz=84, num_updates=5510, lr=9.71122e-06, gnorm=4.892, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=9753
2023-06-29 21:42:50 - progress_bar.py[line:272] - INFO: epoch 009:    250 / 660 loss=3.504, loss_v1=0, loss_v2=0, nll_loss=2.453, ntokens=6154.4, nsentences=84, sample_size=6154.4, sample_size_v1=0, sample_size_v2=0, ppl=5.48, wps=3522.3, ups=0.57, wpb=6154.4, bsz=84, num_updates=5520, lr=9.67092e-06, gnorm=5.214, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=9770
2023-06-29 21:43:08 - progress_bar.py[line:272] - INFO: epoch 009:    260 / 660 loss=3.492, loss_v1=0, loss_v2=0, nll_loss=2.44, ntokens=5924.9, nsentences=84, sample_size=5924.9, sample_size_v1=0, sample_size_v2=0, ppl=5.42, wps=3346, ups=0.56, wpb=5924.9, bsz=84, num_updates=5530, lr=9.63062e-06, gnorm=5.388, clip=100, loss_scale=16, train_wall=18, gb_free=12, wall=9788
2023-06-29 21:43:25 - progress_bar.py[line:272] - INFO: epoch 009:    270 / 660 loss=3.469, loss_v1=0, loss_v2=0, nll_loss=2.412, ntokens=5865.2, nsentences=84, sample_size=5865.2, sample_size_v1=0, sample_size_v2=0, ppl=5.32, wps=3330.2, ups=0.57, wpb=5865.2, bsz=84, num_updates=5540, lr=9.59033e-06, gnorm=5.053, clip=100, loss_scale=16, train_wall=18, gb_free=12, wall=9805
2023-06-29 21:43:43 - progress_bar.py[line:272] - INFO: epoch 009:    280 / 660 loss=3.501, loss_v1=0, loss_v2=0, nll_loss=2.449, ntokens=5965.3, nsentences=84, sample_size=5965.3, sample_size_v1=0, sample_size_v2=0, ppl=5.46, wps=3397.5, ups=0.57, wpb=5965.3, bsz=84, num_updates=5550, lr=9.55003e-06, gnorm=5.128, clip=100, loss_scale=16, train_wall=18, gb_free=11.8, wall=9823
2023-06-29 21:44:00 - progress_bar.py[line:272] - INFO: epoch 009:    290 / 660 loss=3.493, loss_v1=0, loss_v2=0, nll_loss=2.44, ntokens=5937.6, nsentences=84, sample_size=5937.6, sample_size_v1=0, sample_size_v2=0, ppl=5.43, wps=3406.1, ups=0.57, wpb=5937.6, bsz=84, num_updates=5560, lr=9.50974e-06, gnorm=5.138, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=9840
2023-06-29 21:44:18 - progress_bar.py[line:272] - INFO: epoch 009:    300 / 660 loss=3.51, loss_v1=0, loss_v2=0, nll_loss=2.459, ntokens=6163.1, nsentences=84, sample_size=6163.1, sample_size_v1=0, sample_size_v2=0, ppl=5.5, wps=3528.7, ups=0.57, wpb=6163.1, bsz=84, num_updates=5570, lr=9.46944e-06, gnorm=5.175, clip=100, loss_scale=16, train_wall=17, gb_free=12.2, wall=9858
2023-06-29 21:44:35 - progress_bar.py[line:272] - INFO: epoch 009:    310 / 660 loss=3.515, loss_v1=0, loss_v2=0, nll_loss=2.466, ntokens=6322, nsentences=84, sample_size=6322, sample_size_v1=0, sample_size_v2=0, ppl=5.52, wps=3616.7, ups=0.57, wpb=6322, bsz=84, num_updates=5580, lr=9.42915e-06, gnorm=5.203, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=9875
2023-06-29 21:44:53 - progress_bar.py[line:272] - INFO: epoch 009:    320 / 660 loss=3.493, loss_v1=0, loss_v2=0, nll_loss=2.441, ntokens=6182.4, nsentences=84, sample_size=6182.4, sample_size_v1=0, sample_size_v2=0, ppl=5.43, wps=3531.9, ups=0.57, wpb=6182.4, bsz=84, num_updates=5590, lr=9.38885e-06, gnorm=5.175, clip=100, loss_scale=16, train_wall=17, gb_free=11.7, wall=9893
2023-06-29 21:45:10 - progress_bar.py[line:272] - INFO: epoch 009:    330 / 660 loss=3.51, loss_v1=0, loss_v2=0, nll_loss=2.46, ntokens=6016.4, nsentences=84, sample_size=6016.4, sample_size_v1=0, sample_size_v2=0, ppl=5.5, wps=3443.4, ups=0.57, wpb=6016.4, bsz=84, num_updates=5600, lr=9.34856e-06, gnorm=5.491, clip=100, loss_scale=16, train_wall=17, gb_free=12.2, wall=9910
2023-06-29 21:45:28 - progress_bar.py[line:272] - INFO: epoch 009:    340 / 660 loss=3.496, loss_v1=0, loss_v2=0, nll_loss=2.444, ntokens=6052.2, nsentences=84, sample_size=6052.2, sample_size_v1=0, sample_size_v2=0, ppl=5.44, wps=3464.6, ups=0.57, wpb=6052.2, bsz=84, num_updates=5610, lr=9.30826e-06, gnorm=4.982, clip=100, loss_scale=16, train_wall=17, gb_free=11.8, wall=9928
2023-06-29 21:45:45 - progress_bar.py[line:272] - INFO: epoch 009:    350 / 660 loss=3.467, loss_v1=0, loss_v2=0, nll_loss=2.41, ntokens=5966.8, nsentences=84, sample_size=5966.8, sample_size_v1=0, sample_size_v2=0, ppl=5.32, wps=3419.6, ups=0.57, wpb=5966.8, bsz=84, num_updates=5620, lr=9.26797e-06, gnorm=5.253, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=9945
2023-06-29 21:46:03 - progress_bar.py[line:272] - INFO: epoch 009:    360 / 660 loss=3.466, loss_v1=0, loss_v2=0, nll_loss=2.411, ntokens=5964.7, nsentences=84, sample_size=5964.7, sample_size_v1=0, sample_size_v2=0, ppl=5.32, wps=3427.6, ups=0.57, wpb=5964.7, bsz=84, num_updates=5630, lr=9.22767e-06, gnorm=5.307, clip=100, loss_scale=16, train_wall=17, gb_free=12.1, wall=9963
2023-06-29 21:46:20 - progress_bar.py[line:272] - INFO: epoch 009:    370 / 660 loss=3.465, loss_v1=0, loss_v2=0, nll_loss=2.409, ntokens=5813.3, nsentences=84, sample_size=5813.3, sample_size_v1=0, sample_size_v2=0, ppl=5.31, wps=3344, ups=0.58, wpb=5813.3, bsz=84, num_updates=5640, lr=9.18737e-06, gnorm=5.757, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=9980
2023-06-29 21:46:37 - progress_bar.py[line:272] - INFO: epoch 009:    380 / 660 loss=3.475, loss_v1=0, loss_v2=0, nll_loss=2.42, ntokens=5967.6, nsentences=84, sample_size=5967.6, sample_size_v1=0, sample_size_v2=0, ppl=5.35, wps=3431.2, ups=0.57, wpb=5967.6, bsz=84, num_updates=5650, lr=9.14708e-06, gnorm=5.123, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=9997
2023-06-29 21:46:55 - progress_bar.py[line:272] - INFO: epoch 009:    390 / 660 loss=3.469, loss_v1=0, loss_v2=0, nll_loss=2.414, ntokens=5637.5, nsentences=84, sample_size=5637.5, sample_size_v1=0, sample_size_v2=0, ppl=5.33, wps=3259.5, ups=0.58, wpb=5637.5, bsz=84, num_updates=5660, lr=9.10678e-06, gnorm=5.323, clip=100, loss_scale=16, train_wall=17, gb_free=12.4, wall=10015
2023-06-29 21:47:12 - progress_bar.py[line:272] - INFO: epoch 009:    400 / 660 loss=3.464, loss_v1=0, loss_v2=0, nll_loss=2.408, ntokens=5782.9, nsentences=84, sample_size=5782.9, sample_size_v1=0, sample_size_v2=0, ppl=5.31, wps=3338.1, ups=0.58, wpb=5782.9, bsz=84, num_updates=5670, lr=9.06649e-06, gnorm=5.248, clip=100, loss_scale=16, train_wall=17, gb_free=12.2, wall=10032
2023-06-29 21:47:29 - progress_bar.py[line:272] - INFO: epoch 009:    410 / 660 loss=3.473, loss_v1=0, loss_v2=0, nll_loss=2.419, ntokens=5746.6, nsentences=84, sample_size=5746.6, sample_size_v1=0, sample_size_v2=0, ppl=5.35, wps=3328.6, ups=0.58, wpb=5746.6, bsz=84, num_updates=5680, lr=9.02619e-06, gnorm=5.637, clip=100, loss_scale=32, train_wall=17, gb_free=12, wall=10049
2023-06-29 21:47:47 - progress_bar.py[line:272] - INFO: epoch 009:    420 / 660 loss=3.495, loss_v1=0, loss_v2=0, nll_loss=2.442, ntokens=5693.7, nsentences=84, sample_size=5693.7, sample_size_v1=0, sample_size_v2=0, ppl=5.44, wps=3281.6, ups=0.58, wpb=5693.7, bsz=84, num_updates=5690, lr=8.9859e-06, gnorm=5.22, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=10067
2023-06-29 21:48:04 - progress_bar.py[line:272] - INFO: epoch 009:    430 / 660 loss=3.497, loss_v1=0, loss_v2=0, nll_loss=2.444, ntokens=5838.8, nsentences=84, sample_size=5838.8, sample_size_v1=0, sample_size_v2=0, ppl=5.44, wps=3360.8, ups=0.58, wpb=5838.8, bsz=84, num_updates=5700, lr=8.9456e-06, gnorm=5.111, clip=100, loss_scale=32, train_wall=17, gb_free=12, wall=10084
2023-06-29 21:48:18 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-06-29 21:48:23 - progress_bar.py[line:272] - INFO: epoch 009:    441 / 660 loss=3.528, loss_v1=0, loss_v2=0, nll_loss=2.479, ntokens=6133.5, nsentences=84, sample_size=6133.5, sample_size_v1=0, sample_size_v2=0, ppl=5.57, wps=3221.2, ups=0.53, wpb=6133.5, bsz=84, num_updates=5710, lr=8.90531e-06, gnorm=5.576, clip=100, loss_scale=16, train_wall=19, gb_free=11.9, wall=10103
2023-06-29 21:48:40 - progress_bar.py[line:272] - INFO: epoch 009:    451 / 660 loss=3.483, loss_v1=0, loss_v2=0, nll_loss=2.428, ntokens=5685.8, nsentences=84, sample_size=5685.8, sample_size_v1=0, sample_size_v2=0, ppl=5.38, wps=3283, ups=0.58, wpb=5685.8, bsz=84, num_updates=5720, lr=8.86501e-06, gnorm=5.413, clip=100, loss_scale=16, train_wall=17, gb_free=12.7, wall=10120
2023-06-29 21:48:58 - progress_bar.py[line:272] - INFO: epoch 009:    461 / 660 loss=3.504, loss_v1=0, loss_v2=0, nll_loss=2.454, ntokens=5960.4, nsentences=84, sample_size=5960.4, sample_size_v1=0, sample_size_v2=0, ppl=5.48, wps=3425.9, ups=0.57, wpb=5960.4, bsz=84, num_updates=5730, lr=8.82471e-06, gnorm=5.546, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=10138
2023-06-29 21:49:15 - progress_bar.py[line:272] - INFO: epoch 009:    471 / 660 loss=3.513, loss_v1=0, loss_v2=0, nll_loss=2.462, ntokens=6051.8, nsentences=84, sample_size=6051.8, sample_size_v1=0, sample_size_v2=0, ppl=5.51, wps=3473.1, ups=0.57, wpb=6051.8, bsz=84, num_updates=5740, lr=8.78442e-06, gnorm=4.88, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=10155
2023-06-29 21:49:33 - progress_bar.py[line:272] - INFO: epoch 009:    481 / 660 loss=3.484, loss_v1=0, loss_v2=0, nll_loss=2.431, ntokens=5698.5, nsentences=84, sample_size=5698.5, sample_size_v1=0, sample_size_v2=0, ppl=5.39, wps=3280.6, ups=0.58, wpb=5698.5, bsz=84, num_updates=5750, lr=8.74412e-06, gnorm=5.361, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=10173
2023-06-29 21:49:50 - progress_bar.py[line:272] - INFO: epoch 009:    491 / 660 loss=3.477, loss_v1=0, loss_v2=0, nll_loss=2.422, ntokens=5753.5, nsentences=84, sample_size=5753.5, sample_size_v1=0, sample_size_v2=0, ppl=5.36, wps=3319.7, ups=0.58, wpb=5753.5, bsz=84, num_updates=5760, lr=8.70383e-06, gnorm=5.218, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=10190
2023-06-29 21:50:07 - progress_bar.py[line:272] - INFO: epoch 009:    501 / 660 loss=3.495, loss_v1=0, loss_v2=0, nll_loss=2.442, ntokens=5806.7, nsentences=84, sample_size=5806.7, sample_size_v1=0, sample_size_v2=0, ppl=5.43, wps=3311.9, ups=0.57, wpb=5806.7, bsz=84, num_updates=5770, lr=8.66353e-06, gnorm=5.059, clip=100, loss_scale=16, train_wall=17, gb_free=12.2, wall=10208
2023-06-29 21:50:25 - progress_bar.py[line:272] - INFO: epoch 009:    511 / 660 loss=3.486, loss_v1=0, loss_v2=0, nll_loss=2.433, ntokens=5855.9, nsentences=84, sample_size=5855.9, sample_size_v1=0, sample_size_v2=0, ppl=5.4, wps=3365.2, ups=0.57, wpb=5855.9, bsz=84, num_updates=5780, lr=8.62324e-06, gnorm=5.322, clip=100, loss_scale=16, train_wall=17, gb_free=11.8, wall=10225
2023-06-29 21:50:42 - progress_bar.py[line:272] - INFO: epoch 009:    521 / 660 loss=3.508, loss_v1=0, loss_v2=0, nll_loss=2.457, ntokens=5933.8, nsentences=84, sample_size=5933.8, sample_size_v1=0, sample_size_v2=0, ppl=5.49, wps=3421.3, ups=0.58, wpb=5933.8, bsz=84, num_updates=5790, lr=8.58294e-06, gnorm=5.038, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=10242
2023-06-29 21:51:00 - progress_bar.py[line:272] - INFO: epoch 009:    531 / 660 loss=3.498, loss_v1=0, loss_v2=0, nll_loss=2.447, ntokens=6033.6, nsentences=84, sample_size=6033.6, sample_size_v1=0, sample_size_v2=0, ppl=5.45, wps=3465, ups=0.57, wpb=6033.6, bsz=84, num_updates=5800, lr=8.54265e-06, gnorm=5.159, clip=100, loss_scale=16, train_wall=17, gb_free=12.1, wall=10260
2023-06-29 21:51:17 - progress_bar.py[line:272] - INFO: epoch 009:    541 / 660 loss=3.517, loss_v1=0, loss_v2=0, nll_loss=2.467, ntokens=6136, nsentences=84, sample_size=6136, sample_size_v1=0, sample_size_v2=0, ppl=5.53, wps=3511.6, ups=0.57, wpb=6136, bsz=84, num_updates=5810, lr=8.50235e-06, gnorm=5.022, clip=100, loss_scale=16, train_wall=17, gb_free=11.8, wall=10277
2023-06-29 21:51:35 - progress_bar.py[line:272] - INFO: epoch 009:    551 / 660 loss=3.49, loss_v1=0, loss_v2=0, nll_loss=2.436, ntokens=5977, nsentences=84, sample_size=5977, sample_size_v1=0, sample_size_v2=0, ppl=5.41, wps=3414.5, ups=0.57, wpb=5977, bsz=84, num_updates=5820, lr=8.46206e-06, gnorm=5.217, clip=100, loss_scale=16, train_wall=17, gb_free=12.1, wall=10295
2023-06-29 21:51:52 - progress_bar.py[line:272] - INFO: epoch 009:    561 / 660 loss=3.486, loss_v1=0, loss_v2=0, nll_loss=2.432, ntokens=6128.9, nsentences=84, sample_size=6128.9, sample_size_v1=0, sample_size_v2=0, ppl=5.4, wps=3519.7, ups=0.57, wpb=6128.9, bsz=84, num_updates=5830, lr=8.42176e-06, gnorm=5.213, clip=100, loss_scale=16, train_wall=17, gb_free=11.6, wall=10312
2023-06-29 21:52:09 - progress_bar.py[line:272] - INFO: epoch 009:    571 / 660 loss=3.45, loss_v1=0, loss_v2=0, nll_loss=2.391, ntokens=5447.4, nsentences=84, sample_size=5447.4, sample_size_v1=0, sample_size_v2=0, ppl=5.25, wps=3148.2, ups=0.58, wpb=5447.4, bsz=84, num_updates=5840, lr=8.38146e-06, gnorm=5.54, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=10329
2023-06-29 21:52:27 - progress_bar.py[line:272] - INFO: epoch 009:    581 / 660 loss=3.494, loss_v1=0, loss_v2=0, nll_loss=2.441, ntokens=5973.8, nsentences=84, sample_size=5973.8, sample_size_v1=0, sample_size_v2=0, ppl=5.43, wps=3456.6, ups=0.58, wpb=5973.8, bsz=84, num_updates=5850, lr=8.34117e-06, gnorm=5.167, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=10347
2023-06-29 21:52:44 - progress_bar.py[line:272] - INFO: epoch 009:    591 / 660 loss=3.468, loss_v1=0, loss_v2=0, nll_loss=2.413, ntokens=5855.3, nsentences=84, sample_size=5855.3, sample_size_v1=0, sample_size_v2=0, ppl=5.33, wps=3348, ups=0.57, wpb=5855.3, bsz=84, num_updates=5860, lr=8.30087e-06, gnorm=5.263, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=10364
2023-06-29 21:53:01 - progress_bar.py[line:272] - INFO: epoch 009:    601 / 660 loss=3.469, loss_v1=0, loss_v2=0, nll_loss=2.411, ntokens=5963.4, nsentences=84, sample_size=5963.4, sample_size_v1=0, sample_size_v2=0, ppl=5.32, wps=3421.1, ups=0.57, wpb=5963.4, bsz=84, num_updates=5870, lr=8.26058e-06, gnorm=5.073, clip=100, loss_scale=16, train_wall=17, gb_free=12.3, wall=10382
2023-06-29 21:53:19 - progress_bar.py[line:272] - INFO: epoch 009:    611 / 660 loss=3.489, loss_v1=0, loss_v2=0, nll_loss=2.436, ntokens=5971.9, nsentences=84, sample_size=5971.9, sample_size_v1=0, sample_size_v2=0, ppl=5.41, wps=3433.6, ups=0.57, wpb=5971.9, bsz=84, num_updates=5880, lr=8.22028e-06, gnorm=5.091, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=10399
2023-06-29 21:53:36 - progress_bar.py[line:272] - INFO: epoch 009:    621 / 660 loss=3.514, loss_v1=0, loss_v2=0, nll_loss=2.463, ntokens=6071.1, nsentences=84, sample_size=6071.1, sample_size_v1=0, sample_size_v2=0, ppl=5.51, wps=3467.4, ups=0.57, wpb=6071.1, bsz=84, num_updates=5890, lr=8.17999e-06, gnorm=5.397, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=10417
2023-06-29 21:53:54 - progress_bar.py[line:272] - INFO: epoch 009:    631 / 660 loss=3.496, loss_v1=0, loss_v2=0, nll_loss=2.442, ntokens=5969, nsentences=84, sample_size=5969, sample_size_v1=0, sample_size_v2=0, ppl=5.44, wps=3427, ups=0.57, wpb=5969, bsz=84, num_updates=5900, lr=8.13969e-06, gnorm=4.889, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=10434
2023-06-29 21:54:11 - progress_bar.py[line:272] - INFO: epoch 009:    641 / 660 loss=3.506, loss_v1=0, loss_v2=0, nll_loss=2.456, ntokens=6010.2, nsentences=84, sample_size=6010.2, sample_size_v1=0, sample_size_v2=0, ppl=5.49, wps=3451.1, ups=0.57, wpb=6010.2, bsz=84, num_updates=5910, lr=8.0994e-06, gnorm=5.087, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=10451
2023-06-29 21:54:29 - progress_bar.py[line:272] - INFO: epoch 009:    651 / 660 loss=3.498, loss_v1=0, loss_v2=0, nll_loss=2.445, ntokens=5857.1, nsentences=84, sample_size=5857.1, sample_size_v1=0, sample_size_v2=0, ppl=5.45, wps=3350.7, ups=0.57, wpb=5857.1, bsz=84, num_updates=5920, lr=8.0591e-06, gnorm=5.552, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=10469
2023-06-29 21:54:44 - train.py[line:332] - INFO: end of epoch 9 (average epoch stats below)
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
2023-06-29 21:54:44 - progress_bar.py[line:282] - INFO: epoch 009 | loss 3.503 | loss_v1 0 | loss_v2 0 | nll_loss 2.452 | ntokens 5903.93 | nsentences 83.95 | sample_size 5903.93 | sample_size_v1 0 | sample_size_v2 0 | ppl 5.47 | wps 3369.4 | ups 0.57 | wpb 5903.9 | bsz 83.9 | num_updates 5929 | lr 8.02283e-06 | gnorm 5.249 | clip 100 | loss_scale 16 | train_wall 1150 | gb_free 12.3 | wall 10484
2023-06-29 21:54:44 - trainer.py[line:639] - INFO: loading train data for epoch 10
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 row count 18466 total row count 55400
slice_id 2 seek offset 36934
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 row count 18467 total row count 55400
slice_id 1 seek offset 18467
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 row count 18467 total row count 55400
slice_id 0 seek offset 0
2023-06-29 21:54:46 - trainer.py[line:703] - INFO: begin training epoch 10
2023-06-29 21:54:46 - train.py[line:305] - INFO: Start iterating over samples
2023-06-29 21:54:48 - progress_bar.py[line:272] - INFO: epoch 010:      1 / 660 loss=3.512, loss_v1=0, loss_v2=0, nll_loss=2.46, ntokens=5824.7, nsentences=81.9, sample_size=5824.7, sample_size_v1=0, sample_size_v2=0, ppl=5.5, wps=3022.9, ups=0.52, wpb=5824.7, bsz=81.9, num_updates=5930, lr=8.0188e-06, gnorm=5.479, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=10488
2023-06-29 21:55:05 - progress_bar.py[line:272] - INFO: epoch 010:     11 / 660 loss=3.591, loss_v1=0, loss_v2=0, nll_loss=2.547, ntokens=5821.9, nsentences=84, sample_size=5821.9, sample_size_v1=0, sample_size_v2=0, ppl=5.84, wps=3325.1, ups=0.57, wpb=5821.9, bsz=84, num_updates=5940, lr=7.97851e-06, gnorm=5.864, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=10506
2023-06-29 21:55:23 - progress_bar.py[line:272] - INFO: epoch 010:     21 / 660 loss=3.531, loss_v1=0, loss_v2=0, nll_loss=2.48, ntokens=5817.2, nsentences=84, sample_size=5817.2, sample_size_v1=0, sample_size_v2=0, ppl=5.58, wps=3336, ups=0.57, wpb=5817.2, bsz=84, num_updates=5950, lr=7.93821e-06, gnorm=5.702, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=10523
2023-06-29 21:55:40 - progress_bar.py[line:272] - INFO: epoch 010:     31 / 660 loss=3.462, loss_v1=0, loss_v2=0, nll_loss=2.407, ntokens=5377, nsentences=84, sample_size=5377, sample_size_v1=0, sample_size_v2=0, ppl=5.3, wps=3115.5, ups=0.58, wpb=5377, bsz=84, num_updates=5960, lr=7.89792e-06, gnorm=5.524, clip=100, loss_scale=16, train_wall=17, gb_free=11.7, wall=10540
2023-06-29 21:55:58 - progress_bar.py[line:272] - INFO: epoch 010:     41 / 660 loss=3.429, loss_v1=0, loss_v2=0, nll_loss=2.368, ntokens=5746.4, nsentences=84, sample_size=5746.4, sample_size_v1=0, sample_size_v2=0, ppl=5.16, wps=3281.1, ups=0.57, wpb=5746.4, bsz=84, num_updates=5970, lr=7.85762e-06, gnorm=5.63, clip=100, loss_scale=16, train_wall=17, gb_free=11.6, wall=10558
2023-06-29 21:56:15 - progress_bar.py[line:272] - INFO: epoch 010:     51 / 660 loss=3.428, loss_v1=0, loss_v2=0, nll_loss=2.365, ntokens=5780.2, nsentences=84, sample_size=5780.2, sample_size_v1=0, sample_size_v2=0, ppl=5.15, wps=3295.5, ups=0.57, wpb=5780.2, bsz=84, num_updates=5980, lr=7.81733e-06, gnorm=4.868, clip=100, loss_scale=16, train_wall=18, gb_free=11.9, wall=10575
2023-06-29 21:56:33 - progress_bar.py[line:272] - INFO: epoch 010:     61 / 660 loss=3.458, loss_v1=0, loss_v2=0, nll_loss=2.401, ntokens=5711.5, nsentences=84, sample_size=5711.5, sample_size_v1=0, sample_size_v2=0, ppl=5.28, wps=3268.2, ups=0.57, wpb=5711.5, bsz=84, num_updates=5990, lr=7.77703e-06, gnorm=5.452, clip=100, loss_scale=16, train_wall=17, gb_free=12.1, wall=10593
2023-06-29 21:56:50 - progress_bar.py[line:272] - INFO: epoch 010:     71 / 660 loss=3.419, loss_v1=0, loss_v2=0, nll_loss=2.355, ntokens=5883, nsentences=84, sample_size=5883, sample_size_v1=0, sample_size_v2=0, ppl=5.12, wps=3354.9, ups=0.57, wpb=5883, bsz=84, num_updates=6000, lr=7.73674e-06, gnorm=5.999, clip=100, loss_scale=16, train_wall=18, gb_free=11.8, wall=10610
2023-06-29 21:57:08 - progress_bar.py[line:272] - INFO: epoch 010:     81 / 660 loss=3.465, loss_v1=0, loss_v2=0, nll_loss=2.409, ntokens=6173.8, nsentences=84, sample_size=6173.8, sample_size_v1=0, sample_size_v2=0, ppl=5.31, wps=3477.4, ups=0.56, wpb=6173.8, bsz=84, num_updates=6010, lr=7.69644e-06, gnorm=5.067, clip=100, loss_scale=16, train_wall=18, gb_free=11.6, wall=10628
2023-06-29 21:57:26 - progress_bar.py[line:272] - INFO: epoch 010:     91 / 660 loss=3.455, loss_v1=0, loss_v2=0, nll_loss=2.397, ntokens=5898.8, nsentences=84, sample_size=5898.8, sample_size_v1=0, sample_size_v2=0, ppl=5.27, wps=3323.6, ups=0.56, wpb=5898.8, bsz=84, num_updates=6020, lr=7.65615e-06, gnorm=5.188, clip=100, loss_scale=16, train_wall=18, gb_free=11.7, wall=10646
2023-06-29 21:57:43 - progress_bar.py[line:272] - INFO: epoch 010:    101 / 660 loss=3.43, loss_v1=0, loss_v2=0, nll_loss=2.369, ntokens=5672.9, nsentences=84, sample_size=5672.9, sample_size_v1=0, sample_size_v2=0, ppl=5.17, wps=3240.3, ups=0.57, wpb=5672.9, bsz=84, num_updates=6030, lr=7.61585e-06, gnorm=5.336, clip=100, loss_scale=16, train_wall=17, gb_free=11.7, wall=10663
2023-06-29 21:58:01 - progress_bar.py[line:272] - INFO: epoch 010:    111 / 660 loss=3.448, loss_v1=0, loss_v2=0, nll_loss=2.388, ntokens=5905.6, nsentences=84, sample_size=5905.6, sample_size_v1=0, sample_size_v2=0, ppl=5.23, wps=3373.1, ups=0.57, wpb=5905.6, bsz=84, num_updates=6040, lr=7.57555e-06, gnorm=5.236, clip=100, loss_scale=16, train_wall=17, gb_free=11.5, wall=10681
2023-06-29 21:58:18 - progress_bar.py[line:272] - INFO: epoch 010:    121 / 660 loss=3.46, loss_v1=0, loss_v2=0, nll_loss=2.404, ntokens=5607.8, nsentences=84, sample_size=5607.8, sample_size_v1=0, sample_size_v2=0, ppl=5.29, wps=3243.5, ups=0.58, wpb=5607.8, bsz=84, num_updates=6050, lr=7.53526e-06, gnorm=5.393, clip=100, loss_scale=16, train_wall=17, gb_free=11.7, wall=10698
2023-06-29 21:58:36 - progress_bar.py[line:272] - INFO: epoch 010:    131 / 660 loss=3.503, loss_v1=0, loss_v2=0, nll_loss=2.452, ntokens=5783.5, nsentences=84, sample_size=5783.5, sample_size_v1=0, sample_size_v2=0, ppl=5.47, wps=3313.6, ups=0.57, wpb=5783.5, bsz=84, num_updates=6060, lr=7.49496e-06, gnorm=4.957, clip=100, loss_scale=16, train_wall=17, gb_free=11.8, wall=10716
2023-06-29 21:58:53 - progress_bar.py[line:272] - INFO: epoch 010:    141 / 660 loss=3.539, loss_v1=0, loss_v2=0, nll_loss=2.49, ntokens=6005.5, nsentences=84, sample_size=6005.5, sample_size_v1=0, sample_size_v2=0, ppl=5.62, wps=3401, ups=0.57, wpb=6005.5, bsz=84, num_updates=6070, lr=7.45467e-06, gnorm=5.262, clip=100, loss_scale=16, train_wall=18, gb_free=11.7, wall=10733
2023-06-29 21:59:11 - progress_bar.py[line:272] - INFO: epoch 010:    151 / 660 loss=3.514, loss_v1=0, loss_v2=0, nll_loss=2.462, ntokens=5948, nsentences=84, sample_size=5948, sample_size_v1=0, sample_size_v2=0, ppl=5.51, wps=3374.4, ups=0.57, wpb=5948, bsz=84, num_updates=6080, lr=7.41437e-06, gnorm=5.324, clip=100, loss_scale=16, train_wall=18, gb_free=11.8, wall=10751
2023-06-29 21:59:29 - progress_bar.py[line:272] - INFO: epoch 010:    161 / 660 loss=3.527, loss_v1=0, loss_v2=0, nll_loss=2.479, ntokens=6047.1, nsentences=84, sample_size=6047.1, sample_size_v1=0, sample_size_v2=0, ppl=5.57, wps=3416.4, ups=0.56, wpb=6047.1, bsz=84, num_updates=6090, lr=7.37408e-06, gnorm=4.836, clip=100, loss_scale=16, train_wall=18, gb_free=11.9, wall=10769
2023-06-29 21:59:46 - progress_bar.py[line:272] - INFO: epoch 010:    171 / 660 loss=3.49, loss_v1=0, loss_v2=0, nll_loss=2.438, ntokens=5920, nsentences=84, sample_size=5920, sample_size_v1=0, sample_size_v2=0, ppl=5.42, wps=3331.1, ups=0.56, wpb=5920, bsz=84, num_updates=6100, lr=7.33378e-06, gnorm=5.169, clip=100, loss_scale=16, train_wall=18, gb_free=12, wall=10787
2023-06-29 22:00:04 - progress_bar.py[line:272] - INFO: epoch 010:    181 / 660 loss=3.551, loss_v1=0, loss_v2=0, nll_loss=2.505, ntokens=5945.9, nsentences=84, sample_size=5945.9, sample_size_v1=0, sample_size_v2=0, ppl=5.68, wps=3367.6, ups=0.57, wpb=5945.9, bsz=84, num_updates=6110, lr=7.29349e-06, gnorm=4.968, clip=100, loss_scale=16, train_wall=18, gb_free=11.9, wall=10804
2023-06-29 22:00:22 - progress_bar.py[line:272] - INFO: epoch 010:    191 / 660 loss=3.48, loss_v1=0, loss_v2=0, nll_loss=2.426, ntokens=5911.9, nsentences=84, sample_size=5911.9, sample_size_v1=0, sample_size_v2=0, ppl=5.37, wps=3355.3, ups=0.57, wpb=5911.9, bsz=84, num_updates=6120, lr=7.25319e-06, gnorm=4.859, clip=100, loss_scale=16, train_wall=18, gb_free=11.6, wall=10822
2023-06-29 22:00:39 - progress_bar.py[line:272] - INFO: epoch 010:    201 / 660 loss=3.483, loss_v1=0, loss_v2=0, nll_loss=2.429, ntokens=5723.1, nsentences=84, sample_size=5723.1, sample_size_v1=0, sample_size_v2=0, ppl=5.39, wps=3252.5, ups=0.57, wpb=5723.1, bsz=84, num_updates=6130, lr=7.21289e-06, gnorm=5.013, clip=100, loss_scale=16, train_wall=18, gb_free=12, wall=10839
2023-06-29 22:00:57 - progress_bar.py[line:272] - INFO: epoch 010:    211 / 660 loss=3.515, loss_v1=0, loss_v2=0, nll_loss=2.465, ntokens=6039.9, nsentences=84, sample_size=6039.9, sample_size_v1=0, sample_size_v2=0, ppl=5.52, wps=3434.4, ups=0.57, wpb=6039.9, bsz=84, num_updates=6140, lr=7.1726e-06, gnorm=5.452, clip=100, loss_scale=16, train_wall=18, gb_free=12, wall=10857
2023-06-29 22:01:14 - progress_bar.py[line:272] - INFO: epoch 010:    221 / 660 loss=3.517, loss_v1=0, loss_v2=0, nll_loss=2.467, ntokens=6035.2, nsentences=84, sample_size=6035.2, sample_size_v1=0, sample_size_v2=0, ppl=5.53, wps=3422.6, ups=0.57, wpb=6035.2, bsz=84, num_updates=6150, lr=7.1323e-06, gnorm=5.185, clip=100, loss_scale=16, train_wall=18, gb_free=11.6, wall=10875
2023-06-29 22:01:32 - progress_bar.py[line:272] - INFO: epoch 010:    231 / 660 loss=3.505, loss_v1=0, loss_v2=0, nll_loss=2.452, ntokens=6081.8, nsentences=84, sample_size=6081.8, sample_size_v1=0, sample_size_v2=0, ppl=5.47, wps=3478.7, ups=0.57, wpb=6081.8, bsz=84, num_updates=6160, lr=7.09201e-06, gnorm=5.207, clip=100, loss_scale=16, train_wall=17, gb_free=11.8, wall=10892
2023-06-29 22:01:49 - progress_bar.py[line:272] - INFO: epoch 010:    241 / 660 loss=3.461, loss_v1=0, loss_v2=0, nll_loss=2.405, ntokens=5768.1, nsentences=84, sample_size=5768.1, sample_size_v1=0, sample_size_v2=0, ppl=5.3, wps=3304.9, ups=0.57, wpb=5768.1, bsz=84, num_updates=6170, lr=7.05171e-06, gnorm=5.231, clip=100, loss_scale=16, train_wall=17, gb_free=11.7, wall=10910
2023-06-29 22:02:07 - progress_bar.py[line:272] - INFO: epoch 010:    251 / 660 loss=3.463, loss_v1=0, loss_v2=0, nll_loss=2.406, ntokens=6170.8, nsentences=84, sample_size=6170.8, sample_size_v1=0, sample_size_v2=0, ppl=5.3, wps=3547.9, ups=0.57, wpb=6170.8, bsz=84, num_updates=6180, lr=7.01142e-06, gnorm=5.035, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=10927
2023-06-29 22:02:24 - progress_bar.py[line:272] - INFO: epoch 010:    261 / 660 loss=3.438, loss_v1=0, loss_v2=0, nll_loss=2.378, ntokens=5904.5, nsentences=84, sample_size=5904.5, sample_size_v1=0, sample_size_v2=0, ppl=5.2, wps=3381.5, ups=0.57, wpb=5904.5, bsz=84, num_updates=6190, lr=6.97112e-06, gnorm=5.225, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=10944
2023-06-29 22:02:42 - progress_bar.py[line:272] - INFO: epoch 010:    271 / 660 loss=3.445, loss_v1=0, loss_v2=0, nll_loss=2.386, ntokens=5818.3, nsentences=84, sample_size=5818.3, sample_size_v1=0, sample_size_v2=0, ppl=5.23, wps=3327.9, ups=0.57, wpb=5818.3, bsz=84, num_updates=6200, lr=6.93083e-06, gnorm=5.118, clip=100, loss_scale=16, train_wall=17, gb_free=11.6, wall=10962
2023-06-29 22:02:59 - progress_bar.py[line:272] - INFO: epoch 010:    281 / 660 loss=3.479, loss_v1=0, loss_v2=0, nll_loss=2.424, ntokens=5976.1, nsentences=84, sample_size=5976.1, sample_size_v1=0, sample_size_v2=0, ppl=5.37, wps=3406.8, ups=0.57, wpb=5976.1, bsz=84, num_updates=6210, lr=6.89053e-06, gnorm=5.256, clip=100, loss_scale=16, train_wall=18, gb_free=12.1, wall=10979
2023-06-29 22:03:17 - progress_bar.py[line:272] - INFO: epoch 010:    291 / 660 loss=3.465, loss_v1=0, loss_v2=0, nll_loss=2.41, ntokens=6063, nsentences=84, sample_size=6063, sample_size_v1=0, sample_size_v2=0, ppl=5.31, wps=3474.5, ups=0.57, wpb=6063, bsz=84, num_updates=6220, lr=6.85024e-06, gnorm=5.063, clip=100, loss_scale=32, train_wall=17, gb_free=12, wall=10997
2023-06-29 22:03:34 - progress_bar.py[line:272] - INFO: epoch 010:    301 / 660 loss=3.473, loss_v1=0, loss_v2=0, nll_loss=2.417, ntokens=6200.8, nsentences=84, sample_size=6200.8, sample_size_v1=0, sample_size_v2=0, ppl=5.34, wps=3545.1, ups=0.57, wpb=6200.8, bsz=84, num_updates=6230, lr=6.80994e-06, gnorm=5.231, clip=100, loss_scale=32, train_wall=17, gb_free=11.8, wall=11014
2023-06-29 22:03:52 - progress_bar.py[line:272] - INFO: epoch 010:    311 / 660 loss=3.471, loss_v1=0, loss_v2=0, nll_loss=2.415, ntokens=6258.8, nsentences=84, sample_size=6258.8, sample_size_v1=0, sample_size_v2=0, ppl=5.33, wps=3570.3, ups=0.57, wpb=6258.8, bsz=84, num_updates=6240, lr=6.76964e-06, gnorm=5.468, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=11032
2023-06-29 22:04:09 - progress_bar.py[line:272] - INFO: epoch 010:    321 / 660 loss=3.461, loss_v1=0, loss_v2=0, nll_loss=2.405, ntokens=6168.3, nsentences=84, sample_size=6168.3, sample_size_v1=0, sample_size_v2=0, ppl=5.3, wps=3522.4, ups=0.57, wpb=6168.3, bsz=84, num_updates=6250, lr=6.72935e-06, gnorm=5.11, clip=100, loss_scale=32, train_wall=17, gb_free=12.2, wall=11049
2023-06-29 22:04:27 - progress_bar.py[line:272] - INFO: epoch 010:    331 / 660 loss=3.479, loss_v1=0, loss_v2=0, nll_loss=2.424, ntokens=6006.3, nsentences=84, sample_size=6006.3, sample_size_v1=0, sample_size_v2=0, ppl=5.37, wps=3442.5, ups=0.57, wpb=6006.3, bsz=84, num_updates=6260, lr=6.68905e-06, gnorm=5.017, clip=100, loss_scale=32, train_wall=17, gb_free=12.1, wall=11067
2023-06-29 22:04:44 - progress_bar.py[line:272] - INFO: epoch 010:    341 / 660 loss=3.454, loss_v1=0, loss_v2=0, nll_loss=2.396, ntokens=6022.4, nsentences=84, sample_size=6022.4, sample_size_v1=0, sample_size_v2=0, ppl=5.26, wps=3447, ups=0.57, wpb=6022.4, bsz=84, num_updates=6270, lr=6.64876e-06, gnorm=4.898, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=11084
2023-06-29 22:05:02 - progress_bar.py[line:272] - INFO: epoch 010:    351 / 660 loss=3.44, loss_v1=0, loss_v2=0, nll_loss=2.38, ntokens=5985.5, nsentences=84, sample_size=5985.5, sample_size_v1=0, sample_size_v2=0, ppl=5.2, wps=3431.6, ups=0.57, wpb=5985.5, bsz=84, num_updates=6280, lr=6.60846e-06, gnorm=5.129, clip=100, loss_scale=32, train_wall=17, gb_free=12, wall=11102
2023-06-29 22:05:19 - progress_bar.py[line:272] - INFO: epoch 010:    361 / 660 loss=3.459, loss_v1=0, loss_v2=0, nll_loss=2.401, ntokens=6006.1, nsentences=84, sample_size=6006.1, sample_size_v1=0, sample_size_v2=0, ppl=5.28, wps=3441.3, ups=0.57, wpb=6006.1, bsz=84, num_updates=6290, lr=6.56817e-06, gnorm=5.235, clip=100, loss_scale=32, train_wall=17, gb_free=11.8, wall=11119
2023-06-29 22:05:36 - progress_bar.py[line:272] - INFO: epoch 010:    371 / 660 loss=3.421, loss_v1=0, loss_v2=0, nll_loss=2.36, ntokens=5760.9, nsentences=84, sample_size=5760.9, sample_size_v1=0, sample_size_v2=0, ppl=5.13, wps=3328.6, ups=0.58, wpb=5760.9, bsz=84, num_updates=6300, lr=6.52787e-06, gnorm=5.197, clip=100, loss_scale=32, train_wall=17, gb_free=12.3, wall=11137
2023-06-29 22:05:54 - progress_bar.py[line:272] - INFO: epoch 010:    381 / 660 loss=3.444, loss_v1=0, loss_v2=0, nll_loss=2.384, ntokens=5928.4, nsentences=84, sample_size=5928.4, sample_size_v1=0, sample_size_v2=0, ppl=5.22, wps=3410.3, ups=0.58, wpb=5928.4, bsz=84, num_updates=6310, lr=6.48758e-06, gnorm=5.459, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=11154
2023-06-29 22:06:11 - progress_bar.py[line:272] - INFO: epoch 010:    391 / 660 loss=3.452, loss_v1=0, loss_v2=0, nll_loss=2.394, ntokens=5686.2, nsentences=84, sample_size=5686.2, sample_size_v1=0, sample_size_v2=0, ppl=5.26, wps=3281, ups=0.58, wpb=5686.2, bsz=84, num_updates=6320, lr=6.44728e-06, gnorm=5.426, clip=100, loss_scale=32, train_wall=17, gb_free=12.4, wall=11171
2023-06-29 22:06:29 - progress_bar.py[line:272] - INFO: epoch 010:    401 / 660 loss=3.425, loss_v1=0, loss_v2=0, nll_loss=2.363, ntokens=5816.6, nsentences=84, sample_size=5816.6, sample_size_v1=0, sample_size_v2=0, ppl=5.14, wps=3345.1, ups=0.58, wpb=5816.6, bsz=84, num_updates=6330, lr=6.40698e-06, gnorm=5.104, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=11189
2023-06-29 22:06:46 - progress_bar.py[line:272] - INFO: epoch 010:    411 / 660 loss=3.455, loss_v1=0, loss_v2=0, nll_loss=2.398, ntokens=5682.3, nsentences=84, sample_size=5682.3, sample_size_v1=0, sample_size_v2=0, ppl=5.27, wps=3283.6, ups=0.58, wpb=5682.3, bsz=84, num_updates=6340, lr=6.36669e-06, gnorm=5.556, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=11206
2023-06-29 22:07:03 - progress_bar.py[line:272] - INFO: epoch 010:    421 / 660 loss=3.464, loss_v1=0, loss_v2=0, nll_loss=2.408, ntokens=5674.6, nsentences=84, sample_size=5674.6, sample_size_v1=0, sample_size_v2=0, ppl=5.31, wps=3270.2, ups=0.58, wpb=5674.6, bsz=84, num_updates=6350, lr=6.32639e-06, gnorm=5.163, clip=100, loss_scale=32, train_wall=17, gb_free=12.1, wall=11223
2023-06-29 22:07:21 - progress_bar.py[line:272] - INFO: epoch 010:    431 / 660 loss=3.475, loss_v1=0, loss_v2=0, nll_loss=2.419, ntokens=5938.8, nsentences=84, sample_size=5938.8, sample_size_v1=0, sample_size_v2=0, ppl=5.35, wps=3382.7, ups=0.57, wpb=5938.8, bsz=84, num_updates=6360, lr=6.2861e-06, gnorm=5.064, clip=100, loss_scale=32, train_wall=18, gb_free=11.9, wall=11241
2023-06-29 22:07:38 - progress_bar.py[line:272] - INFO: epoch 010:    441 / 660 loss=3.491, loss_v1=0, loss_v2=0, nll_loss=2.437, ntokens=6031.1, nsentences=84, sample_size=6031.1, sample_size_v1=0, sample_size_v2=0, ppl=5.41, wps=3474.8, ups=0.58, wpb=6031.1, bsz=84, num_updates=6370, lr=6.2458e-06, gnorm=5.195, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=11258
2023-06-29 22:07:55 - progress_bar.py[line:272] - INFO: epoch 010:    451 / 660 loss=3.459, loss_v1=0, loss_v2=0, nll_loss=2.402, ntokens=5692.8, nsentences=84, sample_size=5692.8, sample_size_v1=0, sample_size_v2=0, ppl=5.29, wps=3291.7, ups=0.58, wpb=5692.8, bsz=84, num_updates=6380, lr=6.20551e-06, gnorm=5.205, clip=100, loss_scale=32, train_wall=17, gb_free=12.5, wall=11276
2023-06-29 22:08:13 - progress_bar.py[line:272] - INFO: epoch 010:    461 / 660 loss=3.464, loss_v1=0, loss_v2=0, nll_loss=2.408, ntokens=5927.6, nsentences=84, sample_size=5927.6, sample_size_v1=0, sample_size_v2=0, ppl=5.31, wps=3399.3, ups=0.57, wpb=5927.6, bsz=84, num_updates=6390, lr=6.16521e-06, gnorm=5.346, clip=100, loss_scale=32, train_wall=17, gb_free=12, wall=11293
2023-06-29 22:08:30 - progress_bar.py[line:272] - INFO: epoch 010:    471 / 660 loss=3.486, loss_v1=0, loss_v2=0, nll_loss=2.433, ntokens=6050.4, nsentences=84, sample_size=6050.4, sample_size_v1=0, sample_size_v2=0, ppl=5.4, wps=3486.5, ups=0.58, wpb=6050.4, bsz=84, num_updates=6400, lr=6.12492e-06, gnorm=4.777, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=11310
2023-06-29 22:08:48 - progress_bar.py[line:272] - INFO: epoch 010:    481 / 660 loss=3.453, loss_v1=0, loss_v2=0, nll_loss=2.394, ntokens=5703.5, nsentences=84, sample_size=5703.5, sample_size_v1=0, sample_size_v2=0, ppl=5.26, wps=3254.7, ups=0.57, wpb=5703.5, bsz=84, num_updates=6410, lr=6.08462e-06, gnorm=4.918, clip=100, loss_scale=32, train_wall=17, gb_free=12, wall=11328
2023-06-29 22:09:05 - progress_bar.py[line:272] - INFO: epoch 010:    491 / 660 loss=3.452, loss_v1=0, loss_v2=0, nll_loss=2.394, ntokens=5775.8, nsentences=84, sample_size=5775.8, sample_size_v1=0, sample_size_v2=0, ppl=5.25, wps=3332.5, ups=0.58, wpb=5775.8, bsz=84, num_updates=6420, lr=6.04433e-06, gnorm=5.147, clip=100, loss_scale=32, train_wall=17, gb_free=12.1, wall=11345
2023-06-29 22:09:22 - progress_bar.py[line:272] - INFO: epoch 010:    501 / 660 loss=3.479, loss_v1=0, loss_v2=0, nll_loss=2.423, ntokens=5822.1, nsentences=84, sample_size=5822.1, sample_size_v1=0, sample_size_v2=0, ppl=5.36, wps=3351.7, ups=0.58, wpb=5822.1, bsz=84, num_updates=6430, lr=6.00403e-06, gnorm=5.082, clip=100, loss_scale=32, train_wall=17, gb_free=11.7, wall=11363
2023-06-29 22:09:40 - progress_bar.py[line:272] - INFO: epoch 010:    511 / 660 loss=3.457, loss_v1=0, loss_v2=0, nll_loss=2.399, ntokens=5861, nsentences=84, sample_size=5861, sample_size_v1=0, sample_size_v2=0, ppl=5.28, wps=3385, ups=0.58, wpb=5861, bsz=84, num_updates=6440, lr=5.96373e-06, gnorm=5.283, clip=100, loss_scale=32, train_wall=17, gb_free=12.1, wall=11380
2023-06-29 22:09:57 - progress_bar.py[line:272] - INFO: epoch 010:    521 / 660 loss=3.469, loss_v1=0, loss_v2=0, nll_loss=2.414, ntokens=5885.8, nsentences=84, sample_size=5885.8, sample_size_v1=0, sample_size_v2=0, ppl=5.33, wps=3383.4, ups=0.57, wpb=5885.8, bsz=84, num_updates=6450, lr=5.92344e-06, gnorm=5.163, clip=100, loss_scale=32, train_wall=17, gb_free=12, wall=11397
2023-06-29 22:10:15 - progress_bar.py[line:272] - INFO: epoch 010:    531 / 660 loss=3.474, loss_v1=0, loss_v2=0, nll_loss=2.418, ntokens=6063.4, nsentences=84, sample_size=6063.4, sample_size_v1=0, sample_size_v2=0, ppl=5.34, wps=3478.5, ups=0.57, wpb=6063.4, bsz=84, num_updates=6460, lr=5.88314e-06, gnorm=5.214, clip=100, loss_scale=32, train_wall=17, gb_free=12, wall=11415
2023-06-29 22:10:32 - progress_bar.py[line:272] - INFO: epoch 010:    541 / 660 loss=3.497, loss_v1=0, loss_v2=0, nll_loss=2.445, ntokens=6150.8, nsentences=84, sample_size=6150.8, sample_size_v1=0, sample_size_v2=0, ppl=5.44, wps=3516.6, ups=0.57, wpb=6150.8, bsz=84, num_updates=6470, lr=5.84285e-06, gnorm=5.447, clip=100, loss_scale=32, train_wall=17, gb_free=12.1, wall=11432
2023-06-29 22:10:49 - progress_bar.py[line:272] - INFO: epoch 010:    551 / 660 loss=3.459, loss_v1=0, loss_v2=0, nll_loss=2.401, ntokens=5950.7, nsentences=84, sample_size=5950.7, sample_size_v1=0, sample_size_v2=0, ppl=5.28, wps=3429.6, ups=0.58, wpb=5950.7, bsz=84, num_updates=6480, lr=5.80255e-06, gnorm=4.864, clip=100, loss_scale=32, train_wall=17, gb_free=12.1, wall=11450
2023-06-29 22:11:07 - progress_bar.py[line:272] - INFO: epoch 010:    561 / 660 loss=3.458, loss_v1=0, loss_v2=0, nll_loss=2.401, ntokens=6132.6, nsentences=84, sample_size=6132.6, sample_size_v1=0, sample_size_v2=0, ppl=5.28, wps=3500, ups=0.57, wpb=6132.6, bsz=84, num_updates=6490, lr=5.76226e-06, gnorm=5.03, clip=100, loss_scale=32, train_wall=17, gb_free=11.7, wall=11467
2023-06-29 22:11:24 - progress_bar.py[line:272] - INFO: epoch 010:    571 / 660 loss=3.417, loss_v1=0, loss_v2=0, nll_loss=2.355, ntokens=5454, nsentences=84, sample_size=5454, sample_size_v1=0, sample_size_v2=0, ppl=5.11, wps=3151.7, ups=0.58, wpb=5454, bsz=84, num_updates=6500, lr=5.72196e-06, gnorm=5.406, clip=100, loss_scale=32, train_wall=17, gb_free=12.1, wall=11484
2023-06-29 22:11:42 - progress_bar.py[line:272] - INFO: epoch 010:    581 / 660 loss=3.466, loss_v1=0, loss_v2=0, nll_loss=2.408, ntokens=5978.6, nsentences=84, sample_size=5978.6, sample_size_v1=0, sample_size_v2=0, ppl=5.31, wps=3416.6, ups=0.57, wpb=5978.6, bsz=84, num_updates=6510, lr=5.68167e-06, gnorm=5.183, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=11502
2023-06-29 22:11:59 - progress_bar.py[line:272] - INFO: epoch 010:    591 / 660 loss=3.445, loss_v1=0, loss_v2=0, nll_loss=2.387, ntokens=5880.1, nsentences=84, sample_size=5880.1, sample_size_v1=0, sample_size_v2=0, ppl=5.23, wps=3395.5, ups=0.58, wpb=5880.1, bsz=84, num_updates=6520, lr=5.64137e-06, gnorm=5.144, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=11519
2023-06-29 22:12:17 - progress_bar.py[line:272] - INFO: epoch 010:    601 / 660 loss=3.442, loss_v1=0, loss_v2=0, nll_loss=2.383, ntokens=5943.1, nsentences=84, sample_size=5943.1, sample_size_v1=0, sample_size_v2=0, ppl=5.22, wps=3414, ups=0.57, wpb=5943.1, bsz=84, num_updates=6530, lr=5.60107e-06, gnorm=5.097, clip=100, loss_scale=32, train_wall=17, gb_free=12.3, wall=11537
2023-06-29 22:12:34 - progress_bar.py[line:272] - INFO: epoch 010:    611 / 660 loss=3.459, loss_v1=0, loss_v2=0, nll_loss=2.401, ntokens=5965.5, nsentences=84, sample_size=5965.5, sample_size_v1=0, sample_size_v2=0, ppl=5.28, wps=3433, ups=0.58, wpb=5965.5, bsz=84, num_updates=6540, lr=5.56078e-06, gnorm=4.951, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=11554
2023-06-29 22:12:51 - progress_bar.py[line:272] - INFO: epoch 010:    621 / 660 loss=3.493, loss_v1=0, loss_v2=0, nll_loss=2.44, ntokens=6065.2, nsentences=84, sample_size=6065.2, sample_size_v1=0, sample_size_v2=0, ppl=5.43, wps=3464.4, ups=0.57, wpb=6065.2, bsz=84, num_updates=6550, lr=5.52048e-06, gnorm=4.881, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=11572
2023-06-29 22:13:09 - progress_bar.py[line:272] - INFO: epoch 010:    631 / 660 loss=3.473, loss_v1=0, loss_v2=0, nll_loss=2.416, ntokens=5959.5, nsentences=84, sample_size=5959.5, sample_size_v1=0, sample_size_v2=0, ppl=5.34, wps=3421.5, ups=0.57, wpb=5959.5, bsz=84, num_updates=6560, lr=5.48019e-06, gnorm=5.153, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=11589
2023-06-29 22:13:26 - progress_bar.py[line:272] - INFO: epoch 010:    641 / 660 loss=3.49, loss_v1=0, loss_v2=0, nll_loss=2.436, ntokens=6003, nsentences=84, sample_size=6003, sample_size_v1=0, sample_size_v2=0, ppl=5.41, wps=3436.2, ups=0.57, wpb=6003, bsz=84, num_updates=6570, lr=5.43989e-06, gnorm=4.823, clip=100, loss_scale=32, train_wall=17, gb_free=12, wall=11606
2023-06-29 22:13:44 - progress_bar.py[line:272] - INFO: epoch 010:    651 / 660 loss=3.43, loss_v1=0, loss_v2=0, nll_loss=2.416, ntokens=5805.4, nsentences=82.8, sample_size=5805.4, sample_size_v1=0, sample_size_v2=0, ppl=5.34, wps=3329.8, ups=0.57, wpb=5805.4, bsz=82.8, num_updates=6580, lr=5.3996e-06, gnorm=5.087, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=11624
2023-06-29 22:13:59 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 10 @ 6589 updates
2023-06-29 22:13:59 - trainer.py[line:431] - INFO: Saving checkpoint to ../../checkpoints/OFA/sgcls_checkpoints/_12_3e-5_512_base_tgtobj/checkpoint10.pt
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 1 row count 18467 total row count 55400
slice_id 1 seek offset 18467
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 2 row count 18466 total row count 55400
slice_id 2 seek offset 36934
2023-06-29 22:14:02 - trainer.py[line:441] - INFO: Finished saving checkpoint to ../../checkpoints/OFA/sgcls_checkpoints/_12_3e-5_512_base_tgtobj/checkpoint10.pt
2023-06-29 22:14:08 - checkpoint_utils.py[line:133] - INFO: Saved checkpoint ../../checkpoints/OFA/sgcls_checkpoints/_12_3e-5_512_base_tgtobj/checkpoint10.pt (epoch 10 @ 6589 updates, score None) (writing took 8.655789477750659 seconds)
2023-06-29 22:14:08 - train.py[line:332] - INFO: end of epoch 10 (average epoch stats below)
2023-06-29 22:14:08 - progress_bar.py[line:282] - INFO: epoch 010 | loss 3.47 | loss_v1 0 | loss_v2 0 | nll_loss 2.415 | ntokens 5903.05 | nsentences 83.95 | sample_size 5903.05 | sample_size_v1 0 | sample_size_v2 0 | ppl 5.33 | wps 3348 | ups 0.57 | wpb 5903.1 | bsz 84 | num_updates 6589 | lr 5.36333e-06 | gnorm 5.201 | clip 100 | loss_scale 32 | train_wall 1150 | gb_free 12.3 | wall 11648
2023-06-29 22:14:08 - trainer.py[line:639] - INFO: loading train data for epoch 11
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 row count 18467 total row count 55400
slice_id 0 seek offset 0
2023-06-29 22:14:10 - trainer.py[line:703] - INFO: begin training epoch 11
2023-06-29 22:14:10 - train.py[line:305] - INFO: Start iterating over samples
2023-06-29 22:14:12 - progress_bar.py[line:272] - INFO: epoch 011:      1 / 660 loss=3.49, loss_v1=0, loss_v2=0, nll_loss=2.435, ntokens=5824.7, nsentences=81.9, sample_size=5824.7, sample_size_v1=0, sample_size_v2=0, ppl=5.41, wps=2084.7, ups=0.36, wpb=5824.7, bsz=81.9, num_updates=6590, lr=5.3593e-06, gnorm=5.416, clip=100, loss_scale=32, train_wall=17, gb_free=12, wall=11652
2023-06-29 22:14:29 - progress_bar.py[line:272] - INFO: epoch 011:     11 / 660 loss=3.57, loss_v1=0, loss_v2=0, nll_loss=2.524, ntokens=5821.9, nsentences=84, sample_size=5821.9, sample_size_v1=0, sample_size_v2=0, ppl=5.75, wps=3314.7, ups=0.57, wpb=5821.9, bsz=84, num_updates=6600, lr=5.31901e-06, gnorm=5.557, clip=100, loss_scale=32, train_wall=18, gb_free=11.9, wall=11669
2023-06-29 22:14:47 - progress_bar.py[line:272] - INFO: epoch 011:     21 / 660 loss=3.521, loss_v1=0, loss_v2=0, nll_loss=2.468, ntokens=5817.2, nsentences=84, sample_size=5817.2, sample_size_v1=0, sample_size_v2=0, ppl=5.53, wps=3294, ups=0.57, wpb=5817.2, bsz=84, num_updates=6610, lr=5.27871e-06, gnorm=5.791, clip=100, loss_scale=32, train_wall=18, gb_free=11.9, wall=11687
2023-06-29 22:15:04 - progress_bar.py[line:272] - INFO: epoch 011:     31 / 660 loss=3.442, loss_v1=0, loss_v2=0, nll_loss=2.385, ntokens=5377, nsentences=84, sample_size=5377, sample_size_v1=0, sample_size_v2=0, ppl=5.22, wps=3115, ups=0.58, wpb=5377, bsz=84, num_updates=6620, lr=5.23842e-06, gnorm=5.694, clip=100, loss_scale=32, train_wall=17, gb_free=11.7, wall=11704
2023-06-29 22:15:22 - progress_bar.py[line:272] - INFO: epoch 011:     41 / 660 loss=3.409, loss_v1=0, loss_v2=0, nll_loss=2.346, ntokens=5746.4, nsentences=84, sample_size=5746.4, sample_size_v1=0, sample_size_v2=0, ppl=5.08, wps=3277.9, ups=0.57, wpb=5746.4, bsz=84, num_updates=6630, lr=5.19812e-06, gnorm=5.851, clip=100, loss_scale=32, train_wall=17, gb_free=11.6, wall=11722
2023-06-29 22:15:39 - progress_bar.py[line:272] - INFO: epoch 011:     51 / 660 loss=3.408, loss_v1=0, loss_v2=0, nll_loss=2.345, ntokens=5780.2, nsentences=84, sample_size=5780.2, sample_size_v1=0, sample_size_v2=0, ppl=5.08, wps=3298.1, ups=0.57, wpb=5780.2, bsz=84, num_updates=6640, lr=5.15782e-06, gnorm=5.024, clip=100, loss_scale=32, train_wall=17, gb_free=11.9, wall=11739
2023-06-29 22:15:57 - progress_bar.py[line:272] - INFO: epoch 011:     61 / 660 loss=3.434, loss_v1=0, loss_v2=0, nll_loss=2.374, ntokens=5711.5, nsentences=84, sample_size=5711.5, sample_size_v1=0, sample_size_v2=0, ppl=5.18, wps=3269.3, ups=0.57, wpb=5711.5, bsz=84, num_updates=6650, lr=5.11753e-06, gnorm=5.031, clip=100, loss_scale=32, train_wall=17, gb_free=12.1, wall=11757
2023-06-29 22:16:14 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-06-29 22:16:16 - progress_bar.py[line:272] - INFO: epoch 011:     72 / 660 loss=3.402, loss_v1=0, loss_v2=0, nll_loss=2.336, ntokens=5841.1, nsentences=84, sample_size=5841.1, sample_size_v1=0, sample_size_v2=0, ppl=5.05, wps=3035, ups=0.52, wpb=5841.1, bsz=84, num_updates=6660, lr=5.07723e-06, gnorm=5.336, clip=100, loss_scale=16, train_wall=19, gb_free=11.5, wall=11776
2023-06-29 22:16:34 - progress_bar.py[line:272] - INFO: epoch 011:     82 / 660 loss=3.445, loss_v1=0, loss_v2=0, nll_loss=2.385, ntokens=6234.2, nsentences=84, sample_size=6234.2, sample_size_v1=0, sample_size_v2=0, ppl=5.22, wps=3501, ups=0.56, wpb=6234.2, bsz=84, num_updates=6670, lr=5.03694e-06, gnorm=4.986, clip=100, loss_scale=16, train_wall=18, gb_free=11.6, wall=11794
2023-06-29 22:16:51 - progress_bar.py[line:272] - INFO: epoch 011:     92 / 660 loss=3.437, loss_v1=0, loss_v2=0, nll_loss=2.376, ntokens=5792.8, nsentences=84, sample_size=5792.8, sample_size_v1=0, sample_size_v2=0, ppl=5.19, wps=3273.1, ups=0.57, wpb=5792.8, bsz=84, num_updates=6680, lr=4.99664e-06, gnorm=5.471, clip=100, loss_scale=16, train_wall=18, gb_free=11.9, wall=11812
2023-06-29 22:17:09 - progress_bar.py[line:272] - INFO: epoch 011:    102 / 660 loss=3.411, loss_v1=0, loss_v2=0, nll_loss=2.348, ntokens=5721.2, nsentences=84, sample_size=5721.2, sample_size_v1=0, sample_size_v2=0, ppl=5.09, wps=3268.3, ups=0.57, wpb=5721.2, bsz=84, num_updates=6690, lr=4.95635e-06, gnorm=5.435, clip=100, loss_scale=16, train_wall=17, gb_free=11.6, wall=11829
2023-06-29 22:17:26 - progress_bar.py[line:272] - INFO: epoch 011:    112 / 660 loss=3.434, loss_v1=0, loss_v2=0, nll_loss=2.372, ntokens=5851.3, nsentences=84, sample_size=5851.3, sample_size_v1=0, sample_size_v2=0, ppl=5.18, wps=3352.1, ups=0.57, wpb=5851.3, bsz=84, num_updates=6700, lr=4.91605e-06, gnorm=5.107, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=11847
2023-06-29 22:17:44 - progress_bar.py[line:272] - INFO: epoch 011:    122 / 660 loss=3.454, loss_v1=0, loss_v2=0, nll_loss=2.396, ntokens=5683.2, nsentences=84, sample_size=5683.2, sample_size_v1=0, sample_size_v2=0, ppl=5.26, wps=3283.3, ups=0.58, wpb=5683.2, bsz=84, num_updates=6710, lr=4.87576e-06, gnorm=5.294, clip=100, loss_scale=16, train_wall=17, gb_free=11.8, wall=11864
2023-06-29 22:18:01 - progress_bar.py[line:272] - INFO: epoch 011:    132 / 660 loss=3.476, loss_v1=0, loss_v2=0, nll_loss=2.422, ntokens=5766, nsentences=84, sample_size=5766, sample_size_v1=0, sample_size_v2=0, ppl=5.36, wps=3279.6, ups=0.57, wpb=5766, bsz=84, num_updates=6720, lr=4.83546e-06, gnorm=5.223, clip=100, loss_scale=16, train_wall=18, gb_free=11.5, wall=11882
2023-06-29 22:18:19 - progress_bar.py[line:272] - INFO: epoch 011:    142 / 660 loss=3.521, loss_v1=0, loss_v2=0, nll_loss=2.47, ntokens=5988.5, nsentences=84, sample_size=5988.5, sample_size_v1=0, sample_size_v2=0, ppl=5.54, wps=3391.3, ups=0.57, wpb=5988.5, bsz=84, num_updates=6730, lr=4.79516e-06, gnorm=4.87, clip=100, loss_scale=16, train_wall=18, gb_free=11.8, wall=11899
2023-06-29 22:18:37 - progress_bar.py[line:272] - INFO: epoch 011:    152 / 660 loss=3.489, loss_v1=0, loss_v2=0, nll_loss=2.434, ntokens=5986.8, nsentences=84, sample_size=5986.8, sample_size_v1=0, sample_size_v2=0, ppl=5.4, wps=3402.4, ups=0.57, wpb=5986.8, bsz=84, num_updates=6740, lr=4.75487e-06, gnorm=4.85, clip=100, loss_scale=16, train_wall=18, gb_free=11.6, wall=11917
2023-06-29 22:18:54 - progress_bar.py[line:272] - INFO: epoch 011:    162 / 660 loss=3.493, loss_v1=0, loss_v2=0, nll_loss=2.439, ntokens=5960.6, nsentences=84, sample_size=5960.6, sample_size_v1=0, sample_size_v2=0, ppl=5.42, wps=3369.5, ups=0.57, wpb=5960.6, bsz=84, num_updates=6750, lr=4.71457e-06, gnorm=4.865, clip=100, loss_scale=16, train_wall=18, gb_free=11.8, wall=11934
2023-06-29 22:19:12 - progress_bar.py[line:272] - INFO: epoch 011:    172 / 660 loss=3.486, loss_v1=0, loss_v2=0, nll_loss=2.432, ntokens=5945.2, nsentences=84, sample_size=5945.2, sample_size_v1=0, sample_size_v2=0, ppl=5.4, wps=3351, ups=0.56, wpb=5945.2, bsz=84, num_updates=6760, lr=4.67428e-06, gnorm=4.972, clip=100, loss_scale=16, train_wall=18, gb_free=11.9, wall=11952
2023-06-29 22:19:30 - progress_bar.py[line:272] - INFO: epoch 011:    182 / 660 loss=3.524, loss_v1=0, loss_v2=0, nll_loss=2.474, ntokens=5945.7, nsentences=84, sample_size=5945.7, sample_size_v1=0, sample_size_v2=0, ppl=5.56, wps=3365, ups=0.57, wpb=5945.7, bsz=84, num_updates=6770, lr=4.63398e-06, gnorm=5.331, clip=100, loss_scale=16, train_wall=18, gb_free=12.4, wall=11970
2023-06-29 22:19:47 - progress_bar.py[line:272] - INFO: epoch 011:    192 / 660 loss=3.457, loss_v1=0, loss_v2=0, nll_loss=2.4, ntokens=5877.2, nsentences=84, sample_size=5877.2, sample_size_v1=0, sample_size_v2=0, ppl=5.28, wps=3362.5, ups=0.57, wpb=5877.2, bsz=84, num_updates=6780, lr=4.59369e-06, gnorm=4.913, clip=100, loss_scale=16, train_wall=17, gb_free=12.1, wall=11987
2023-06-29 22:20:05 - progress_bar.py[line:272] - INFO: epoch 011:    202 / 660 loss=3.464, loss_v1=0, loss_v2=0, nll_loss=2.408, ntokens=5768.4, nsentences=84, sample_size=5768.4, sample_size_v1=0, sample_size_v2=0, ppl=5.31, wps=3308.8, ups=0.57, wpb=5768.4, bsz=84, num_updates=6790, lr=4.55339e-06, gnorm=5.045, clip=100, loss_scale=16, train_wall=17, gb_free=11.7, wall=12005
2023-06-29 22:20:22 - progress_bar.py[line:272] - INFO: epoch 011:    212 / 660 loss=3.494, loss_v1=0, loss_v2=0, nll_loss=2.44, ntokens=6026.8, nsentences=84, sample_size=6026.8, sample_size_v1=0, sample_size_v2=0, ppl=5.43, wps=3411.8, ups=0.57, wpb=6026.8, bsz=84, num_updates=6800, lr=4.5131e-06, gnorm=5.133, clip=100, loss_scale=16, train_wall=18, gb_free=11.3, wall=12022
2023-06-29 22:20:40 - progress_bar.py[line:272] - INFO: epoch 011:    222 / 660 loss=3.512, loss_v1=0, loss_v2=0, nll_loss=2.46, ntokens=6094.2, nsentences=84, sample_size=6094.2, sample_size_v1=0, sample_size_v2=0, ppl=5.5, wps=3470.9, ups=0.57, wpb=6094.2, bsz=84, num_updates=6810, lr=4.4728e-06, gnorm=4.792, clip=100, loss_scale=16, train_wall=18, gb_free=12, wall=12040
2023-06-29 22:20:57 - progress_bar.py[line:272] - INFO: epoch 011:    232 / 660 loss=3.477, loss_v1=0, loss_v2=0, nll_loss=2.422, ntokens=6080.2, nsentences=84, sample_size=6080.2, sample_size_v1=0, sample_size_v2=0, ppl=5.36, wps=3483.5, ups=0.57, wpb=6080.2, bsz=84, num_updates=6820, lr=4.43251e-06, gnorm=4.991, clip=100, loss_scale=16, train_wall=17, gb_free=12, wall=12058
2023-06-29 22:21:15 - progress_bar.py[line:272] - INFO: epoch 011:    242 / 660 loss=3.45, loss_v1=0, loss_v2=0, nll_loss=2.391, ntokens=5757.6, nsentences=84, sample_size=5757.6, sample_size_v1=0, sample_size_v2=0, ppl=5.25, wps=3294.9, ups=0.57, wpb=5757.6, bsz=84, num_updates=6830, lr=4.39221e-06, gnorm=5.122, clip=100, loss_scale=16, train_wall=17, gb_free=11.9, wall=12075
2023-06-29 22:21:32 - progress_bar.py[line:272] - INFO: epoch 011:    252 / 660 loss=3.431, loss_v1=0, loss_v2=0, nll_loss=2.37, ntokens=6117.9, nsentences=84, sample_size=6117.9, sample_size_v1=0, sample_size_v2=0, ppl=5.17, wps=3515, ups=0.57, wpb=6117.9, bsz=84, num_updates=6840, lr=4.35191e-06, gnorm=5.074, clip=100, loss_scale=16, train_wall=17, gb_free=12.1, wall=12092
Traceback (most recent call last):
WARNING:torch.distributed.elastic.agent.server.api:Received 2 death signal, shutting down workers
  File "../../train.py", line 539, in <module>
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2378603 closing signal SIGINT
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2378607 closing signal SIGINT
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2378610 closing signal SIGINT
  File "../../train.py", line 532, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/zcai75/Github/OFA/fairseq/fairseq/distributed/utils.py", line 374, in call_main
    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
  File "/home/zcai75/Github/OFA/fairseq/fairseq/distributed/utils.py", line 348, in distributed_main
    main(cfg, **kwargs)
Traceback (most recent call last):
  File "../../train.py", line 539, in <module>
    cli_main()
  File "../../train.py", line 532, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/zcai75/Github/OFA/fairseq/fairseq/distributed/utils.py", line 374, in call_main
    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
  File "/home/zcai75/Github/OFA/fairseq/fairseq/distributed/utils.py", line 348, in distributed_main
    main(cfg, **kwargs)
  File "../../train.py", line 199, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "../../train.py", line 310, in train
      File "../../train.py", line 199, in main
log_output = trainer.train_step(samples)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/contextlib.py", line 75, in inner
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "../../train.py", line 310, in train
    return func(*args, **kwds)
  File "/home/zcai75/Github/OFA_forked/trainer.py", line 773, in train_step
    log_output = trainer.train_step(samples)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zcai75/Github/OFA_forked/trainer.py", line 773, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/home/zcai75/Github/OFA_forked/tasks/ofa_task.py", line 334, in train_step
    loss, sample_size, logging_output = criterion(model, sample, update_num=update_num)
      File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
loss, sample_size_i, logging_output = self.task.train_step(
  File "/home/zcai75/Github/OFA_forked/tasks/ofa_task.py", line 334, in train_step
    loss, sample_size, logging_output = criterion(model, sample, update_num=update_num)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zcai75/Github/OFA_forked/criterions/label_smoothed_cross_entropy.py", line 199, in forward
    net_output = model(**sample["net_input"])
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zcai75/Github/OFA_forked/criterions/label_smoothed_cross_entropy.py", line 199, in forward
    net_output = model(**sample["net_input"])
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zcai75/Github/OFA/fairseq/fairseq/distributed/module_proxy_wrapper.py", line 55, in forward
    return self.module(*args, **kwargs)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zcai75/Github/OFA/fairseq/fairseq/distributed/module_proxy_wrapper.py", line 55, in forward
    return self.module(*args, **kwargs)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1008, in forward
    return forward_call(*input, **kwargs)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1008, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 969, in _run_ddp_forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 969, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return module_to_run(*inputs[0], **kwargs[0])
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zcai75/Github/OFA_forked/models/ofa/ofa.py", line 89, in forward
        return forward_call(*input, **kwargs)encoder_out = self.encoder(

  File "/home/zcai75/Github/OFA_forked/models/ofa/ofa.py", line 89, in forward
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    encoder_out = self.encoder(
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zcai75/Github/OFA_forked/models/ofa/unify_transformer.py", line 797, in forward
    return forward_call(*input, **kwargs)
  File "/home/zcai75/Github/OFA_forked/models/ofa/unify_transformer.py", line 797, in forward
    return self.forward_scriptable(src_tokens,
  File "/home/zcai75/Github/OFA_forked/models/ofa/unify_transformer.py", line 935, in forward_scriptable
        x = layer(x, encoder_padding_mask=encoder_padding_mask if has_pads else None, \
KeyboardInterrupt
return self.forward_scriptable(src_tokens,
  File "/home/zcai75/Github/OFA_forked/models/ofa/unify_transformer.py", line 935, in forward_scriptable
    x = layer(x, encoder_padding_mask=encoder_padding_mask if has_pads else None, \
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zcai75/Github/OFA_forked/models/ofa/unify_transformer_layer.py", line 259, in forward
    x, _ = self.self_attn(
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zcai75/Github/OFA_forked/models/ofa/unify_multihead_attention.py", line 216, in forward
    v = self.v_proj(query)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Traceback (most recent call last):
  File "../../train.py", line 539, in <module>
    cli_main()
  File "../../train.py", line 532, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/zcai75/Github/OFA/fairseq/fairseq/distributed/utils.py", line 374, in call_main
    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
  File "/home/zcai75/Github/OFA/fairseq/fairseq/distributed/utils.py", line 348, in distributed_main
    main(cfg, **kwargs)
  File "../../train.py", line 199, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "../../train.py", line 310, in train
    log_output = trainer.train_step(samples)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zcai75/Github/OFA_forked/trainer.py", line 773, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/home/zcai75/Github/OFA_forked/tasks/ofa_task.py", line 334, in train_step
    loss, sample_size, logging_output = criterion(model, sample, update_num=update_num)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zcai75/Github/OFA_forked/criterions/label_smoothed_cross_entropy.py", line 199, in forward
    net_output = model(**sample["net_input"])
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zcai75/Github/OFA/fairseq/fairseq/distributed/module_proxy_wrapper.py", line 55, in forward
    return self.module(*args, **kwargs)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1008, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 969, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zcai75/Github/OFA_forked/models/ofa/ofa.py", line 89, in forward
    encoder_out = self.encoder(
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zcai75/Github/OFA_forked/models/ofa/unify_transformer.py", line 797, in forward
    return self.forward_scriptable(src_tokens,
  File "/home/zcai75/Github/OFA_forked/models/ofa/unify_transformer.py", line 935, in forward_scriptable
    x = layer(x, encoder_padding_mask=encoder_padding_mask if has_pads else None, \
KeyboardInterrupt
wandb: Waiting for W&B process to finish... (failed 255). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:                  train/bsz ▁█▁▁▁▁▁▁▁█
wandb:                 train/clip ▃▁████████
wandb:              train/gb_free ▁▁▁▁▁▁▁▁▁▁
wandb:                train/gnorm ▃▁▃▅▆█████
wandb:                 train/loss █▄▄▃▂▂▁▁▁▁
wandb:           train/loss_scale ▄█▂▂▂▂▁▁▁▂
wandb:              train/loss_v1 ▁▁▁▁▁▁▁▁▁▁
wandb:              train/loss_v2 ▁▁▁▁▁▁▁▁▁▁
wandb:                   train/lr █▇▆▆▅▄▃▃▂▁
wandb:             train/nll_loss █▄▄▃▂▂▁▁▁▁
wandb:           train/nsentences █████▁████
wandb:              train/ntokens ▄▆█▄▆▁▄▅█▆
wandb:                  train/ppl █▂▂▂▁▁▁▁▁▁
wandb:          train/sample_size ▄▆█▄▆▁▄▅█▆
wandb:       train/sample_size_v1 ▁▁▁▁▁▁▁▁▁▁
wandb:       train/sample_size_v2 ▁▁▁▁▁▁▁▁▁▁
wandb:           train/train_wall ▂█▃▂▂▁▁▁▁▁
wandb:                  train/ups █▁█▁███▁██
wandb:                 train/wall ▁▂▃▃▄▅▆▆▇█
wandb:                  train/wpb ▄▆█▅▆▁▄▅█▆
wandb:                  train/wps █▃▆▁▇▆▇▁▇▆
wandb:            train_inner/bsz ███████████████▁████████████████████████
wandb:           train_inner/clip █▇▁▁▂▅▁▇████████████████████████████████
wandb:        train_inner/gb_free ▃▅▇▆▄▅▅▇▅▅▇▆▅▆▅▂▂▆▆▅▅█▆█▄▅▆█▄▇▅▃▅▅▅▄▅▅▅▁
wandb:          train_inner/gnorm ▅▁▁▁▁▁▁▁▂▂▄▄▅▅▅▅▅▅▆▆▇█▇█▇█▇█▇▇██▇▇███▇▇▇
wandb:           train_inner/loss █▅▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_inner/loss_scale ▂▂▂▄▄▄▄█▄▄▄▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▂▂▂▁
wandb:        train_inner/loss_v1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        train_inner/loss_v2 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:             train_inner/lr ▁▄▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:       train_inner/nll_loss █▅▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_inner/nsentences ███████████████▁████████████████████████
wandb:        train_inner/ntokens █▃▄▇▂▆▄▆▂▇▅▅▃▆▃▃▄▆▁▆▇▁▇▂▇▃▄▄▄▃▄▄▇▃▄▅█▆▅▅
wandb:            train_inner/ppl █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    train_inner/sample_size █▃▄▇▂▆▄▆▂▇▅▅▃▆▃▃▄▆▁▆▇▁▇▂▇▃▄▄▄▃▄▄▇▃▄▅█▆▅▅
wandb: train_inner/sample_size_v1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_inner/sample_size_v2 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_inner/train_wall ▅▁▁▁▁▅▅▅▁▁▁▁▅▁█▁▅▁▁▁▅▁▁█▅▁▁▁▁▁▁▅▁▁▁▅▁▁▁▅
wandb:            train_inner/ups ███████▇██████▇▁███████▇████████████████
wandb:           train_inner/wall ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            train_inner/wpb █▃▄▇▂▆▄▆▂▇▅▅▃▆▃▃▄▆▁▆▇▁▇▂▇▃▄▄▄▃▄▄▇▃▄▅█▆▅▅
wandb:            train_inner/wps █▇▇█▇█▇▇▇███▇█▆▁▇█▇██▇█▆█▇▇▇▇▇▇▇█▇▇▇███▇
wandb: 
wandb: Run summary:
wandb:                  train/bsz 84.0
wandb:                 train/clip 100.0
wandb:              train/gb_free 12.3
wandb:                train/gnorm 5.201
wandb:                 train/loss 3.47
wandb:           train/loss_scale 32.0
wandb:              train/loss_v1 0.0
wandb:              train/loss_v2 0.0
wandb:                   train/lr 1e-05
wandb:             train/nll_loss 2.415
wandb:           train/nsentences 83.95
wandb:              train/ntokens 5903.053
wandb:                  train/ppl 5.33
wandb:          train/sample_size 5903.053
wandb:       train/sample_size_v1 0.0
wandb:       train/sample_size_v2 0.0
wandb:           train/train_wall 1150.0
wandb:                  train/ups 0.57
wandb:                 train/wall 11648.0
wandb:                  train/wpb 5903.1
wandb:                  train/wps 3348.0
wandb:            train_inner/bsz 84.0
wandb:           train_inner/clip 100.0
wandb:        train_inner/gb_free 12.1
wandb:          train_inner/gnorm 5.074
wandb:           train_inner/loss 3.431
wandb:     train_inner/loss_scale 16.0
wandb:        train_inner/loss_v1 0.0
wandb:        train_inner/loss_v2 0.0
wandb:             train_inner/lr 0.0
wandb:       train_inner/nll_loss 2.37
wandb:     train_inner/nsentences 84.0
wandb:        train_inner/ntokens 6117.9
wandb:            train_inner/ppl 5.17
wandb:    train_inner/sample_size 6117.9
wandb: train_inner/sample_size_v1 0.0
wandb: train_inner/sample_size_v2 0.0
wandb:     train_inner/train_wall 17.0
wandb:            train_inner/ups 0.57
wandb:           train_inner/wall 12092.0
wandb:            train_inner/wpb 6117.9
wandb:            train_inner/wps 3515.0
wandb: 
wandb: 🚀 View run _12_3e-5_512_base_tgtobj at: https://wandb.ai/jackcai1206/OFA-VG/runs/1yh6pjmr
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230629_190003-1yh6pjmr/logs
Traceback (most recent call last):
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 850, in _invoke_run
    time.sleep(monitor_interval)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 60, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2378565 got signal: 2

2023-08-08 16:07:45 - utils.py[line:255] - INFO: distributed init (rank 0): env://
2023-08-08 16:07:45 - utils.py[line:261] - INFO: Start init
2023-08-08 16:07:45 - distributed_c10d.py[line:228] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-08-08 16:07:45 - distributed_c10d.py[line:262] - INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
2023-08-08 16:07:45 - utils.py[line:271] - INFO: initialized host AMD4RTX3090GPU14 as rank 0
single-machine distributed training is initialized.
2023-08-08 16:07:45 - train.py[line:77] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './tensorboard/_4_3e-5_512', 'wandb_project': 'OFA-VG', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 2097152, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 7, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 30, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 7, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 4, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [4], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '../../checkpoints/OFA/sgcls_checkpoints/_4_3e-5_512', 'restore_file': '../../checkpoints/OFA/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_type_embedding=True, all_gather_list_size=2097152, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=7, batch_size_valid=7, best_checkpoint_metric='loss', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='../../dataset/OFA_data/sgcls/vg_train_full.tsv,../../dataset/OFA_data/sgcls/vg_val_full.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, decoder_prompt=True, decoder_prompt_length=100, decoder_prompt_type='prefix', device_id=0, disable_entangle=True, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, encoder_prompt=True, encoder_prompt_length=100, encoder_prompt_type='prefix', end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"max_len_a":0,"max_len_b":200}', eval_print_samples=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=False, freeze_encoder_embedding=False, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=4, max_source_positions=1024, max_src_length=150, max_target_positions=1024, max_tgt_length=150, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=True, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_bins=1000, num_shards=1, num_workers=0, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=512, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='../../checkpoints/OFA/ofa_base.pt', sample_patch_num=196, save_dir='../../checkpoints/OFA/sgcls_checkpoints/_4_3e-5_512', save_interval=2, save_interval_updates=0, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols=None, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, simul_type=None, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, sync_bn=False, task='sgcls', tensorboard_logdir='./tensorboard/_4_3e-5_512', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[4], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', valid_subset='valid', validate_after_updates=0, validate_interval=30, validate_interval_updates=0, vg_json_dir=None, wandb_project='OFA-VG', warmup_ratio=0.06, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'sgcls', 'data': '../../dataset/OFA_data/sgcls/vg_train_full.tsv,../../dataset/OFA_data/sgcls/vg_val_full.tsv', 'selected_cols': None, 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 150, 'max_tgt_length': 150, 'code_dict_size': 8192, 'patch_image_size': 512, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'eval_args': '{"beam":5,"max_len_a":0,"max_len_b":200}', 'eval_print_samples': False, 'vg_json_dir': None}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.06, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2023-08-08 16:07:45 - sg_cls.py[line:87] - INFO: sgcls setup: source dictionary: 59460 types
2023-08-08 16:07:45 - sg_cls.py[line:88] - INFO: sgcls setup: target dictionary: 59460 types
/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
2023-08-08 16:07:48 - train.py[line:110] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_prompt_encoder): PromptEncoder(
      (embedding): Embedding(100, 9216)
    )
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59460, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (decoder_prompt_encoder): PromptEncoder(
      (embedding): Embedding(100, 9216)
    )
    (decoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59460, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59460, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-08-08 16:07:48 - train.py[line:111] - INFO: task: SGClsTask
2023-08-08 16:07:48 - train.py[line:112] - INFO: model: OFAModel
2023-08-08 16:07:48 - train.py[line:113] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-08-08 16:07:48 - train.py[line:114] - INFO: num. shared model params: 184,084,040 (num. trained: 1,843,200)
2023-08-08 16:07:48 - train.py[line:121] - INFO: num. expert model params: 0 (num. trained: 0)
local datafile ../../dataset/OFA_data/sgcls/vg_val_full.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_val_full.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_val_full.tsv slice_id 0 row count 26446 total row count 26446
/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-08-08 16:07:49 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-08-08 16:07:49 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 1 workers***********************
2023-08-08 16:07:49 - utils.py[line:761] - INFO: rank   0: capabilities =  8.6  ; total memory = 23.688 GB ; name = NVIDIA GeForce RTX 3090 Ti              
2023-08-08 16:07:49 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 1 workers***********************
2023-08-08 16:07:49 - train.py[line:152] - INFO: training on 1 devices (GPUs/TPUs)
2023-08-08 16:07:49 - train.py[line:157] - INFO: max tokens per device = None and max sentences per device = 7
2023-08-08 16:07:49 - trainer.py[line:458] - INFO: Preparing to load checkpoint ../../checkpoints/OFA/ofa_base.pt
2023-08-08 16:07:50 - trainer.py[line:617] - INFO: Loaded checkpoint ../../checkpoints/OFA/ofa_base.pt (epoch 48 @ 0 updates)
2023-08-08 16:07:50 - trainer.py[line:639] - INFO: loading train data for epoch 1
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 row count 62723 total row count 62723
slice_id 0 seek offset 0
Total steps 8964, warmup steps 537, warmup_factor 0.00186219739292365
wandb: Currently logged in as: jackcai1206. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /home/zcai75/Github/OFA_forked/run_scripts/sgcls/wandb/run-20230808_160753-303vy8x0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run _4_3e-5_512
wandb: ⭐️ View project at https://wandb.ai/jackcai1206/OFA-VG
wandb: 🚀 View run at https://wandb.ai/jackcai1206/OFA-VG/runs/303vy8x0
2023-08-08 16:07:59 - trainer.py[line:703] - INFO: begin training epoch 1
2023-08-08 16:07:59 - train.py[line:305] - INFO: Start iterating over samples
/home/zcai75/Github/OFA/fairseq/fairseq/utils.py:372: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2023-08-08 16:08:02 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-08 16:08:03 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-08 16:08:05 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-08 16:08:07 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-08 16:08:08 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-08-08 16:08:14 - progress_bar.py[line:272] - INFO: epoch 001:     15 / 2241 loss=13.864, loss_v1=0, loss_v2=0, nll_loss=13.276, ntokens=849.6, nsentences=28, sample_size=849.6, sample_size_v1=0, sample_size_v2=0, ppl=9921.11, wps=723.2, ups=0.87, wpb=849.6, bsz=28, num_updates=10, lr=5.58659e-07, gnorm=0.397, clip=0, loss_scale=4, train_wall=15, gb_free=17.2, wall=26
2023-08-08 16:08:19 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-08-08 16:08:24 - progress_bar.py[line:272] - INFO: epoch 001:     26 / 2241 loss=13.991, loss_v1=0, loss_v2=0, nll_loss=13.407, ntokens=873, nsentences=27.6, sample_size=873, sample_size_v1=0, sample_size_v2=0, ppl=10861.5, wps=934.8, ups=1.07, wpb=873, bsz=27.6, num_updates=20, lr=1.11732e-06, gnorm=0.407, clip=0, loss_scale=2, train_wall=9, gb_free=17.3, wall=35
2023-08-08 16:08:32 - progress_bar.py[line:272] - INFO: epoch 001:     36 / 2241 loss=13.588, loss_v1=0, loss_v2=0, nll_loss=13.006, ntokens=685.1, nsentences=28, sample_size=685.1, sample_size_v1=0, sample_size_v2=0, ppl=8227.24, wps=817.8, ups=1.19, wpb=685.1, bsz=28, num_updates=30, lr=1.67598e-06, gnorm=0.395, clip=0, loss_scale=2, train_wall=8, gb_free=17.6, wall=43
2023-08-08 16:08:38 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2023-08-08 16:08:42 - progress_bar.py[line:272] - INFO: epoch 001:     47 / 2241 loss=13.668, loss_v1=0, loss_v2=0, nll_loss=13.023, ntokens=873.8, nsentences=28, sample_size=873.8, sample_size_v1=0, sample_size_v2=0, ppl=8323.51, wps=913.3, ups=1.05, wpb=873.8, bsz=28, num_updates=40, lr=2.23464e-06, gnorm=0.471, clip=0, loss_scale=1, train_wall=10, gb_free=16.6, wall=53
2023-08-08 16:08:50 - progress_bar.py[line:272] - INFO: epoch 001:     57 / 2241 loss=13.57, loss_v1=0, loss_v2=0, nll_loss=12.947, ntokens=867.3, nsentences=28, sample_size=867.3, sample_size_v1=0, sample_size_v2=0, ppl=7899.07, wps=1006.6, ups=1.16, wpb=867.3, bsz=28, num_updates=50, lr=2.7933e-06, gnorm=0.394, clip=0, loss_scale=1, train_wall=9, gb_free=17, wall=62
2023-08-08 16:08:59 - progress_bar.py[line:272] - INFO: epoch 001:     67 / 2241 loss=13.804, loss_v1=0, loss_v2=0, nll_loss=13.194, ntokens=802.9, nsentences=28, sample_size=802.9, sample_size_v1=0, sample_size_v2=0, ppl=9368.88, wps=935.5, ups=1.17, wpb=802.9, bsz=28, num_updates=60, lr=3.35196e-06, gnorm=0.423, clip=0, loss_scale=1, train_wall=9, gb_free=16.6, wall=70
2023-08-08 16:09:07 - progress_bar.py[line:272] - INFO: epoch 001:     77 / 2241 loss=13.663, loss_v1=0, loss_v2=0, nll_loss=13.031, ntokens=731.1, nsentences=28, sample_size=731.1, sample_size_v1=0, sample_size_v2=0, ppl=8369.33, wps=862.5, ups=1.18, wpb=731.1, bsz=28, num_updates=70, lr=3.91061e-06, gnorm=0.415, clip=0, loss_scale=1, train_wall=8, gb_free=16.7, wall=79
2023-08-08 16:09:16 - progress_bar.py[line:272] - INFO: epoch 001:     87 / 2241 loss=13.842, loss_v1=0, loss_v2=0, nll_loss=13.224, ntokens=949.3, nsentences=28, sample_size=949.3, sample_size_v1=0, sample_size_v2=0, ppl=9567.91, wps=1085.6, ups=1.14, wpb=949.3, bsz=28, num_updates=80, lr=4.46927e-06, gnorm=0.419, clip=0, loss_scale=1, train_wall=9, gb_free=17, wall=87
2023-08-08 16:09:25 - progress_bar.py[line:272] - INFO: epoch 001:     97 / 2241 loss=13.948, loss_v1=0, loss_v2=0, nll_loss=13.341, ntokens=1108.1, nsentences=28, sample_size=1108.1, sample_size_v1=0, sample_size_v2=0, ppl=10379.2, wps=1252.9, ups=1.13, wpb=1108.1, bsz=28, num_updates=90, lr=5.02793e-06, gnorm=0.428, clip=0, loss_scale=1, train_wall=9, gb_free=16.3, wall=96
2023-08-08 16:09:32 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2023-08-08 16:09:35 - progress_bar.py[line:272] - INFO: epoch 001:    108 / 2241 loss=13.631, loss_v1=0, loss_v2=0, nll_loss=13.036, ntokens=1040.1, nsentences=28, sample_size=1040.1, sample_size_v1=0, sample_size_v2=0, ppl=8397.71, wps=1066.8, ups=1.03, wpb=1040.1, bsz=28, num_updates=100, lr=5.58659e-06, gnorm=0.423, clip=0, loss_scale=0.5, train_wall=10, gb_free=16.3, wall=106
2023-08-08 16:09:43 - progress_bar.py[line:272] - INFO: epoch 001:    118 / 2241 loss=13.598, loss_v1=0, loss_v2=0, nll_loss=12.999, ntokens=843.8, nsentences=28, sample_size=843.8, sample_size_v1=0, sample_size_v2=0, ppl=8185.06, wps=991.5, ups=1.17, wpb=843.8, bsz=28, num_updates=110, lr=6.14525e-06, gnorm=0.416, clip=0, loss_scale=0.5, train_wall=8, gb_free=16.2, wall=114
2023-08-08 16:09:52 - progress_bar.py[line:272] - INFO: epoch 001:    128 / 2241 loss=13.596, loss_v1=0, loss_v2=0, nll_loss=12.976, ntokens=880.1, nsentences=28, sample_size=880.1, sample_size_v1=0, sample_size_v2=0, ppl=8057.63, wps=1025.4, ups=1.17, wpb=880.1, bsz=28, num_updates=120, lr=6.70391e-06, gnorm=0.423, clip=0, loss_scale=0.5, train_wall=9, gb_free=16.7, wall=123
2023-08-08 16:10:00 - progress_bar.py[line:272] - INFO: epoch 001:    138 / 2241 loss=13.391, loss_v1=0, loss_v2=0, nll_loss=12.804, ntokens=740.2, nsentences=28, sample_size=740.2, sample_size_v1=0, sample_size_v2=0, ppl=7150.38, wps=872.2, ups=1.18, wpb=740.2, bsz=28, num_updates=130, lr=7.26257e-06, gnorm=0.395, clip=0, loss_scale=0.5, train_wall=8, gb_free=16.4, wall=132
2023-08-08 16:10:09 - progress_bar.py[line:272] - INFO: epoch 001:    148 / 2241 loss=13.581, loss_v1=0, loss_v2=0, nll_loss=12.995, ntokens=786.6, nsentences=28, sample_size=786.6, sample_size_v1=0, sample_size_v2=0, ppl=8162.06, wps=924.3, ups=1.18, wpb=786.6, bsz=28, num_updates=140, lr=7.82123e-06, gnorm=0.413, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.6, wall=140
2023-08-08 16:10:17 - progress_bar.py[line:272] - INFO: epoch 001:    158 / 2241 loss=13.448, loss_v1=0, loss_v2=0, nll_loss=12.864, ntokens=801.2, nsentences=28, sample_size=801.2, sample_size_v1=0, sample_size_v2=0, ppl=7454.54, wps=936.5, ups=1.17, wpb=801.2, bsz=28, num_updates=150, lr=8.37989e-06, gnorm=0.364, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.1, wall=149
2023-08-08 16:10:26 - progress_bar.py[line:272] - INFO: epoch 001:    168 / 2241 loss=13.44, loss_v1=0, loss_v2=0, nll_loss=12.883, ntokens=922, nsentences=28, sample_size=922, sample_size_v1=0, sample_size_v2=0, ppl=7555.37, wps=1060.3, ups=1.15, wpb=922, bsz=28, num_updates=160, lr=8.93855e-06, gnorm=0.37, clip=0, loss_scale=0.5, train_wall=9, gb_free=16.4, wall=157
2023-08-08 16:10:35 - progress_bar.py[line:272] - INFO: epoch 001:    178 / 2241 loss=13.497, loss_v1=0, loss_v2=0, nll_loss=12.93, ntokens=929.3, nsentences=28, sample_size=929.3, sample_size_v1=0, sample_size_v2=0, ppl=7803.39, wps=1062.1, ups=1.14, wpb=929.3, bsz=28, num_updates=170, lr=9.49721e-06, gnorm=0.356, clip=0, loss_scale=0.5, train_wall=9, gb_free=16.8, wall=166
2023-08-08 16:10:44 - progress_bar.py[line:272] - INFO: epoch 001:    188 / 2241 loss=13.493, loss_v1=0, loss_v2=0, nll_loss=12.928, ntokens=952.9, nsentences=28, sample_size=952.9, sample_size_v1=0, sample_size_v2=0, ppl=7794.65, wps=1100.3, ups=1.15, wpb=952.9, bsz=28, num_updates=180, lr=1.00559e-05, gnorm=0.377, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.3, wall=175
2023-08-08 16:10:52 - progress_bar.py[line:272] - INFO: epoch 001:    198 / 2241 loss=13.433, loss_v1=0, loss_v2=0, nll_loss=12.861, ntokens=963.7, nsentences=28, sample_size=963.7, sample_size_v1=0, sample_size_v2=0, ppl=7440.62, wps=1097.5, ups=1.14, wpb=963.7, bsz=28, num_updates=190, lr=1.06145e-05, gnorm=0.352, clip=0, loss_scale=0.5, train_wall=9, gb_free=16.9, wall=184
2023-08-08 16:11:01 - progress_bar.py[line:272] - INFO: epoch 001:    208 / 2241 loss=13.435, loss_v1=0, loss_v2=0, nll_loss=12.866, ntokens=895.4, nsentences=28, sample_size=895.4, sample_size_v1=0, sample_size_v2=0, ppl=7466.48, wps=1030.6, ups=1.15, wpb=895.4, bsz=28, num_updates=200, lr=1.11732e-05, gnorm=0.357, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.1, wall=192
2023-08-08 16:11:10 - progress_bar.py[line:272] - INFO: epoch 001:    218 / 2241 loss=13.29, loss_v1=0, loss_v2=0, nll_loss=12.726, ntokens=817.5, nsentences=28, sample_size=817.5, sample_size_v1=0, sample_size_v2=0, ppl=6776.8, wps=944.4, ups=1.16, wpb=817.5, bsz=28, num_updates=210, lr=1.17318e-05, gnorm=0.351, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.6, wall=201
2023-08-08 16:11:18 - progress_bar.py[line:272] - INFO: epoch 001:    228 / 2241 loss=13.27, loss_v1=0, loss_v2=0, nll_loss=12.702, ntokens=761.7, nsentences=28, sample_size=761.7, sample_size_v1=0, sample_size_v2=0, ppl=6661.5, wps=889.9, ups=1.17, wpb=761.7, bsz=28, num_updates=220, lr=1.22905e-05, gnorm=0.338, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.2, wall=209
2023-08-08 16:11:27 - progress_bar.py[line:272] - INFO: epoch 001:    238 / 2241 loss=13.377, loss_v1=0, loss_v2=0, nll_loss=12.79, ntokens=650.1, nsentences=28, sample_size=650.1, sample_size_v1=0, sample_size_v2=0, ppl=7080.63, wps=768.7, ups=1.18, wpb=650.1, bsz=28, num_updates=230, lr=1.28492e-05, gnorm=0.36, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.4, wall=218
2023-08-08 16:11:35 - progress_bar.py[line:272] - INFO: epoch 001:    248 / 2241 loss=13.414, loss_v1=0, loss_v2=0, nll_loss=12.846, ntokens=935.7, nsentences=28, sample_size=935.7, sample_size_v1=0, sample_size_v2=0, ppl=7365.03, wps=1075.9, ups=1.15, wpb=935.7, bsz=28, num_updates=240, lr=1.34078e-05, gnorm=0.363, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.3, wall=227
2023-08-08 16:11:44 - progress_bar.py[line:272] - INFO: epoch 001:    258 / 2241 loss=13.174, loss_v1=0, loss_v2=0, nll_loss=12.61, ntokens=826.9, nsentences=28, sample_size=826.9, sample_size_v1=0, sample_size_v2=0, ppl=6250.26, wps=948.3, ups=1.15, wpb=826.9, bsz=28, num_updates=250, lr=1.39665e-05, gnorm=0.388, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.1, wall=235
2023-08-08 16:11:53 - progress_bar.py[line:272] - INFO: epoch 001:    268 / 2241 loss=13.329, loss_v1=0, loss_v2=0, nll_loss=12.766, ntokens=858.9, nsentences=28, sample_size=858.9, sample_size_v1=0, sample_size_v2=0, ppl=6966.91, wps=990.1, ups=1.15, wpb=858.9, bsz=28, num_updates=260, lr=1.45251e-05, gnorm=0.352, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.1, wall=244
2023-08-08 16:12:01 - progress_bar.py[line:272] - INFO: epoch 001:    278 / 2241 loss=13.473, loss_v1=0, loss_v2=0, nll_loss=12.922, ntokens=871.9, nsentences=28, sample_size=871.9, sample_size_v1=0, sample_size_v2=0, ppl=7761.93, wps=1016.4, ups=1.17, wpb=871.9, bsz=28, num_updates=270, lr=1.50838e-05, gnorm=0.36, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.1, wall=253
2023-08-08 16:12:10 - progress_bar.py[line:272] - INFO: epoch 001:    288 / 2241 loss=13.636, loss_v1=0, loss_v2=0, nll_loss=13.092, ntokens=771.9, nsentences=28, sample_size=771.9, sample_size_v1=0, sample_size_v2=0, ppl=8731.61, wps=912.1, ups=1.18, wpb=771.9, bsz=28, num_updates=280, lr=1.56425e-05, gnorm=0.388, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.4, wall=261
2023-08-08 16:12:18 - progress_bar.py[line:272] - INFO: epoch 001:    298 / 2241 loss=13.496, loss_v1=0, loss_v2=0, nll_loss=12.95, ntokens=947.8, nsentences=28, sample_size=947.8, sample_size_v1=0, sample_size_v2=0, ppl=7911.94, wps=1110.2, ups=1.17, wpb=947.8, bsz=28, num_updates=290, lr=1.62011e-05, gnorm=0.365, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.4, wall=270
2023-08-08 16:12:27 - progress_bar.py[line:272] - INFO: epoch 001:    308 / 2241 loss=13.472, loss_v1=0, loss_v2=0, nll_loss=12.903, ntokens=911.1, nsentences=28, sample_size=911.1, sample_size_v1=0, sample_size_v2=0, ppl=7657.95, wps=1058.5, ups=1.16, wpb=911.1, bsz=28, num_updates=300, lr=1.67598e-05, gnorm=0.356, clip=0, loss_scale=0.5, train_wall=9, gb_free=17, wall=278
2023-08-08 16:12:36 - progress_bar.py[line:272] - INFO: epoch 001:    318 / 2241 loss=13.43, loss_v1=0, loss_v2=0, nll_loss=12.893, ntokens=897, nsentences=28, sample_size=897, sample_size_v1=0, sample_size_v2=0, ppl=7603.91, wps=1047.8, ups=1.17, wpb=897, bsz=28, num_updates=310, lr=1.73184e-05, gnorm=0.347, clip=0, loss_scale=0.5, train_wall=9, gb_free=17, wall=287
2023-08-08 16:12:44 - progress_bar.py[line:272] - INFO: epoch 001:    328 / 2241 loss=13.551, loss_v1=0, loss_v2=0, nll_loss=13.018, ntokens=932.4, nsentences=28, sample_size=932.4, sample_size_v1=0, sample_size_v2=0, ppl=8296.7, wps=1086.3, ups=1.17, wpb=932.4, bsz=28, num_updates=320, lr=1.78771e-05, gnorm=0.323, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.2, wall=295
2023-08-08 16:12:53 - progress_bar.py[line:272] - INFO: epoch 001:    338 / 2241 loss=13.41, loss_v1=0, loss_v2=0, nll_loss=12.867, ntokens=959.6, nsentences=28, sample_size=959.6, sample_size_v1=0, sample_size_v2=0, ppl=7469.38, wps=1119.7, ups=1.17, wpb=959.6, bsz=28, num_updates=330, lr=1.84358e-05, gnorm=0.32, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.3, wall=304
2023-08-08 16:13:01 - progress_bar.py[line:272] - INFO: epoch 001:    348 / 2241 loss=13.437, loss_v1=0, loss_v2=0, nll_loss=12.901, ntokens=917.4, nsentences=28, sample_size=917.4, sample_size_v1=0, sample_size_v2=0, ppl=7647.76, wps=1075.6, ups=1.17, wpb=917.4, bsz=28, num_updates=340, lr=1.89944e-05, gnorm=0.336, clip=0, loss_scale=0.5, train_wall=9, gb_free=16.5, wall=312
2023-08-08 16:13:10 - progress_bar.py[line:272] - INFO: epoch 001:    358 / 2241 loss=13.268, loss_v1=0, loss_v2=0, nll_loss=12.709, ntokens=969, nsentences=28, sample_size=969, sample_size_v1=0, sample_size_v2=0, ppl=6696.26, wps=1127, ups=1.16, wpb=969, bsz=28, num_updates=350, lr=1.95531e-05, gnorm=0.311, clip=0, loss_scale=0.5, train_wall=9, gb_free=16.4, wall=321
2023-08-08 16:13:19 - progress_bar.py[line:272] - INFO: epoch 001:    368 / 2241 loss=13.444, loss_v1=0, loss_v2=0, nll_loss=12.897, ntokens=938.4, nsentences=28, sample_size=938.4, sample_size_v1=0, sample_size_v2=0, ppl=7627.64, wps=1086, ups=1.16, wpb=938.4, bsz=28, num_updates=360, lr=2.01117e-05, gnorm=0.354, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.4, wall=330
2023-08-08 16:13:27 - progress_bar.py[line:272] - INFO: epoch 001:    378 / 2241 loss=13.314, loss_v1=0, loss_v2=0, nll_loss=12.773, ntokens=934.1, nsentences=28, sample_size=934.1, sample_size_v1=0, sample_size_v2=0, ppl=7000.13, wps=1086.6, ups=1.16, wpb=934.1, bsz=28, num_updates=370, lr=2.06704e-05, gnorm=0.301, clip=0, loss_scale=0.5, train_wall=9, gb_free=17, wall=338
2023-08-08 16:13:36 - progress_bar.py[line:272] - INFO: epoch 001:    388 / 2241 loss=13.345, loss_v1=0, loss_v2=0, nll_loss=12.794, ntokens=913.5, nsentences=28, sample_size=913.5, sample_size_v1=0, sample_size_v2=0, ppl=7101.35, wps=1070.5, ups=1.17, wpb=913.5, bsz=28, num_updates=380, lr=2.12291e-05, gnorm=0.335, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.6, wall=347
2023-08-08 16:13:44 - progress_bar.py[line:272] - INFO: epoch 001:    398 / 2241 loss=13.349, loss_v1=0, loss_v2=0, nll_loss=12.798, ntokens=950.2, nsentences=28, sample_size=950.2, sample_size_v1=0, sample_size_v2=0, ppl=7121.21, wps=1113.9, ups=1.17, wpb=950.2, bsz=28, num_updates=390, lr=2.17877e-05, gnorm=0.337, clip=0, loss_scale=0.5, train_wall=9, gb_free=16.9, wall=355
2023-08-08 16:13:53 - progress_bar.py[line:272] - INFO: epoch 001:    408 / 2241 loss=13.145, loss_v1=0, loss_v2=0, nll_loss=12.585, ntokens=897, nsentences=28, sample_size=897, sample_size_v1=0, sample_size_v2=0, ppl=6142.48, wps=1054.9, ups=1.18, wpb=897, bsz=28, num_updates=400, lr=2.23464e-05, gnorm=0.328, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.2, wall=364
2023-08-08 16:14:01 - progress_bar.py[line:272] - INFO: epoch 001:    418 / 2241 loss=12.991, loss_v1=0, loss_v2=0, nll_loss=12.442, ntokens=839.7, nsentences=28, sample_size=839.7, sample_size_v1=0, sample_size_v2=0, ppl=5564.27, wps=995.9, ups=1.19, wpb=839.7, bsz=28, num_updates=410, lr=2.2905e-05, gnorm=0.304, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.3, wall=372
2023-08-08 16:14:10 - progress_bar.py[line:272] - INFO: epoch 001:    428 / 2241 loss=13.067, loss_v1=0, loss_v2=0, nll_loss=12.527, ntokens=818.1, nsentences=28, sample_size=818.1, sample_size_v1=0, sample_size_v2=0, ppl=5902.14, wps=966.3, ups=1.18, wpb=818.1, bsz=28, num_updates=420, lr=2.34637e-05, gnorm=0.296, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.2, wall=381
2023-08-08 16:14:18 - progress_bar.py[line:272] - INFO: epoch 001:    438 / 2241 loss=13.055, loss_v1=0, loss_v2=0, nll_loss=12.517, ntokens=844.2, nsentences=28, sample_size=844.2, sample_size_v1=0, sample_size_v2=0, ppl=5860.94, wps=1005.1, ups=1.19, wpb=844.2, bsz=28, num_updates=430, lr=2.40223e-05, gnorm=0.29, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.4, wall=389
2023-08-08 16:14:26 - progress_bar.py[line:272] - INFO: epoch 001:    448 / 2241 loss=12.883, loss_v1=0, loss_v2=0, nll_loss=12.334, ntokens=754.4, nsentences=28, sample_size=754.4, sample_size_v1=0, sample_size_v2=0, ppl=5162.8, wps=896.3, ups=1.19, wpb=754.4, bsz=28, num_updates=440, lr=2.4581e-05, gnorm=0.291, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.7, wall=398
2023-08-08 16:14:35 - progress_bar.py[line:272] - INFO: epoch 001:    458 / 2241 loss=12.984, loss_v1=0, loss_v2=0, nll_loss=12.444, ntokens=737.2, nsentences=28, sample_size=737.2, sample_size_v1=0, sample_size_v2=0, ppl=5570.44, wps=876.1, ups=1.19, wpb=737.2, bsz=28, num_updates=450, lr=2.51397e-05, gnorm=0.29, clip=0, loss_scale=0.5, train_wall=8, gb_free=17, wall=406
2023-08-08 16:14:43 - progress_bar.py[line:272] - INFO: epoch 001:    468 / 2241 loss=12.949, loss_v1=0, loss_v2=0, nll_loss=12.412, ntokens=783.3, nsentences=28, sample_size=783.3, sample_size_v1=0, sample_size_v2=0, ppl=5450.66, wps=934.5, ups=1.19, wpb=783.3, bsz=28, num_updates=460, lr=2.56983e-05, gnorm=0.283, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.2, wall=414
2023-08-08 16:14:52 - progress_bar.py[line:272] - INFO: epoch 001:    478 / 2241 loss=12.78, loss_v1=0, loss_v2=0, nll_loss=12.235, ntokens=735.5, nsentences=28, sample_size=735.5, sample_size_v1=0, sample_size_v2=0, ppl=4821.84, wps=876.9, ups=1.19, wpb=735.5, bsz=28, num_updates=470, lr=2.6257e-05, gnorm=0.325, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.1, wall=423
2023-08-08 16:15:00 - progress_bar.py[line:272] - INFO: epoch 001:    488 / 2241 loss=12.866, loss_v1=0, loss_v2=0, nll_loss=12.317, ntokens=742.4, nsentences=28, sample_size=742.4, sample_size_v1=0, sample_size_v2=0, ppl=5100.86, wps=880.1, ups=1.19, wpb=742.4, bsz=28, num_updates=480, lr=2.68156e-05, gnorm=0.278, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.4, wall=431
2023-08-08 16:15:09 - progress_bar.py[line:272] - INFO: epoch 001:    498 / 2241 loss=12.963, loss_v1=0, loss_v2=0, nll_loss=12.435, ntokens=890.4, nsentences=28, sample_size=890.4, sample_size_v1=0, sample_size_v2=0, ppl=5538.48, wps=1047.7, ups=1.18, wpb=890.4, bsz=28, num_updates=490, lr=2.73743e-05, gnorm=0.278, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.1, wall=440
2023-08-08 16:15:17 - progress_bar.py[line:272] - INFO: epoch 001:    508 / 2241 loss=12.906, loss_v1=0, loss_v2=0, nll_loss=12.372, ntokens=876.1, nsentences=28, sample_size=876.1, sample_size_v1=0, sample_size_v2=0, ppl=5301.85, wps=1036, ups=1.18, wpb=876.1, bsz=28, num_updates=500, lr=2.7933e-05, gnorm=0.305, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.4, wall=448
2023-08-08 16:15:25 - progress_bar.py[line:272] - INFO: epoch 001:    518 / 2241 loss=12.853, loss_v1=0, loss_v2=0, nll_loss=12.324, ntokens=747.3, nsentences=28, sample_size=747.3, sample_size_v1=0, sample_size_v2=0, ppl=5128.91, wps=890.3, ups=1.19, wpb=747.3, bsz=28, num_updates=510, lr=2.84916e-05, gnorm=0.286, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.7, wall=457
2023-08-08 16:15:34 - progress_bar.py[line:272] - INFO: epoch 001:    528 / 2241 loss=12.616, loss_v1=0, loss_v2=0, nll_loss=12.084, ntokens=832, nsentences=28, sample_size=832, sample_size_v1=0, sample_size_v2=0, ppl=4340.42, wps=981.9, ups=1.18, wpb=832, bsz=28, num_updates=520, lr=2.90503e-05, gnorm=0.257, clip=0, loss_scale=0.5, train_wall=8, gb_free=16.9, wall=465
2023-08-08 16:15:42 - progress_bar.py[line:272] - INFO: epoch 001:    538 / 2241 loss=12.611, loss_v1=0, loss_v2=0, nll_loss=12.091, ntokens=886.7, nsentences=28, sample_size=886.7, sample_size_v1=0, sample_size_v2=0, ppl=4361.29, wps=1043.9, ups=1.18, wpb=886.7, bsz=28, num_updates=530, lr=2.96089e-05, gnorm=0.264, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.3, wall=474
2023-08-08 16:15:51 - progress_bar.py[line:272] - INFO: epoch 001:    548 / 2241 loss=12.768, loss_v1=0, loss_v2=0, nll_loss=12.24, ntokens=823.3, nsentences=28, sample_size=823.3, sample_size_v1=0, sample_size_v2=0, ppl=4838.11, wps=942.8, ups=1.15, wpb=823.3, bsz=28, num_updates=540, lr=2.99893e-05, gnorm=0.273, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.6, wall=482
2023-08-08 16:16:00 - progress_bar.py[line:272] - INFO: epoch 001:    558 / 2241 loss=12.681, loss_v1=0, loss_v2=0, nll_loss=12.15, ntokens=799, nsentences=28, sample_size=799, sample_size_v1=0, sample_size_v2=0, ppl=4544.54, wps=942.6, ups=1.18, wpb=799, bsz=28, num_updates=550, lr=2.99537e-05, gnorm=0.287, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.7, wall=491
2023-08-08 16:16:08 - progress_bar.py[line:272] - INFO: epoch 001:    568 / 2241 loss=12.656, loss_v1=0, loss_v2=0, nll_loss=12.125, ntokens=850.7, nsentences=28, sample_size=850.7, sample_size_v1=0, sample_size_v2=0, ppl=4467.04, wps=1000.8, ups=1.18, wpb=850.7, bsz=28, num_updates=560, lr=2.99181e-05, gnorm=0.268, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.4, wall=499
2023-08-08 16:16:17 - progress_bar.py[line:272] - INFO: epoch 001:    578 / 2241 loss=12.626, loss_v1=0, loss_v2=0, nll_loss=12.103, ntokens=804.6, nsentences=28, sample_size=804.6, sample_size_v1=0, sample_size_v2=0, ppl=4400.08, wps=952.4, ups=1.18, wpb=804.6, bsz=28, num_updates=570, lr=2.98825e-05, gnorm=0.26, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.7, wall=508
2023-08-08 16:16:25 - progress_bar.py[line:272] - INFO: epoch 001:    588 / 2241 loss=12.638, loss_v1=0, loss_v2=0, nll_loss=12.095, ntokens=750.9, nsentences=28, sample_size=750.9, sample_size_v1=0, sample_size_v2=0, ppl=4374.39, wps=890.6, ups=1.19, wpb=750.9, bsz=28, num_updates=580, lr=2.98469e-05, gnorm=0.279, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.3, wall=516
2023-08-08 16:16:33 - progress_bar.py[line:272] - INFO: epoch 001:    598 / 2241 loss=12.558, loss_v1=0, loss_v2=0, nll_loss=12.028, ntokens=776.9, nsentences=28, sample_size=776.9, sample_size_v1=0, sample_size_v2=0, ppl=4177.04, wps=925.9, ups=1.19, wpb=776.9, bsz=28, num_updates=590, lr=2.98113e-05, gnorm=0.256, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.4, wall=525
2023-08-08 16:16:42 - progress_bar.py[line:272] - INFO: epoch 001:    608 / 2241 loss=12.497, loss_v1=0, loss_v2=0, nll_loss=11.977, ntokens=887.3, nsentences=28, sample_size=887.3, sample_size_v1=0, sample_size_v2=0, ppl=4032.51, wps=1041.9, ups=1.17, wpb=887.3, bsz=28, num_updates=600, lr=2.97757e-05, gnorm=0.261, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.4, wall=533
2023-08-08 16:16:50 - progress_bar.py[line:272] - INFO: epoch 001:    618 / 2241 loss=12.549, loss_v1=0, loss_v2=0, nll_loss=12.034, ntokens=860.2, nsentences=28, sample_size=860.2, sample_size_v1=0, sample_size_v2=0, ppl=4194.83, wps=1011, ups=1.18, wpb=860.2, bsz=28, num_updates=610, lr=2.97401e-05, gnorm=0.284, clip=0, loss_scale=1, train_wall=8, gb_free=17.2, wall=542
2023-08-08 16:16:59 - progress_bar.py[line:272] - INFO: epoch 001:    628 / 2241 loss=12.491, loss_v1=0, loss_v2=0, nll_loss=11.973, ntokens=863.6, nsentences=28, sample_size=863.6, sample_size_v1=0, sample_size_v2=0, ppl=4020.5, wps=1018.9, ups=1.18, wpb=863.6, bsz=28, num_updates=620, lr=2.97045e-05, gnorm=0.3, clip=0, loss_scale=1, train_wall=8, gb_free=15.5, wall=550
2023-08-08 16:17:07 - progress_bar.py[line:272] - INFO: epoch 001:    638 / 2241 loss=12.383, loss_v1=0, loss_v2=0, nll_loss=11.853, ntokens=715.2, nsentences=28, sample_size=715.2, sample_size_v1=0, sample_size_v2=0, ppl=3698, wps=861.2, ups=1.2, wpb=715.2, bsz=28, num_updates=630, lr=2.96689e-05, gnorm=0.251, clip=0, loss_scale=1, train_wall=8, gb_free=17.8, wall=558
2023-08-08 16:17:16 - progress_bar.py[line:272] - INFO: epoch 001:    648 / 2241 loss=12.441, loss_v1=0, loss_v2=0, nll_loss=11.916, ntokens=756.9, nsentences=28, sample_size=756.9, sample_size_v1=0, sample_size_v2=0, ppl=3863.1, wps=894.6, ups=1.18, wpb=756.9, bsz=28, num_updates=640, lr=2.96333e-05, gnorm=0.26, clip=0, loss_scale=1, train_wall=8, gb_free=17.4, wall=567
2023-08-08 16:17:24 - progress_bar.py[line:272] - INFO: epoch 001:    658 / 2241 loss=12.296, loss_v1=0, loss_v2=0, nll_loss=11.768, ntokens=750.9, nsentences=28, sample_size=750.9, sample_size_v1=0, sample_size_v2=0, ppl=3487.86, wps=898.9, ups=1.2, wpb=750.9, bsz=28, num_updates=650, lr=2.95977e-05, gnorm=0.245, clip=0, loss_scale=1, train_wall=8, gb_free=17.5, wall=575
2023-08-08 16:17:32 - progress_bar.py[line:272] - INFO: epoch 001:    668 / 2241 loss=12.242, loss_v1=0, loss_v2=0, nll_loss=11.725, ntokens=858.6, nsentences=28, sample_size=858.6, sample_size_v1=0, sample_size_v2=0, ppl=3385.85, wps=1019.6, ups=1.19, wpb=858.6, bsz=28, num_updates=660, lr=2.95621e-05, gnorm=0.24, clip=0, loss_scale=1, train_wall=8, gb_free=17.7, wall=584
2023-08-08 16:17:41 - progress_bar.py[line:272] - INFO: epoch 001:    678 / 2241 loss=12.299, loss_v1=0, loss_v2=0, nll_loss=11.788, ntokens=830.5, nsentences=28, sample_size=830.5, sample_size_v1=0, sample_size_v2=0, ppl=3536.15, wps=985.6, ups=1.19, wpb=830.5, bsz=28, num_updates=670, lr=2.95265e-05, gnorm=0.245, clip=0, loss_scale=1, train_wall=8, gb_free=17.6, wall=592
2023-08-08 16:17:49 - progress_bar.py[line:272] - INFO: epoch 001:    688 / 2241 loss=12.242, loss_v1=0, loss_v2=0, nll_loss=11.727, ntokens=771.6, nsentences=28, sample_size=771.6, sample_size_v1=0, sample_size_v2=0, ppl=3390.46, wps=925.9, ups=1.2, wpb=771.6, bsz=28, num_updates=680, lr=2.94909e-05, gnorm=0.24, clip=0, loss_scale=1, train_wall=8, gb_free=17.6, wall=600
2023-08-08 16:17:58 - progress_bar.py[line:272] - INFO: epoch 001:    698 / 2241 loss=12.282, loss_v1=0, loss_v2=0, nll_loss=11.764, ntokens=789.2, nsentences=28, sample_size=789.2, sample_size_v1=0, sample_size_v2=0, ppl=3477.21, wps=937.9, ups=1.19, wpb=789.2, bsz=28, num_updates=690, lr=2.94553e-05, gnorm=0.263, clip=0, loss_scale=1, train_wall=8, gb_free=17.3, wall=609
2023-08-08 16:18:06 - progress_bar.py[line:272] - INFO: epoch 001:    708 / 2241 loss=12.192, loss_v1=0, loss_v2=0, nll_loss=11.669, ntokens=795.2, nsentences=28, sample_size=795.2, sample_size_v1=0, sample_size_v2=0, ppl=3256.38, wps=943.1, ups=1.19, wpb=795.2, bsz=28, num_updates=700, lr=2.94197e-05, gnorm=0.253, clip=0, loss_scale=1, train_wall=8, gb_free=17, wall=617
2023-08-08 16:18:15 - progress_bar.py[line:272] - INFO: epoch 001:    718 / 2241 loss=12.318, loss_v1=0, loss_v2=0, nll_loss=11.805, ntokens=855.9, nsentences=28, sample_size=855.9, sample_size_v1=0, sample_size_v2=0, ppl=3578.54, wps=1006.6, ups=1.18, wpb=855.9, bsz=28, num_updates=710, lr=2.93841e-05, gnorm=0.242, clip=0, loss_scale=1, train_wall=8, gb_free=16.9, wall=626
2023-08-08 16:18:23 - progress_bar.py[line:272] - INFO: epoch 001:    728 / 2241 loss=12.129, loss_v1=0, loss_v2=0, nll_loss=11.622, ntokens=823.2, nsentences=28, sample_size=823.2, sample_size_v1=0, sample_size_v2=0, ppl=3151.22, wps=969, ups=1.18, wpb=823.2, bsz=28, num_updates=720, lr=2.93485e-05, gnorm=0.24, clip=0, loss_scale=1, train_wall=8, gb_free=16.5, wall=634
2023-08-08 16:18:32 - progress_bar.py[line:272] - INFO: epoch 001:    738 / 2241 loss=12.131, loss_v1=0, loss_v2=0, nll_loss=11.613, ntokens=820.8, nsentences=28, sample_size=820.8, sample_size_v1=0, sample_size_v2=0, ppl=3133.28, wps=974.3, ups=1.19, wpb=820.8, bsz=28, num_updates=730, lr=2.93129e-05, gnorm=0.237, clip=0, loss_scale=1, train_wall=8, gb_free=17.1, wall=643
2023-08-08 16:18:40 - progress_bar.py[line:272] - INFO: epoch 001:    748 / 2241 loss=12.041, loss_v1=0, loss_v2=0, nll_loss=11.524, ntokens=827.2, nsentences=28, sample_size=827.2, sample_size_v1=0, sample_size_v2=0, ppl=2945.9, wps=976.5, ups=1.18, wpb=827.2, bsz=28, num_updates=740, lr=2.92773e-05, gnorm=0.238, clip=0, loss_scale=1, train_wall=8, gb_free=17.3, wall=651
2023-08-08 16:18:49 - progress_bar.py[line:272] - INFO: epoch 001:    758 / 2241 loss=12.114, loss_v1=0, loss_v2=0, nll_loss=11.596, ntokens=796.9, nsentences=28, sample_size=796.9, sample_size_v1=0, sample_size_v2=0, ppl=3095.83, wps=938.6, ups=1.18, wpb=796.9, bsz=28, num_updates=750, lr=2.92417e-05, gnorm=0.235, clip=0, loss_scale=1, train_wall=8, gb_free=17.4, wall=660
2023-08-08 16:18:57 - progress_bar.py[line:272] - INFO: epoch 001:    768 / 2241 loss=11.995, loss_v1=0, loss_v2=0, nll_loss=11.474, ntokens=731.8, nsentences=28, sample_size=731.8, sample_size_v1=0, sample_size_v2=0, ppl=2845.5, wps=865.6, ups=1.18, wpb=731.8, bsz=28, num_updates=760, lr=2.92061e-05, gnorm=0.247, clip=0, loss_scale=1, train_wall=8, gb_free=17.8, wall=668
2023-08-08 16:19:05 - progress_bar.py[line:272] - INFO: epoch 001:    778 / 2241 loss=11.898, loss_v1=0, loss_v2=0, nll_loss=11.378, ntokens=781.9, nsentences=28, sample_size=781.9, sample_size_v1=0, sample_size_v2=0, ppl=2662.08, wps=926.3, ups=1.18, wpb=781.9, bsz=28, num_updates=770, lr=2.91705e-05, gnorm=0.231, clip=0, loss_scale=1, train_wall=8, gb_free=17.7, wall=677
2023-08-08 16:19:14 - progress_bar.py[line:272] - INFO: epoch 001:    788 / 2241 loss=11.888, loss_v1=0, loss_v2=0, nll_loss=11.373, ntokens=672.8, nsentences=28, sample_size=672.8, sample_size_v1=0, sample_size_v2=0, ppl=2652.88, wps=807.6, ups=1.2, wpb=672.8, bsz=28, num_updates=780, lr=2.91349e-05, gnorm=0.219, clip=0, loss_scale=1, train_wall=8, gb_free=17.6, wall=685
2023-08-08 16:19:22 - progress_bar.py[line:272] - INFO: epoch 001:    798 / 2241 loss=11.857, loss_v1=0, loss_v2=0, nll_loss=11.345, ntokens=721.2, nsentences=28, sample_size=721.2, sample_size_v1=0, sample_size_v2=0, ppl=2601.05, wps=864.4, ups=1.2, wpb=721.2, bsz=28, num_updates=790, lr=2.90993e-05, gnorm=0.226, clip=0, loss_scale=1, train_wall=8, gb_free=17.7, wall=693
2023-08-08 16:19:30 - progress_bar.py[line:272] - INFO: epoch 001:    808 / 2241 loss=11.886, loss_v1=0, loss_v2=0, nll_loss=11.381, ntokens=648.6, nsentences=28, sample_size=648.6, sample_size_v1=0, sample_size_v2=0, ppl=2666.26, wps=779.4, ups=1.2, wpb=648.6, bsz=28, num_updates=800, lr=2.90637e-05, gnorm=0.236, clip=0, loss_scale=1, train_wall=8, gb_free=17.5, wall=702
2023-08-08 16:19:39 - progress_bar.py[line:272] - INFO: epoch 001:    818 / 2241 loss=11.811, loss_v1=0, loss_v2=0, nll_loss=11.296, ntokens=731.2, nsentences=28, sample_size=731.2, sample_size_v1=0, sample_size_v2=0, ppl=2513.98, wps=869, ups=1.19, wpb=731.2, bsz=28, num_updates=810, lr=2.90281e-05, gnorm=0.222, clip=0, loss_scale=1, train_wall=8, gb_free=17.8, wall=710
2023-08-08 16:19:47 - progress_bar.py[line:272] - INFO: epoch 001:    828 / 2241 loss=11.76, loss_v1=0, loss_v2=0, nll_loss=11.246, ntokens=714.5, nsentences=28, sample_size=714.5, sample_size_v1=0, sample_size_v2=0, ppl=2428.26, wps=852.4, ups=1.19, wpb=714.5, bsz=28, num_updates=820, lr=2.89925e-05, gnorm=0.236, clip=0, loss_scale=1, train_wall=8, gb_free=17.8, wall=718
2023-08-08 16:19:56 - progress_bar.py[line:272] - INFO: epoch 001:    838 / 2241 loss=11.861, loss_v1=0, loss_v2=0, nll_loss=11.359, ntokens=788.3, nsentences=28, sample_size=788.3, sample_size_v1=0, sample_size_v2=0, ppl=2626.56, wps=931.6, ups=1.18, wpb=788.3, bsz=28, num_updates=830, lr=2.89569e-05, gnorm=0.221, clip=0, loss_scale=1, train_wall=8, gb_free=17.1, wall=727
2023-08-08 16:20:04 - progress_bar.py[line:272] - INFO: epoch 001:    848 / 2241 loss=11.843, loss_v1=0, loss_v2=0, nll_loss=11.342, ntokens=793.3, nsentences=28, sample_size=793.3, sample_size_v1=0, sample_size_v2=0, ppl=2594.99, wps=919.2, ups=1.16, wpb=793.3, bsz=28, num_updates=840, lr=2.89213e-05, gnorm=0.243, clip=0, loss_scale=1, train_wall=9, gb_free=17.2, wall=735
2023-08-08 16:20:13 - progress_bar.py[line:272] - INFO: epoch 001:    858 / 2241 loss=11.787, loss_v1=0, loss_v2=0, nll_loss=11.284, ntokens=664.4, nsentences=28, sample_size=664.4, sample_size_v1=0, sample_size_v2=0, ppl=2493.31, wps=790.2, ups=1.19, wpb=664.4, bsz=28, num_updates=850, lr=2.88857e-05, gnorm=0.224, clip=0, loss_scale=1, train_wall=8, gb_free=17.2, wall=744
2023-08-08 16:20:21 - progress_bar.py[line:272] - INFO: epoch 001:    868 / 2241 loss=11.611, loss_v1=0, loss_v2=0, nll_loss=11.098, ntokens=688.3, nsentences=28, sample_size=688.3, sample_size_v1=0, sample_size_v2=0, ppl=2191.33, wps=787, ups=1.14, wpb=688.3, bsz=28, num_updates=860, lr=2.88501e-05, gnorm=0.211, clip=0, loss_scale=1, train_wall=9, gb_free=17.7, wall=753
2023-08-08 16:20:30 - progress_bar.py[line:272] - INFO: epoch 001:    878 / 2241 loss=11.701, loss_v1=0, loss_v2=0, nll_loss=11.193, ntokens=765, nsentences=28, sample_size=765, sample_size_v1=0, sample_size_v2=0, ppl=2341.54, wps=906.5, ups=1.19, wpb=765, bsz=28, num_updates=870, lr=2.88145e-05, gnorm=0.211, clip=0, loss_scale=1, train_wall=8, gb_free=17.5, wall=761
2023-08-08 16:20:38 - progress_bar.py[line:272] - INFO: epoch 001:    888 / 2241 loss=11.613, loss_v1=0, loss_v2=0, nll_loss=11.114, ntokens=765.1, nsentences=28, sample_size=765.1, sample_size_v1=0, sample_size_v2=0, ppl=2215.76, wps=911.4, ups=1.19, wpb=765.1, bsz=28, num_updates=880, lr=2.87789e-05, gnorm=0.211, clip=0, loss_scale=1, train_wall=8, gb_free=17.2, wall=769
2023-08-08 16:20:47 - progress_bar.py[line:272] - INFO: epoch 001:    898 / 2241 loss=11.661, loss_v1=0, loss_v2=0, nll_loss=11.153, ntokens=734.4, nsentences=28, sample_size=734.4, sample_size_v1=0, sample_size_v2=0, ppl=2276.36, wps=879.1, ups=1.2, wpb=734.4, bsz=28, num_updates=890, lr=2.87433e-05, gnorm=0.227, clip=0, loss_scale=1, train_wall=8, gb_free=17.4, wall=778
2023-08-08 16:20:55 - progress_bar.py[line:272] - INFO: epoch 001:    908 / 2241 loss=11.699, loss_v1=0, loss_v2=0, nll_loss=11.2, ntokens=821.1, nsentences=28, sample_size=821.1, sample_size_v1=0, sample_size_v2=0, ppl=2352.81, wps=967.6, ups=1.18, wpb=821.1, bsz=28, num_updates=900, lr=2.87077e-05, gnorm=0.21, clip=0, loss_scale=1, train_wall=8, gb_free=17.6, wall=786
2023-08-08 16:21:04 - progress_bar.py[line:272] - INFO: epoch 001:    918 / 2241 loss=11.431, loss_v1=0, loss_v2=0, nll_loss=10.921, ntokens=718.1, nsentences=28, sample_size=718.1, sample_size_v1=0, sample_size_v2=0, ppl=1938.85, wps=860.5, ups=1.2, wpb=718.1, bsz=28, num_updates=910, lr=2.86721e-05, gnorm=0.211, clip=0, loss_scale=1, train_wall=8, gb_free=17.4, wall=795
2023-08-08 16:21:12 - progress_bar.py[line:272] - INFO: epoch 001:    928 / 2241 loss=11.564, loss_v1=0, loss_v2=0, nll_loss=11.069, ntokens=726.3, nsentences=28, sample_size=726.3, sample_size_v1=0, sample_size_v2=0, ppl=2147.72, wps=868.5, ups=1.2, wpb=726.3, bsz=28, num_updates=920, lr=2.86365e-05, gnorm=0.244, clip=0, loss_scale=1, train_wall=8, gb_free=17.1, wall=803
2023-08-08 16:21:20 - progress_bar.py[line:272] - INFO: epoch 001:    938 / 2241 loss=11.432, loss_v1=0, loss_v2=0, nll_loss=10.93, ntokens=669.5, nsentences=28, sample_size=669.5, sample_size_v1=0, sample_size_v2=0, ppl=1950.33, wps=801.2, ups=1.2, wpb=669.5, bsz=28, num_updates=930, lr=2.86009e-05, gnorm=0.209, clip=0, loss_scale=1, train_wall=8, gb_free=17.6, wall=811
2023-08-08 16:21:29 - progress_bar.py[line:272] - INFO: epoch 001:    948 / 2241 loss=11.453, loss_v1=0, loss_v2=0, nll_loss=10.944, ntokens=715.8, nsentences=28, sample_size=715.8, sample_size_v1=0, sample_size_v2=0, ppl=1969.94, wps=851.5, ups=1.19, wpb=715.8, bsz=28, num_updates=940, lr=2.85653e-05, gnorm=0.21, clip=0, loss_scale=1, train_wall=8, gb_free=17.5, wall=820
2023-08-08 16:21:37 - progress_bar.py[line:272] - INFO: epoch 001:    958 / 2241 loss=11.585, loss_v1=0, loss_v2=0, nll_loss=11.084, ntokens=760.8, nsentences=28, sample_size=760.8, sample_size_v1=0, sample_size_v2=0, ppl=2170.3, wps=901.6, ups=1.19, wpb=760.8, bsz=28, num_updates=950, lr=2.85297e-05, gnorm=0.212, clip=0, loss_scale=1, train_wall=8, gb_free=16.7, wall=828
2023-08-08 16:21:45 - progress_bar.py[line:272] - INFO: epoch 001:    968 / 2241 loss=11.45, loss_v1=0, loss_v2=0, nll_loss=10.952, ntokens=802, nsentences=28, sample_size=802, sample_size_v1=0, sample_size_v2=0, ppl=1980.59, wps=953.8, ups=1.19, wpb=802, bsz=28, num_updates=960, lr=2.84941e-05, gnorm=0.199, clip=0, loss_scale=1, train_wall=8, gb_free=17.3, wall=837
2023-08-08 16:21:54 - progress_bar.py[line:272] - INFO: epoch 001:    978 / 2241 loss=11.588, loss_v1=0, loss_v2=0, nll_loss=11.091, ntokens=791.8, nsentences=28, sample_size=791.8, sample_size_v1=0, sample_size_v2=0, ppl=2181.1, wps=938.2, ups=1.18, wpb=791.8, bsz=28, num_updates=970, lr=2.84585e-05, gnorm=0.209, clip=0, loss_scale=1, train_wall=8, gb_free=17.5, wall=845
2023-08-08 16:22:02 - progress_bar.py[line:272] - INFO: epoch 001:    988 / 2241 loss=11.412, loss_v1=0, loss_v2=0, nll_loss=10.922, ntokens=760, nsentences=28, sample_size=760, sample_size_v1=0, sample_size_v2=0, ppl=1940.49, wps=901.9, ups=1.19, wpb=760, bsz=28, num_updates=980, lr=2.84229e-05, gnorm=0.201, clip=0, loss_scale=1, train_wall=8, gb_free=17.5, wall=854
2023-08-08 16:22:11 - progress_bar.py[line:272] - INFO: epoch 001:    998 / 2241 loss=11.327, loss_v1=0, loss_v2=0, nll_loss=10.835, ntokens=768, nsentences=28, sample_size=768, sample_size_v1=0, sample_size_v2=0, ppl=1826.4, wps=920.5, ups=1.2, wpb=768, bsz=28, num_updates=990, lr=2.83873e-05, gnorm=0.191, clip=0, loss_scale=1, train_wall=8, gb_free=17.6, wall=862
2023-08-08 16:22:19 - progress_bar.py[line:272] - INFO: epoch 001:   1008 / 2241 loss=11.433, loss_v1=0, loss_v2=0, nll_loss=10.938, ntokens=783.1, nsentences=28, sample_size=783.1, sample_size_v1=0, sample_size_v2=0, ppl=1961.69, wps=929.1, ups=1.19, wpb=783.1, bsz=28, num_updates=1000, lr=2.83517e-05, gnorm=0.201, clip=0, loss_scale=1, train_wall=8, gb_free=16, wall=870
2023-08-08 16:22:28 - progress_bar.py[line:272] - INFO: epoch 001:   1018 / 2241 loss=11.454, loss_v1=0, loss_v2=0, nll_loss=10.955, ntokens=857.2, nsentences=28, sample_size=857.2, sample_size_v1=0, sample_size_v2=0, ppl=1985.34, wps=1011.4, ups=1.18, wpb=857.2, bsz=28, num_updates=1010, lr=2.83161e-05, gnorm=0.193, clip=0, loss_scale=1, train_wall=8, gb_free=16.7, wall=879
2023-08-08 16:22:36 - progress_bar.py[line:272] - INFO: epoch 001:   1028 / 2241 loss=11.369, loss_v1=0, loss_v2=0, nll_loss=10.878, ntokens=773.3, nsentences=28, sample_size=773.3, sample_size_v1=0, sample_size_v2=0, ppl=1881.57, wps=921.9, ups=1.19, wpb=773.3, bsz=28, num_updates=1020, lr=2.82805e-05, gnorm=0.199, clip=0, loss_scale=1, train_wall=8, gb_free=17.4, wall=887
2023-08-08 16:22:44 - progress_bar.py[line:272] - INFO: epoch 001:   1038 / 2241 loss=11.257, loss_v1=0, loss_v2=0, nll_loss=10.767, ntokens=844.8, nsentences=28, sample_size=844.8, sample_size_v1=0, sample_size_v2=0, ppl=1742.46, wps=1000.6, ups=1.18, wpb=844.8, bsz=28, num_updates=1030, lr=2.82449e-05, gnorm=0.191, clip=0, loss_scale=1, train_wall=8, gb_free=17.1, wall=896
2023-08-08 16:22:53 - progress_bar.py[line:272] - INFO: epoch 001:   1048 / 2241 loss=11.269, loss_v1=0, loss_v2=0, nll_loss=10.778, ntokens=731.2, nsentences=28, sample_size=731.2, sample_size_v1=0, sample_size_v2=0, ppl=1755.87, wps=869.9, ups=1.19, wpb=731.2, bsz=28, num_updates=1040, lr=2.82093e-05, gnorm=0.199, clip=0, loss_scale=1, train_wall=8, gb_free=16.8, wall=904
2023-08-08 16:23:01 - progress_bar.py[line:272] - INFO: epoch 001:   1058 / 2241 loss=11.292, loss_v1=0, loss_v2=0, nll_loss=10.798, ntokens=739.2, nsentences=28, sample_size=739.2, sample_size_v1=0, sample_size_v2=0, ppl=1779.87, wps=882.3, ups=1.19, wpb=739.2, bsz=28, num_updates=1050, lr=2.81737e-05, gnorm=0.196, clip=0, loss_scale=1, train_wall=8, gb_free=17.4, wall=912
2023-08-08 16:23:10 - progress_bar.py[line:272] - INFO: epoch 001:   1068 / 2241 loss=11.19, loss_v1=0, loss_v2=0, nll_loss=10.706, ntokens=710.5, nsentences=28, sample_size=710.5, sample_size_v1=0, sample_size_v2=0, ppl=1670.23, wps=840.7, ups=1.18, wpb=710.5, bsz=28, num_updates=1060, lr=2.81381e-05, gnorm=0.196, clip=0, loss_scale=1, train_wall=8, gb_free=17.8, wall=921
2023-08-08 16:23:18 - progress_bar.py[line:272] - INFO: epoch 001:   1078 / 2241 loss=11.189, loss_v1=0, loss_v2=0, nll_loss=10.697, ntokens=725.8, nsentences=28, sample_size=725.8, sample_size_v1=0, sample_size_v2=0, ppl=1659.63, wps=853.6, ups=1.18, wpb=725.8, bsz=28, num_updates=1070, lr=2.81025e-05, gnorm=0.188, clip=0, loss_scale=1, train_wall=8, gb_free=17.7, wall=929
2023-08-08 16:23:27 - progress_bar.py[line:272] - INFO: epoch 001:   1088 / 2241 loss=11.043, loss_v1=0, loss_v2=0, nll_loss=10.546, ntokens=734.9, nsentences=28, sample_size=734.9, sample_size_v1=0, sample_size_v2=0, ppl=1495.11, wps=878.4, ups=1.2, wpb=734.9, bsz=28, num_updates=1080, lr=2.80669e-05, gnorm=0.185, clip=0, loss_scale=1, train_wall=8, gb_free=17.4, wall=938
2023-08-08 16:23:35 - progress_bar.py[line:272] - INFO: epoch 001:   1098 / 2241 loss=11.113, loss_v1=0, loss_v2=0, nll_loss=10.622, ntokens=702.9, nsentences=28, sample_size=702.9, sample_size_v1=0, sample_size_v2=0, ppl=1576.25, wps=842.8, ups=1.2, wpb=702.9, bsz=28, num_updates=1090, lr=2.80313e-05, gnorm=0.195, clip=0, loss_scale=1, train_wall=8, gb_free=17.4, wall=946
2023-08-08 16:23:43 - progress_bar.py[line:272] - INFO: epoch 001:   1108 / 2241 loss=11.206, loss_v1=0, loss_v2=0, nll_loss=10.713, ntokens=822.5, nsentences=28, sample_size=822.5, sample_size_v1=0, sample_size_v2=0, ppl=1678.59, wps=973, ups=1.18, wpb=822.5, bsz=28, num_updates=1100, lr=2.79957e-05, gnorm=0.198, clip=0, loss_scale=1, train_wall=8, gb_free=17, wall=955
2023-08-08 16:23:52 - progress_bar.py[line:272] - INFO: epoch 001:   1118 / 2241 loss=11.107, loss_v1=0, loss_v2=0, nll_loss=10.617, ntokens=749.1, nsentences=28, sample_size=749.1, sample_size_v1=0, sample_size_v2=0, ppl=1570.8, wps=892.7, ups=1.19, wpb=749.1, bsz=28, num_updates=1110, lr=2.79601e-05, gnorm=0.196, clip=0, loss_scale=1, train_wall=8, gb_free=17.5, wall=963
2023-08-08 16:24:00 - progress_bar.py[line:272] - INFO: epoch 001:   1128 / 2241 loss=11.084, loss_v1=0, loss_v2=0, nll_loss=10.606, ntokens=778.7, nsentences=28, sample_size=778.7, sample_size_v1=0, sample_size_v2=0, ppl=1558.66, wps=920, ups=1.18, wpb=778.7, bsz=28, num_updates=1120, lr=2.79245e-05, gnorm=0.181, clip=0, loss_scale=1, train_wall=8, gb_free=17.3, wall=971
2023-08-08 16:24:09 - progress_bar.py[line:272] - INFO: epoch 001:   1138 / 2241 loss=11.062, loss_v1=0, loss_v2=0, nll_loss=10.571, ntokens=765.6, nsentences=28, sample_size=765.6, sample_size_v1=0, sample_size_v2=0, ppl=1521.14, wps=923, ups=1.21, wpb=765.6, bsz=28, num_updates=1130, lr=2.78889e-05, gnorm=0.196, clip=0, loss_scale=2, train_wall=8, gb_free=17.6, wall=980
2023-08-08 16:24:17 - progress_bar.py[line:272] - INFO: epoch 001:   1148 / 2241 loss=11.067, loss_v1=0, loss_v2=0, nll_loss=10.568, ntokens=817.5, nsentences=28, sample_size=817.5, sample_size_v1=0, sample_size_v2=0, ppl=1518.25, wps=968.8, ups=1.19, wpb=817.5, bsz=28, num_updates=1140, lr=2.78533e-05, gnorm=0.193, clip=0, loss_scale=2, train_wall=8, gb_free=17.5, wall=988
2023-08-08 16:24:25 - progress_bar.py[line:272] - INFO: epoch 001:   1158 / 2241 loss=11.072, loss_v1=0, loss_v2=0, nll_loss=10.583, ntokens=770.8, nsentences=28, sample_size=770.8, sample_size_v1=0, sample_size_v2=0, ppl=1534.41, wps=917, ups=1.19, wpb=770.8, bsz=28, num_updates=1150, lr=2.78177e-05, gnorm=0.191, clip=0, loss_scale=2, train_wall=8, gb_free=16.9, wall=997
2023-08-08 16:24:34 - progress_bar.py[line:272] - INFO: epoch 001:   1168 / 2241 loss=11.041, loss_v1=0, loss_v2=0, nll_loss=10.544, ntokens=841.7, nsentences=28, sample_size=841.7, sample_size_v1=0, sample_size_v2=0, ppl=1492.61, wps=992.1, ups=1.18, wpb=841.7, bsz=28, num_updates=1160, lr=2.77821e-05, gnorm=0.188, clip=0, loss_scale=2, train_wall=8, gb_free=17.3, wall=1005
2023-08-08 16:24:42 - progress_bar.py[line:272] - INFO: epoch 001:   1178 / 2241 loss=11.071, loss_v1=0, loss_v2=0, nll_loss=10.582, ntokens=797.8, nsentences=28, sample_size=797.8, sample_size_v1=0, sample_size_v2=0, ppl=1533.35, wps=938.2, ups=1.18, wpb=797.8, bsz=28, num_updates=1170, lr=2.77465e-05, gnorm=0.19, clip=0, loss_scale=2, train_wall=8, gb_free=17, wall=1014
2023-08-08 16:24:51 - progress_bar.py[line:272] - INFO: epoch 001:   1188 / 2241 loss=11.002, loss_v1=0, loss_v2=0, nll_loss=10.519, ntokens=756.6, nsentences=28, sample_size=756.6, sample_size_v1=0, sample_size_v2=0, ppl=1467.28, wps=896.4, ups=1.18, wpb=756.6, bsz=28, num_updates=1180, lr=2.77109e-05, gnorm=0.186, clip=0, loss_scale=2, train_wall=8, gb_free=17.5, wall=1022
2023-08-08 16:24:59 - progress_bar.py[line:272] - INFO: epoch 001:   1198 / 2241 loss=11.052, loss_v1=0, loss_v2=0, nll_loss=10.575, ntokens=801.8, nsentences=28, sample_size=801.8, sample_size_v1=0, sample_size_v2=0, ppl=1524.97, wps=948.6, ups=1.18, wpb=801.8, bsz=28, num_updates=1190, lr=2.76753e-05, gnorm=0.185, clip=0, loss_scale=2, train_wall=8, gb_free=16.4, wall=1030
2023-08-08 16:25:08 - progress_bar.py[line:272] - INFO: epoch 001:   1208 / 2241 loss=10.922, loss_v1=0, loss_v2=0, nll_loss=10.435, ntokens=799.8, nsentences=28, sample_size=799.8, sample_size_v1=0, sample_size_v2=0, ppl=1383.98, wps=947.3, ups=1.18, wpb=799.8, bsz=28, num_updates=1200, lr=2.76397e-05, gnorm=0.181, clip=0, loss_scale=2, train_wall=8, gb_free=17.1, wall=1039
2023-08-08 16:25:16 - progress_bar.py[line:272] - INFO: epoch 001:   1218 / 2241 loss=10.92, loss_v1=0, loss_v2=0, nll_loss=10.439, ntokens=872, nsentences=28, sample_size=872, sample_size_v1=0, sample_size_v2=0, ppl=1388.33, wps=1033.5, ups=1.19, wpb=872, bsz=28, num_updates=1210, lr=2.76041e-05, gnorm=0.182, clip=0, loss_scale=2, train_wall=8, gb_free=17.7, wall=1047
2023-08-08 16:25:25 - progress_bar.py[line:272] - INFO: epoch 001:   1228 / 2241 loss=10.947, loss_v1=0, loss_v2=0, nll_loss=10.461, ntokens=860.5, nsentences=28, sample_size=860.5, sample_size_v1=0, sample_size_v2=0, ppl=1409.92, wps=1009.9, ups=1.17, wpb=860.5, bsz=28, num_updates=1220, lr=2.75685e-05, gnorm=0.186, clip=0, loss_scale=2, train_wall=8, gb_free=17.4, wall=1056
2023-08-08 16:25:33 - progress_bar.py[line:272] - INFO: epoch 001:   1238 / 2241 loss=10.832, loss_v1=0, loss_v2=0, nll_loss=10.332, ntokens=840, nsentences=28, sample_size=840, sample_size_v1=0, sample_size_v2=0, ppl=1289.01, wps=986.3, ups=1.17, wpb=840, bsz=28, num_updates=1230, lr=2.75329e-05, gnorm=0.187, clip=0, loss_scale=2, train_wall=8, gb_free=17.5, wall=1064
2023-08-08 16:25:42 - progress_bar.py[line:272] - INFO: epoch 001:   1248 / 2241 loss=10.805, loss_v1=0, loss_v2=0, nll_loss=10.309, ntokens=854.1, nsentences=28, sample_size=854.1, sample_size_v1=0, sample_size_v2=0, ppl=1268.91, wps=1008.9, ups=1.18, wpb=854.1, bsz=28, num_updates=1240, lr=2.74973e-05, gnorm=0.184, clip=0, loss_scale=2, train_wall=8, gb_free=17.3, wall=1073
2023-08-08 16:25:50 - progress_bar.py[line:272] - INFO: epoch 001:   1258 / 2241 loss=10.863, loss_v1=0, loss_v2=0, nll_loss=10.368, ntokens=863.7, nsentences=28, sample_size=863.7, sample_size_v1=0, sample_size_v2=0, ppl=1321.78, wps=1020.1, ups=1.18, wpb=863.7, bsz=28, num_updates=1250, lr=2.74617e-05, gnorm=0.175, clip=0, loss_scale=2, train_wall=8, gb_free=17.5, wall=1081
2023-08-08 16:25:59 - progress_bar.py[line:272] - INFO: epoch 001:   1268 / 2241 loss=10.861, loss_v1=0, loss_v2=0, nll_loss=10.373, ntokens=814.5, nsentences=28, sample_size=814.5, sample_size_v1=0, sample_size_v2=0, ppl=1326.33, wps=960.8, ups=1.18, wpb=814.5, bsz=28, num_updates=1260, lr=2.74261e-05, gnorm=0.176, clip=0, loss_scale=2, train_wall=8, gb_free=17.2, wall=1090
2023-08-08 16:26:07 - progress_bar.py[line:272] - INFO: epoch 001:   1278 / 2241 loss=10.913, loss_v1=0, loss_v2=0, nll_loss=10.43, ntokens=808.9, nsentences=28, sample_size=808.9, sample_size_v1=0, sample_size_v2=0, ppl=1379.34, wps=954.5, ups=1.18, wpb=808.9, bsz=28, num_updates=1270, lr=2.73905e-05, gnorm=0.217, clip=0, loss_scale=2, train_wall=8, gb_free=17.2, wall=1098
2023-08-08 16:26:16 - progress_bar.py[line:272] - INFO: epoch 001:   1288 / 2241 loss=10.771, loss_v1=0, loss_v2=0, nll_loss=10.291, ntokens=874.9, nsentences=28, sample_size=874.9, sample_size_v1=0, sample_size_v2=0, ppl=1252.68, wps=1028.5, ups=1.18, wpb=874.9, bsz=28, num_updates=1280, lr=2.73549e-05, gnorm=0.175, clip=0, loss_scale=2, train_wall=8, gb_free=17.2, wall=1107
2023-08-08 16:26:24 - progress_bar.py[line:272] - INFO: epoch 001:   1298 / 2241 loss=10.732, loss_v1=0, loss_v2=0, nll_loss=10.241, ntokens=812.3, nsentences=28, sample_size=812.3, sample_size_v1=0, sample_size_v2=0, ppl=1209.91, wps=961.7, ups=1.18, wpb=812.3, bsz=28, num_updates=1290, lr=2.73193e-05, gnorm=0.175, clip=0, loss_scale=2, train_wall=8, gb_free=17.5, wall=1115
2023-08-08 16:26:33 - progress_bar.py[line:272] - INFO: epoch 001:   1308 / 2241 loss=10.627, loss_v1=0, loss_v2=0, nll_loss=10.141, ntokens=792.3, nsentences=28, sample_size=792.3, sample_size_v1=0, sample_size_v2=0, ppl=1129.4, wps=938, ups=1.18, wpb=792.3, bsz=28, num_updates=1300, lr=2.72837e-05, gnorm=0.174, clip=0, loss_scale=2, train_wall=8, gb_free=17.2, wall=1124
2023-08-08 16:26:41 - progress_bar.py[line:272] - INFO: epoch 001:   1318 / 2241 loss=10.748, loss_v1=0, loss_v2=0, nll_loss=10.263, ntokens=788.2, nsentences=28, sample_size=788.2, sample_size_v1=0, sample_size_v2=0, ppl=1228.73, wps=935.4, ups=1.19, wpb=788.2, bsz=28, num_updates=1310, lr=2.72481e-05, gnorm=0.178, clip=0, loss_scale=2, train_wall=8, gb_free=17.4, wall=1132
2023-08-08 16:26:49 - progress_bar.py[line:272] - INFO: epoch 001:   1328 / 2241 loss=10.748, loss_v1=0, loss_v2=0, nll_loss=10.252, ntokens=854.1, nsentences=28, sample_size=854.1, sample_size_v1=0, sample_size_v2=0, ppl=1219.72, wps=1001.6, ups=1.17, wpb=854.1, bsz=28, num_updates=1320, lr=2.72125e-05, gnorm=0.182, clip=0, loss_scale=2, train_wall=9, gb_free=17.2, wall=1141
2023-08-08 16:26:58 - progress_bar.py[line:272] - INFO: epoch 001:   1338 / 2241 loss=10.745, loss_v1=0, loss_v2=0, nll_loss=10.251, ntokens=851.1, nsentences=28, sample_size=851.1, sample_size_v1=0, sample_size_v2=0, ppl=1218.45, wps=995.6, ups=1.17, wpb=851.1, bsz=28, num_updates=1330, lr=2.71769e-05, gnorm=0.181, clip=0, loss_scale=2, train_wall=9, gb_free=16.9, wall=1149
2023-08-08 16:27:07 - progress_bar.py[line:272] - INFO: epoch 001:   1348 / 2241 loss=10.716, loss_v1=0, loss_v2=0, nll_loss=10.23, ntokens=918.5, nsentences=28, sample_size=918.5, sample_size_v1=0, sample_size_v2=0, ppl=1200.6, wps=1063.9, ups=1.16, wpb=918.5, bsz=28, num_updates=1340, lr=2.71413e-05, gnorm=0.179, clip=0, loss_scale=2, train_wall=9, gb_free=16.8, wall=1158
2023-08-08 16:27:15 - progress_bar.py[line:272] - INFO: epoch 001:   1358 / 2241 loss=10.574, loss_v1=0, loss_v2=0, nll_loss=10.089, ntokens=842.3, nsentences=28, sample_size=842.3, sample_size_v1=0, sample_size_v2=0, ppl=1088.8, wps=987.3, ups=1.17, wpb=842.3, bsz=28, num_updates=1350, lr=2.71057e-05, gnorm=0.193, clip=0, loss_scale=2, train_wall=9, gb_free=17.6, wall=1166
2023-08-08 16:27:24 - progress_bar.py[line:272] - INFO: epoch 001:   1368 / 2241 loss=10.547, loss_v1=0, loss_v2=0, nll_loss=10.058, ntokens=824.2, nsentences=28, sample_size=824.2, sample_size_v1=0, sample_size_v2=0, ppl=1066.28, wps=971.1, ups=1.18, wpb=824.2, bsz=28, num_updates=1360, lr=2.70701e-05, gnorm=0.172, clip=0, loss_scale=2, train_wall=8, gb_free=17.4, wall=1175
2023-08-08 16:27:32 - progress_bar.py[line:272] - INFO: epoch 001:   1378 / 2241 loss=10.607, loss_v1=0, loss_v2=0, nll_loss=10.115, ntokens=877.5, nsentences=28, sample_size=877.5, sample_size_v1=0, sample_size_v2=0, ppl=1108.85, wps=1029.2, ups=1.17, wpb=877.5, bsz=28, num_updates=1370, lr=2.70345e-05, gnorm=0.176, clip=0, loss_scale=2, train_wall=9, gb_free=17.3, wall=1183
2023-08-08 16:27:41 - progress_bar.py[line:272] - INFO: epoch 001:   1388 / 2241 loss=10.418, loss_v1=0, loss_v2=0, nll_loss=9.924, ntokens=791.9, nsentences=28, sample_size=791.9, sample_size_v1=0, sample_size_v2=0, ppl=971.26, wps=935.4, ups=1.18, wpb=791.9, bsz=28, num_updates=1380, lr=2.69989e-05, gnorm=0.173, clip=0, loss_scale=2, train_wall=8, gb_free=17.6, wall=1192
2023-08-08 16:27:49 - progress_bar.py[line:272] - INFO: epoch 001:   1398 / 2241 loss=10.613, loss_v1=0, loss_v2=0, nll_loss=10.137, ntokens=811.3, nsentences=28, sample_size=811.3, sample_size_v1=0, sample_size_v2=0, ppl=1126.28, wps=955.8, ups=1.18, wpb=811.3, bsz=28, num_updates=1390, lr=2.69633e-05, gnorm=0.173, clip=0, loss_scale=2, train_wall=8, gb_free=17.1, wall=1200
2023-08-08 16:27:58 - progress_bar.py[line:272] - INFO: epoch 001:   1408 / 2241 loss=10.642, loss_v1=0, loss_v2=0, nll_loss=10.155, ntokens=871.1, nsentences=28, sample_size=871.1, sample_size_v1=0, sample_size_v2=0, ppl=1140.42, wps=1026.1, ups=1.18, wpb=871.1, bsz=28, num_updates=1400, lr=2.69277e-05, gnorm=0.171, clip=0, loss_scale=2, train_wall=8, gb_free=16.8, wall=1209
2023-08-08 16:28:06 - progress_bar.py[line:272] - INFO: epoch 001:   1418 / 2241 loss=10.554, loss_v1=0, loss_v2=0, nll_loss=10.062, ntokens=861.9, nsentences=28, sample_size=861.9, sample_size_v1=0, sample_size_v2=0, ppl=1068.91, wps=1017.6, ups=1.18, wpb=861.9, bsz=28, num_updates=1410, lr=2.68921e-05, gnorm=0.173, clip=0, loss_scale=2, train_wall=8, gb_free=17.1, wall=1217
2023-08-08 16:28:15 - progress_bar.py[line:272] - INFO: epoch 001:   1428 / 2241 loss=10.455, loss_v1=0, loss_v2=0, nll_loss=9.959, ntokens=806.5, nsentences=28, sample_size=806.5, sample_size_v1=0, sample_size_v2=0, ppl=995.56, wps=954.1, ups=1.18, wpb=806.5, bsz=28, num_updates=1420, lr=2.68565e-05, gnorm=0.177, clip=0, loss_scale=2, train_wall=8, gb_free=17, wall=1226
2023-08-08 16:28:23 - progress_bar.py[line:272] - INFO: epoch 001:   1438 / 2241 loss=10.606, loss_v1=0, loss_v2=0, nll_loss=10.112, ntokens=863.4, nsentences=28, sample_size=863.4, sample_size_v1=0, sample_size_v2=0, ppl=1106.94, wps=1005.1, ups=1.16, wpb=863.4, bsz=28, num_updates=1430, lr=2.68209e-05, gnorm=0.176, clip=0, loss_scale=2, train_wall=9, gb_free=16.9, wall=1234
2023-08-08 16:28:32 - progress_bar.py[line:272] - INFO: epoch 001:   1448 / 2241 loss=10.408, loss_v1=0, loss_v2=0, nll_loss=9.928, ntokens=737.9, nsentences=28, sample_size=737.9, sample_size_v1=0, sample_size_v2=0, ppl=974.31, wps=876, ups=1.19, wpb=737.9, bsz=28, num_updates=1440, lr=2.67853e-05, gnorm=0.168, clip=0, loss_scale=2, train_wall=8, gb_free=17.3, wall=1243
2023-08-08 16:28:40 - progress_bar.py[line:272] - INFO: epoch 001:   1458 / 2241 loss=10.397, loss_v1=0, loss_v2=0, nll_loss=9.912, ntokens=757.1, nsentences=28, sample_size=757.1, sample_size_v1=0, sample_size_v2=0, ppl=963.29, wps=892.5, ups=1.18, wpb=757.1, bsz=28, num_updates=1450, lr=2.67497e-05, gnorm=0.193, clip=0, loss_scale=2, train_wall=8, gb_free=17.5, wall=1251
2023-08-08 16:28:49 - progress_bar.py[line:272] - INFO: epoch 001:   1468 / 2241 loss=10.49, loss_v1=0, loss_v2=0, nll_loss=9.994, ntokens=788.1, nsentences=28, sample_size=788.1, sample_size_v1=0, sample_size_v2=0, ppl=1019.64, wps=924.7, ups=1.17, wpb=788.1, bsz=28, num_updates=1460, lr=2.67141e-05, gnorm=0.181, clip=0, loss_scale=2, train_wall=9, gb_free=16.9, wall=1260
2023-08-08 16:28:57 - progress_bar.py[line:272] - INFO: epoch 001:   1478 / 2241 loss=10.303, loss_v1=0, loss_v2=0, nll_loss=9.81, ntokens=746.9, nsentences=28, sample_size=746.9, sample_size_v1=0, sample_size_v2=0, ppl=897.58, wps=889, ups=1.19, wpb=746.9, bsz=28, num_updates=1470, lr=2.66785e-05, gnorm=0.171, clip=0, loss_scale=2, train_wall=8, gb_free=16.8, wall=1268
2023-08-08 16:29:06 - progress_bar.py[line:272] - INFO: epoch 001:   1488 / 2241 loss=10.421, loss_v1=0, loss_v2=0, nll_loss=9.942, ntokens=844.6, nsentences=28, sample_size=844.6, sample_size_v1=0, sample_size_v2=0, ppl=983.53, wps=991.1, ups=1.17, wpb=844.6, bsz=28, num_updates=1480, lr=2.66429e-05, gnorm=0.168, clip=0, loss_scale=2, train_wall=9, gb_free=17.5, wall=1277
2023-08-08 16:29:14 - progress_bar.py[line:272] - INFO: epoch 001:   1498 / 2241 loss=10.441, loss_v1=0, loss_v2=0, nll_loss=9.961, ntokens=793.9, nsentences=28, sample_size=793.9, sample_size_v1=0, sample_size_v2=0, ppl=996.58, wps=935.6, ups=1.18, wpb=793.9, bsz=28, num_updates=1490, lr=2.66073e-05, gnorm=0.166, clip=0, loss_scale=2, train_wall=8, gb_free=16.6, wall=1285
2023-08-08 16:29:23 - progress_bar.py[line:272] - INFO: epoch 001:   1508 / 2241 loss=10.32, loss_v1=0, loss_v2=0, nll_loss=9.827, ntokens=792.1, nsentences=28, sample_size=792.1, sample_size_v1=0, sample_size_v2=0, ppl=908.29, wps=935.3, ups=1.18, wpb=792.1, bsz=28, num_updates=1500, lr=2.65717e-05, gnorm=0.167, clip=0, loss_scale=2, train_wall=8, gb_free=17.3, wall=1294
2023-08-08 16:29:31 - progress_bar.py[line:272] - INFO: epoch 001:   1518 / 2241 loss=10.259, loss_v1=0, loss_v2=0, nll_loss=9.765, ntokens=811.5, nsentences=28, sample_size=811.5, sample_size_v1=0, sample_size_v2=0, ppl=870.08, wps=958.7, ups=1.18, wpb=811.5, bsz=28, num_updates=1510, lr=2.65361e-05, gnorm=0.161, clip=0, loss_scale=2, train_wall=8, gb_free=17.2, wall=1302
2023-08-08 16:29:40 - progress_bar.py[line:272] - INFO: epoch 001:   1528 / 2241 loss=10.338, loss_v1=0, loss_v2=0, nll_loss=9.854, ntokens=837.9, nsentences=28, sample_size=837.9, sample_size_v1=0, sample_size_v2=0, ppl=925.58, wps=981.9, ups=1.17, wpb=837.9, bsz=28, num_updates=1520, lr=2.65005e-05, gnorm=0.171, clip=0, loss_scale=2, train_wall=9, gb_free=17.4, wall=1311
2023-08-08 16:29:48 - progress_bar.py[line:272] - INFO: epoch 001:   1538 / 2241 loss=10.336, loss_v1=0, loss_v2=0, nll_loss=9.851, ntokens=777, nsentences=28, sample_size=777, sample_size_v1=0, sample_size_v2=0, ppl=923.72, wps=921.6, ups=1.19, wpb=777, bsz=28, num_updates=1530, lr=2.64649e-05, gnorm=0.164, clip=0, loss_scale=2, train_wall=8, gb_free=17.2, wall=1319
2023-08-08 16:29:56 - progress_bar.py[line:272] - INFO: epoch 001:   1548 / 2241 loss=10.4, loss_v1=0, loss_v2=0, nll_loss=9.923, ntokens=759.4, nsentences=28, sample_size=759.4, sample_size_v1=0, sample_size_v2=0, ppl=971.05, wps=898.9, ups=1.18, wpb=759.4, bsz=28, num_updates=1540, lr=2.64293e-05, gnorm=0.17, clip=0, loss_scale=2, train_wall=8, gb_free=17, wall=1328
2023-08-08 16:30:05 - progress_bar.py[line:272] - INFO: epoch 001:   1558 / 2241 loss=10.394, loss_v1=0, loss_v2=0, nll_loss=9.903, ntokens=880.3, nsentences=28, sample_size=880.3, sample_size_v1=0, sample_size_v2=0, ppl=957.69, wps=1039.5, ups=1.18, wpb=880.3, bsz=28, num_updates=1550, lr=2.63937e-05, gnorm=0.162, clip=0, loss_scale=2, train_wall=8, gb_free=16.1, wall=1336
2023-08-08 16:30:13 - progress_bar.py[line:272] - INFO: epoch 001:   1568 / 2241 loss=10.383, loss_v1=0, loss_v2=0, nll_loss=9.89, ntokens=933.4, nsentences=28, sample_size=933.4, sample_size_v1=0, sample_size_v2=0, ppl=948.56, wps=1083.9, ups=1.16, wpb=933.4, bsz=28, num_updates=1560, lr=2.63581e-05, gnorm=0.166, clip=0, loss_scale=2, train_wall=9, gb_free=17.2, wall=1345
2023-08-08 16:30:22 - progress_bar.py[line:272] - INFO: epoch 001:   1578 / 2241 loss=10.235, loss_v1=0, loss_v2=0, nll_loss=9.746, ntokens=786.5, nsentences=28, sample_size=786.5, sample_size_v1=0, sample_size_v2=0, ppl=858.99, wps=932.4, ups=1.19, wpb=786.5, bsz=28, num_updates=1570, lr=2.63225e-05, gnorm=0.167, clip=0, loss_scale=2, train_wall=8, gb_free=17.1, wall=1353
2023-08-08 16:30:30 - progress_bar.py[line:272] - INFO: epoch 001:   1588 / 2241 loss=10.215, loss_v1=0, loss_v2=0, nll_loss=9.728, ntokens=821.7, nsentences=28, sample_size=821.7, sample_size_v1=0, sample_size_v2=0, ppl=848.03, wps=978, ups=1.19, wpb=821.7, bsz=28, num_updates=1580, lr=2.62869e-05, gnorm=0.162, clip=0, loss_scale=2, train_wall=8, gb_free=17.1, wall=1361
2023-08-08 16:30:39 - progress_bar.py[line:272] - INFO: epoch 001:   1598 / 2241 loss=10.219, loss_v1=0, loss_v2=0, nll_loss=9.735, ntokens=825.6, nsentences=28, sample_size=825.6, sample_size_v1=0, sample_size_v2=0, ppl=852.09, wps=981.3, ups=1.19, wpb=825.6, bsz=28, num_updates=1590, lr=2.62513e-05, gnorm=0.162, clip=0, loss_scale=2, train_wall=8, gb_free=17.5, wall=1370
2023-08-08 16:30:47 - progress_bar.py[line:272] - INFO: epoch 001:   1608 / 2241 loss=10.253, loss_v1=0, loss_v2=0, nll_loss=9.773, ntokens=829.1, nsentences=28, sample_size=829.1, sample_size_v1=0, sample_size_v2=0, ppl=874.72, wps=978.1, ups=1.18, wpb=829.1, bsz=28, num_updates=1600, lr=2.62157e-05, gnorm=0.159, clip=0, loss_scale=2, train_wall=8, gb_free=16.4, wall=1378
2023-08-08 16:30:56 - progress_bar.py[line:272] - INFO: epoch 001:   1618 / 2241 loss=10.125, loss_v1=0, loss_v2=0, nll_loss=9.619, ntokens=872, nsentences=28, sample_size=872, sample_size_v1=0, sample_size_v2=0, ppl=786.43, wps=1029, ups=1.18, wpb=872, bsz=28, num_updates=1610, lr=2.61801e-05, gnorm=0.163, clip=0, loss_scale=2, train_wall=8, gb_free=17.5, wall=1387
2023-08-08 16:31:04 - progress_bar.py[line:272] - INFO: epoch 001:   1628 / 2241 loss=10.289, loss_v1=0, loss_v2=0, nll_loss=9.799, ntokens=882.6, nsentences=28, sample_size=882.6, sample_size_v1=0, sample_size_v2=0, ppl=890.62, wps=1034.6, ups=1.17, wpb=882.6, bsz=28, num_updates=1620, lr=2.61445e-05, gnorm=0.167, clip=0, loss_scale=2, train_wall=9, gb_free=17.4, wall=1395
2023-08-08 16:31:13 - progress_bar.py[line:272] - INFO: epoch 001:   1638 / 2241 loss=10.261, loss_v1=0, loss_v2=0, nll_loss=9.781, ntokens=848.6, nsentences=28, sample_size=848.6, sample_size_v1=0, sample_size_v2=0, ppl=879.85, wps=997.9, ups=1.18, wpb=848.6, bsz=28, num_updates=1630, lr=2.61089e-05, gnorm=0.16, clip=0, loss_scale=2, train_wall=8, gb_free=17.6, wall=1404
2023-08-08 16:31:21 - progress_bar.py[line:272] - INFO: epoch 001:   1648 / 2241 loss=10.192, loss_v1=0, loss_v2=0, nll_loss=9.701, ntokens=830.5, nsentences=28, sample_size=830.5, sample_size_v1=0, sample_size_v2=0, ppl=832.45, wps=976.1, ups=1.18, wpb=830.5, bsz=28, num_updates=1640, lr=2.60733e-05, gnorm=0.164, clip=0, loss_scale=4, train_wall=8, gb_free=17.7, wall=1412
2023-08-08 16:31:30 - progress_bar.py[line:272] - INFO: epoch 001:   1658 / 2241 loss=10.12, loss_v1=0, loss_v2=0, nll_loss=9.634, ntokens=835.8, nsentences=28, sample_size=835.8, sample_size_v1=0, sample_size_v2=0, ppl=794.38, wps=986.7, ups=1.18, wpb=835.8, bsz=28, num_updates=1650, lr=2.60377e-05, gnorm=0.163, clip=0, loss_scale=4, train_wall=8, gb_free=17.3, wall=1421
2023-08-08 16:31:36 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-08-08 16:31:39 - progress_bar.py[line:272] - INFO: epoch 001:   1669 / 2241 loss=10.148, loss_v1=0, loss_v2=0, nll_loss=9.664, ntokens=848.1, nsentences=28, sample_size=848.1, sample_size_v1=0, sample_size_v2=0, ppl=811.01, wps=903.9, ups=1.07, wpb=848.1, bsz=28, num_updates=1660, lr=2.60021e-05, gnorm=0.161, clip=0, loss_scale=2, train_wall=9, gb_free=16.1, wall=1430
2023-08-08 16:31:48 - progress_bar.py[line:272] - INFO: epoch 001:   1679 / 2241 loss=10.104, loss_v1=0, loss_v2=0, nll_loss=9.612, ntokens=880.9, nsentences=28, sample_size=880.9, sample_size_v1=0, sample_size_v2=0, ppl=782.69, wps=1034.5, ups=1.17, wpb=880.9, bsz=28, num_updates=1670, lr=2.59665e-05, gnorm=0.158, clip=0, loss_scale=2, train_wall=8, gb_free=17.3, wall=1439
2023-08-08 16:31:56 - progress_bar.py[line:272] - INFO: epoch 001:   1689 / 2241 loss=10.118, loss_v1=0, loss_v2=0, nll_loss=9.633, ntokens=893.6, nsentences=28, sample_size=893.6, sample_size_v1=0, sample_size_v2=0, ppl=794.01, wps=1050, ups=1.18, wpb=893.6, bsz=28, num_updates=1680, lr=2.59309e-05, gnorm=0.161, clip=0, loss_scale=2, train_wall=8, gb_free=17.3, wall=1447
2023-08-08 16:32:05 - progress_bar.py[line:272] - INFO: epoch 001:   1699 / 2241 loss=10.149, loss_v1=0, loss_v2=0, nll_loss=9.664, ntokens=885.4, nsentences=28, sample_size=885.4, sample_size_v1=0, sample_size_v2=0, ppl=810.98, wps=1036.3, ups=1.17, wpb=885.4, bsz=28, num_updates=1690, lr=2.58953e-05, gnorm=0.16, clip=0, loss_scale=2, train_wall=9, gb_free=16.9, wall=1456
2023-08-08 16:32:13 - progress_bar.py[line:272] - INFO: epoch 001:   1709 / 2241 loss=10.131, loss_v1=0, loss_v2=0, nll_loss=9.644, ntokens=821.1, nsentences=28, sample_size=821.1, sample_size_v1=0, sample_size_v2=0, ppl=800.17, wps=968.1, ups=1.18, wpb=821.1, bsz=28, num_updates=1700, lr=2.58597e-05, gnorm=0.165, clip=0, loss_scale=2, train_wall=8, gb_free=17.3, wall=1464
2023-08-08 16:32:22 - progress_bar.py[line:272] - INFO: epoch 001:   1719 / 2241 loss=10.093, loss_v1=0, loss_v2=0, nll_loss=9.614, ntokens=892.2, nsentences=28, sample_size=892.2, sample_size_v1=0, sample_size_v2=0, ppl=783.62, wps=1038.8, ups=1.16, wpb=892.2, bsz=28, num_updates=1710, lr=2.58241e-05, gnorm=0.165, clip=0, loss_scale=2, train_wall=9, gb_free=16.7, wall=1473
2023-08-08 16:32:30 - progress_bar.py[line:272] - INFO: epoch 001:   1729 / 2241 loss=10.029, loss_v1=0, loss_v2=0, nll_loss=9.53, ntokens=858.8, nsentences=28, sample_size=858.8, sample_size_v1=0, sample_size_v2=0, ppl=739.04, wps=1008.3, ups=1.17, wpb=858.8, bsz=28, num_updates=1720, lr=2.57885e-05, gnorm=0.165, clip=0, loss_scale=2, train_wall=8, gb_free=17.2, wall=1481
2023-08-08 16:32:39 - progress_bar.py[line:272] - INFO: epoch 001:   1739 / 2241 loss=10.114, loss_v1=0, loss_v2=0, nll_loss=9.625, ntokens=922.3, nsentences=28, sample_size=922.3, sample_size_v1=0, sample_size_v2=0, ppl=789.36, wps=1082.1, ups=1.17, wpb=922.3, bsz=28, num_updates=1730, lr=2.57529e-05, gnorm=0.166, clip=0, loss_scale=2, train_wall=9, gb_free=17.5, wall=1490
2023-08-08 16:32:47 - progress_bar.py[line:272] - INFO: epoch 001:   1749 / 2241 loss=10.11, loss_v1=0, loss_v2=0, nll_loss=9.616, ntokens=981.3, nsentences=28, sample_size=981.3, sample_size_v1=0, sample_size_v2=0, ppl=784.83, wps=1140.1, ups=1.16, wpb=981.3, bsz=28, num_updates=1740, lr=2.57173e-05, gnorm=0.156, clip=0, loss_scale=2, train_wall=9, gb_free=16.8, wall=1499
2023-08-08 16:32:56 - progress_bar.py[line:272] - INFO: epoch 001:   1759 / 2241 loss=10.119, loss_v1=0, loss_v2=0, nll_loss=9.616, ntokens=930, nsentences=28, sample_size=930, sample_size_v1=0, sample_size_v2=0, ppl=784.67, wps=1086.2, ups=1.17, wpb=930, bsz=28, num_updates=1750, lr=2.56817e-05, gnorm=0.157, clip=0, loss_scale=2, train_wall=9, gb_free=17.1, wall=1507
2023-08-08 16:33:05 - progress_bar.py[line:272] - INFO: epoch 001:   1769 / 2241 loss=10.037, loss_v1=0, loss_v2=0, nll_loss=9.544, ntokens=909.5, nsentences=28, sample_size=909.5, sample_size_v1=0, sample_size_v2=0, ppl=746.37, wps=1068.6, ups=1.17, wpb=909.5, bsz=28, num_updates=1760, lr=2.56461e-05, gnorm=0.154, clip=0, loss_scale=2, train_wall=8, gb_free=17.3, wall=1516
2023-08-08 16:33:13 - progress_bar.py[line:272] - INFO: epoch 001:   1779 / 2241 loss=10.048, loss_v1=0, loss_v2=0, nll_loss=9.554, ntokens=886.7, nsentences=28, sample_size=886.7, sample_size_v1=0, sample_size_v2=0, ppl=751.58, wps=1041.1, ups=1.17, wpb=886.7, bsz=28, num_updates=1770, lr=2.56105e-05, gnorm=0.155, clip=0, loss_scale=2, train_wall=8, gb_free=16.6, wall=1524
2023-08-08 16:33:22 - progress_bar.py[line:272] - INFO: epoch 001:   1789 / 2241 loss=10.026, loss_v1=0, loss_v2=0, nll_loss=9.537, ntokens=897.8, nsentences=28, sample_size=897.8, sample_size_v1=0, sample_size_v2=0, ppl=743.08, wps=1052.6, ups=1.17, wpb=897.8, bsz=28, num_updates=1780, lr=2.55749e-05, gnorm=0.183, clip=0, loss_scale=2, train_wall=9, gb_free=17.6, wall=1533
2023-08-08 16:33:30 - progress_bar.py[line:272] - INFO: epoch 001:   1799 / 2241 loss=10.026, loss_v1=0, loss_v2=0, nll_loss=9.533, ntokens=973.8, nsentences=28, sample_size=973.8, sample_size_v1=0, sample_size_v2=0, ppl=740.61, wps=1129.1, ups=1.16, wpb=973.8, bsz=28, num_updates=1790, lr=2.55393e-05, gnorm=0.151, clip=0, loss_scale=2, train_wall=9, gb_free=16.8, wall=1541
2023-08-08 16:33:39 - progress_bar.py[line:272] - INFO: epoch 001:   1809 / 2241 loss=9.848, loss_v1=0, loss_v2=0, nll_loss=9.362, ntokens=821.2, nsentences=28, sample_size=821.2, sample_size_v1=0, sample_size_v2=0, ppl=657.92, wps=963.6, ups=1.17, wpb=821.2, bsz=28, num_updates=1800, lr=2.55037e-05, gnorm=0.148, clip=0, loss_scale=2, train_wall=9, gb_free=17.2, wall=1550
2023-08-08 16:33:47 - progress_bar.py[line:272] - INFO: epoch 001:   1819 / 2241 loss=10.019, loss_v1=0, loss_v2=0, nll_loss=9.528, ntokens=970.2, nsentences=28, sample_size=970.2, sample_size_v1=0, sample_size_v2=0, ppl=738.26, wps=1136.7, ups=1.17, wpb=970.2, bsz=28, num_updates=1810, lr=2.54681e-05, gnorm=0.155, clip=0, loss_scale=2, train_wall=9, gb_free=17.6, wall=1558
2023-08-08 16:33:56 - progress_bar.py[line:272] - INFO: epoch 001:   1829 / 2241 loss=9.964, loss_v1=0, loss_v2=0, nll_loss=9.478, ntokens=964.6, nsentences=28, sample_size=964.6, sample_size_v1=0, sample_size_v2=0, ppl=713.03, wps=1125.7, ups=1.17, wpb=964.6, bsz=28, num_updates=1820, lr=2.54325e-05, gnorm=0.151, clip=0, loss_scale=2, train_wall=9, gb_free=17.1, wall=1567
2023-08-08 16:34:04 - progress_bar.py[line:272] - INFO: epoch 001:   1839 / 2241 loss=10.09, loss_v1=0, loss_v2=0, nll_loss=9.604, ntokens=1088.7, nsentences=28, sample_size=1088.7, sample_size_v1=0, sample_size_v2=0, ppl=778.45, wps=1258.2, ups=1.16, wpb=1088.7, bsz=28, num_updates=1830, lr=2.53969e-05, gnorm=0.149, clip=0, loss_scale=2, train_wall=9, gb_free=17, wall=1576
2023-08-08 16:34:13 - progress_bar.py[line:272] - INFO: epoch 001:   1849 / 2241 loss=10.071, loss_v1=0, loss_v2=0, nll_loss=9.561, ntokens=1030.2, nsentences=28, sample_size=1030.2, sample_size_v1=0, sample_size_v2=0, ppl=755.59, wps=1183.4, ups=1.15, wpb=1030.2, bsz=28, num_updates=1840, lr=2.53613e-05, gnorm=0.159, clip=0, loss_scale=2, train_wall=9, gb_free=15.7, wall=1584
2023-08-08 16:34:22 - progress_bar.py[line:272] - INFO: epoch 001:   1859 / 2241 loss=10.023, loss_v1=0, loss_v2=0, nll_loss=9.53, ntokens=1010.9, nsentences=28, sample_size=1010.9, sample_size_v1=0, sample_size_v2=0, ppl=739.48, wps=1185, ups=1.17, wpb=1010.9, bsz=28, num_updates=1850, lr=2.53257e-05, gnorm=0.164, clip=0, loss_scale=2, train_wall=9, gb_free=17.2, wall=1593
2023-08-08 16:34:30 - progress_bar.py[line:272] - INFO: epoch 001:   1869 / 2241 loss=9.976, loss_v1=0, loss_v2=0, nll_loss=9.489, ntokens=934, nsentences=28, sample_size=934, sample_size_v1=0, sample_size_v2=0, ppl=718.56, wps=1092.2, ups=1.17, wpb=934, bsz=28, num_updates=1860, lr=2.52901e-05, gnorm=0.147, clip=0, loss_scale=2, train_wall=9, gb_free=16.6, wall=1601
2023-08-08 16:34:39 - progress_bar.py[line:272] - INFO: epoch 001:   1879 / 2241 loss=9.918, loss_v1=0, loss_v2=0, nll_loss=9.425, ntokens=906.3, nsentences=28, sample_size=906.3, sample_size_v1=0, sample_size_v2=0, ppl=687.17, wps=1065.8, ups=1.18, wpb=906.3, bsz=28, num_updates=1870, lr=2.52545e-05, gnorm=0.155, clip=0, loss_scale=2, train_wall=8, gb_free=17.2, wall=1610
2023-08-08 16:34:47 - progress_bar.py[line:272] - INFO: epoch 001:   1889 / 2241 loss=9.969, loss_v1=0, loss_v2=0, nll_loss=9.485, ntokens=938.1, nsentences=28, sample_size=938.1, sample_size_v1=0, sample_size_v2=0, ppl=716.73, wps=1098.2, ups=1.17, wpb=938.1, bsz=28, num_updates=1880, lr=2.52189e-05, gnorm=0.151, clip=0, loss_scale=2, train_wall=9, gb_free=17.2, wall=1618
2023-08-08 16:34:56 - progress_bar.py[line:272] - INFO: epoch 001:   1899 / 2241 loss=9.912, loss_v1=0, loss_v2=0, nll_loss=9.41, ntokens=962.8, nsentences=28, sample_size=962.8, sample_size_v1=0, sample_size_v2=0, ppl=680.52, wps=1109.3, ups=1.15, wpb=962.8, bsz=28, num_updates=1890, lr=2.51833e-05, gnorm=0.146, clip=0, loss_scale=2, train_wall=9, gb_free=17.1, wall=1627
2023-08-08 16:35:05 - progress_bar.py[line:272] - INFO: epoch 001:   1909 / 2241 loss=9.84, loss_v1=0, loss_v2=0, nll_loss=9.35, ntokens=880.3, nsentences=28, sample_size=880.3, sample_size_v1=0, sample_size_v2=0, ppl=652.44, wps=1027.1, ups=1.17, wpb=880.3, bsz=28, num_updates=1900, lr=2.51477e-05, gnorm=0.144, clip=0, loss_scale=2, train_wall=9, gb_free=17.5, wall=1636
2023-08-08 16:35:13 - progress_bar.py[line:272] - INFO: epoch 001:   1919 / 2241 loss=9.81, loss_v1=0, loss_v2=0, nll_loss=9.316, ntokens=847.3, nsentences=28, sample_size=847.3, sample_size_v1=0, sample_size_v2=0, ppl=637.22, wps=990, ups=1.17, wpb=847.3, bsz=28, num_updates=1910, lr=2.51121e-05, gnorm=0.147, clip=0, loss_scale=2, train_wall=9, gb_free=17.2, wall=1644
2023-08-08 16:35:22 - progress_bar.py[line:272] - INFO: epoch 001:   1929 / 2241 loss=9.874, loss_v1=0, loss_v2=0, nll_loss=9.377, ntokens=926.4, nsentences=28, sample_size=926.4, sample_size_v1=0, sample_size_v2=0, ppl=664.77, wps=1083.2, ups=1.17, wpb=926.4, bsz=28, num_updates=1920, lr=2.50765e-05, gnorm=0.151, clip=0, loss_scale=2, train_wall=9, gb_free=17.2, wall=1653
2023-08-08 16:35:30 - progress_bar.py[line:272] - INFO: epoch 001:   1939 / 2241 loss=9.827, loss_v1=0, loss_v2=0, nll_loss=9.331, ntokens=879.9, nsentences=28, sample_size=879.9, sample_size_v1=0, sample_size_v2=0, ppl=643.83, wps=1031.6, ups=1.17, wpb=879.9, bsz=28, num_updates=1930, lr=2.50409e-05, gnorm=0.154, clip=0, loss_scale=2, train_wall=9, gb_free=16.7, wall=1661
2023-08-08 16:35:39 - progress_bar.py[line:272] - INFO: epoch 001:   1949 / 2241 loss=9.82, loss_v1=0, loss_v2=0, nll_loss=9.321, ntokens=914.1, nsentences=28, sample_size=914.1, sample_size_v1=0, sample_size_v2=0, ppl=639.54, wps=1062.3, ups=1.16, wpb=914.1, bsz=28, num_updates=1940, lr=2.50053e-05, gnorm=0.144, clip=0, loss_scale=2, train_wall=9, gb_free=16.8, wall=1670
2023-08-08 16:35:47 - progress_bar.py[line:272] - INFO: epoch 001:   1959 / 2241 loss=9.696, loss_v1=0, loss_v2=0, nll_loss=9.2, ntokens=810, nsentences=28, sample_size=810, sample_size_v1=0, sample_size_v2=0, ppl=588.16, wps=953.4, ups=1.18, wpb=810, bsz=28, num_updates=1950, lr=2.49697e-05, gnorm=0.144, clip=0, loss_scale=2, train_wall=8, gb_free=17.2, wall=1679
2023-08-08 16:35:56 - progress_bar.py[line:272] - INFO: epoch 001:   1969 / 2241 loss=9.749, loss_v1=0, loss_v2=0, nll_loss=9.253, ntokens=842, nsentences=28, sample_size=842, sample_size_v1=0, sample_size_v2=0, ppl=610.19, wps=990.7, ups=1.18, wpb=842, bsz=28, num_updates=1960, lr=2.49341e-05, gnorm=0.139, clip=0, loss_scale=2, train_wall=8, gb_free=17.6, wall=1687
2023-08-08 16:36:04 - progress_bar.py[line:272] - INFO: epoch 001:   1979 / 2241 loss=9.743, loss_v1=0, loss_v2=0, nll_loss=9.246, ntokens=843.8, nsentences=28, sample_size=843.8, sample_size_v1=0, sample_size_v2=0, ppl=607.22, wps=989.7, ups=1.17, wpb=843.8, bsz=28, num_updates=1970, lr=2.48985e-05, gnorm=0.172, clip=0, loss_scale=2, train_wall=9, gb_free=16.9, wall=1696
2023-08-08 16:36:13 - progress_bar.py[line:272] - INFO: epoch 001:   1989 / 2241 loss=9.783, loss_v1=0, loss_v2=0, nll_loss=9.277, ntokens=839.4, nsentences=28, sample_size=839.4, sample_size_v1=0, sample_size_v2=0, ppl=620.47, wps=984.6, ups=1.17, wpb=839.4, bsz=28, num_updates=1980, lr=2.48629e-05, gnorm=0.148, clip=0, loss_scale=2, train_wall=9, gb_free=16.8, wall=1704
2023-08-08 16:36:22 - progress_bar.py[line:272] - INFO: epoch 001:   1999 / 2241 loss=9.734, loss_v1=0, loss_v2=0, nll_loss=9.234, ntokens=907.2, nsentences=28, sample_size=907.2, sample_size_v1=0, sample_size_v2=0, ppl=602.08, wps=1052.5, ups=1.16, wpb=907.2, bsz=28, num_updates=1990, lr=2.48273e-05, gnorm=0.137, clip=0, loss_scale=2, train_wall=9, gb_free=16.7, wall=1713
2023-08-08 16:36:30 - progress_bar.py[line:272] - INFO: epoch 001:   2009 / 2241 loss=9.717, loss_v1=0, loss_v2=0, nll_loss=9.216, ntokens=862.7, nsentences=28, sample_size=862.7, sample_size_v1=0, sample_size_v2=0, ppl=594.67, wps=1011.7, ups=1.17, wpb=862.7, bsz=28, num_updates=2000, lr=2.47917e-05, gnorm=0.143, clip=0, loss_scale=2, train_wall=9, gb_free=17.3, wall=1721
2023-08-08 16:36:39 - progress_bar.py[line:272] - INFO: epoch 001:   2019 / 2241 loss=9.686, loss_v1=0, loss_v2=0, nll_loss=9.182, ntokens=874.1, nsentences=28, sample_size=874.1, sample_size_v1=0, sample_size_v2=0, ppl=580.9, wps=1023, ups=1.17, wpb=874.1, bsz=28, num_updates=2010, lr=2.47561e-05, gnorm=0.155, clip=0, loss_scale=2, train_wall=9, gb_free=16.8, wall=1730
2023-08-08 16:36:47 - progress_bar.py[line:272] - INFO: epoch 001:   2029 / 2241 loss=9.714, loss_v1=0, loss_v2=0, nll_loss=9.222, ntokens=861.1, nsentences=28, sample_size=861.1, sample_size_v1=0, sample_size_v2=0, ppl=597.06, wps=1006, ups=1.17, wpb=861.1, bsz=28, num_updates=2020, lr=2.47205e-05, gnorm=0.158, clip=0, loss_scale=2, train_wall=9, gb_free=17.3, wall=1738
2023-08-08 16:36:56 - progress_bar.py[line:272] - INFO: epoch 001:   2039 / 2241 loss=9.765, loss_v1=0, loss_v2=0, nll_loss=9.267, ntokens=842.6, nsentences=28, sample_size=842.6, sample_size_v1=0, sample_size_v2=0, ppl=615.91, wps=982.7, ups=1.17, wpb=842.6, bsz=28, num_updates=2030, lr=2.46849e-05, gnorm=0.145, clip=0, loss_scale=2, train_wall=9, gb_free=17.2, wall=1747
2023-08-08 16:37:04 - progress_bar.py[line:272] - INFO: epoch 001:   2049 / 2241 loss=9.688, loss_v1=0, loss_v2=0, nll_loss=9.196, ntokens=718.8, nsentences=28, sample_size=718.8, sample_size_v1=0, sample_size_v2=0, ppl=586.34, wps=857.9, ups=1.19, wpb=718.8, bsz=28, num_updates=2040, lr=2.46493e-05, gnorm=0.149, clip=0, loss_scale=2, train_wall=8, gb_free=17.1, wall=1755
2023-08-08 16:37:13 - progress_bar.py[line:272] - INFO: epoch 001:   2059 / 2241 loss=9.711, loss_v1=0, loss_v2=0, nll_loss=9.197, ntokens=920.8, nsentences=28, sample_size=920.8, sample_size_v1=0, sample_size_v2=0, ppl=586.83, wps=1069.4, ups=1.16, wpb=920.8, bsz=28, num_updates=2050, lr=2.46137e-05, gnorm=0.144, clip=0, loss_scale=2, train_wall=9, gb_free=17.4, wall=1764
2023-08-08 16:37:21 - progress_bar.py[line:272] - INFO: epoch 001:   2069 / 2241 loss=9.651, loss_v1=0, loss_v2=0, nll_loss=9.151, ntokens=875.1, nsentences=28, sample_size=875.1, sample_size_v1=0, sample_size_v2=0, ppl=568.3, wps=1025, ups=1.17, wpb=875.1, bsz=28, num_updates=2060, lr=2.45781e-05, gnorm=0.144, clip=0, loss_scale=2, train_wall=9, gb_free=17.2, wall=1772
2023-08-08 16:37:30 - progress_bar.py[line:272] - INFO: epoch 001:   2079 / 2241 loss=9.701, loss_v1=0, loss_v2=0, nll_loss=9.202, ntokens=899, nsentences=28, sample_size=899, sample_size_v1=0, sample_size_v2=0, ppl=589.12, wps=1050.5, ups=1.17, wpb=899, bsz=28, num_updates=2070, lr=2.45425e-05, gnorm=0.149, clip=0, loss_scale=2, train_wall=9, gb_free=16.7, wall=1781
2023-08-08 16:37:38 - progress_bar.py[line:272] - INFO: epoch 001:   2089 / 2241 loss=9.747, loss_v1=0, loss_v2=0, nll_loss=9.241, ntokens=919.6, nsentences=28, sample_size=919.6, sample_size_v1=0, sample_size_v2=0, ppl=605.01, wps=1071.9, ups=1.17, wpb=919.6, bsz=28, num_updates=2080, lr=2.45069e-05, gnorm=0.146, clip=0, loss_scale=2, train_wall=9, gb_free=17.4, wall=1790
2023-08-08 16:37:47 - progress_bar.py[line:272] - INFO: epoch 001:   2099 / 2241 loss=9.704, loss_v1=0, loss_v2=0, nll_loss=9.203, ntokens=917.1, nsentences=28, sample_size=917.1, sample_size_v1=0, sample_size_v2=0, ppl=589.36, wps=1066.8, ups=1.16, wpb=917.1, bsz=28, num_updates=2090, lr=2.44713e-05, gnorm=0.144, clip=0, loss_scale=2, train_wall=9, gb_free=16.8, wall=1798
2023-08-08 16:37:56 - progress_bar.py[line:272] - INFO: epoch 001:   2109 / 2241 loss=9.803, loss_v1=0, loss_v2=0, nll_loss=9.302, ntokens=949.7, nsentences=28, sample_size=949.7, sample_size_v1=0, sample_size_v2=0, ppl=631.36, wps=1102.4, ups=1.16, wpb=949.7, bsz=28, num_updates=2100, lr=2.44357e-05, gnorm=0.147, clip=0, loss_scale=2, train_wall=9, gb_free=16.8, wall=1807
2023-08-08 16:38:04 - progress_bar.py[line:272] - INFO: epoch 001:   2119 / 2241 loss=9.615, loss_v1=0, loss_v2=0, nll_loss=9.108, ntokens=877.5, nsentences=28, sample_size=877.5, sample_size_v1=0, sample_size_v2=0, ppl=551.83, wps=1025.7, ups=1.17, wpb=877.5, bsz=28, num_updates=2110, lr=2.44001e-05, gnorm=0.148, clip=0, loss_scale=2, train_wall=9, gb_free=17.6, wall=1815
2023-08-08 16:38:13 - progress_bar.py[line:272] - INFO: epoch 001:   2129 / 2241 loss=9.787, loss_v1=0, loss_v2=0, nll_loss=9.286, ntokens=1084, nsentences=28, sample_size=1084, sample_size_v1=0, sample_size_v2=0, ppl=624.2, wps=1247.1, ups=1.15, wpb=1084, bsz=28, num_updates=2120, lr=2.43645e-05, gnorm=0.152, clip=0, loss_scale=2, train_wall=9, gb_free=17.2, wall=1824
2023-08-08 16:38:14 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2023-08-08 16:38:22 - progress_bar.py[line:272] - INFO: epoch 001:   2140 / 2241 loss=9.556, loss_v1=0, loss_v2=0, nll_loss=9.059, ntokens=753.4, nsentences=28, sample_size=753.4, sample_size_v1=0, sample_size_v2=0, ppl=533.45, wps=804, ups=1.07, wpb=753.4, bsz=28, num_updates=2130, lr=2.43289e-05, gnorm=0.15, clip=0, loss_scale=1, train_wall=9, gb_free=17.5, wall=1833
2023-08-08 16:38:31 - progress_bar.py[line:272] - INFO: epoch 001:   2150 / 2241 loss=9.5, loss_v1=0, loss_v2=0, nll_loss=8.997, ntokens=799.2, nsentences=28, sample_size=799.2, sample_size_v1=0, sample_size_v2=0, ppl=510.95, wps=937, ups=1.17, wpb=799.2, bsz=28, num_updates=2140, lr=2.42933e-05, gnorm=0.141, clip=0, loss_scale=1, train_wall=9, gb_free=17, wall=1842
2023-08-08 16:38:39 - progress_bar.py[line:272] - INFO: epoch 001:   2160 / 2241 loss=9.63, loss_v1=0, loss_v2=0, nll_loss=9.134, ntokens=793.8, nsentences=28, sample_size=793.8, sample_size_v1=0, sample_size_v2=0, ppl=561.92, wps=934.6, ups=1.18, wpb=793.8, bsz=28, num_updates=2150, lr=2.42577e-05, gnorm=0.147, clip=0, loss_scale=1, train_wall=8, gb_free=17.7, wall=1850
2023-08-08 16:38:48 - progress_bar.py[line:272] - INFO: epoch 001:   2170 / 2241 loss=9.645, loss_v1=0, loss_v2=0, nll_loss=9.135, ntokens=919.8, nsentences=28, sample_size=919.8, sample_size_v1=0, sample_size_v2=0, ppl=562.12, wps=1073.1, ups=1.17, wpb=919.8, bsz=28, num_updates=2160, lr=2.42221e-05, gnorm=0.152, clip=0, loss_scale=1, train_wall=9, gb_free=17.1, wall=1859
2023-08-08 16:38:56 - progress_bar.py[line:272] - INFO: epoch 001:   2180 / 2241 loss=9.607, loss_v1=0, loss_v2=0, nll_loss=9.093, ntokens=918, nsentences=28, sample_size=918, sample_size_v1=0, sample_size_v2=0, ppl=546.24, wps=1076.7, ups=1.17, wpb=918, bsz=28, num_updates=2170, lr=2.41865e-05, gnorm=0.14, clip=0, loss_scale=1, train_wall=9, gb_free=17.2, wall=1868
2023-08-08 16:39:05 - progress_bar.py[line:272] - INFO: epoch 001:   2190 / 2241 loss=9.654, loss_v1=0, loss_v2=0, nll_loss=9.153, ntokens=1007.7, nsentences=28, sample_size=1007.7, sample_size_v1=0, sample_size_v2=0, ppl=569.3, wps=1160.7, ups=1.15, wpb=1007.7, bsz=28, num_updates=2180, lr=2.41509e-05, gnorm=0.14, clip=0, loss_scale=1, train_wall=9, gb_free=16.8, wall=1876
2023-08-08 16:39:14 - progress_bar.py[line:272] - INFO: epoch 001:   2200 / 2241 loss=9.701, loss_v1=0, loss_v2=0, nll_loss=9.192, ntokens=1070.4, nsentences=28, sample_size=1070.4, sample_size_v1=0, sample_size_v2=0, ppl=584.75, wps=1227.8, ups=1.15, wpb=1070.4, bsz=28, num_updates=2190, lr=2.41153e-05, gnorm=0.136, clip=0, loss_scale=1, train_wall=9, gb_free=17.3, wall=1885
2023-08-08 16:39:22 - progress_bar.py[line:272] - INFO: epoch 001:   2210 / 2241 loss=9.69, loss_v1=0, loss_v2=0, nll_loss=9.18, ntokens=936.6, nsentences=28, sample_size=936.6, sample_size_v1=0, sample_size_v2=0, ppl=580.14, wps=1094, ups=1.17, wpb=936.6, bsz=28, num_updates=2200, lr=2.40797e-05, gnorm=0.149, clip=0, loss_scale=1, train_wall=9, gb_free=17.3, wall=1894
2023-08-08 16:39:31 - progress_bar.py[line:272] - INFO: epoch 001:   2220 / 2241 loss=9.524, loss_v1=0, loss_v2=0, nll_loss=9.006, ntokens=989.9, nsentences=28, sample_size=989.9, sample_size_v1=0, sample_size_v2=0, ppl=514.13, wps=1143.4, ups=1.16, wpb=989.9, bsz=28, num_updates=2210, lr=2.40441e-05, gnorm=0.135, clip=0, loss_scale=1, train_wall=9, gb_free=16.6, wall=1902
2023-08-08 16:39:40 - progress_bar.py[line:272] - INFO: epoch 001:   2230 / 2241 loss=9.688, loss_v1=0, loss_v2=0, nll_loss=9.193, ntokens=929.2, nsentences=28, sample_size=929.2, sample_size_v1=0, sample_size_v2=0, ppl=585.38, wps=1078.9, ups=1.16, wpb=929.2, bsz=28, num_updates=2220, lr=2.40085e-05, gnorm=0.141, clip=0, loss_scale=1, train_wall=9, gb_free=16.6, wall=1911
2023-08-08 16:39:48 - progress_bar.py[line:272] - INFO: epoch 001:   2240 / 2241 loss=9.514, loss_v1=0, loss_v2=0, nll_loss=8.996, ntokens=936.3, nsentences=28, sample_size=936.3, sample_size_v1=0, sample_size_v2=0, ppl=510.51, wps=1092.9, ups=1.17, wpb=936.3, bsz=28, num_updates=2230, lr=2.39729e-05, gnorm=0.134, clip=0, loss_scale=1, train_wall=9, gb_free=17.4, wall=1919
2023-08-08 16:39:48 - train.py[line:332] - INFO: end of epoch 1 (average epoch stats below)
2023-08-08 16:39:48 - progress_bar.py[line:282] - INFO: epoch 001 | loss 11.364 | loss_v1 0 | loss_v2 0 | nll_loss 10.85 | ntokens 840.107 | nsentences 27.989 | sample_size 840.107 | sample_size_v1 0 | sample_size_v2 0 | ppl 1845.21 | wps 983.7 | ups 1.17 | wpb 840.1 | bsz 28 | num_updates 2231 | lr 2.39694e-05 | gnorm 0.225 | clip 0 | loss_scale 1 | train_wall 1904 | gb_free 17.4 | wall 1920
2023-08-08 16:39:48 - trainer.py[line:639] - INFO: loading train data for epoch 2
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 row count 62723 total row count 62723
/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
slice_id 0 seek offset 0
2023-08-08 16:39:50 - trainer.py[line:703] - INFO: begin training epoch 2
2023-08-08 16:39:50 - train.py[line:305] - INFO: Start iterating over samples
2023-08-08 16:39:58 - progress_bar.py[line:272] - INFO: epoch 002:      9 / 2241 loss=9.552, loss_v1=0, loss_v2=0, nll_loss=9.024, ntokens=825.6, nsentences=25.9, sample_size=825.6, sample_size_v1=0, sample_size_v2=0, ppl=520.77, wps=809.9, ups=0.98, wpb=825.6, bsz=25.9, num_updates=2240, lr=2.39373e-05, gnorm=0.155, clip=0, loss_scale=1, train_wall=8, gb_free=17, wall=1930
2023-08-08 16:40:07 - progress_bar.py[line:272] - INFO: epoch 002:     19 / 2241 loss=9.825, loss_v1=0, loss_v2=0, nll_loss=9.299, ntokens=884.7, nsentences=28, sample_size=884.7, sample_size_v1=0, sample_size_v2=0, ppl=629.97, wps=1010.9, ups=1.14, wpb=884.7, bsz=28, num_updates=2250, lr=2.39017e-05, gnorm=0.169, clip=0, loss_scale=1, train_wall=9, gb_free=16.8, wall=1938
2023-08-08 16:40:16 - progress_bar.py[line:272] - INFO: epoch 002:     29 / 2241 loss=9.487, loss_v1=0, loss_v2=0, nll_loss=8.952, ntokens=809.8, nsentences=28, sample_size=809.8, sample_size_v1=0, sample_size_v2=0, ppl=495.12, wps=949.2, ups=1.17, wpb=809.8, bsz=28, num_updates=2260, lr=2.38661e-05, gnorm=0.163, clip=0, loss_scale=1, train_wall=9, gb_free=17.7, wall=1947
2023-08-08 16:40:24 - progress_bar.py[line:272] - INFO: epoch 002:     39 / 2241 loss=9.34, loss_v1=0, loss_v2=0, nll_loss=8.816, ntokens=677.1, nsentences=28, sample_size=677.1, sample_size_v1=0, sample_size_v2=0, ppl=450.65, wps=804.7, ups=1.19, wpb=677.1, bsz=28, num_updates=2270, lr=2.38305e-05, gnorm=0.152, clip=0, loss_scale=1, train_wall=8, gb_free=17.3, wall=1955
2023-08-08 16:40:33 - progress_bar.py[line:272] - INFO: epoch 002:     49 / 2241 loss=9.579, loss_v1=0, loss_v2=0, nll_loss=9.002, ntokens=909.8, nsentences=28, sample_size=909.8, sample_size_v1=0, sample_size_v2=0, ppl=512.63, wps=1056.1, ups=1.16, wpb=909.8, bsz=28, num_updates=2280, lr=2.37949e-05, gnorm=0.178, clip=0, loss_scale=1, train_wall=9, gb_free=16.6, wall=1964
2023-08-08 16:40:41 - progress_bar.py[line:272] - INFO: epoch 002:     59 / 2241 loss=9.592, loss_v1=0, loss_v2=0, nll_loss=9.015, ntokens=851, nsentences=28, sample_size=851, sample_size_v1=0, sample_size_v2=0, ppl=517.41, wps=986.8, ups=1.16, wpb=851, bsz=28, num_updates=2290, lr=2.37593e-05, gnorm=0.181, clip=0, loss_scale=1, train_wall=9, gb_free=16.7, wall=1973
2023-08-08 16:40:48 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2023-08-08 16:40:51 - progress_bar.py[line:272] - INFO: epoch 002:     70 / 2241 loss=9.521, loss_v1=0, loss_v2=0, nll_loss=8.972, ntokens=764.6, nsentences=28, sample_size=764.6, sample_size_v1=0, sample_size_v2=0, ppl=502.22, wps=810.6, ups=1.06, wpb=764.6, bsz=28, num_updates=2300, lr=2.37237e-05, gnorm=0.178, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.7, wall=1982
2023-08-08 16:40:59 - progress_bar.py[line:272] - INFO: epoch 002:     80 / 2241 loss=9.376, loss_v1=0, loss_v2=0, nll_loss=8.811, ntokens=715, nsentences=28, sample_size=715, sample_size_v1=0, sample_size_v2=0, ppl=449.17, wps=843.4, ups=1.18, wpb=715, bsz=28, num_updates=2310, lr=2.36881e-05, gnorm=0.181, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.8, wall=1990
2023-08-08 16:41:08 - progress_bar.py[line:272] - INFO: epoch 002:     90 / 2241 loss=9.595, loss_v1=0, loss_v2=0, nll_loss=9.014, ntokens=1082.2, nsentences=28, sample_size=1082.2, sample_size_v1=0, sample_size_v2=0, ppl=517.05, wps=1225.9, ups=1.13, wpb=1082.2, bsz=28, num_updates=2320, lr=2.36525e-05, gnorm=0.174, clip=0, loss_scale=0.5, train_wall=9, gb_free=16.6, wall=1999
2023-08-08 16:41:17 - progress_bar.py[line:272] - INFO: epoch 002:    100 / 2241 loss=9.701, loss_v1=0, loss_v2=0, nll_loss=9.138, ntokens=1128.8, nsentences=28, sample_size=1128.8, sample_size_v1=0, sample_size_v2=0, ppl=563.44, wps=1267.4, ups=1.12, wpb=1128.8, bsz=28, num_updates=2330, lr=2.36169e-05, gnorm=0.181, clip=0, loss_scale=0.5, train_wall=9, gb_free=16.7, wall=2008
2023-08-08 16:41:26 - progress_bar.py[line:272] - INFO: epoch 002:    110 / 2241 loss=9.639, loss_v1=0, loss_v2=0, nll_loss=9.088, ntokens=972.9, nsentences=28, sample_size=972.9, sample_size_v1=0, sample_size_v2=0, ppl=544.03, wps=1102, ups=1.13, wpb=972.9, bsz=28, num_updates=2340, lr=2.35813e-05, gnorm=0.171, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.4, wall=2017
2023-08-08 16:41:34 - progress_bar.py[line:272] - INFO: epoch 002:    120 / 2241 loss=9.39, loss_v1=0, loss_v2=0, nll_loss=8.853, ntokens=841, nsentences=28, sample_size=841, sample_size_v1=0, sample_size_v2=0, ppl=462.38, wps=975.4, ups=1.16, wpb=841, bsz=28, num_updates=2350, lr=2.35457e-05, gnorm=0.15, clip=0, loss_scale=0.5, train_wall=9, gb_free=16.8, wall=2026
2023-08-08 16:41:43 - progress_bar.py[line:272] - INFO: epoch 002:    130 / 2241 loss=9.345, loss_v1=0, loss_v2=0, nll_loss=8.785, ntokens=838.7, nsentences=28, sample_size=838.7, sample_size_v1=0, sample_size_v2=0, ppl=441.22, wps=977, ups=1.16, wpb=838.7, bsz=28, num_updates=2360, lr=2.35101e-05, gnorm=0.182, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.7, wall=2034
2023-08-08 16:41:52 - progress_bar.py[line:272] - INFO: epoch 002:    140 / 2241 loss=9.409, loss_v1=0, loss_v2=0, nll_loss=8.891, ntokens=785.6, nsentences=28, sample_size=785.6, sample_size_v1=0, sample_size_v2=0, ppl=474.69, wps=919.4, ups=1.17, wpb=785.6, bsz=28, num_updates=2370, lr=2.34745e-05, gnorm=0.146, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.1, wall=2043
2023-08-08 16:42:00 - progress_bar.py[line:272] - INFO: epoch 002:    150 / 2241 loss=9.371, loss_v1=0, loss_v2=0, nll_loss=8.856, ntokens=779.7, nsentences=28, sample_size=779.7, sample_size_v1=0, sample_size_v2=0, ppl=463.26, wps=912.4, ups=1.17, wpb=779.7, bsz=28, num_updates=2380, lr=2.34389e-05, gnorm=0.138, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.4, wall=2051
2023-08-08 16:42:09 - progress_bar.py[line:272] - INFO: epoch 002:    160 / 2241 loss=9.362, loss_v1=0, loss_v2=0, nll_loss=8.826, ntokens=806, nsentences=28, sample_size=806, sample_size_v1=0, sample_size_v2=0, ppl=453.96, wps=937.3, ups=1.16, wpb=806, bsz=28, num_updates=2390, lr=2.34033e-05, gnorm=0.136, clip=0, loss_scale=0.5, train_wall=9, gb_free=16.7, wall=2060
2023-08-08 16:42:18 - progress_bar.py[line:272] - INFO: epoch 002:    170 / 2241 loss=9.437, loss_v1=0, loss_v2=0, nll_loss=8.917, ntokens=944.7, nsentences=28, sample_size=944.7, sample_size_v1=0, sample_size_v2=0, ppl=483.28, wps=1080.7, ups=1.14, wpb=944.7, bsz=28, num_updates=2400, lr=2.33677e-05, gnorm=0.141, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.1, wall=2069
2023-08-08 16:42:26 - progress_bar.py[line:272] - INFO: epoch 002:    180 / 2241 loss=9.331, loss_v1=0, loss_v2=0, nll_loss=8.796, ntokens=930.5, nsentences=28, sample_size=930.5, sample_size_v1=0, sample_size_v2=0, ppl=444.46, wps=1066.6, ups=1.15, wpb=930.5, bsz=28, num_updates=2410, lr=2.33321e-05, gnorm=0.143, clip=0, loss_scale=0.5, train_wall=9, gb_free=16.7, wall=2077
2023-08-08 16:42:35 - progress_bar.py[line:272] - INFO: epoch 002:    190 / 2241 loss=9.322, loss_v1=0, loss_v2=0, nll_loss=8.792, ntokens=976.2, nsentences=28, sample_size=976.2, sample_size_v1=0, sample_size_v2=0, ppl=443.11, wps=1118.7, ups=1.15, wpb=976.2, bsz=28, num_updates=2420, lr=2.32965e-05, gnorm=0.135, clip=0, loss_scale=0.5, train_wall=9, gb_free=17, wall=2086
2023-08-08 16:42:44 - progress_bar.py[line:272] - INFO: epoch 002:    200 / 2241 loss=9.35, loss_v1=0, loss_v2=0, nll_loss=8.802, ntokens=934.6, nsentences=28, sample_size=934.6, sample_size_v1=0, sample_size_v2=0, ppl=446.32, wps=1068.6, ups=1.14, wpb=934.6, bsz=28, num_updates=2430, lr=2.32609e-05, gnorm=0.151, clip=0, loss_scale=0.5, train_wall=9, gb_free=16.9, wall=2095
2023-08-08 16:42:52 - progress_bar.py[line:272] - INFO: epoch 002:    210 / 2241 loss=9.356, loss_v1=0, loss_v2=0, nll_loss=8.821, ntokens=907.5, nsentences=28, sample_size=907.5, sample_size_v1=0, sample_size_v2=0, ppl=452.24, wps=1041.8, ups=1.15, wpb=907.5, bsz=28, num_updates=2440, lr=2.32253e-05, gnorm=0.135, clip=0, loss_scale=0.5, train_wall=9, gb_free=16.8, wall=2104
2023-08-08 16:43:01 - progress_bar.py[line:272] - INFO: epoch 002:    220 / 2241 loss=9.105, loss_v1=0, loss_v2=0, nll_loss=8.557, ntokens=774.2, nsentences=28, sample_size=774.2, sample_size_v1=0, sample_size_v2=0, ppl=376.57, wps=899.6, ups=1.16, wpb=774.2, bsz=28, num_updates=2450, lr=2.31897e-05, gnorm=0.144, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.4, wall=2112
2023-08-08 16:43:10 - progress_bar.py[line:272] - INFO: epoch 002:    230 / 2241 loss=9.235, loss_v1=0, loss_v2=0, nll_loss=8.702, ntokens=749.8, nsentences=28, sample_size=749.8, sample_size_v1=0, sample_size_v2=0, ppl=416.39, wps=871.5, ups=1.16, wpb=749.8, bsz=28, num_updates=2460, lr=2.31541e-05, gnorm=0.146, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.7, wall=2121
2023-08-08 16:43:18 - progress_bar.py[line:272] - INFO: epoch 002:    240 / 2241 loss=9.075, loss_v1=0, loss_v2=0, nll_loss=8.537, ntokens=699.2, nsentences=28, sample_size=699.2, sample_size_v1=0, sample_size_v2=0, ppl=371.49, wps=822.1, ups=1.18, wpb=699.2, bsz=28, num_updates=2470, lr=2.31185e-05, gnorm=0.142, clip=0, loss_scale=0.5, train_wall=8, gb_free=16.5, wall=2129
2023-08-08 16:43:27 - progress_bar.py[line:272] - INFO: epoch 002:    250 / 2241 loss=9.324, loss_v1=0, loss_v2=0, nll_loss=8.777, ntokens=915.9, nsentences=28, sample_size=915.9, sample_size_v1=0, sample_size_v2=0, ppl=438.62, wps=1055.2, ups=1.15, wpb=915.9, bsz=28, num_updates=2480, lr=2.30829e-05, gnorm=0.147, clip=0, loss_scale=0.5, train_wall=9, gb_free=17, wall=2138
2023-08-08 16:43:36 - progress_bar.py[line:272] - INFO: epoch 002:    260 / 2241 loss=9.114, loss_v1=0, loss_v2=0, nll_loss=8.564, ntokens=808.7, nsentences=28, sample_size=808.7, sample_size_v1=0, sample_size_v2=0, ppl=378.33, wps=932.4, ups=1.15, wpb=808.7, bsz=28, num_updates=2490, lr=2.30473e-05, gnorm=0.133, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.4, wall=2147
2023-08-08 16:43:44 - progress_bar.py[line:272] - INFO: epoch 002:    270 / 2241 loss=9.18, loss_v1=0, loss_v2=0, nll_loss=8.638, ntokens=912.2, nsentences=28, sample_size=912.2, sample_size_v1=0, sample_size_v2=0, ppl=398.36, wps=1051.4, ups=1.15, wpb=912.2, bsz=28, num_updates=2500, lr=2.30117e-05, gnorm=0.142, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.1, wall=2155
2023-08-08 16:43:53 - progress_bar.py[line:272] - INFO: epoch 002:    280 / 2241 loss=9.386, loss_v1=0, loss_v2=0, nll_loss=8.875, ntokens=788.7, nsentences=28, sample_size=788.7, sample_size_v1=0, sample_size_v2=0, ppl=469.4, wps=931.3, ups=1.18, wpb=788.7, bsz=28, num_updates=2510, lr=2.29761e-05, gnorm=0.144, clip=0, loss_scale=0.5, train_wall=8, gb_free=16.9, wall=2164
2023-08-08 16:44:01 - progress_bar.py[line:272] - INFO: epoch 002:    290 / 2241 loss=9.536, loss_v1=0, loss_v2=0, nll_loss=9.023, ntokens=829.3, nsentences=28, sample_size=829.3, sample_size_v1=0, sample_size_v2=0, ppl=520.23, wps=973.3, ups=1.17, wpb=829.3, bsz=28, num_updates=2520, lr=2.29405e-05, gnorm=0.139, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.1, wall=2172
2023-08-08 16:44:10 - progress_bar.py[line:272] - INFO: epoch 002:    300 / 2241 loss=9.537, loss_v1=0, loss_v2=0, nll_loss=9.023, ntokens=938.5, nsentences=28, sample_size=938.5, sample_size_v1=0, sample_size_v2=0, ppl=520.32, wps=1097.3, ups=1.17, wpb=938.5, bsz=28, num_updates=2530, lr=2.29049e-05, gnorm=0.134, clip=0, loss_scale=0.5, train_wall=9, gb_free=16.9, wall=2181
2023-08-08 16:44:18 - progress_bar.py[line:272] - INFO: epoch 002:    310 / 2241 loss=9.477, loss_v1=0, loss_v2=0, nll_loss=8.954, ntokens=929.4, nsentences=28, sample_size=929.4, sample_size_v1=0, sample_size_v2=0, ppl=495.95, wps=1092.6, ups=1.18, wpb=929.4, bsz=28, num_updates=2540, lr=2.28693e-05, gnorm=0.132, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.2, wall=2189
2023-08-08 16:44:27 - progress_bar.py[line:272] - INFO: epoch 002:    320 / 2241 loss=9.545, loss_v1=0, loss_v2=0, nll_loss=9.042, ntokens=879.9, nsentences=28, sample_size=879.9, sample_size_v1=0, sample_size_v2=0, ppl=527.2, wps=1026.6, ups=1.17, wpb=879.9, bsz=28, num_updates=2550, lr=2.28337e-05, gnorm=0.133, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.3, wall=2198
2023-08-08 16:44:36 - progress_bar.py[line:272] - INFO: epoch 002:    330 / 2241 loss=9.66, loss_v1=0, loss_v2=0, nll_loss=9.158, ntokens=971.9, nsentences=28, sample_size=971.9, sample_size_v1=0, sample_size_v2=0, ppl=571.2, wps=1122.8, ups=1.16, wpb=971.9, bsz=28, num_updates=2560, lr=2.27981e-05, gnorm=0.134, clip=0, loss_scale=0.5, train_wall=9, gb_free=16.4, wall=2207
2023-08-08 16:44:44 - progress_bar.py[line:272] - INFO: epoch 002:    340 / 2241 loss=9.497, loss_v1=0, loss_v2=0, nll_loss=8.979, ntokens=918.6, nsentences=28, sample_size=918.6, sample_size_v1=0, sample_size_v2=0, ppl=504.51, wps=1077.1, ups=1.17, wpb=918.6, bsz=28, num_updates=2570, lr=2.27625e-05, gnorm=0.134, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.7, wall=2215
2023-08-08 16:44:53 - progress_bar.py[line:272] - INFO: epoch 002:    350 / 2241 loss=9.469, loss_v1=0, loss_v2=0, nll_loss=8.962, ntokens=930.1, nsentences=28, sample_size=930.1, sample_size_v1=0, sample_size_v2=0, ppl=498.57, wps=1087.8, ups=1.17, wpb=930.1, bsz=28, num_updates=2580, lr=2.27269e-05, gnorm=0.133, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.3, wall=2224
2023-08-08 16:45:01 - progress_bar.py[line:272] - INFO: epoch 002:    360 / 2241 loss=9.622, loss_v1=0, loss_v2=0, nll_loss=9.11, ntokens=967.2, nsentences=28, sample_size=967.2, sample_size_v1=0, sample_size_v2=0, ppl=552.63, wps=1123.7, ups=1.16, wpb=967.2, bsz=28, num_updates=2590, lr=2.26913e-05, gnorm=0.135, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.3, wall=2232
2023-08-08 16:45:10 - progress_bar.py[line:272] - INFO: epoch 002:    370 / 2241 loss=9.445, loss_v1=0, loss_v2=0, nll_loss=8.932, ntokens=933.6, nsentences=28, sample_size=933.6, sample_size_v1=0, sample_size_v2=0, ppl=488.59, wps=1091.8, ups=1.17, wpb=933.6, bsz=28, num_updates=2600, lr=2.26557e-05, gnorm=0.13, clip=0, loss_scale=0.5, train_wall=9, gb_free=16.7, wall=2241
2023-08-08 16:45:18 - progress_bar.py[line:272] - INFO: epoch 002:    380 / 2241 loss=9.393, loss_v1=0, loss_v2=0, nll_loss=8.871, ntokens=926.8, nsentences=28, sample_size=926.8, sample_size_v1=0, sample_size_v2=0, ppl=468.25, wps=1078.5, ups=1.16, wpb=926.8, bsz=28, num_updates=2610, lr=2.26201e-05, gnorm=0.133, clip=0, loss_scale=0.5, train_wall=9, gb_free=16.8, wall=2250
2023-08-08 16:45:27 - progress_bar.py[line:272] - INFO: epoch 002:    390 / 2241 loss=9.384, loss_v1=0, loss_v2=0, nll_loss=8.865, ntokens=931.1, nsentences=28, sample_size=931.1, sample_size_v1=0, sample_size_v2=0, ppl=466.16, wps=1079.6, ups=1.16, wpb=931.1, bsz=28, num_updates=2620, lr=2.25845e-05, gnorm=0.13, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.4, wall=2258
2023-08-08 16:45:36 - progress_bar.py[line:272] - INFO: epoch 002:    400 / 2241 loss=9.45, loss_v1=0, loss_v2=0, nll_loss=8.935, ntokens=948.1, nsentences=28, sample_size=948.1, sample_size_v1=0, sample_size_v2=0, ppl=489.42, wps=1096.8, ups=1.16, wpb=948.1, bsz=28, num_updates=2630, lr=2.25489e-05, gnorm=0.131, clip=0, loss_scale=0.5, train_wall=9, gb_free=16.1, wall=2267
2023-08-08 16:45:44 - progress_bar.py[line:272] - INFO: epoch 002:    410 / 2241 loss=9.302, loss_v1=0, loss_v2=0, nll_loss=8.779, ntokens=880.1, nsentences=28, sample_size=880.1, sample_size_v1=0, sample_size_v2=0, ppl=439.38, wps=1033, ups=1.17, wpb=880.1, bsz=28, num_updates=2640, lr=2.25133e-05, gnorm=0.135, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.2, wall=2275
2023-08-08 16:45:53 - progress_bar.py[line:272] - INFO: epoch 002:    420 / 2241 loss=9.329, loss_v1=0, loss_v2=0, nll_loss=8.804, ntokens=839.6, nsentences=28, sample_size=839.6, sample_size_v1=0, sample_size_v2=0, ppl=446.96, wps=988.4, ups=1.18, wpb=839.6, bsz=28, num_updates=2650, lr=2.24778e-05, gnorm=0.13, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.5, wall=2284
2023-08-08 16:46:01 - progress_bar.py[line:272] - INFO: epoch 002:    430 / 2241 loss=9.334, loss_v1=0, loss_v2=0, nll_loss=8.819, ntokens=814, nsentences=28, sample_size=814, sample_size_v1=0, sample_size_v2=0, ppl=451.77, wps=959.8, ups=1.18, wpb=814, bsz=28, num_updates=2660, lr=2.24422e-05, gnorm=0.132, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.2, wall=2292
2023-08-08 16:46:10 - progress_bar.py[line:272] - INFO: epoch 002:    440 / 2241 loss=9.29, loss_v1=0, loss_v2=0, nll_loss=8.773, ntokens=843.5, nsentences=28, sample_size=843.5, sample_size_v1=0, sample_size_v2=0, ppl=437.55, wps=996.7, ups=1.18, wpb=843.5, bsz=28, num_updates=2670, lr=2.24066e-05, gnorm=0.13, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.1, wall=2301
2023-08-08 16:46:18 - progress_bar.py[line:272] - INFO: epoch 002:    450 / 2241 loss=9.205, loss_v1=0, loss_v2=0, nll_loss=8.674, ntokens=736, nsentences=28, sample_size=736, sample_size_v1=0, sample_size_v2=0, ppl=408.45, wps=871.9, ups=1.18, wpb=736, bsz=28, num_updates=2680, lr=2.2371e-05, gnorm=0.137, clip=0, loss_scale=0.5, train_wall=8, gb_free=16.8, wall=2309
2023-08-08 16:46:26 - progress_bar.py[line:272] - INFO: epoch 002:    460 / 2241 loss=9.281, loss_v1=0, loss_v2=0, nll_loss=8.772, ntokens=751.2, nsentences=28, sample_size=751.2, sample_size_v1=0, sample_size_v2=0, ppl=437.01, wps=895.3, ups=1.19, wpb=751.2, bsz=28, num_updates=2690, lr=2.23354e-05, gnorm=0.133, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.6, wall=2318
2023-08-08 16:46:35 - progress_bar.py[line:272] - INFO: epoch 002:    470 / 2241 loss=9.274, loss_v1=0, loss_v2=0, nll_loss=8.751, ntokens=776.6, nsentences=28, sample_size=776.6, sample_size_v1=0, sample_size_v2=0, ppl=430.86, wps=920.9, ups=1.19, wpb=776.6, bsz=28, num_updates=2700, lr=2.22998e-05, gnorm=0.135, clip=0, loss_scale=0.5, train_wall=8, gb_free=16.9, wall=2326
2023-08-08 16:46:43 - progress_bar.py[line:272] - INFO: epoch 002:    480 / 2241 loss=9.176, loss_v1=0, loss_v2=0, nll_loss=8.658, ntokens=727.7, nsentences=28, sample_size=727.7, sample_size_v1=0, sample_size_v2=0, ppl=403.8, wps=867.2, ups=1.19, wpb=727.7, bsz=28, num_updates=2710, lr=2.22642e-05, gnorm=0.133, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.3, wall=2334
2023-08-08 16:46:52 - progress_bar.py[line:272] - INFO: epoch 002:    490 / 2241 loss=9.237, loss_v1=0, loss_v2=0, nll_loss=8.707, ntokens=772.4, nsentences=28, sample_size=772.4, sample_size_v1=0, sample_size_v2=0, ppl=417.98, wps=909, ups=1.18, wpb=772.4, bsz=28, num_updates=2720, lr=2.22286e-05, gnorm=0.134, clip=0, loss_scale=0.5, train_wall=8, gb_free=16.5, wall=2343
2023-08-08 16:47:00 - progress_bar.py[line:272] - INFO: epoch 002:    500 / 2241 loss=9.355, loss_v1=0, loss_v2=0, nll_loss=8.84, ntokens=870.4, nsentences=28, sample_size=870.4, sample_size_v1=0, sample_size_v2=0, ppl=458.37, wps=1028, ups=1.18, wpb=870.4, bsz=28, num_updates=2730, lr=2.2193e-05, gnorm=0.136, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.3, wall=2351
2023-08-08 16:47:09 - progress_bar.py[line:272] - INFO: epoch 002:    510 / 2241 loss=9.335, loss_v1=0, loss_v2=0, nll_loss=8.815, ntokens=896.1, nsentences=28, sample_size=896.1, sample_size_v1=0, sample_size_v2=0, ppl=450.41, wps=1052.4, ups=1.17, wpb=896.1, bsz=28, num_updates=2740, lr=2.21574e-05, gnorm=0.128, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.4, wall=2360
2023-08-08 16:47:17 - progress_bar.py[line:272] - INFO: epoch 002:    520 / 2241 loss=9.28, loss_v1=0, loss_v2=0, nll_loss=8.77, ntokens=707.4, nsentences=28, sample_size=707.4, sample_size_v1=0, sample_size_v2=0, ppl=436.49, wps=847.6, ups=1.2, wpb=707.4, bsz=28, num_updates=2750, lr=2.21218e-05, gnorm=0.136, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.8, wall=2368
2023-08-08 16:47:26 - progress_bar.py[line:272] - INFO: epoch 002:    530 / 2241 loss=9.293, loss_v1=0, loss_v2=0, nll_loss=8.764, ntokens=871.9, nsentences=28, sample_size=871.9, sample_size_v1=0, sample_size_v2=0, ppl=434.83, wps=1020.8, ups=1.17, wpb=871.9, bsz=28, num_updates=2760, lr=2.20862e-05, gnorm=0.131, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.5, wall=2377
2023-08-08 16:47:34 - progress_bar.py[line:272] - INFO: epoch 002:    540 / 2241 loss=9.27, loss_v1=0, loss_v2=0, nll_loss=8.754, ntokens=885.5, nsentences=28, sample_size=885.5, sample_size_v1=0, sample_size_v2=0, ppl=431.72, wps=1036.7, ups=1.17, wpb=885.5, bsz=28, num_updates=2770, lr=2.20506e-05, gnorm=0.129, clip=0, loss_scale=0.5, train_wall=9, gb_free=17.3, wall=2385
2023-08-08 16:47:43 - progress_bar.py[line:272] - INFO: epoch 002:    550 / 2241 loss=9.182, loss_v1=0, loss_v2=0, nll_loss=8.665, ntokens=797.4, nsentences=28, sample_size=797.4, sample_size_v1=0, sample_size_v2=0, ppl=405.95, wps=938.4, ups=1.18, wpb=797.4, bsz=28, num_updates=2780, lr=2.2015e-05, gnorm=0.128, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.1, wall=2394
2023-08-08 16:47:51 - progress_bar.py[line:272] - INFO: epoch 002:    560 / 2241 loss=9.241, loss_v1=0, loss_v2=0, nll_loss=8.719, ntokens=813.6, nsentences=28, sample_size=813.6, sample_size_v1=0, sample_size_v2=0, ppl=421.4, wps=959, ups=1.18, wpb=813.6, bsz=28, num_updates=2790, lr=2.19794e-05, gnorm=0.155, clip=0, loss_scale=0.5, train_wall=8, gb_free=16.9, wall=2402
2023-08-08 16:48:00 - progress_bar.py[line:272] - INFO: epoch 002:    570 / 2241 loss=9.238, loss_v1=0, loss_v2=0, nll_loss=8.707, ntokens=847.3, nsentences=28, sample_size=847.3, sample_size_v1=0, sample_size_v2=0, ppl=418.02, wps=1003, ups=1.18, wpb=847.3, bsz=28, num_updates=2800, lr=2.19438e-05, gnorm=0.131, clip=0, loss_scale=0.5, train_wall=8, gb_free=17.5, wall=2411
2023-08-08 16:48:08 - progress_bar.py[line:272] - INFO: epoch 002:    580 / 2241 loss=9.327, loss_v1=0, loss_v2=0, nll_loss=8.81, ntokens=792.3, nsentences=28, sample_size=792.3, sample_size_v1=0, sample_size_v2=0, ppl=448.75, wps=942.4, ups=1.19, wpb=792.3, bsz=28, num_updates=2810, lr=2.19082e-05, gnorm=0.137, clip=0, loss_scale=1, train_wall=8, gb_free=16.6, wall=2419
2023-08-08 16:48:16 - progress_bar.py[line:272] - INFO: epoch 002:    590 / 2241 loss=9.141, loss_v1=0, loss_v2=0, nll_loss=8.615, ntokens=742.2, nsentences=28, sample_size=742.2, sample_size_v1=0, sample_size_v2=0, ppl=391.97, wps=887.8, ups=1.2, wpb=742.2, bsz=28, num_updates=2820, lr=2.18726e-05, gnorm=0.131, clip=0, loss_scale=1, train_wall=8, gb_free=17.3, wall=2428
2023-08-08 16:48:25 - progress_bar.py[line:272] - INFO: epoch 002:    600 / 2241 loss=9.175, loss_v1=0, loss_v2=0, nll_loss=8.651, ntokens=820.9, nsentences=28, sample_size=820.9, sample_size_v1=0, sample_size_v2=0, ppl=401.86, wps=967.3, ups=1.18, wpb=820.9, bsz=28, num_updates=2830, lr=2.1837e-05, gnorm=0.129, clip=0, loss_scale=1, train_wall=8, gb_free=17.4, wall=2436
2023-08-08 16:48:33 - progress_bar.py[line:272] - INFO: epoch 002:    610 / 2241 loss=9.187, loss_v1=0, loss_v2=0, nll_loss=8.657, ntokens=867.6, nsentences=28, sample_size=867.6, sample_size_v1=0, sample_size_v2=0, ppl=403.64, wps=1013.5, ups=1.17, wpb=867.6, bsz=28, num_updates=2840, lr=2.18014e-05, gnorm=0.13, clip=0, loss_scale=1, train_wall=9, gb_free=17.3, wall=2445
2023-08-08 16:48:42 - progress_bar.py[line:272] - INFO: epoch 002:    620 / 2241 loss=9.251, loss_v1=0, loss_v2=0, nll_loss=8.727, ntokens=857.7, nsentences=28, sample_size=857.7, sample_size_v1=0, sample_size_v2=0, ppl=423.71, wps=1007.3, ups=1.17, wpb=857.7, bsz=28, num_updates=2850, lr=2.17658e-05, gnorm=0.126, clip=0, loss_scale=1, train_wall=8, gb_free=17.5, wall=2453
2023-08-08 16:48:50 - progress_bar.py[line:272] - INFO: epoch 002:    630 / 2241 loss=9.215, loss_v1=0, loss_v2=0, nll_loss=8.68, ntokens=820.1, nsentences=28, sample_size=820.1, sample_size_v1=0, sample_size_v2=0, ppl=410.23, wps=978, ups=1.19, wpb=820.1, bsz=28, num_updates=2860, lr=2.17302e-05, gnorm=0.133, clip=0, loss_scale=1, train_wall=8, gb_free=18, wall=2462
2023-08-08 16:48:59 - progress_bar.py[line:272] - INFO: epoch 002:    640 / 2241 loss=9.127, loss_v1=0, loss_v2=0, nll_loss=8.597, ntokens=751, nsentences=28, sample_size=751, sample_size_v1=0, sample_size_v2=0, ppl=387.31, wps=894.4, ups=1.19, wpb=751, bsz=28, num_updates=2870, lr=2.16946e-05, gnorm=0.131, clip=0, loss_scale=1, train_wall=8, gb_free=17.5, wall=2470
2023-08-08 16:49:07 - progress_bar.py[line:272] - INFO: epoch 002:    650 / 2241 loss=9.165, loss_v1=0, loss_v2=0, nll_loss=8.642, ntokens=743.8, nsentences=28, sample_size=743.8, sample_size_v1=0, sample_size_v2=0, ppl=399.6, wps=882, ups=1.19, wpb=743.8, bsz=28, num_updates=2880, lr=2.1659e-05, gnorm=0.131, clip=0, loss_scale=1, train_wall=8, gb_free=16.8, wall=2478
2023-08-08 16:49:16 - progress_bar.py[line:272] - INFO: epoch 002:    660 / 2241 loss=9.138, loss_v1=0, loss_v2=0, nll_loss=8.613, ntokens=763.4, nsentences=28, sample_size=763.4, sample_size_v1=0, sample_size_v2=0, ppl=391.57, wps=906.2, ups=1.19, wpb=763.4, bsz=28, num_updates=2890, lr=2.16234e-05, gnorm=0.128, clip=0, loss_scale=1, train_wall=8, gb_free=17.2, wall=2487
2023-08-08 16:49:24 - progress_bar.py[line:272] - INFO: epoch 002:    670 / 2241 loss=9.214, loss_v1=0, loss_v2=0, nll_loss=8.687, ntokens=882.7, nsentences=28, sample_size=882.7, sample_size_v1=0, sample_size_v2=0, ppl=412.12, wps=1042.2, ups=1.18, wpb=882.7, bsz=28, num_updates=2900, lr=2.15878e-05, gnorm=0.132, clip=0, loss_scale=1, train_wall=8, gb_free=16, wall=2495
2023-08-08 16:49:33 - progress_bar.py[line:272] - INFO: epoch 002:    680 / 2241 loss=9.088, loss_v1=0, loss_v2=0, nll_loss=8.56, ntokens=795.2, nsentences=28, sample_size=795.2, sample_size_v1=0, sample_size_v2=0, ppl=377.33, wps=942.5, ups=1.19, wpb=795.2, bsz=28, num_updates=2910, lr=2.15522e-05, gnorm=0.129, clip=0, loss_scale=1, train_wall=8, gb_free=17.7, wall=2504
2023-08-08 16:49:41 - progress_bar.py[line:272] - INFO: epoch 002:    690 / 2241 loss=9.11, loss_v1=0, loss_v2=0, nll_loss=8.583, ntokens=773.7, nsentences=28, sample_size=773.7, sample_size_v1=0, sample_size_v2=0, ppl=383.41, wps=924.1, ups=1.19, wpb=773.7, bsz=28, num_updates=2920, lr=2.15166e-05, gnorm=0.132, clip=0, loss_scale=1, train_wall=8, gb_free=17.8, wall=2512
2023-08-08 16:49:49 - progress_bar.py[line:272] - INFO: epoch 002:    700 / 2241 loss=9.094, loss_v1=0, loss_v2=0, nll_loss=8.556, ntokens=794.9, nsentences=28, sample_size=794.9, sample_size_v1=0, sample_size_v2=0, ppl=376.41, wps=945.6, ups=1.19, wpb=794.9, bsz=28, num_updates=2930, lr=2.1481e-05, gnorm=0.136, clip=0, loss_scale=1, train_wall=8, gb_free=17.7, wall=2520
2023-08-08 16:49:58 - progress_bar.py[line:272] - INFO: epoch 002:    710 / 2241 loss=9.155, loss_v1=0, loss_v2=0, nll_loss=8.626, ntokens=817.2, nsentences=28, sample_size=817.2, sample_size_v1=0, sample_size_v2=0, ppl=395, wps=970.6, ups=1.19, wpb=817.2, bsz=28, num_updates=2940, lr=2.14454e-05, gnorm=0.134, clip=0, loss_scale=1, train_wall=8, gb_free=17.6, wall=2529
2023-08-08 16:50:06 - progress_bar.py[line:272] - INFO: epoch 002:    720 / 2241 loss=9.197, loss_v1=0, loss_v2=0, nll_loss=8.668, ntokens=834.1, nsentences=28, sample_size=834.1, sample_size_v1=0, sample_size_v2=0, ppl=406.74, wps=983.6, ups=1.18, wpb=834.1, bsz=28, num_updates=2950, lr=2.14098e-05, gnorm=0.14, clip=0, loss_scale=1, train_wall=8, gb_free=17.1, wall=2537
2023-08-08 16:50:15 - progress_bar.py[line:272] - INFO: epoch 002:    730 / 2241 loss=9.105, loss_v1=0, loss_v2=0, nll_loss=8.567, ntokens=829.6, nsentences=28, sample_size=829.6, sample_size_v1=0, sample_size_v2=0, ppl=379.25, wps=976.8, ups=1.18, wpb=829.6, bsz=28, num_updates=2960, lr=2.13742e-05, gnorm=0.13, clip=0, loss_scale=1, train_wall=8, gb_free=17.7, wall=2546
2023-08-08 16:50:23 - progress_bar.py[line:272] - INFO: epoch 002:    740 / 2241 loss=9.171, loss_v1=0, loss_v2=0, nll_loss=8.641, ntokens=822.7, nsentences=28, sample_size=822.7, sample_size_v1=0, sample_size_v2=0, ppl=399.21, wps=974.7, ups=1.18, wpb=822.7, bsz=28, num_updates=2970, lr=2.13386e-05, gnorm=0.133, clip=0, loss_scale=1, train_wall=8, gb_free=17.2, wall=2554
2023-08-08 16:50:32 - progress_bar.py[line:272] - INFO: epoch 002:    750 / 2241 loss=9.139, loss_v1=0, loss_v2=0, nll_loss=8.595, ntokens=827, nsentences=28, sample_size=827, sample_size_v1=0, sample_size_v2=0, ppl=386.58, wps=973.9, ups=1.18, wpb=827, bsz=28, num_updates=2980, lr=2.1303e-05, gnorm=0.136, clip=0, loss_scale=1, train_wall=8, gb_free=17, wall=2563
2023-08-08 16:50:40 - progress_bar.py[line:272] - INFO: epoch 002:    760 / 2241 loss=9.128, loss_v1=0, loss_v2=0, nll_loss=8.601, ntokens=792.9, nsentences=28, sample_size=792.9, sample_size_v1=0, sample_size_v2=0, ppl=388.2, wps=938.4, ups=1.18, wpb=792.9, bsz=28, num_updates=2990, lr=2.12674e-05, gnorm=0.128, clip=0, loss_scale=1, train_wall=8, gb_free=17.3, wall=2571
2023-08-08 16:50:49 - progress_bar.py[line:272] - INFO: epoch 002:    770 / 2241 loss=9.072, loss_v1=0, loss_v2=0, nll_loss=8.527, ntokens=734, nsentences=28, sample_size=734, sample_size_v1=0, sample_size_v2=0, ppl=368.95, wps=865.8, ups=1.18, wpb=734, bsz=28, num_updates=3000, lr=2.12318e-05, gnorm=0.134, clip=0, loss_scale=1, train_wall=8, gb_free=16.5, wall=2580
2023-08-08 16:50:57 - progress_bar.py[line:272] - INFO: epoch 002:    780 / 2241 loss=9.025, loss_v1=0, loss_v2=0, nll_loss=8.467, ntokens=743.4, nsentences=28, sample_size=743.4, sample_size_v1=0, sample_size_v2=0, ppl=353.88, wps=884, ups=1.19, wpb=743.4, bsz=28, num_updates=3010, lr=2.11962e-05, gnorm=0.134, clip=0, loss_scale=1, train_wall=8, gb_free=17.8, wall=2588
2023-08-08 16:51:05 - progress_bar.py[line:272] - INFO: epoch 002:    790 / 2241 loss=9.004, loss_v1=0, loss_v2=0, nll_loss=8.458, ntokens=676.8, nsentences=28, sample_size=676.8, sample_size_v1=0, sample_size_v2=0, ppl=351.72, wps=803.9, ups=1.19, wpb=676.8, bsz=28, num_updates=3020, lr=2.11606e-05, gnorm=0.142, clip=0, loss_scale=1, train_wall=8, gb_free=17.8, wall=2597
2023-08-08 16:51:14 - progress_bar.py[line:272] - INFO: epoch 002:    800 / 2241 loss=9.121, loss_v1=0, loss_v2=0, nll_loss=8.583, ntokens=719.6, nsentences=28, sample_size=719.6, sample_size_v1=0, sample_size_v2=0, ppl=383.58, wps=850.2, ups=1.18, wpb=719.6, bsz=28, num_updates=3030, lr=2.1125e-05, gnorm=0.137, clip=0, loss_scale=1, train_wall=8, gb_free=17.3, wall=2605
2023-08-08 16:51:22 - progress_bar.py[line:272] - INFO: epoch 002:    810 / 2241 loss=9.003, loss_v1=0, loss_v2=0, nll_loss=8.466, ntokens=662.3, nsentences=28, sample_size=662.3, sample_size_v1=0, sample_size_v2=0, ppl=353.69, wps=786.5, ups=1.19, wpb=662.3, bsz=28, num_updates=3040, lr=2.10894e-05, gnorm=0.142, clip=0, loss_scale=1, train_wall=8, gb_free=17.7, wall=2613
2023-08-08 16:51:31 - progress_bar.py[line:272] - INFO: epoch 002:    820 / 2241 loss=9.043, loss_v1=0, loss_v2=0, nll_loss=8.495, ntokens=723.7, nsentences=28, sample_size=723.7, sample_size_v1=0, sample_size_v2=0, ppl=360.87, wps=863.3, ups=1.19, wpb=723.7, bsz=28, num_updates=3050, lr=2.10538e-05, gnorm=0.131, clip=0, loss_scale=1, train_wall=8, gb_free=17.5, wall=2622
2023-08-08 16:51:39 - progress_bar.py[line:272] - INFO: epoch 002:    830 / 2241 loss=8.995, loss_v1=0, loss_v2=0, nll_loss=8.459, ntokens=737.4, nsentences=28, sample_size=737.4, sample_size_v1=0, sample_size_v2=0, ppl=351.79, wps=885.4, ups=1.2, wpb=737.4, bsz=28, num_updates=3060, lr=2.10182e-05, gnorm=0.134, clip=0, loss_scale=1, train_wall=8, gb_free=17.3, wall=2630
2023-08-08 16:51:48 - progress_bar.py[line:272] - INFO: epoch 002:    840 / 2241 loss=9.091, loss_v1=0, loss_v2=0, nll_loss=8.551, ntokens=795.4, nsentences=28, sample_size=795.4, sample_size_v1=0, sample_size_v2=0, ppl=375, wps=935.7, ups=1.18, wpb=795.4, bsz=28, num_updates=3070, lr=2.09826e-05, gnorm=0.13, clip=0, loss_scale=1, train_wall=8, gb_free=17.2, wall=2639
2023-08-08 16:51:56 - progress_bar.py[line:272] - INFO: epoch 002:    850 / 2241 loss=9.087, loss_v1=0, loss_v2=0, nll_loss=8.557, ntokens=763, nsentences=28, sample_size=763, sample_size_v1=0, sample_size_v2=0, ppl=376.69, wps=910.9, ups=1.19, wpb=763, bsz=28, num_updates=3080, lr=2.0947e-05, gnorm=0.132, clip=0, loss_scale=1, train_wall=8, gb_free=17.9, wall=2647
2023-08-08 16:52:04 - progress_bar.py[line:272] - INFO: epoch 002:    860 / 2241 loss=9.052, loss_v1=0, loss_v2=0, nll_loss=8.51, ntokens=661.8, nsentences=28, sample_size=661.8, sample_size_v1=0, sample_size_v2=0, ppl=364.44, wps=795.7, ups=1.2, wpb=661.8, bsz=28, num_updates=3090, lr=2.09114e-05, gnorm=0.138, clip=0, loss_scale=1, train_wall=8, gb_free=17.8, wall=2655
2023-08-08 16:52:13 - progress_bar.py[line:272] - INFO: epoch 002:    870 / 2241 loss=8.959, loss_v1=0, loss_v2=0, nll_loss=8.411, ntokens=718.8, nsentences=28, sample_size=718.8, sample_size_v1=0, sample_size_v2=0, ppl=340.33, wps=863.5, ups=1.2, wpb=718.8, bsz=28, num_updates=3100, lr=2.08758e-05, gnorm=0.133, clip=0, loss_scale=1, train_wall=8, gb_free=17.6, wall=2664
2023-08-08 16:52:21 - progress_bar.py[line:272] - INFO: epoch 002:    880 / 2241 loss=8.98, loss_v1=0, loss_v2=0, nll_loss=8.434, ntokens=757, nsentences=28, sample_size=757, sample_size_v1=0, sample_size_v2=0, ppl=345.76, wps=902.4, ups=1.19, wpb=757, bsz=28, num_updates=3110, lr=2.08402e-05, gnorm=0.133, clip=0, loss_scale=1, train_wall=8, gb_free=17.7, wall=2672
2023-08-08 16:52:29 - progress_bar.py[line:272] - INFO: epoch 002:    890 / 2241 loss=9.013, loss_v1=0, loss_v2=0, nll_loss=8.472, ntokens=768.8, nsentences=28, sample_size=768.8, sample_size_v1=0, sample_size_v2=0, ppl=355.1, wps=913.5, ups=1.19, wpb=768.8, bsz=28, num_updates=3120, lr=2.08046e-05, gnorm=0.134, clip=0, loss_scale=1, train_wall=8, gb_free=16.9, wall=2681
2023-08-08 16:52:38 - progress_bar.py[line:272] - INFO: epoch 002:    900 / 2241 loss=8.993, loss_v1=0, loss_v2=0, nll_loss=8.451, ntokens=731.2, nsentences=28, sample_size=731.2, sample_size_v1=0, sample_size_v2=0, ppl=349.93, wps=865.3, ups=1.18, wpb=731.2, bsz=28, num_updates=3130, lr=2.0769e-05, gnorm=0.133, clip=0, loss_scale=1, train_wall=8, gb_free=16.4, wall=2689
2023-08-08 16:52:46 - progress_bar.py[line:272] - INFO: epoch 002:    910 / 2241 loss=9.023, loss_v1=0, loss_v2=0, nll_loss=8.476, ntokens=827.2, nsentences=28, sample_size=827.2, sample_size_v1=0, sample_size_v2=0, ppl=356.18, wps=978, ups=1.18, wpb=827.2, bsz=28, num_updates=3140, lr=2.07334e-05, gnorm=0.13, clip=0, loss_scale=1, train_wall=8, gb_free=17, wall=2697
2023-08-08 16:52:55 - progress_bar.py[line:272] - INFO: epoch 002:    920 / 2241 loss=8.789, loss_v1=0, loss_v2=0, nll_loss=8.229, ntokens=696.3, nsentences=28, sample_size=696.3, sample_size_v1=0, sample_size_v2=0, ppl=300.09, wps=834.2, ups=1.2, wpb=696.3, bsz=28, num_updates=3150, lr=2.06978e-05, gnorm=0.135, clip=0, loss_scale=1, train_wall=8, gb_free=17.7, wall=2706
2023-08-08 16:53:03 - progress_bar.py[line:272] - INFO: epoch 002:    930 / 2241 loss=8.96, loss_v1=0, loss_v2=0, nll_loss=8.412, ntokens=717.4, nsentences=28, sample_size=717.4, sample_size_v1=0, sample_size_v2=0, ppl=340.51, wps=857.2, ups=1.19, wpb=717.4, bsz=28, num_updates=3160, lr=2.06622e-05, gnorm=0.133, clip=0, loss_scale=1, train_wall=8, gb_free=17.8, wall=2714
2023-08-08 16:53:11 - progress_bar.py[line:272] - INFO: epoch 002:    940 / 2241 loss=8.889, loss_v1=0, loss_v2=0, nll_loss=8.338, ntokens=652.7, nsentences=28, sample_size=652.7, sample_size_v1=0, sample_size_v2=0, ppl=323.52, wps=780.1, ups=1.2, wpb=652.7, bsz=28, num_updates=3170, lr=2.06266e-05, gnorm=0.137, clip=0, loss_scale=1, train_wall=8, gb_free=17.7, wall=2723
2023-08-08 16:53:20 - progress_bar.py[line:272] - INFO: epoch 002:    950 / 2241 loss=8.87, loss_v1=0, loss_v2=0, nll_loss=8.315, ntokens=744.9, nsentences=28, sample_size=744.9, sample_size_v1=0, sample_size_v2=0, ppl=318.5, wps=884.1, ups=1.19, wpb=744.9, bsz=28, num_updates=3180, lr=2.0591e-05, gnorm=0.131, clip=0, loss_scale=1, train_wall=8, gb_free=17.7, wall=2731
2023-08-08 16:53:28 - progress_bar.py[line:272] - INFO: epoch 002:    960 / 2241 loss=8.939, loss_v1=0, loss_v2=0, nll_loss=8.377, ntokens=772, nsentences=28, sample_size=772, sample_size_v1=0, sample_size_v2=0, ppl=332.38, wps=911.3, ups=1.18, wpb=772, bsz=28, num_updates=3190, lr=2.05554e-05, gnorm=0.133, clip=0, loss_scale=1, train_wall=8, gb_free=17.7, wall=2739
2023-08-08 16:53:37 - progress_bar.py[line:272] - INFO: epoch 002:    970 / 2241 loss=8.98, loss_v1=0, loss_v2=0, nll_loss=8.432, ntokens=819.6, nsentences=28, sample_size=819.6, sample_size_v1=0, sample_size_v2=0, ppl=345.26, wps=972.9, ups=1.19, wpb=819.6, bsz=28, num_updates=3200, lr=2.05198e-05, gnorm=0.131, clip=0, loss_scale=1, train_wall=8, gb_free=17.7, wall=2748
2023-08-08 16:53:45 - progress_bar.py[line:272] - INFO: epoch 002:    980 / 2241 loss=9, loss_v1=0, loss_v2=0, nll_loss=8.447, ntokens=774, nsentences=28, sample_size=774, sample_size_v1=0, sample_size_v2=0, ppl=349.03, wps=911.7, ups=1.18, wpb=774, bsz=28, num_updates=3210, lr=2.04842e-05, gnorm=0.134, clip=0, loss_scale=1, train_wall=8, gb_free=17.7, wall=2756
2023-08-08 16:53:54 - progress_bar.py[line:272] - INFO: epoch 002:    990 / 2241 loss=8.943, loss_v1=0, loss_v2=0, nll_loss=8.389, ntokens=754.1, nsentences=28, sample_size=754.1, sample_size_v1=0, sample_size_v2=0, ppl=335.13, wps=894.3, ups=1.19, wpb=754.1, bsz=28, num_updates=3220, lr=2.04486e-05, gnorm=0.133, clip=0, loss_scale=1, train_wall=8, gb_free=17.2, wall=2765
2023-08-08 16:54:02 - progress_bar.py[line:272] - INFO: epoch 002:   1000 / 2241 loss=8.846, loss_v1=0, loss_v2=0, nll_loss=8.296, ntokens=760, nsentences=28, sample_size=760, sample_size_v1=0, sample_size_v2=0, ppl=314.28, wps=910.5, ups=1.2, wpb=760, bsz=28, num_updates=3230, lr=2.0413e-05, gnorm=0.129, clip=0, loss_scale=1, train_wall=8, gb_free=17.6, wall=2773
2023-08-08 16:54:10 - progress_bar.py[line:272] - INFO: epoch 002:   1010 / 2241 loss=8.972, loss_v1=0, loss_v2=0, nll_loss=8.42, ntokens=832.5, nsentences=28, sample_size=832.5, sample_size_v1=0, sample_size_v2=0, ppl=342.44, wps=986.4, ups=1.18, wpb=832.5, bsz=28, num_updates=3240, lr=2.03774e-05, gnorm=0.142, clip=0, loss_scale=1, train_wall=8, gb_free=17.5, wall=2782
2023-08-08 16:54:19 - progress_bar.py[line:272] - INFO: epoch 002:   1020 / 2241 loss=8.952, loss_v1=0, loss_v2=0, nll_loss=8.399, ntokens=810.7, nsentences=28, sample_size=810.7, sample_size_v1=0, sample_size_v2=0, ppl=337.67, wps=959.5, ups=1.18, wpb=810.7, bsz=28, num_updates=3250, lr=2.03418e-05, gnorm=0.14, clip=0, loss_scale=1, train_wall=8, gb_free=17.5, wall=2790
2023-08-08 16:54:27 - progress_bar.py[line:272] - INFO: epoch 002:   1030 / 2241 loss=8.958, loss_v1=0, loss_v2=0, nll_loss=8.403, ntokens=785.9, nsentences=28, sample_size=785.9, sample_size_v1=0, sample_size_v2=0, ppl=338.52, wps=934.8, ups=1.19, wpb=785.9, bsz=28, num_updates=3260, lr=2.03062e-05, gnorm=0.133, clip=0, loss_scale=1, train_wall=8, gb_free=17.6, wall=2798
2023-08-08 16:54:36 - progress_bar.py[line:272] - INFO: epoch 002:   1040 / 2241 loss=8.951, loss_v1=0, loss_v2=0, nll_loss=8.394, ntokens=861.2, nsentences=28, sample_size=861.2, sample_size_v1=0, sample_size_v2=0, ppl=336.34, wps=1022.9, ups=1.19, wpb=861.2, bsz=28, num_updates=3270, lr=2.02706e-05, gnorm=0.131, clip=0, loss_scale=1, train_wall=8, gb_free=17.5, wall=2807
2023-08-08 16:54:44 - progress_bar.py[line:272] - INFO: epoch 002:   1050 / 2241 loss=8.867, loss_v1=0, loss_v2=0, nll_loss=8.32, ntokens=698.3, nsentences=28, sample_size=698.3, sample_size_v1=0, sample_size_v2=0, ppl=319.55, wps=833.6, ups=1.19, wpb=698.3, bsz=28, num_updates=3280, lr=2.0235e-05, gnorm=0.132, clip=0, loss_scale=1, train_wall=8, gb_free=16.6, wall=2815
2023-08-08 16:54:52 - progress_bar.py[line:272] - INFO: epoch 002:   1060 / 2241 loss=8.852, loss_v1=0, loss_v2=0, nll_loss=8.301, ntokens=741.3, nsentences=28, sample_size=741.3, sample_size_v1=0, sample_size_v2=0, ppl=315.38, wps=886.4, ups=1.2, wpb=741.3, bsz=28, num_updates=3290, lr=2.01994e-05, gnorm=0.129, clip=0, loss_scale=1, train_wall=8, gb_free=17.6, wall=2824
2023-08-08 16:55:01 - progress_bar.py[line:272] - INFO: epoch 002:   1070 / 2241 loss=8.946, loss_v1=0, loss_v2=0, nll_loss=8.393, ntokens=725.9, nsentences=28, sample_size=725.9, sample_size_v1=0, sample_size_v2=0, ppl=336.17, wps=858.2, ups=1.18, wpb=725.9, bsz=28, num_updates=3300, lr=2.01638e-05, gnorm=0.137, clip=0, loss_scale=1, train_wall=8, gb_free=17.1, wall=2832
2023-08-08 16:55:09 - progress_bar.py[line:272] - INFO: epoch 002:   1080 / 2241 loss=8.837, loss_v1=0, loss_v2=0, nll_loss=8.273, ntokens=722.6, nsentences=28, sample_size=722.6, sample_size_v1=0, sample_size_v2=0, ppl=309.31, wps=855, ups=1.18, wpb=722.6, bsz=28, num_updates=3310, lr=2.01282e-05, gnorm=0.132, clip=0, loss_scale=1, train_wall=8, gb_free=17.5, wall=2841
2023-08-08 16:55:18 - progress_bar.py[line:272] - INFO: epoch 002:   1090 / 2241 loss=8.722, loss_v1=0, loss_v2=0, nll_loss=8.156, ntokens=714, nsentences=28, sample_size=714, sample_size_v1=0, sample_size_v2=0, ppl=285.16, wps=847.1, ups=1.19, wpb=714, bsz=28, num_updates=3320, lr=2.00926e-05, gnorm=0.146, clip=0, loss_scale=1, train_wall=8, gb_free=17.7, wall=2849
2023-08-08 16:55:26 - progress_bar.py[line:272] - INFO: epoch 002:   1100 / 2241 loss=8.785, loss_v1=0, loss_v2=0, nll_loss=8.225, ntokens=748.4, nsentences=28, sample_size=748.4, sample_size_v1=0, sample_size_v2=0, ppl=299.25, wps=894.7, ups=1.2, wpb=748.4, bsz=28, num_updates=3330, lr=2.0057e-05, gnorm=0.13, clip=0, loss_scale=2, train_wall=8, gb_free=17.7, wall=2857
2023-08-08 16:55:35 - progress_bar.py[line:272] - INFO: epoch 002:   1110 / 2241 loss=8.836, loss_v1=0, loss_v2=0, nll_loss=8.275, ntokens=804, nsentences=28, sample_size=804, sample_size_v1=0, sample_size_v2=0, ppl=309.74, wps=955.6, ups=1.19, wpb=804, bsz=28, num_updates=3340, lr=2.00214e-05, gnorm=0.13, clip=0, loss_scale=2, train_wall=8, gb_free=17.7, wall=2866
2023-08-08 16:55:43 - progress_bar.py[line:272] - INFO: epoch 002:   1120 / 2241 loss=8.855, loss_v1=0, loss_v2=0, nll_loss=8.297, ntokens=741, nsentences=28, sample_size=741, sample_size_v1=0, sample_size_v2=0, ppl=314.56, wps=884.9, ups=1.19, wpb=741, bsz=28, num_updates=3350, lr=1.99858e-05, gnorm=0.134, clip=0, loss_scale=2, train_wall=8, gb_free=17.3, wall=2874
2023-08-08 16:55:51 - progress_bar.py[line:272] - INFO: epoch 002:   1130 / 2241 loss=8.925, loss_v1=0, loss_v2=0, nll_loss=8.366, ntokens=768.7, nsentences=28, sample_size=768.7, sample_size_v1=0, sample_size_v2=0, ppl=329.85, wps=906.5, ups=1.18, wpb=768.7, bsz=28, num_updates=3360, lr=1.99502e-05, gnorm=0.137, clip=0, loss_scale=2, train_wall=8, gb_free=17.7, wall=2883
2023-08-08 16:56:00 - progress_bar.py[line:272] - INFO: epoch 002:   1140 / 2241 loss=8.779, loss_v1=0, loss_v2=0, nll_loss=8.217, ntokens=792.2, nsentences=28, sample_size=792.2, sample_size_v1=0, sample_size_v2=0, ppl=297.51, wps=944.6, ups=1.19, wpb=792.2, bsz=28, num_updates=3370, lr=1.99146e-05, gnorm=0.127, clip=0, loss_scale=2, train_wall=8, gb_free=17.4, wall=2891
2023-08-08 16:56:08 - progress_bar.py[line:272] - INFO: epoch 002:   1150 / 2241 loss=8.803, loss_v1=0, loss_v2=0, nll_loss=8.232, ntokens=809, nsentences=28, sample_size=809, sample_size_v1=0, sample_size_v2=0, ppl=300.59, wps=953.8, ups=1.18, wpb=809, bsz=28, num_updates=3380, lr=1.9879e-05, gnorm=0.141, clip=0, loss_scale=2, train_wall=8, gb_free=17.9, wall=2899
2023-08-08 16:56:17 - progress_bar.py[line:272] - INFO: epoch 002:   1160 / 2241 loss=8.972, loss_v1=0, loss_v2=0, nll_loss=8.408, ntokens=789.4, nsentences=28, sample_size=789.4, sample_size_v1=0, sample_size_v2=0, ppl=339.65, wps=936.4, ups=1.19, wpb=789.4, bsz=28, num_updates=3390, lr=1.98434e-05, gnorm=0.135, clip=0, loss_scale=2, train_wall=8, gb_free=17.7, wall=2908
2023-08-08 16:56:25 - progress_bar.py[line:272] - INFO: epoch 002:   1170 / 2241 loss=8.892, loss_v1=0, loss_v2=0, nll_loss=8.316, ntokens=825.5, nsentences=28, sample_size=825.5, sample_size_v1=0, sample_size_v2=0, ppl=318.75, wps=968.5, ups=1.17, wpb=825.5, bsz=28, num_updates=3400, lr=1.98078e-05, gnorm=0.128, clip=0, loss_scale=2, train_wall=9, gb_free=17.1, wall=2916
2023-08-08 16:56:34 - progress_bar.py[line:272] - INFO: epoch 002:   1180 / 2241 loss=8.882, loss_v1=0, loss_v2=0, nll_loss=8.324, ntokens=803.8, nsentences=28, sample_size=803.8, sample_size_v1=0, sample_size_v2=0, ppl=320.42, wps=943.2, ups=1.17, wpb=803.8, bsz=28, num_updates=3410, lr=1.97722e-05, gnorm=0.129, clip=0, loss_scale=2, train_wall=8, gb_free=17.2, wall=2925
2023-08-08 16:56:42 - progress_bar.py[line:272] - INFO: epoch 002:   1190 / 2241 loss=8.784, loss_v1=0, loss_v2=0, nll_loss=8.214, ntokens=726.3, nsentences=28, sample_size=726.3, sample_size_v1=0, sample_size_v2=0, ppl=296.9, wps=864.9, ups=1.19, wpb=726.3, bsz=28, num_updates=3420, lr=1.97366e-05, gnorm=0.138, clip=0, loss_scale=2, train_wall=8, gb_free=17.4, wall=2933
2023-08-08 16:56:51 - progress_bar.py[line:272] - INFO: epoch 002:   1200 / 2241 loss=8.87, loss_v1=0, loss_v2=0, nll_loss=8.307, ntokens=824.1, nsentences=28, sample_size=824.1, sample_size_v1=0, sample_size_v2=0, ppl=316.67, wps=973.1, ups=1.18, wpb=824.1, bsz=28, num_updates=3430, lr=1.9701e-05, gnorm=0.134, clip=0, loss_scale=2, train_wall=8, gb_free=17.4, wall=2942
2023-08-08 16:56:59 - progress_bar.py[line:272] - INFO: epoch 002:   1210 / 2241 loss=8.762, loss_v1=0, loss_v2=0, nll_loss=8.187, ntokens=835.1, nsentences=28, sample_size=835.1, sample_size_v1=0, sample_size_v2=0, ppl=291.41, wps=985.3, ups=1.18, wpb=835.1, bsz=28, num_updates=3440, lr=1.96654e-05, gnorm=0.13, clip=0, loss_scale=2, train_wall=8, gb_free=17.2, wall=2950
2023-08-08 16:57:08 - progress_bar.py[line:272] - INFO: epoch 002:   1220 / 2241 loss=8.806, loss_v1=0, loss_v2=0, nll_loss=8.243, ntokens=872.7, nsentences=28, sample_size=872.7, sample_size_v1=0, sample_size_v2=0, ppl=302.88, wps=1025.2, ups=1.17, wpb=872.7, bsz=28, num_updates=3450, lr=1.96298e-05, gnorm=0.131, clip=0, loss_scale=2, train_wall=8, gb_free=17.3, wall=2959
2023-08-08 16:57:16 - progress_bar.py[line:272] - INFO: epoch 002:   1230 / 2241 loss=8.883, loss_v1=0, loss_v2=0, nll_loss=8.307, ntokens=846.1, nsentences=28, sample_size=846.1, sample_size_v1=0, sample_size_v2=0, ppl=316.78, wps=987.4, ups=1.17, wpb=846.1, bsz=28, num_updates=3460, lr=1.95942e-05, gnorm=0.127, clip=0, loss_scale=2, train_wall=9, gb_free=17.1, wall=2967
2023-08-08 16:57:25 - progress_bar.py[line:272] - INFO: epoch 002:   1240 / 2241 loss=8.674, loss_v1=0, loss_v2=0, nll_loss=8.084, ntokens=832.1, nsentences=28, sample_size=832.1, sample_size_v1=0, sample_size_v2=0, ppl=271.44, wps=986, ups=1.18, wpb=832.1, bsz=28, num_updates=3470, lr=1.95586e-05, gnorm=0.133, clip=0, loss_scale=2, train_wall=8, gb_free=17.7, wall=2976
2023-08-08 16:57:33 - progress_bar.py[line:272] - INFO: epoch 002:   1250 / 2241 loss=8.867, loss_v1=0, loss_v2=0, nll_loss=8.285, ntokens=855.4, nsentences=28, sample_size=855.4, sample_size_v1=0, sample_size_v2=0, ppl=311.91, wps=1004.9, ups=1.17, wpb=855.4, bsz=28, num_updates=3480, lr=1.9523e-05, gnorm=0.135, clip=0, loss_scale=2, train_wall=8, gb_free=17.2, wall=2984
2023-08-08 16:57:42 - progress_bar.py[line:272] - INFO: epoch 002:   1260 / 2241 loss=8.789, loss_v1=0, loss_v2=0, nll_loss=8.213, ntokens=840.2, nsentences=28, sample_size=840.2, sample_size_v1=0, sample_size_v2=0, ppl=296.64, wps=991.4, ups=1.18, wpb=840.2, bsz=28, num_updates=3490, lr=1.94874e-05, gnorm=0.135, clip=0, loss_scale=2, train_wall=8, gb_free=17.3, wall=2993
2023-08-08 16:57:50 - progress_bar.py[line:272] - INFO: epoch 002:   1270 / 2241 loss=8.863, loss_v1=0, loss_v2=0, nll_loss=8.291, ntokens=844, nsentences=28, sample_size=844, sample_size_v1=0, sample_size_v2=0, ppl=313.13, wps=990.8, ups=1.17, wpb=844, bsz=28, num_updates=3500, lr=1.94518e-05, gnorm=0.128, clip=0, loss_scale=2, train_wall=8, gb_free=17.5, wall=3001
2023-08-08 16:57:59 - progress_bar.py[line:272] - INFO: epoch 002:   1280 / 2241 loss=8.773, loss_v1=0, loss_v2=0, nll_loss=8.193, ntokens=821, nsentences=28, sample_size=821, sample_size_v1=0, sample_size_v2=0, ppl=292.7, wps=969.4, ups=1.18, wpb=821, bsz=28, num_updates=3510, lr=1.94162e-05, gnorm=0.132, clip=0, loss_scale=2, train_wall=8, gb_free=17.3, wall=3010
2023-08-08 16:58:07 - progress_bar.py[line:272] - INFO: epoch 002:   1290 / 2241 loss=8.778, loss_v1=0, loss_v2=0, nll_loss=8.2, ntokens=834.3, nsentences=28, sample_size=834.3, sample_size_v1=0, sample_size_v2=0, ppl=294.12, wps=986.3, ups=1.18, wpb=834.3, bsz=28, num_updates=3520, lr=1.93806e-05, gnorm=0.129, clip=0, loss_scale=2, train_wall=8, gb_free=17.4, wall=3018
2023-08-08 16:58:16 - progress_bar.py[line:272] - INFO: epoch 002:   1300 / 2241 loss=8.766, loss_v1=0, loss_v2=0, nll_loss=8.18, ntokens=819.6, nsentences=28, sample_size=819.6, sample_size_v1=0, sample_size_v2=0, ppl=290.1, wps=971.2, ups=1.18, wpb=819.6, bsz=28, num_updates=3530, lr=1.9345e-05, gnorm=0.134, clip=0, loss_scale=2, train_wall=8, gb_free=17.6, wall=3027
2023-08-08 16:58:24 - progress_bar.py[line:272] - INFO: epoch 002:   1310 / 2241 loss=8.649, loss_v1=0, loss_v2=0, nll_loss=8.065, ntokens=811.8, nsentences=28, sample_size=811.8, sample_size_v1=0, sample_size_v2=0, ppl=267.82, wps=966.2, ups=1.19, wpb=811.8, bsz=28, num_updates=3540, lr=1.93094e-05, gnorm=0.127, clip=0, loss_scale=2, train_wall=8, gb_free=17.2, wall=3035
2023-08-08 16:58:32 - progress_bar.py[line:272] - INFO: epoch 002:   1320 / 2241 loss=8.71, loss_v1=0, loss_v2=0, nll_loss=8.13, ntokens=783.5, nsentences=28, sample_size=783.5, sample_size_v1=0, sample_size_v2=0, ppl=280.17, wps=927.7, ups=1.18, wpb=783.5, bsz=28, num_updates=3550, lr=1.92738e-05, gnorm=0.133, clip=0, loss_scale=2, train_wall=8, gb_free=17.4, wall=3044
2023-08-08 16:58:41 - progress_bar.py[line:272] - INFO: epoch 002:   1330 / 2241 loss=8.746, loss_v1=0, loss_v2=0, nll_loss=8.164, ntokens=845.2, nsentences=28, sample_size=845.2, sample_size_v1=0, sample_size_v2=0, ppl=286.75, wps=997.6, ups=1.18, wpb=845.2, bsz=28, num_updates=3560, lr=1.92382e-05, gnorm=0.128, clip=0, loss_scale=2, train_wall=8, gb_free=17.5, wall=3052
2023-08-08 16:58:50 - progress_bar.py[line:272] - INFO: epoch 002:   1340 / 2241 loss=8.8, loss_v1=0, loss_v2=0, nll_loss=8.203, ntokens=908.5, nsentences=28, sample_size=908.5, sample_size_v1=0, sample_size_v2=0, ppl=294.68, wps=1052.7, ups=1.16, wpb=908.5, bsz=28, num_updates=3570, lr=1.92026e-05, gnorm=0.133, clip=0, loss_scale=2, train_wall=9, gb_free=16.4, wall=3061
2023-08-08 16:58:58 - progress_bar.py[line:272] - INFO: epoch 002:   1350 / 2241 loss=8.779, loss_v1=0, loss_v2=0, nll_loss=8.191, ntokens=885.8, nsentences=28, sample_size=885.8, sample_size_v1=0, sample_size_v2=0, ppl=292.31, wps=1041.1, ups=1.18, wpb=885.8, bsz=28, num_updates=3580, lr=1.9167e-05, gnorm=0.174, clip=0, loss_scale=2, train_wall=8, gb_free=17.4, wall=3069
2023-08-08 16:59:07 - progress_bar.py[line:272] - INFO: epoch 002:   1360 / 2241 loss=8.673, loss_v1=0, loss_v2=0, nll_loss=8.09, ntokens=811.4, nsentences=28, sample_size=811.4, sample_size_v1=0, sample_size_v2=0, ppl=272.53, wps=953.8, ups=1.18, wpb=811.4, bsz=28, num_updates=3590, lr=1.91314e-05, gnorm=0.128, clip=0, loss_scale=2, train_wall=8, gb_free=17.7, wall=3078
2023-08-08 16:59:15 - progress_bar.py[line:272] - INFO: epoch 002:   1370 / 2241 loss=8.605, loss_v1=0, loss_v2=0, nll_loss=8.022, ntokens=845.3, nsentences=28, sample_size=845.3, sample_size_v1=0, sample_size_v2=0, ppl=259.89, wps=996.6, ups=1.18, wpb=845.3, bsz=28, num_updates=3600, lr=1.90958e-05, gnorm=0.127, clip=0, loss_scale=2, train_wall=8, gb_free=17.5, wall=3086
2023-08-08 16:59:24 - progress_bar.py[line:272] - INFO: epoch 002:   1380 / 2241 loss=8.658, loss_v1=0, loss_v2=0, nll_loss=8.07, ntokens=845.3, nsentences=28, sample_size=845.3, sample_size_v1=0, sample_size_v2=0, ppl=268.74, wps=986.3, ups=1.17, wpb=845.3, bsz=28, num_updates=3610, lr=1.90602e-05, gnorm=0.126, clip=0, loss_scale=2, train_wall=9, gb_free=17.5, wall=3095
2023-08-08 16:59:32 - progress_bar.py[line:272] - INFO: epoch 002:   1390 / 2241 loss=8.616, loss_v1=0, loss_v2=0, nll_loss=8.022, ntokens=806.3, nsentences=28, sample_size=806.3, sample_size_v1=0, sample_size_v2=0, ppl=259.89, wps=957.8, ups=1.19, wpb=806.3, bsz=28, num_updates=3620, lr=1.90246e-05, gnorm=0.13, clip=0, loss_scale=2, train_wall=8, gb_free=17.7, wall=3103
2023-08-08 16:59:41 - progress_bar.py[line:272] - INFO: epoch 002:   1400 / 2241 loss=8.806, loss_v1=0, loss_v2=0, nll_loss=8.232, ntokens=815.9, nsentences=28, sample_size=815.9, sample_size_v1=0, sample_size_v2=0, ppl=300.69, wps=960.9, ups=1.18, wpb=815.9, bsz=28, num_updates=3630, lr=1.8989e-05, gnorm=0.131, clip=0, loss_scale=2, train_wall=8, gb_free=17.3, wall=3112
2023-08-08 16:59:49 - progress_bar.py[line:272] - INFO: epoch 002:   1410 / 2241 loss=8.716, loss_v1=0, loss_v2=0, nll_loss=8.129, ntokens=883.7, nsentences=28, sample_size=883.7, sample_size_v1=0, sample_size_v2=0, ppl=279.92, wps=1041.1, ups=1.18, wpb=883.7, bsz=28, num_updates=3640, lr=1.89534e-05, gnorm=0.141, clip=0, loss_scale=2, train_wall=8, gb_free=17.2, wall=3120
2023-08-08 16:59:57 - progress_bar.py[line:272] - INFO: epoch 002:   1420 / 2241 loss=8.708, loss_v1=0, loss_v2=0, nll_loss=8.121, ntokens=829, nsentences=28, sample_size=829, sample_size_v1=0, sample_size_v2=0, ppl=278.36, wps=987.5, ups=1.19, wpb=829, bsz=28, num_updates=3650, lr=1.89178e-05, gnorm=0.133, clip=0, loss_scale=2, train_wall=8, gb_free=17.3, wall=3129
2023-08-08 17:00:06 - progress_bar.py[line:272] - INFO: epoch 002:   1430 / 2241 loss=8.593, loss_v1=0, loss_v2=0, nll_loss=7.994, ntokens=813.3, nsentences=28, sample_size=813.3, sample_size_v1=0, sample_size_v2=0, ppl=254.9, wps=967, ups=1.19, wpb=813.3, bsz=28, num_updates=3660, lr=1.88822e-05, gnorm=0.13, clip=0, loss_scale=2, train_wall=8, gb_free=17.2, wall=3137
2023-08-08 17:00:14 - progress_bar.py[line:272] - INFO: epoch 002:   1440 / 2241 loss=8.641, loss_v1=0, loss_v2=0, nll_loss=8.05, ntokens=883.8, nsentences=28, sample_size=883.8, sample_size_v1=0, sample_size_v2=0, ppl=265.05, wps=1037.4, ups=1.17, wpb=883.8, bsz=28, num_updates=3670, lr=1.88466e-05, gnorm=0.13, clip=0, loss_scale=2, train_wall=8, gb_free=17.3, wall=3146
2023-08-08 17:00:23 - progress_bar.py[line:272] - INFO: epoch 002:   1450 / 2241 loss=8.518, loss_v1=0, loss_v2=0, nll_loss=7.926, ntokens=708.6, nsentences=28, sample_size=708.6, sample_size_v1=0, sample_size_v2=0, ppl=243.23, wps=842.2, ups=1.19, wpb=708.6, bsz=28, num_updates=3680, lr=1.8811e-05, gnorm=0.129, clip=0, loss_scale=2, train_wall=8, gb_free=17.3, wall=3154
2023-08-08 17:00:31 - progress_bar.py[line:272] - INFO: epoch 002:   1460 / 2241 loss=8.675, loss_v1=0, loss_v2=0, nll_loss=8.086, ntokens=777.4, nsentences=28, sample_size=777.4, sample_size_v1=0, sample_size_v2=0, ppl=271.78, wps=918.1, ups=1.18, wpb=777.4, bsz=28, num_updates=3690, lr=1.87754e-05, gnorm=0.133, clip=0, loss_scale=2, train_wall=8, gb_free=17.2, wall=3162
2023-08-08 17:00:40 - progress_bar.py[line:272] - INFO: epoch 002:   1470 / 2241 loss=8.653, loss_v1=0, loss_v2=0, nll_loss=8.046, ntokens=786.9, nsentences=28, sample_size=786.9, sample_size_v1=0, sample_size_v2=0, ppl=264.29, wps=925.4, ups=1.18, wpb=786.9, bsz=28, num_updates=3700, lr=1.87398e-05, gnorm=0.133, clip=0, loss_scale=2, train_wall=8, gb_free=17.5, wall=3171
2023-08-08 17:00:48 - progress_bar.py[line:272] - INFO: epoch 002:   1480 / 2241 loss=8.699, loss_v1=0, loss_v2=0, nll_loss=8.1, ntokens=770.6, nsentences=28, sample_size=770.6, sample_size_v1=0, sample_size_v2=0, ppl=274.39, wps=913.9, ups=1.19, wpb=770.6, bsz=28, num_updates=3710, lr=1.87042e-05, gnorm=0.138, clip=0, loss_scale=2, train_wall=8, gb_free=16.3, wall=3179
2023-08-08 17:00:57 - progress_bar.py[line:272] - INFO: epoch 002:   1490 / 2241 loss=8.8, loss_v1=0, loss_v2=0, nll_loss=8.213, ntokens=808.4, nsentences=28, sample_size=808.4, sample_size_v1=0, sample_size_v2=0, ppl=296.81, wps=961.7, ups=1.19, wpb=808.4, bsz=28, num_updates=3720, lr=1.86686e-05, gnorm=0.13, clip=0, loss_scale=2, train_wall=8, gb_free=17.4, wall=3188
2023-08-08 17:01:05 - progress_bar.py[line:272] - INFO: epoch 002:   1500 / 2241 loss=8.672, loss_v1=0, loss_v2=0, nll_loss=8.074, ntokens=822.3, nsentences=28, sample_size=822.3, sample_size_v1=0, sample_size_v2=0, ppl=269.55, wps=971.7, ups=1.18, wpb=822.3, bsz=28, num_updates=3730, lr=1.8633e-05, gnorm=0.133, clip=0, loss_scale=2, train_wall=8, gb_free=17.3, wall=3196
2023-08-08 17:01:14 - progress_bar.py[line:272] - INFO: epoch 002:   1510 / 2241 loss=8.608, loss_v1=0, loss_v2=0, nll_loss=8.005, ntokens=781.6, nsentences=28, sample_size=781.6, sample_size_v1=0, sample_size_v2=0, ppl=256.82, wps=920.5, ups=1.18, wpb=781.6, bsz=28, num_updates=3740, lr=1.85974e-05, gnorm=0.132, clip=0, loss_scale=2, train_wall=8, gb_free=17.2, wall=3205
2023-08-08 17:01:22 - progress_bar.py[line:272] - INFO: epoch 002:   1520 / 2241 loss=8.68, loss_v1=0, loss_v2=0, nll_loss=8.083, ntokens=786.8, nsentences=28, sample_size=786.8, sample_size_v1=0, sample_size_v2=0, ppl=271.17, wps=931.4, ups=1.18, wpb=786.8, bsz=28, num_updates=3750, lr=1.85618e-05, gnorm=0.13, clip=0, loss_scale=2, train_wall=8, gb_free=17.7, wall=3213
2023-08-08 17:01:31 - progress_bar.py[line:272] - INFO: epoch 002:   1530 / 2241 loss=8.73, loss_v1=0, loss_v2=0, nll_loss=8.131, ntokens=871.3, nsentences=28, sample_size=871.3, sample_size_v1=0, sample_size_v2=0, ppl=280.29, wps=1020.1, ups=1.17, wpb=871.3, bsz=28, num_updates=3760, lr=1.85262e-05, gnorm=0.128, clip=0, loss_scale=2, train_wall=9, gb_free=16.8, wall=3222
2023-08-08 17:01:39 - progress_bar.py[line:272] - INFO: epoch 002:   1540 / 2241 loss=8.624, loss_v1=0, loss_v2=0, nll_loss=8.023, ntokens=759.9, nsentences=28, sample_size=759.9, sample_size_v1=0, sample_size_v2=0, ppl=260.09, wps=902.5, ups=1.19, wpb=759.9, bsz=28, num_updates=3770, lr=1.84906e-05, gnorm=0.134, clip=0, loss_scale=2, train_wall=8, gb_free=16.9, wall=3230
2023-08-08 17:01:47 - progress_bar.py[line:272] - INFO: epoch 002:   1550 / 2241 loss=8.644, loss_v1=0, loss_v2=0, nll_loss=8.057, ntokens=764.7, nsentences=28, sample_size=764.7, sample_size_v1=0, sample_size_v2=0, ppl=266.33, wps=910.5, ups=1.19, wpb=764.7, bsz=28, num_updates=3780, lr=1.8455e-05, gnorm=0.153, clip=0, loss_scale=2, train_wall=8, gb_free=17.4, wall=3239
2023-08-08 17:01:56 - progress_bar.py[line:272] - INFO: epoch 002:   1560 / 2241 loss=8.765, loss_v1=0, loss_v2=0, nll_loss=8.174, ntokens=923.8, nsentences=28, sample_size=923.8, sample_size_v1=0, sample_size_v2=0, ppl=288.78, wps=1083.7, ups=1.17, wpb=923.8, bsz=28, num_updates=3790, lr=1.84194e-05, gnorm=0.131, clip=0, loss_scale=2, train_wall=9, gb_free=17.1, wall=3247
2023-08-08 17:02:04 - progress_bar.py[line:272] - INFO: epoch 002:   1570 / 2241 loss=8.703, loss_v1=0, loss_v2=0, nll_loss=8.1, ntokens=880.1, nsentences=28, sample_size=880.1, sample_size_v1=0, sample_size_v2=0, ppl=274.3, wps=1034, ups=1.17, wpb=880.1, bsz=28, num_updates=3800, lr=1.83838e-05, gnorm=0.134, clip=0, loss_scale=2, train_wall=8, gb_free=17.6, wall=3256
2023-08-08 17:02:13 - progress_bar.py[line:272] - INFO: epoch 002:   1580 / 2241 loss=8.489, loss_v1=0, loss_v2=0, nll_loss=7.879, ntokens=803.3, nsentences=28, sample_size=803.3, sample_size_v1=0, sample_size_v2=0, ppl=235.36, wps=953.5, ups=1.19, wpb=803.3, bsz=28, num_updates=3810, lr=1.83482e-05, gnorm=0.125, clip=0, loss_scale=2, train_wall=8, gb_free=17.6, wall=3264
2023-08-08 17:02:21 - progress_bar.py[line:272] - INFO: epoch 002:   1590 / 2241 loss=8.7, loss_v1=0, loss_v2=0, nll_loss=8.113, ntokens=818.5, nsentences=28, sample_size=818.5, sample_size_v1=0, sample_size_v2=0, ppl=276.81, wps=969.4, ups=1.18, wpb=818.5, bsz=28, num_updates=3820, lr=1.83126e-05, gnorm=0.125, clip=0, loss_scale=2, train_wall=8, gb_free=17.4, wall=3272
2023-08-08 17:02:30 - progress_bar.py[line:272] - INFO: epoch 002:   1600 / 2241 loss=8.625, loss_v1=0, loss_v2=0, nll_loss=8.026, ntokens=836.7, nsentences=28, sample_size=836.7, sample_size_v1=0, sample_size_v2=0, ppl=260.59, wps=985.9, ups=1.18, wpb=836.7, bsz=28, num_updates=3830, lr=1.8277e-05, gnorm=0.132, clip=0, loss_scale=2, train_wall=8, gb_free=16.7, wall=3281
2023-08-08 17:02:38 - progress_bar.py[line:272] - INFO: epoch 002:   1610 / 2241 loss=8.723, loss_v1=0, loss_v2=0, nll_loss=8.129, ntokens=811.5, nsentences=28, sample_size=811.5, sample_size_v1=0, sample_size_v2=0, ppl=279.92, wps=953.9, ups=1.18, wpb=811.5, bsz=28, num_updates=3840, lr=1.82414e-05, gnorm=0.132, clip=0, loss_scale=4, train_wall=8, gb_free=17.4, wall=3289
2023-08-08 17:02:47 - progress_bar.py[line:272] - INFO: epoch 002:   1620 / 2241 loss=8.588, loss_v1=0, loss_v2=0, nll_loss=7.972, ntokens=910.2, nsentences=28, sample_size=910.2, sample_size_v1=0, sample_size_v2=0, ppl=251.1, wps=1064.8, ups=1.17, wpb=910.2, bsz=28, num_updates=3850, lr=1.82058e-05, gnorm=0.131, clip=0, loss_scale=4, train_wall=9, gb_free=17.2, wall=3298
2023-08-08 17:02:55 - progress_bar.py[line:272] - INFO: epoch 002:   1630 / 2241 loss=8.646, loss_v1=0, loss_v2=0, nll_loss=8.045, ntokens=882.4, nsentences=28, sample_size=882.4, sample_size_v1=0, sample_size_v2=0, ppl=264.16, wps=1031.1, ups=1.17, wpb=882.4, bsz=28, num_updates=3860, lr=1.81702e-05, gnorm=0.129, clip=0, loss_scale=4, train_wall=9, gb_free=17, wall=3307
2023-08-08 17:03:04 - progress_bar.py[line:272] - INFO: epoch 002:   1640 / 2241 loss=8.55, loss_v1=0, loss_v2=0, nll_loss=7.949, ntokens=803.1, nsentences=28, sample_size=803.1, sample_size_v1=0, sample_size_v2=0, ppl=247.12, wps=952.5, ups=1.19, wpb=803.1, bsz=28, num_updates=3870, lr=1.81346e-05, gnorm=0.131, clip=0, loss_scale=4, train_wall=8, gb_free=17.7, wall=3315
2023-08-08 17:03:12 - progress_bar.py[line:272] - INFO: epoch 002:   1650 / 2241 loss=8.726, loss_v1=0, loss_v2=0, nll_loss=8.136, ntokens=831.1, nsentences=28, sample_size=831.1, sample_size_v1=0, sample_size_v2=0, ppl=281.21, wps=984, ups=1.18, wpb=831.1, bsz=28, num_updates=3880, lr=1.8099e-05, gnorm=0.134, clip=0, loss_scale=4, train_wall=8, gb_free=17.6, wall=3323
2023-08-08 17:03:21 - progress_bar.py[line:272] - INFO: epoch 002:   1660 / 2241 loss=8.676, loss_v1=0, loss_v2=0, nll_loss=8.081, ntokens=867.5, nsentences=28, sample_size=867.5, sample_size_v1=0, sample_size_v2=0, ppl=270.83, wps=1029.9, ups=1.19, wpb=867.5, bsz=28, num_updates=3890, lr=1.80634e-05, gnorm=0.127, clip=0, loss_scale=4, train_wall=8, gb_free=16.7, wall=3332
2023-08-08 17:03:29 - progress_bar.py[line:272] - INFO: epoch 002:   1670 / 2241 loss=8.652, loss_v1=0, loss_v2=0, nll_loss=8.052, ntokens=851.6, nsentences=28, sample_size=851.6, sample_size_v1=0, sample_size_v2=0, ppl=265.47, wps=996.4, ups=1.17, wpb=851.6, bsz=28, num_updates=3900, lr=1.80278e-05, gnorm=0.132, clip=0, loss_scale=4, train_wall=9, gb_free=17.5, wall=3340
2023-08-08 17:03:38 - progress_bar.py[line:272] - INFO: epoch 002:   1680 / 2241 loss=8.666, loss_v1=0, loss_v2=0, nll_loss=8.069, ntokens=895.7, nsentences=28, sample_size=895.7, sample_size_v1=0, sample_size_v2=0, ppl=268.46, wps=1054.1, ups=1.18, wpb=895.7, bsz=28, num_updates=3910, lr=1.79922e-05, gnorm=0.13, clip=0, loss_scale=4, train_wall=8, gb_free=17.2, wall=3349
2023-08-08 17:03:46 - progress_bar.py[line:272] - INFO: epoch 002:   1690 / 2241 loss=8.607, loss_v1=0, loss_v2=0, nll_loss=8.006, ntokens=869.1, nsentences=28, sample_size=869.1, sample_size_v1=0, sample_size_v2=0, ppl=257.11, wps=1021.7, ups=1.18, wpb=869.1, bsz=28, num_updates=3920, lr=1.79566e-05, gnorm=0.129, clip=0, loss_scale=4, train_wall=8, gb_free=17.5, wall=3357
2023-08-08 17:03:55 - progress_bar.py[line:272] - INFO: epoch 002:   1700 / 2241 loss=8.708, loss_v1=0, loss_v2=0, nll_loss=8.108, ntokens=893.2, nsentences=28, sample_size=893.2, sample_size_v1=0, sample_size_v2=0, ppl=275.91, wps=1045.4, ups=1.17, wpb=893.2, bsz=28, num_updates=3930, lr=1.7921e-05, gnorm=0.137, clip=0, loss_scale=4, train_wall=9, gb_free=17.3, wall=3366
2023-08-08 17:04:03 - progress_bar.py[line:272] - INFO: epoch 002:   1710 / 2241 loss=8.622, loss_v1=0, loss_v2=0, nll_loss=8.017, ntokens=803.5, nsentences=28, sample_size=803.5, sample_size_v1=0, sample_size_v2=0, ppl=258.98, wps=941.4, ups=1.17, wpb=803.5, bsz=28, num_updates=3940, lr=1.78854e-05, gnorm=0.138, clip=0, loss_scale=4, train_wall=9, gb_free=17, wall=3375
2023-08-08 17:04:12 - progress_bar.py[line:272] - INFO: epoch 002:   1720 / 2241 loss=8.735, loss_v1=0, loss_v2=0, nll_loss=8.136, ntokens=934.5, nsentences=28, sample_size=934.5, sample_size_v1=0, sample_size_v2=0, ppl=281.31, wps=1091.3, ups=1.17, wpb=934.5, bsz=28, num_updates=3950, lr=1.78498e-05, gnorm=0.137, clip=0, loss_scale=4, train_wall=9, gb_free=16.7, wall=3383
2023-08-08 17:04:20 - progress_bar.py[line:272] - INFO: epoch 002:   1730 / 2241 loss=8.585, loss_v1=0, loss_v2=0, nll_loss=7.96, ntokens=841.4, nsentences=28, sample_size=841.4, sample_size_v1=0, sample_size_v2=0, ppl=248.99, wps=989.1, ups=1.18, wpb=841.4, bsz=28, num_updates=3960, lr=1.78142e-05, gnorm=0.137, clip=0, loss_scale=4, train_wall=8, gb_free=16.7, wall=3392
2023-08-08 17:04:29 - progress_bar.py[line:272] - INFO: epoch 002:   1740 / 2241 loss=8.608, loss_v1=0, loss_v2=0, nll_loss=8.013, ntokens=933.5, nsentences=28, sample_size=933.5, sample_size_v1=0, sample_size_v2=0, ppl=258.24, wps=1095, ups=1.17, wpb=933.5, bsz=28, num_updates=3970, lr=1.77786e-05, gnorm=0.133, clip=0, loss_scale=4, train_wall=9, gb_free=16.9, wall=3400
2023-08-08 17:04:38 - progress_bar.py[line:272] - INFO: epoch 002:   1750 / 2241 loss=8.725, loss_v1=0, loss_v2=0, nll_loss=8.124, ntokens=963.4, nsentences=28, sample_size=963.4, sample_size_v1=0, sample_size_v2=0, ppl=279.06, wps=1116, ups=1.16, wpb=963.4, bsz=28, num_updates=3980, lr=1.7743e-05, gnorm=0.13, clip=0, loss_scale=4, train_wall=9, gb_free=17.4, wall=3409
2023-08-08 17:04:46 - progress_bar.py[line:272] - INFO: epoch 002:   1760 / 2241 loss=8.65, loss_v1=0, loss_v2=0, nll_loss=8.041, ntokens=928, nsentences=28, sample_size=928, sample_size_v1=0, sample_size_v2=0, ppl=263.31, wps=1084.6, ups=1.17, wpb=928, bsz=28, num_updates=3990, lr=1.77074e-05, gnorm=0.131, clip=0, loss_scale=4, train_wall=9, gb_free=17.4, wall=3417
2023-08-08 17:04:55 - progress_bar.py[line:272] - INFO: epoch 002:   1770 / 2241 loss=8.637, loss_v1=0, loss_v2=0, nll_loss=8.027, ntokens=913.8, nsentences=28, sample_size=913.8, sample_size_v1=0, sample_size_v2=0, ppl=260.77, wps=1074.4, ups=1.18, wpb=913.8, bsz=28, num_updates=4000, lr=1.76718e-05, gnorm=0.132, clip=0, loss_scale=4, train_wall=8, gb_free=17.2, wall=3426
2023-08-08 17:05:03 - progress_bar.py[line:272] - INFO: epoch 002:   1780 / 2241 loss=8.657, loss_v1=0, loss_v2=0, nll_loss=8.051, ntokens=895.5, nsentences=28, sample_size=895.5, sample_size_v1=0, sample_size_v2=0, ppl=265.21, wps=1055.9, ups=1.18, wpb=895.5, bsz=28, num_updates=4010, lr=1.76362e-05, gnorm=0.131, clip=0, loss_scale=4, train_wall=8, gb_free=17.3, wall=3434
2023-08-08 17:05:12 - progress_bar.py[line:272] - INFO: epoch 002:   1790 / 2241 loss=8.601, loss_v1=0, loss_v2=0, nll_loss=8.002, ntokens=902.4, nsentences=28, sample_size=902.4, sample_size_v1=0, sample_size_v2=0, ppl=256.37, wps=1066.4, ups=1.18, wpb=902.4, bsz=28, num_updates=4020, lr=1.76006e-05, gnorm=0.129, clip=0, loss_scale=4, train_wall=8, gb_free=17, wall=3443
2023-08-08 17:05:20 - progress_bar.py[line:272] - INFO: epoch 002:   1800 / 2241 loss=8.693, loss_v1=0, loss_v2=0, nll_loss=8.097, ntokens=986.9, nsentences=28, sample_size=986.9, sample_size_v1=0, sample_size_v2=0, ppl=273.83, wps=1149.9, ups=1.17, wpb=986.9, bsz=28, num_updates=4030, lr=1.7565e-05, gnorm=0.123, clip=0, loss_scale=4, train_wall=9, gb_free=17.5, wall=3451
2023-08-08 17:05:29 - progress_bar.py[line:272] - INFO: epoch 002:   1810 / 2241 loss=8.595, loss_v1=0, loss_v2=0, nll_loss=7.991, ntokens=791.4, nsentences=27.6, sample_size=791.4, sample_size_v1=0, sample_size_v2=0, ppl=254.33, wps=946.2, ups=1.2, wpb=791.4, bsz=27.6, num_updates=4040, lr=1.75294e-05, gnorm=0.13, clip=0, loss_scale=4, train_wall=8, gb_free=17.5, wall=3460
2023-08-08 17:05:37 - progress_bar.py[line:272] - INFO: epoch 002:   1820 / 2241 loss=8.636, loss_v1=0, loss_v2=0, nll_loss=8.029, ntokens=972.3, nsentences=28, sample_size=972.3, sample_size_v1=0, sample_size_v2=0, ppl=261.17, wps=1138.6, ups=1.17, wpb=972.3, bsz=28, num_updates=4050, lr=1.74938e-05, gnorm=0.127, clip=0, loss_scale=4, train_wall=9, gb_free=16.1, wall=3468
2023-08-08 17:05:46 - progress_bar.py[line:272] - INFO: epoch 002:   1830 / 2241 loss=8.721, loss_v1=0, loss_v2=0, nll_loss=8.12, ntokens=989.4, nsentences=28, sample_size=989.4, sample_size_v1=0, sample_size_v2=0, ppl=278.12, wps=1159.9, ups=1.17, wpb=989.4, bsz=28, num_updates=4060, lr=1.74582e-05, gnorm=0.131, clip=0, loss_scale=4, train_wall=9, gb_free=17.2, wall=3477
2023-08-08 17:05:55 - progress_bar.py[line:272] - INFO: epoch 002:   1840 / 2241 loss=8.838, loss_v1=0, loss_v2=0, nll_loss=8.24, ntokens=1073.2, nsentences=28, sample_size=1073.2, sample_size_v1=0, sample_size_v2=0, ppl=302.33, wps=1213.7, ups=1.13, wpb=1073.2, bsz=28, num_updates=4070, lr=1.74226e-05, gnorm=0.132, clip=0, loss_scale=4, train_wall=9, gb_free=17, wall=3486
2023-08-08 17:06:03 - progress_bar.py[line:272] - INFO: epoch 002:   1850 / 2241 loss=8.674, loss_v1=0, loss_v2=0, nll_loss=8.058, ntokens=1022.3, nsentences=28, sample_size=1022.3, sample_size_v1=0, sample_size_v2=0, ppl=266.53, wps=1178, ups=1.15, wpb=1022.3, bsz=28, num_updates=4080, lr=1.7387e-05, gnorm=0.127, clip=0, loss_scale=4, train_wall=9, gb_free=16.3, wall=3494
2023-08-08 17:06:12 - progress_bar.py[line:272] - INFO: epoch 002:   1860 / 2241 loss=8.704, loss_v1=0, loss_v2=0, nll_loss=8.102, ntokens=1011.6, nsentences=28, sample_size=1011.6, sample_size_v1=0, sample_size_v2=0, ppl=274.72, wps=1184.3, ups=1.17, wpb=1011.6, bsz=28, num_updates=4090, lr=1.73514e-05, gnorm=0.13, clip=0, loss_scale=4, train_wall=9, gb_free=17.3, wall=3503
2023-08-08 17:06:20 - progress_bar.py[line:272] - INFO: epoch 002:   1870 / 2241 loss=8.757, loss_v1=0, loss_v2=0, nll_loss=8.153, ntokens=927.6, nsentences=28, sample_size=927.6, sample_size_v1=0, sample_size_v2=0, ppl=284.63, wps=1086, ups=1.17, wpb=927.6, bsz=28, num_updates=4100, lr=1.73158e-05, gnorm=0.128, clip=0, loss_scale=4, train_wall=9, gb_free=17.5, wall=3511
2023-08-08 17:06:29 - progress_bar.py[line:272] - INFO: epoch 002:   1880 / 2241 loss=8.756, loss_v1=0, loss_v2=0, nll_loss=8.155, ntokens=932.2, nsentences=28, sample_size=932.2, sample_size_v1=0, sample_size_v2=0, ppl=285, wps=1096.4, ups=1.18, wpb=932.2, bsz=28, num_updates=4110, lr=1.72802e-05, gnorm=0.135, clip=0, loss_scale=4, train_wall=8, gb_free=16.6, wall=3520
2023-08-08 17:06:37 - progress_bar.py[line:272] - INFO: epoch 002:   1890 / 2241 loss=8.639, loss_v1=0, loss_v2=0, nll_loss=8.034, ntokens=931.6, nsentences=28, sample_size=931.6, sample_size_v1=0, sample_size_v2=0, ppl=262.13, wps=1092.8, ups=1.17, wpb=931.6, bsz=28, num_updates=4120, lr=1.72446e-05, gnorm=0.131, clip=0, loss_scale=4, train_wall=9, gb_free=16.6, wall=3528
2023-08-08 17:06:39 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-08-08 17:06:47 - progress_bar.py[line:272] - INFO: epoch 002:   1901 / 2241 loss=8.698, loss_v1=0, loss_v2=0, nll_loss=8.085, ntokens=950.7, nsentences=28, sample_size=950.7, sample_size_v1=0, sample_size_v2=0, ppl=271.56, wps=1007.2, ups=1.06, wpb=950.7, bsz=28, num_updates=4130, lr=1.7209e-05, gnorm=0.128, clip=0, loss_scale=2, train_wall=9, gb_free=17.2, wall=3538
2023-08-08 17:06:55 - progress_bar.py[line:272] - INFO: epoch 002:   1911 / 2241 loss=8.563, loss_v1=0, loss_v2=0, nll_loss=7.964, ntokens=832.2, nsentences=28, sample_size=832.2, sample_size_v1=0, sample_size_v2=0, ppl=249.73, wps=976.8, ups=1.17, wpb=832.2, bsz=28, num_updates=4140, lr=1.71734e-05, gnorm=0.127, clip=0, loss_scale=2, train_wall=8, gb_free=17.3, wall=3546
2023-08-08 17:07:04 - progress_bar.py[line:272] - INFO: epoch 002:   1921 / 2241 loss=8.578, loss_v1=0, loss_v2=0, nll_loss=7.963, ntokens=867.2, nsentences=28, sample_size=867.2, sample_size_v1=0, sample_size_v2=0, ppl=249.59, wps=1016.6, ups=1.17, wpb=867.2, bsz=28, num_updates=4150, lr=1.71378e-05, gnorm=0.127, clip=0, loss_scale=2, train_wall=9, gb_free=16, wall=3555
2023-08-08 17:07:12 - progress_bar.py[line:272] - INFO: epoch 002:   1931 / 2241 loss=8.603, loss_v1=0, loss_v2=0, nll_loss=7.999, ntokens=916.9, nsentences=28, sample_size=916.9, sample_size_v1=0, sample_size_v2=0, ppl=255.86, wps=1077.7, ups=1.18, wpb=916.9, bsz=28, num_updates=4160, lr=1.71022e-05, gnorm=0.129, clip=0, loss_scale=2, train_wall=8, gb_free=17.4, wall=3563
2023-08-08 17:07:21 - progress_bar.py[line:272] - INFO: epoch 002:   1941 / 2241 loss=8.548, loss_v1=0, loss_v2=0, nll_loss=7.947, ntokens=865.5, nsentences=28, sample_size=865.5, sample_size_v1=0, sample_size_v2=0, ppl=246.76, wps=1027.9, ups=1.19, wpb=865.5, bsz=28, num_updates=4170, lr=1.70666e-05, gnorm=0.124, clip=0, loss_scale=2, train_wall=8, gb_free=17.7, wall=3572
2023-08-08 17:07:29 - progress_bar.py[line:272] - INFO: epoch 002:   1951 / 2241 loss=8.609, loss_v1=0, loss_v2=0, nll_loss=7.993, ntokens=926.1, nsentences=28, sample_size=926.1, sample_size_v1=0, sample_size_v2=0, ppl=254.75, wps=1087.2, ups=1.17, wpb=926.1, bsz=28, num_updates=4180, lr=1.7031e-05, gnorm=0.128, clip=0, loss_scale=2, train_wall=8, gb_free=17.5, wall=3580
2023-08-08 17:07:38 - progress_bar.py[line:272] - INFO: epoch 002:   1961 / 2241 loss=8.541, loss_v1=0, loss_v2=0, nll_loss=7.929, ntokens=827.9, nsentences=28, sample_size=827.9, sample_size_v1=0, sample_size_v2=0, ppl=243.79, wps=983.4, ups=1.19, wpb=827.9, bsz=28, num_updates=4190, lr=1.69954e-05, gnorm=0.124, clip=0, loss_scale=2, train_wall=8, gb_free=17.1, wall=3589
2023-08-08 17:07:46 - progress_bar.py[line:272] - INFO: epoch 002:   1971 / 2241 loss=8.564, loss_v1=0, loss_v2=0, nll_loss=7.953, ntokens=824.8, nsentences=28, sample_size=824.8, sample_size_v1=0, sample_size_v2=0, ppl=247.82, wps=987.1, ups=1.2, wpb=824.8, bsz=28, num_updates=4200, lr=1.69598e-05, gnorm=0.13, clip=0, loss_scale=2, train_wall=8, gb_free=17.6, wall=3597
2023-08-08 17:07:55 - progress_bar.py[line:272] - INFO: epoch 002:   1981 / 2241 loss=8.498, loss_v1=0, loss_v2=0, nll_loss=7.874, ntokens=848.4, nsentences=28, sample_size=848.4, sample_size_v1=0, sample_size_v2=0, ppl=234.62, wps=999.9, ups=1.18, wpb=848.4, bsz=28, num_updates=4210, lr=1.69242e-05, gnorm=0.125, clip=0, loss_scale=2, train_wall=8, gb_free=16.9, wall=3606
2023-08-08 17:08:03 - progress_bar.py[line:272] - INFO: epoch 002:   1991 / 2241 loss=8.597, loss_v1=0, loss_v2=0, nll_loss=7.981, ntokens=859.5, nsentences=28, sample_size=859.5, sample_size_v1=0, sample_size_v2=0, ppl=252.57, wps=1010.7, ups=1.18, wpb=859.5, bsz=28, num_updates=4220, lr=1.68886e-05, gnorm=0.13, clip=0, loss_scale=2, train_wall=8, gb_free=17.2, wall=3614
2023-08-08 17:08:12 - progress_bar.py[line:272] - INFO: epoch 002:   2001 / 2241 loss=8.633, loss_v1=0, loss_v2=0, nll_loss=8.021, ntokens=897.9, nsentences=28, sample_size=897.9, sample_size_v1=0, sample_size_v2=0, ppl=259.73, wps=1048.7, ups=1.17, wpb=897.9, bsz=28, num_updates=4230, lr=1.6853e-05, gnorm=0.133, clip=0, loss_scale=2, train_wall=9, gb_free=17.4, wall=3623
2023-08-08 17:08:20 - progress_bar.py[line:272] - INFO: epoch 002:   2011 / 2241 loss=8.549, loss_v1=0, loss_v2=0, nll_loss=7.941, ntokens=855.1, nsentences=28, sample_size=855.1, sample_size_v1=0, sample_size_v2=0, ppl=245.69, wps=1015.7, ups=1.19, wpb=855.1, bsz=28, num_updates=4240, lr=1.68174e-05, gnorm=0.123, clip=0, loss_scale=2, train_wall=8, gb_free=17.2, wall=3631
2023-08-08 17:08:29 - progress_bar.py[line:272] - INFO: epoch 002:   2021 / 2241 loss=8.583, loss_v1=0, loss_v2=0, nll_loss=7.958, ntokens=891.5, nsentences=28, sample_size=891.5, sample_size_v1=0, sample_size_v2=0, ppl=248.65, wps=1052.4, ups=1.18, wpb=891.5, bsz=28, num_updates=4250, lr=1.67818e-05, gnorm=0.13, clip=0, loss_scale=2, train_wall=8, gb_free=16.4, wall=3640
2023-08-08 17:08:37 - progress_bar.py[line:272] - INFO: epoch 002:   2031 / 2241 loss=8.494, loss_v1=0, loss_v2=0, nll_loss=7.882, ntokens=832.9, nsentences=28, sample_size=832.9, sample_size_v1=0, sample_size_v2=0, ppl=235.82, wps=984.6, ups=1.18, wpb=832.9, bsz=28, num_updates=4260, lr=1.67462e-05, gnorm=0.124, clip=0, loss_scale=2, train_wall=8, gb_free=17.3, wall=3648
2023-08-08 17:08:46 - progress_bar.py[line:272] - INFO: epoch 002:   2041 / 2241 loss=8.598, loss_v1=0, loss_v2=0, nll_loss=7.986, ntokens=810.9, nsentences=28, sample_size=810.9, sample_size_v1=0, sample_size_v2=0, ppl=253.47, wps=946.1, ups=1.17, wpb=810.9, bsz=28, num_updates=4270, lr=1.67106e-05, gnorm=0.133, clip=0, loss_scale=2, train_wall=9, gb_free=17.7, wall=3657
2023-08-08 17:08:54 - progress_bar.py[line:272] - INFO: epoch 002:   2051 / 2241 loss=8.578, loss_v1=0, loss_v2=0, nll_loss=7.97, ntokens=811.8, nsentences=28, sample_size=811.8, sample_size_v1=0, sample_size_v2=0, ppl=250.8, wps=954.7, ups=1.18, wpb=811.8, bsz=28, num_updates=4280, lr=1.6675e-05, gnorm=0.134, clip=0, loss_scale=2, train_wall=8, gb_free=16.5, wall=3665
2023-08-08 17:09:03 - progress_bar.py[line:272] - INFO: epoch 002:   2061 / 2241 loss=8.462, loss_v1=0, loss_v2=0, nll_loss=7.837, ntokens=869.2, nsentences=28, sample_size=869.2, sample_size_v1=0, sample_size_v2=0, ppl=228.69, wps=1023, ups=1.18, wpb=869.2, bsz=28, num_updates=4290, lr=1.66394e-05, gnorm=0.126, clip=0, loss_scale=2, train_wall=8, gb_free=16.6, wall=3674
2023-08-08 17:09:11 - progress_bar.py[line:272] - INFO: epoch 002:   2071 / 2241 loss=8.533, loss_v1=0, loss_v2=0, nll_loss=7.918, ntokens=878.4, nsentences=28, sample_size=878.4, sample_size_v1=0, sample_size_v2=0, ppl=241.9, wps=1044.4, ups=1.19, wpb=878.4, bsz=28, num_updates=4300, lr=1.66038e-05, gnorm=0.124, clip=0, loss_scale=2, train_wall=8, gb_free=17.4, wall=3682
2023-08-08 17:09:19 - progress_bar.py[line:272] - INFO: epoch 002:   2081 / 2241 loss=8.585, loss_v1=0, loss_v2=0, nll_loss=7.968, ntokens=909.9, nsentences=28, sample_size=909.9, sample_size_v1=0, sample_size_v2=0, ppl=250.43, wps=1066.9, ups=1.17, wpb=909.9, bsz=28, num_updates=4310, lr=1.65682e-05, gnorm=0.141, clip=0, loss_scale=2, train_wall=9, gb_free=16.9, wall=3691
2023-08-08 17:09:28 - progress_bar.py[line:272] - INFO: epoch 002:   2091 / 2241 loss=8.651, loss_v1=0, loss_v2=0, nll_loss=8.037, ntokens=917.1, nsentences=28, sample_size=917.1, sample_size_v1=0, sample_size_v2=0, ppl=262.73, wps=1075.3, ups=1.17, wpb=917.1, bsz=28, num_updates=4320, lr=1.65326e-05, gnorm=0.142, clip=0, loss_scale=2, train_wall=9, gb_free=17.4, wall=3699
2023-08-08 17:09:37 - progress_bar.py[line:272] - INFO: epoch 002:   2101 / 2241 loss=8.494, loss_v1=0, loss_v2=0, nll_loss=7.871, ntokens=908.9, nsentences=28, sample_size=908.9, sample_size_v1=0, sample_size_v2=0, ppl=234.18, wps=1070.7, ups=1.18, wpb=908.9, bsz=28, num_updates=4330, lr=1.6497e-05, gnorm=0.126, clip=0, loss_scale=2, train_wall=8, gb_free=17.3, wall=3708
2023-08-08 17:09:45 - progress_bar.py[line:272] - INFO: epoch 002:   2111 / 2241 loss=8.845, loss_v1=0, loss_v2=0, nll_loss=8.23, ntokens=963.5, nsentences=28, sample_size=963.5, sample_size_v1=0, sample_size_v2=0, ppl=300.16, wps=1118.7, ups=1.16, wpb=963.5, bsz=28, num_updates=4340, lr=1.64614e-05, gnorm=0.135, clip=0, loss_scale=2, train_wall=9, gb_free=16.4, wall=3716
2023-08-08 17:09:54 - progress_bar.py[line:272] - INFO: epoch 002:   2121 / 2241 loss=8.481, loss_v1=0, loss_v2=0, nll_loss=7.857, ntokens=872.4, nsentences=28, sample_size=872.4, sample_size_v1=0, sample_size_v2=0, ppl=231.87, wps=1030.7, ups=1.18, wpb=872.4, bsz=28, num_updates=4350, lr=1.64258e-05, gnorm=0.134, clip=0, loss_scale=2, train_wall=8, gb_free=17.1, wall=3725
2023-08-08 17:10:02 - progress_bar.py[line:272] - INFO: epoch 002:   2131 / 2241 loss=8.676, loss_v1=0, loss_v2=0, nll_loss=8.068, ntokens=1114.3, nsentences=28, sample_size=1114.3, sample_size_v1=0, sample_size_v2=0, ppl=268.36, wps=1283.7, ups=1.15, wpb=1114.3, bsz=28, num_updates=4360, lr=1.63902e-05, gnorm=0.124, clip=0, loss_scale=2, train_wall=9, gb_free=17.3, wall=3733
2023-08-08 17:10:11 - progress_bar.py[line:272] - INFO: epoch 002:   2141 / 2241 loss=8.475, loss_v1=0, loss_v2=0, nll_loss=7.86, ntokens=738.1, nsentences=28, sample_size=738.1, sample_size_v1=0, sample_size_v2=0, ppl=232.37, wps=878.6, ups=1.19, wpb=738.1, bsz=28, num_updates=4370, lr=1.63546e-05, gnorm=0.127, clip=0, loss_scale=2, train_wall=8, gb_free=17.3, wall=3742
2023-08-08 17:10:19 - progress_bar.py[line:272] - INFO: epoch 002:   2151 / 2241 loss=8.39, loss_v1=0, loss_v2=0, nll_loss=7.772, ntokens=812.5, nsentences=28, sample_size=812.5, sample_size_v1=0, sample_size_v2=0, ppl=218.51, wps=956, ups=1.18, wpb=812.5, bsz=28, num_updates=4380, lr=1.6319e-05, gnorm=0.122, clip=0, loss_scale=2, train_wall=8, gb_free=17, wall=3750
2023-08-08 17:10:28 - progress_bar.py[line:272] - INFO: epoch 002:   2161 / 2241 loss=8.51, loss_v1=0, loss_v2=0, nll_loss=7.896, ntokens=774.6, nsentences=28, sample_size=774.6, sample_size_v1=0, sample_size_v2=0, ppl=238.11, wps=925.2, ups=1.19, wpb=774.6, bsz=28, num_updates=4390, lr=1.62834e-05, gnorm=0.136, clip=0, loss_scale=2, train_wall=8, gb_free=17.7, wall=3759
2023-08-08 17:10:32 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2023-08-08 17:10:37 - progress_bar.py[line:272] - INFO: epoch 002:   2172 / 2241 loss=8.558, loss_v1=0, loss_v2=0, nll_loss=7.937, ntokens=933.3, nsentences=28, sample_size=933.3, sample_size_v1=0, sample_size_v2=0, ppl=245.11, wps=990.3, ups=1.06, wpb=933.3, bsz=28, num_updates=4400, lr=1.62478e-05, gnorm=0.13, clip=0, loss_scale=1, train_wall=9, gb_free=16.7, wall=3768
2023-08-08 17:10:45 - progress_bar.py[line:272] - INFO: epoch 002:   2182 / 2241 loss=8.518, loss_v1=0, loss_v2=0, nll_loss=7.898, ntokens=901.4, nsentences=28, sample_size=901.4, sample_size_v1=0, sample_size_v2=0, ppl=238.57, wps=1070.1, ups=1.19, wpb=901.4, bsz=28, num_updates=4410, lr=1.62122e-05, gnorm=0.128, clip=0, loss_scale=1, train_wall=8, gb_free=17.3, wall=3777
2023-08-08 17:10:54 - progress_bar.py[line:272] - INFO: epoch 002:   2192 / 2241 loss=8.685, loss_v1=0, loss_v2=0, nll_loss=8.074, ntokens=1040.5, nsentences=28, sample_size=1040.5, sample_size_v1=0, sample_size_v2=0, ppl=269.42, wps=1201.6, ups=1.15, wpb=1040.5, bsz=28, num_updates=4420, lr=1.61766e-05, gnorm=0.13, clip=0, loss_scale=1, train_wall=9, gb_free=17, wall=3785
2023-08-08 17:11:03 - progress_bar.py[line:272] - INFO: epoch 002:   2202 / 2241 loss=8.684, loss_v1=0, loss_v2=0, nll_loss=8.069, ntokens=1034.6, nsentences=28, sample_size=1034.6, sample_size_v1=0, sample_size_v2=0, ppl=268.53, wps=1193.4, ups=1.15, wpb=1034.6, bsz=28, num_updates=4430, lr=1.6141e-05, gnorm=0.134, clip=0, loss_scale=1, train_wall=9, gb_free=17.3, wall=3794
2023-08-08 17:11:11 - progress_bar.py[line:272] - INFO: epoch 002:   2212 / 2241 loss=8.579, loss_v1=0, loss_v2=0, nll_loss=7.974, ntokens=966.7, nsentences=28, sample_size=966.7, sample_size_v1=0, sample_size_v2=0, ppl=251.37, wps=1129, ups=1.17, wpb=966.7, bsz=28, num_updates=4440, lr=1.61054e-05, gnorm=0.122, clip=0, loss_scale=1, train_wall=9, gb_free=16.3, wall=3802
2023-08-08 17:11:20 - progress_bar.py[line:272] - INFO: epoch 002:   2222 / 2241 loss=8.503, loss_v1=0, loss_v2=0, nll_loss=7.875, ntokens=994.2, nsentences=28, sample_size=994.2, sample_size_v1=0, sample_size_v2=0, ppl=234.76, wps=1153.2, ups=1.16, wpb=994.2, bsz=28, num_updates=4450, lr=1.60698e-05, gnorm=0.129, clip=0, loss_scale=1, train_wall=9, gb_free=17, wall=3811
2023-08-08 17:11:28 - progress_bar.py[line:272] - INFO: epoch 002:   2232 / 2241 loss=8.511, loss_v1=0, loss_v2=0, nll_loss=7.892, ntokens=868.1, nsentences=28, sample_size=868.1, sample_size_v1=0, sample_size_v2=0, ppl=237.58, wps=1020.8, ups=1.18, wpb=868.1, bsz=28, num_updates=4460, lr=1.60342e-05, gnorm=0.131, clip=0, loss_scale=1, train_wall=8, gb_free=17.7, wall=3820
2023-08-08 17:11:36 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 2 @ 4469 updates
2023-08-08 17:11:36 - trainer.py[line:431] - INFO: Saving checkpoint to ../../checkpoints/OFA/sgcls_checkpoints/_4_3e-5_512/checkpoint2.pt
2023-08-08 17:11:37 - trainer.py[line:441] - INFO: Finished saving checkpoint to ../../checkpoints/OFA/sgcls_checkpoints/_4_3e-5_512/checkpoint2.pt
2023-08-08 17:11:38 - checkpoint_utils.py[line:133] - INFO: Saved checkpoint ../../checkpoints/OFA/sgcls_checkpoints/_4_3e-5_512/checkpoint2.pt (epoch 2 @ 4469 updates, score None) (writing took 2.0586731290677562 seconds)
2023-08-08 17:11:38 - train.py[line:332] - INFO: end of epoch 2 (average epoch stats below)
2023-08-08 17:11:38 - progress_bar.py[line:282] - INFO: epoch 002 | loss 8.936 | loss_v1 0 | loss_v2 0 | nll_loss 8.368 | ntokens 840.27 | nsentences 27.989 | sample_size 840.27 | sample_size_v1 0 | sample_size_v2 0 | ppl 330.33 | wps 985 | ups 1.17 | wpb 840.3 | bsz 28 | num_updates 4469 | lr 1.60021e-05 | gnorm 0.135 | clip 0 | loss_scale 1 | train_wall 1899 | gb_free 17.4 | wall 3829
2023-08-08 17:11:38 - trainer.py[line:639] - INFO: loading train data for epoch 3
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 row count 62723 total row count 62723
slice_id 0 seek offset 0
2023-08-08 17:11:40 - trainer.py[line:703] - INFO: begin training epoch 3
2023-08-08 17:11:40 - train.py[line:305] - INFO: Start iterating over samples
2023-08-08 17:11:41 - progress_bar.py[line:272] - INFO: epoch 003:      1 / 2241 loss=8.555, loss_v1=0, loss_v2=0, nll_loss=7.937, ntokens=905.6, nsentences=25.9, sample_size=905.6, sample_size_v1=0, sample_size_v2=0, ppl=245.1, wps=745.4, ups=0.82, wpb=905.6, bsz=25.9, num_updates=4470, lr=1.59986e-05, gnorm=0.136, clip=0, loss_scale=1, train_wall=8, gb_free=17, wall=3832
2023-08-08 17:11:49 - progress_bar.py[line:272] - INFO: epoch 003:     11 / 2241 loss=8.545, loss_v1=0, loss_v2=0, nll_loss=7.91, ntokens=899, nsentences=28, sample_size=899, sample_size_v1=0, sample_size_v2=0, ppl=240.44, wps=1048.7, ups=1.17, wpb=899, bsz=28, num_updates=4480, lr=1.5963e-05, gnorm=0.163, clip=0, loss_scale=1, train_wall=9, gb_free=17, wall=3840
2023-08-08 17:11:58 - progress_bar.py[line:272] - INFO: epoch 003:     21 / 2241 loss=8.704, loss_v1=0, loss_v2=0, nll_loss=8.076, ntokens=845.2, nsentences=28, sample_size=845.2, sample_size_v1=0, sample_size_v2=0, ppl=269.85, wps=988.9, ups=1.17, wpb=845.2, bsz=28, num_updates=4490, lr=1.59274e-05, gnorm=0.155, clip=0, loss_scale=1, train_wall=9, gb_free=17.2, wall=3849
2023-08-08 17:12:06 - progress_bar.py[line:272] - INFO: epoch 003:     31 / 2241 loss=8.563, loss_v1=0, loss_v2=0, nll_loss=7.934, ntokens=777.2, nsentences=28, sample_size=777.2, sample_size_v1=0, sample_size_v2=0, ppl=244.56, wps=917.2, ups=1.18, wpb=777.2, bsz=28, num_updates=4500, lr=1.58918e-05, gnorm=0.154, clip=0, loss_scale=1, train_wall=8, gb_free=17.7, wall=3857
2023-08-08 17:12:15 - progress_bar.py[line:272] - INFO: epoch 003:     41 / 2241 loss=8.295, loss_v1=0, loss_v2=0, nll_loss=7.642, ntokens=729.9, nsentences=28, sample_size=729.9, sample_size_v1=0, sample_size_v2=0, ppl=199.71, wps=868.9, ups=1.19, wpb=729.9, bsz=28, num_updates=4510, lr=1.58562e-05, gnorm=0.142, clip=0, loss_scale=1, train_wall=8, gb_free=17.2, wall=3866
2023-08-08 17:12:23 - progress_bar.py[line:272] - INFO: epoch 003:     51 / 2241 loss=8.583, loss_v1=0, loss_v2=0, nll_loss=7.909, ntokens=931.7, nsentences=28, sample_size=931.7, sample_size_v1=0, sample_size_v2=0, ppl=240.32, wps=1076, ups=1.15, wpb=931.7, bsz=28, num_updates=4520, lr=1.58206e-05, gnorm=0.169, clip=0, loss_scale=1, train_wall=9, gb_free=17.2, wall=3874
2023-08-08 17:12:32 - progress_bar.py[line:272] - INFO: epoch 003:     61 / 2241 loss=8.496, loss_v1=0, loss_v2=0, nll_loss=7.835, ntokens=807.8, nsentences=28, sample_size=807.8, sample_size_v1=0, sample_size_v2=0, ppl=228.26, wps=947.6, ups=1.17, wpb=807.8, bsz=28, num_updates=4530, lr=1.5785e-05, gnorm=0.183, clip=0, loss_scale=1, train_wall=9, gb_free=17, wall=3883
2023-08-08 17:12:40 - progress_bar.py[line:272] - INFO: epoch 003:     71 / 2241 loss=8.506, loss_v1=0, loss_v2=0, nll_loss=7.842, ntokens=770.5, nsentences=28, sample_size=770.5, sample_size_v1=0, sample_size_v2=0, ppl=229.37, wps=905.8, ups=1.18, wpb=770.5, bsz=28, num_updates=4540, lr=1.57494e-05, gnorm=0.171, clip=0, loss_scale=1, train_wall=8, gb_free=17.2, wall=3891
2023-08-08 17:12:49 - progress_bar.py[line:272] - INFO: epoch 003:     81 / 2241 loss=8.177, loss_v1=0, loss_v2=0, nll_loss=7.5, ntokens=722.6, nsentences=28, sample_size=722.6, sample_size_v1=0, sample_size_v2=0, ppl=181.03, wps=858.5, ups=1.19, wpb=722.6, bsz=28, num_updates=4550, lr=1.57138e-05, gnorm=0.168, clip=0, loss_scale=1, train_wall=8, gb_free=17.3, wall=3900
2023-08-08 17:12:58 - progress_bar.py[line:272] - INFO: epoch 003:     91 / 2241 loss=8.682, loss_v1=0, loss_v2=0, nll_loss=8.007, ntokens=1093.7, nsentences=28, sample_size=1093.7, sample_size_v1=0, sample_size_v2=0, ppl=257.2, wps=1242.1, ups=1.14, wpb=1093.7, bsz=28, num_updates=4560, lr=1.56782e-05, gnorm=0.182, clip=0, loss_scale=1, train_wall=9, gb_free=16.7, wall=3909
2023-08-08 17:13:06 - progress_bar.py[line:272] - INFO: epoch 003:    101 / 2241 loss=8.807, loss_v1=0, loss_v2=0, nll_loss=8.161, ntokens=1167.6, nsentences=28, sample_size=1167.6, sample_size_v1=0, sample_size_v2=0, ppl=286.14, wps=1319.3, ups=1.13, wpb=1167.6, bsz=28, num_updates=4570, lr=1.56426e-05, gnorm=0.182, clip=0, loss_scale=1, train_wall=9, gb_free=15.8, wall=3918
2023-08-08 17:13:15 - progress_bar.py[line:272] - INFO: epoch 003:    111 / 2241 loss=8.576, loss_v1=0, loss_v2=0, nll_loss=7.927, ntokens=933.6, nsentences=28, sample_size=933.6, sample_size_v1=0, sample_size_v2=0, ppl=243.42, wps=1069.5, ups=1.15, wpb=933.6, bsz=28, num_updates=4580, lr=1.5607e-05, gnorm=0.156, clip=0, loss_scale=1, train_wall=9, gb_free=17.1, wall=3926
2023-08-08 17:13:24 - progress_bar.py[line:272] - INFO: epoch 003:    121 / 2241 loss=8.52, loss_v1=0, loss_v2=0, nll_loss=7.88, ntokens=822.8, nsentences=28, sample_size=822.8, sample_size_v1=0, sample_size_v2=0, ppl=235.64, wps=964.5, ups=1.17, wpb=822.8, bsz=28, num_updates=4590, lr=1.55714e-05, gnorm=0.147, clip=0, loss_scale=1, train_wall=9, gb_free=16.9, wall=3935
2023-08-08 17:13:32 - progress_bar.py[line:272] - INFO: epoch 003:    131 / 2241 loss=8.306, loss_v1=0, loss_v2=0, nll_loss=7.639, ntokens=834.5, nsentences=28, sample_size=834.5, sample_size_v1=0, sample_size_v2=0, ppl=199.38, wps=980.6, ups=1.18, wpb=834.5, bsz=28, num_updates=4600, lr=1.55358e-05, gnorm=0.168, clip=0, loss_scale=1, train_wall=8, gb_free=17.5, wall=3943
2023-08-08 17:13:41 - progress_bar.py[line:272] - INFO: epoch 003:    141 / 2241 loss=8.429, loss_v1=0, loss_v2=0, nll_loss=7.803, ntokens=790, nsentences=28, sample_size=790, sample_size_v1=0, sample_size_v2=0, ppl=223.34, wps=930.8, ups=1.18, wpb=790, bsz=28, num_updates=4610, lr=1.55002e-05, gnorm=0.134, clip=0, loss_scale=1, train_wall=8, gb_free=17.3, wall=3952
2023-08-08 17:13:49 - progress_bar.py[line:272] - INFO: epoch 003:    151 / 2241 loss=8.465, loss_v1=0, loss_v2=0, nll_loss=7.842, ntokens=793.9, nsentences=28, sample_size=793.9, sample_size_v1=0, sample_size_v2=0, ppl=229.44, wps=932.3, ups=1.17, wpb=793.9, bsz=28, num_updates=4620, lr=1.54646e-05, gnorm=0.126, clip=0, loss_scale=1, train_wall=8, gb_free=16.9, wall=3960
2023-08-08 17:13:58 - progress_bar.py[line:272] - INFO: epoch 003:    161 / 2241 loss=8.438, loss_v1=0, loss_v2=0, nll_loss=7.794, ntokens=819.7, nsentences=28, sample_size=819.7, sample_size_v1=0, sample_size_v2=0, ppl=221.89, wps=951.9, ups=1.16, wpb=819.7, bsz=28, num_updates=4630, lr=1.5429e-05, gnorm=0.134, clip=0, loss_scale=1, train_wall=9, gb_free=16.9, wall=3969
2023-08-08 17:14:07 - progress_bar.py[line:272] - INFO: epoch 003:    171 / 2241 loss=8.605, loss_v1=0, loss_v2=0, nll_loss=7.982, ntokens=943.3, nsentences=28, sample_size=943.3, sample_size_v1=0, sample_size_v2=0, ppl=252.91, wps=1076.1, ups=1.14, wpb=943.3, bsz=28, num_updates=4640, lr=1.53934e-05, gnorm=0.134, clip=0, loss_scale=1, train_wall=9, gb_free=15.9, wall=3978
2023-08-08 17:14:15 - progress_bar.py[line:272] - INFO: epoch 003:    181 / 2241 loss=8.466, loss_v1=0, loss_v2=0, nll_loss=7.826, ntokens=921.6, nsentences=28, sample_size=921.6, sample_size_v1=0, sample_size_v2=0, ppl=226.93, wps=1066, ups=1.16, wpb=921.6, bsz=28, num_updates=4650, lr=1.53578e-05, gnorm=0.13, clip=0, loss_scale=1, train_wall=9, gb_free=16.9, wall=3986
2023-08-08 17:14:24 - progress_bar.py[line:272] - INFO: epoch 003:    191 / 2241 loss=8.484, loss_v1=0, loss_v2=0, nll_loss=7.842, ntokens=993.1, nsentences=28, sample_size=993.1, sample_size_v1=0, sample_size_v2=0, ppl=229.43, wps=1145.1, ups=1.15, wpb=993.1, bsz=28, num_updates=4660, lr=1.53222e-05, gnorm=0.131, clip=0, loss_scale=1, train_wall=9, gb_free=16, wall=3995
2023-08-08 17:14:33 - progress_bar.py[line:272] - INFO: epoch 003:    201 / 2241 loss=8.495, loss_v1=0, loss_v2=0, nll_loss=7.853, ntokens=920.4, nsentences=28, sample_size=920.4, sample_size_v1=0, sample_size_v2=0, ppl=231.27, wps=1058.7, ups=1.15, wpb=920.4, bsz=28, num_updates=4670, lr=1.52866e-05, gnorm=0.134, clip=0, loss_scale=1, train_wall=9, gb_free=17.1, wall=4004
2023-08-08 17:14:41 - progress_bar.py[line:272] - INFO: epoch 003:    211 / 2241 loss=8.498, loss_v1=0, loss_v2=0, nll_loss=7.868, ntokens=878.1, nsentences=28, sample_size=878.1, sample_size_v1=0, sample_size_v2=0, ppl=233.69, wps=1012.2, ups=1.15, wpb=878.1, bsz=28, num_updates=4680, lr=1.5251e-05, gnorm=0.128, clip=0, loss_scale=1, train_wall=9, gb_free=16.6, wall=4012
2023-08-08 17:14:50 - progress_bar.py[line:272] - INFO: epoch 003:    221 / 2241 loss=8.182, loss_v1=0, loss_v2=0, nll_loss=7.527, ntokens=779.7, nsentences=28, sample_size=779.7, sample_size_v1=0, sample_size_v2=0, ppl=184.43, wps=914, ups=1.17, wpb=779.7, bsz=28, num_updates=4690, lr=1.52154e-05, gnorm=0.13, clip=0, loss_scale=1, train_wall=9, gb_free=17.4, wall=4021
2023-08-08 17:14:58 - progress_bar.py[line:272] - INFO: epoch 003:    231 / 2241 loss=8.356, loss_v1=0, loss_v2=0, nll_loss=7.706, ntokens=740.4, nsentences=28, sample_size=740.4, sample_size_v1=0, sample_size_v2=0, ppl=208.86, wps=865, ups=1.17, wpb=740.4, bsz=28, num_updates=4700, lr=1.51798e-05, gnorm=0.136, clip=0, loss_scale=1, train_wall=9, gb_free=17.5, wall=4030
2023-08-08 17:15:07 - progress_bar.py[line:272] - INFO: epoch 003:    241 / 2241 loss=8.236, loss_v1=0, loss_v2=0, nll_loss=7.576, ntokens=741.8, nsentences=28, sample_size=741.8, sample_size_v1=0, sample_size_v2=0, ppl=190.85, wps=870, ups=1.17, wpb=741.8, bsz=28, num_updates=4710, lr=1.51442e-05, gnorm=0.139, clip=0, loss_scale=1, train_wall=9, gb_free=16.9, wall=4038
2023-08-08 17:15:16 - progress_bar.py[line:272] - INFO: epoch 003:    251 / 2241 loss=8.46, loss_v1=0, loss_v2=0, nll_loss=7.822, ntokens=879, nsentences=28, sample_size=879, sample_size_v1=0, sample_size_v2=0, ppl=226.28, wps=1020.3, ups=1.16, wpb=879, bsz=28, num_updates=4720, lr=1.51086e-05, gnorm=0.136, clip=0, loss_scale=1, train_wall=9, gb_free=17.2, wall=4047
2023-08-08 17:15:24 - progress_bar.py[line:272] - INFO: epoch 003:    261 / 2241 loss=8.268, loss_v1=0, loss_v2=0, nll_loss=7.622, ntokens=840.3, nsentences=28, sample_size=840.3, sample_size_v1=0, sample_size_v2=0, ppl=197.05, wps=974.3, ups=1.16, wpb=840.3, bsz=28, num_updates=4730, lr=1.5073e-05, gnorm=0.132, clip=0, loss_scale=1, train_wall=9, gb_free=16.9, wall=4055
2023-08-08 17:15:33 - progress_bar.py[line:272] - INFO: epoch 003:    271 / 2241 loss=8.465, loss_v1=0, loss_v2=0, nll_loss=7.828, ntokens=900.2, nsentences=28, sample_size=900.2, sample_size_v1=0, sample_size_v2=0, ppl=227.29, wps=1043.2, ups=1.16, wpb=900.2, bsz=28, num_updates=4740, lr=1.50374e-05, gnorm=0.138, clip=0, loss_scale=1, train_wall=9, gb_free=17.3, wall=4064
2023-08-08 17:15:41 - progress_bar.py[line:272] - INFO: epoch 003:    281 / 2241 loss=8.584, loss_v1=0, loss_v2=0, nll_loss=7.975, ntokens=777.2, nsentences=28, sample_size=777.2, sample_size_v1=0, sample_size_v2=0, ppl=251.62, wps=921.7, ups=1.19, wpb=777.2, bsz=28, num_updates=4750, lr=1.50018e-05, gnorm=0.133, clip=0, loss_scale=1, train_wall=8, gb_free=17.3, wall=4072
2023-08-08 17:15:50 - progress_bar.py[line:272] - INFO: epoch 003:    291 / 2241 loss=8.701, loss_v1=0, loss_v2=0, nll_loss=8.093, ntokens=873.9, nsentences=28, sample_size=873.9, sample_size_v1=0, sample_size_v2=0, ppl=273.04, wps=1023.6, ups=1.17, wpb=873.9, bsz=28, num_updates=4760, lr=1.49662e-05, gnorm=0.132, clip=0, loss_scale=1, train_wall=9, gb_free=16.8, wall=4081
2023-08-08 17:15:58 - progress_bar.py[line:272] - INFO: epoch 003:    301 / 2241 loss=8.668, loss_v1=0, loss_v2=0, nll_loss=8.058, ntokens=912.8, nsentences=28, sample_size=912.8, sample_size_v1=0, sample_size_v2=0, ppl=266.45, wps=1076.4, ups=1.18, wpb=912.8, bsz=28, num_updates=4770, lr=1.49306e-05, gnorm=0.13, clip=0, loss_scale=1, train_wall=8, gb_free=16.4, wall=4089
2023-08-08 17:16:07 - progress_bar.py[line:272] - INFO: epoch 003:    311 / 2241 loss=8.684, loss_v1=0, loss_v2=0, nll_loss=8.067, ntokens=908, nsentences=28, sample_size=908, sample_size_v1=0, sample_size_v2=0, ppl=268.25, wps=1067.7, ups=1.18, wpb=908, bsz=28, num_updates=4780, lr=1.4895e-05, gnorm=0.132, clip=0, loss_scale=1, train_wall=8, gb_free=17.6, wall=4098
2023-08-08 17:16:15 - progress_bar.py[line:272] - INFO: epoch 003:    321 / 2241 loss=8.706, loss_v1=0, loss_v2=0, nll_loss=8.112, ntokens=897.5, nsentences=28, sample_size=897.5, sample_size_v1=0, sample_size_v2=0, ppl=276.64, wps=1049.7, ups=1.17, wpb=897.5, bsz=28, num_updates=4790, lr=1.48594e-05, gnorm=0.128, clip=0, loss_scale=1, train_wall=9, gb_free=16.6, wall=4106
2023-08-08 17:16:24 - progress_bar.py[line:272] - INFO: epoch 003:    331 / 2241 loss=8.763, loss_v1=0, loss_v2=0, nll_loss=8.173, ntokens=981.8, nsentences=28, sample_size=981.8, sample_size_v1=0, sample_size_v2=0, ppl=288.61, wps=1141.9, ups=1.16, wpb=981.8, bsz=28, num_updates=4800, lr=1.48238e-05, gnorm=0.126, clip=0, loss_scale=1, train_wall=9, gb_free=17.1, wall=4115
2023-08-08 17:16:32 - progress_bar.py[line:272] - INFO: epoch 003:    341 / 2241 loss=8.732, loss_v1=0, loss_v2=0, nll_loss=8.134, ntokens=922, nsentences=28, sample_size=922, sample_size_v1=0, sample_size_v2=0, ppl=280.9, wps=1086.1, ups=1.18, wpb=922, bsz=28, num_updates=4810, lr=1.47882e-05, gnorm=0.133, clip=0, loss_scale=1, train_wall=8, gb_free=16.9, wall=4124
2023-08-08 17:16:41 - progress_bar.py[line:272] - INFO: epoch 003:    351 / 2241 loss=8.73, loss_v1=0, loss_v2=0, nll_loss=8.117, ntokens=919.8, nsentences=28, sample_size=919.8, sample_size_v1=0, sample_size_v2=0, ppl=277.67, wps=1081.3, ups=1.18, wpb=919.8, bsz=28, num_updates=4820, lr=1.47526e-05, gnorm=0.134, clip=0, loss_scale=1, train_wall=8, gb_free=17.2, wall=4132
2023-08-08 17:16:49 - progress_bar.py[line:272] - INFO: epoch 003:    361 / 2241 loss=8.743, loss_v1=0, loss_v2=0, nll_loss=8.142, ntokens=964.8, nsentences=28, sample_size=964.8, sample_size_v1=0, sample_size_v2=0, ppl=282.43, wps=1125.4, ups=1.17, wpb=964.8, bsz=28, num_updates=4830, lr=1.4717e-05, gnorm=0.129, clip=0, loss_scale=1, train_wall=9, gb_free=17.3, wall=4141
2023-08-08 17:16:58 - progress_bar.py[line:272] - INFO: epoch 003:    371 / 2241 loss=8.711, loss_v1=0, loss_v2=0, nll_loss=8.1, ntokens=946.3, nsentences=28, sample_size=946.3, sample_size_v1=0, sample_size_v2=0, ppl=274.3, wps=1103.9, ups=1.17, wpb=946.3, bsz=28, num_updates=4840, lr=1.46814e-05, gnorm=0.137, clip=0, loss_scale=1, train_wall=9, gb_free=17.3, wall=4149
2023-08-08 17:17:07 - progress_bar.py[line:272] - INFO: epoch 003:    381 / 2241 loss=8.688, loss_v1=0, loss_v2=0, nll_loss=8.084, ntokens=936.9, nsentences=28, sample_size=936.9, sample_size_v1=0, sample_size_v2=0, ppl=271.36, wps=1086.4, ups=1.16, wpb=936.9, bsz=28, num_updates=4850, lr=1.46458e-05, gnorm=0.131, clip=0, loss_scale=1, train_wall=9, gb_free=16.9, wall=4158
2023-08-08 17:17:15 - progress_bar.py[line:272] - INFO: epoch 003:    391 / 2241 loss=8.759, loss_v1=0, loss_v2=0, nll_loss=8.142, ntokens=906.9, nsentences=28, sample_size=906.9, sample_size_v1=0, sample_size_v2=0, ppl=282.56, wps=1060.1, ups=1.17, wpb=906.9, bsz=28, num_updates=4860, lr=1.46102e-05, gnorm=0.137, clip=0, loss_scale=1, train_wall=9, gb_free=17, wall=4166
2023-08-08 17:17:24 - progress_bar.py[line:272] - INFO: epoch 003:    401 / 2241 loss=8.62, loss_v1=0, loss_v2=0, nll_loss=8.009, ntokens=955.3, nsentences=28, sample_size=955.3, sample_size_v1=0, sample_size_v2=0, ppl=257.6, wps=1109.8, ups=1.16, wpb=955.3, bsz=28, num_updates=4870, lr=1.45746e-05, gnorm=0.124, clip=0, loss_scale=1, train_wall=9, gb_free=17.2, wall=4175
2023-08-08 17:17:32 - progress_bar.py[line:272] - INFO: epoch 003:    411 / 2241 loss=8.569, loss_v1=0, loss_v2=0, nll_loss=7.946, ntokens=866.2, nsentences=28, sample_size=866.2, sample_size_v1=0, sample_size_v2=0, ppl=246.6, wps=1023.4, ups=1.18, wpb=866.2, bsz=28, num_updates=4880, lr=1.4539e-05, gnorm=0.134, clip=0, loss_scale=1, train_wall=8, gb_free=17.4, wall=4183
2023-08-08 17:17:41 - progress_bar.py[line:272] - INFO: epoch 003:    421 / 2241 loss=8.626, loss_v1=0, loss_v2=0, nll_loss=8.014, ntokens=844.6, nsentences=28, sample_size=844.6, sample_size_v1=0, sample_size_v2=0, ppl=258.53, wps=999.7, ups=1.18, wpb=844.6, bsz=28, num_updates=4890, lr=1.45034e-05, gnorm=0.128, clip=0, loss_scale=1, train_wall=8, gb_free=17.4, wall=4192
2023-08-08 17:17:49 - progress_bar.py[line:272] - INFO: epoch 003:    431 / 2241 loss=8.548, loss_v1=0, loss_v2=0, nll_loss=7.94, ntokens=808.7, nsentences=28, sample_size=808.7, sample_size_v1=0, sample_size_v2=0, ppl=245.52, wps=961.2, ups=1.19, wpb=808.7, bsz=28, num_updates=4900, lr=1.44678e-05, gnorm=0.128, clip=0, loss_scale=1, train_wall=8, gb_free=17.6, wall=4200
2023-08-08 17:17:58 - progress_bar.py[line:272] - INFO: epoch 003:    441 / 2241 loss=8.579, loss_v1=0, loss_v2=0, nll_loss=7.969, ntokens=830.7, nsentences=28, sample_size=830.7, sample_size_v1=0, sample_size_v2=0, ppl=250.63, wps=986, ups=1.19, wpb=830.7, bsz=28, num_updates=4910, lr=1.44322e-05, gnorm=0.125, clip=0, loss_scale=2, train_wall=8, gb_free=17.1, wall=4209
2023-08-08 17:18:06 - progress_bar.py[line:272] - INFO: epoch 003:    451 / 2241 loss=8.413, loss_v1=0, loss_v2=0, nll_loss=7.782, ntokens=741.5, nsentences=28, sample_size=741.5, sample_size_v1=0, sample_size_v2=0, ppl=220.14, wps=883.3, ups=1.19, wpb=741.5, bsz=28, num_updates=4920, lr=1.43966e-05, gnorm=0.127, clip=0, loss_scale=2, train_wall=8, gb_free=17.5, wall=4217
2023-08-08 17:18:14 - progress_bar.py[line:272] - INFO: epoch 003:    461 / 2241 loss=8.452, loss_v1=0, loss_v2=0, nll_loss=7.827, ntokens=743.7, nsentences=28, sample_size=743.7, sample_size_v1=0, sample_size_v2=0, ppl=227.01, wps=888, ups=1.19, wpb=743.7, bsz=28, num_updates=4930, lr=1.4361e-05, gnorm=0.132, clip=0, loss_scale=2, train_wall=8, gb_free=17.7, wall=4226
2023-08-08 17:18:23 - progress_bar.py[line:272] - INFO: epoch 003:    471 / 2241 loss=8.504, loss_v1=0, loss_v2=0, nll_loss=7.894, ntokens=774.7, nsentences=28, sample_size=774.7, sample_size_v1=0, sample_size_v2=0, ppl=237.81, wps=921.9, ups=1.19, wpb=774.7, bsz=28, num_updates=4940, lr=1.43254e-05, gnorm=0.124, clip=0, loss_scale=2, train_wall=8, gb_free=17.5, wall=4234
2023-08-08 17:18:31 - progress_bar.py[line:272] - INFO: epoch 003:    481 / 2241 loss=8.463, loss_v1=0, loss_v2=0, nll_loss=7.846, ntokens=731.5, nsentences=28, sample_size=731.5, sample_size_v1=0, sample_size_v2=0, ppl=230.05, wps=876, ups=1.2, wpb=731.5, bsz=28, num_updates=4950, lr=1.42898e-05, gnorm=0.129, clip=0, loss_scale=2, train_wall=8, gb_free=17.5, wall=4242
2023-08-08 17:18:40 - progress_bar.py[line:272] - INFO: epoch 003:    491 / 2241 loss=8.605, loss_v1=0, loss_v2=0, nll_loss=7.984, ntokens=791.3, nsentences=28, sample_size=791.3, sample_size_v1=0, sample_size_v2=0, ppl=253.23, wps=938.9, ups=1.19, wpb=791.3, bsz=28, num_updates=4960, lr=1.42542e-05, gnorm=0.131, clip=0, loss_scale=2, train_wall=8, gb_free=16.9, wall=4251
2023-08-08 17:18:48 - progress_bar.py[line:272] - INFO: epoch 003:    501 / 2241 loss=8.576, loss_v1=0, loss_v2=0, nll_loss=7.972, ntokens=889.1, nsentences=28, sample_size=889.1, sample_size_v1=0, sample_size_v2=0, ppl=251.12, wps=1059.3, ups=1.19, wpb=889.1, bsz=28, num_updates=4970, lr=1.42186e-05, gnorm=0.125, clip=0, loss_scale=2, train_wall=8, gb_free=16.3, wall=4259
2023-08-08 17:18:56 - progress_bar.py[line:272] - INFO: epoch 003:    511 / 2241 loss=8.599, loss_v1=0, loss_v2=0, nll_loss=7.99, ntokens=857.9, nsentences=28, sample_size=857.9, sample_size_v1=0, sample_size_v2=0, ppl=254.16, wps=1017.4, ups=1.19, wpb=857.9, bsz=28, num_updates=4980, lr=1.4183e-05, gnorm=0.133, clip=0, loss_scale=2, train_wall=8, gb_free=17.7, wall=4268
2023-08-08 17:19:05 - progress_bar.py[line:272] - INFO: epoch 003:    521 / 2241 loss=8.535, loss_v1=0, loss_v2=0, nll_loss=7.933, ntokens=712.9, nsentences=28, sample_size=712.9, sample_size_v1=0, sample_size_v2=0, ppl=244.4, wps=854.2, ups=1.2, wpb=712.9, bsz=28, num_updates=4990, lr=1.41474e-05, gnorm=0.13, clip=0, loss_scale=2, train_wall=8, gb_free=17, wall=4276
2023-08-08 17:19:13 - progress_bar.py[line:272] - INFO: epoch 003:    531 / 2241 loss=8.593, loss_v1=0, loss_v2=0, nll_loss=7.979, ntokens=898.9, nsentences=28, sample_size=898.9, sample_size_v1=0, sample_size_v2=0, ppl=252.36, wps=1057.9, ups=1.18, wpb=898.9, bsz=28, num_updates=5000, lr=1.41118e-05, gnorm=0.129, clip=0, loss_scale=2, train_wall=8, gb_free=17.1, wall=4284
2023-08-08 17:19:22 - progress_bar.py[line:272] - INFO: epoch 003:    541 / 2241 loss=8.56, loss_v1=0, loss_v2=0, nll_loss=7.954, ntokens=874.2, nsentences=28, sample_size=874.2, sample_size_v1=0, sample_size_v2=0, ppl=248.02, wps=1030.6, ups=1.18, wpb=874.2, bsz=28, num_updates=5010, lr=1.40762e-05, gnorm=0.123, clip=0, loss_scale=2, train_wall=8, gb_free=16.8, wall=4293
2023-08-08 17:19:30 - progress_bar.py[line:272] - INFO: epoch 003:    551 / 2241 loss=8.564, loss_v1=0, loss_v2=0, nll_loss=7.952, ntokens=800.4, nsentences=28, sample_size=800.4, sample_size_v1=0, sample_size_v2=0, ppl=247.62, wps=943.2, ups=1.18, wpb=800.4, bsz=28, num_updates=5020, lr=1.40406e-05, gnorm=0.13, clip=0, loss_scale=2, train_wall=8, gb_free=17.1, wall=4301
2023-08-08 17:19:39 - progress_bar.py[line:272] - INFO: epoch 003:    561 / 2241 loss=8.593, loss_v1=0, loss_v2=0, nll_loss=7.987, ntokens=804.5, nsentences=28, sample_size=804.5, sample_size_v1=0, sample_size_v2=0, ppl=253.76, wps=952.4, ups=1.18, wpb=804.5, bsz=28, num_updates=5030, lr=1.4005e-05, gnorm=0.141, clip=0, loss_scale=2, train_wall=8, gb_free=16.9, wall=4310
2023-08-08 17:19:47 - progress_bar.py[line:272] - INFO: epoch 003:    571 / 2241 loss=8.536, loss_v1=0, loss_v2=0, nll_loss=7.923, ntokens=830.9, nsentences=28, sample_size=830.9, sample_size_v1=0, sample_size_v2=0, ppl=242.63, wps=988, ups=1.19, wpb=830.9, bsz=28, num_updates=5040, lr=1.39694e-05, gnorm=0.123, clip=0, loss_scale=2, train_wall=8, gb_free=17.4, wall=4318
2023-08-08 17:19:56 - progress_bar.py[line:272] - INFO: epoch 003:    581 / 2241 loss=8.56, loss_v1=0, loss_v2=0, nll_loss=7.956, ntokens=780.7, nsentences=28, sample_size=780.7, sample_size_v1=0, sample_size_v2=0, ppl=248.38, wps=925.2, ups=1.19, wpb=780.7, bsz=28, num_updates=5050, lr=1.39338e-05, gnorm=0.125, clip=0, loss_scale=2, train_wall=8, gb_free=17.8, wall=4327
2023-08-08 17:20:04 - progress_bar.py[line:272] - INFO: epoch 003:    591 / 2241 loss=8.424, loss_v1=0, loss_v2=0, nll_loss=7.805, ntokens=753.2, nsentences=28, sample_size=753.2, sample_size_v1=0, sample_size_v2=0, ppl=223.7, wps=899.9, ups=1.19, wpb=753.2, bsz=28, num_updates=5060, lr=1.38982e-05, gnorm=0.126, clip=0, loss_scale=2, train_wall=8, gb_free=17.7, wall=4335
2023-08-08 17:20:12 - progress_bar.py[line:272] - INFO: epoch 003:    601 / 2241 loss=8.61, loss_v1=0, loss_v2=0, nll_loss=8.001, ntokens=853.6, nsentences=28, sample_size=853.6, sample_size_v1=0, sample_size_v2=0, ppl=256.21, wps=1011.9, ups=1.19, wpb=853.6, bsz=28, num_updates=5070, lr=1.38626e-05, gnorm=0.128, clip=0, loss_scale=2, train_wall=8, gb_free=17.1, wall=4344
2023-08-08 17:20:21 - progress_bar.py[line:272] - INFO: epoch 003:    611 / 2241 loss=8.603, loss_v1=0, loss_v2=0, nll_loss=7.99, ntokens=867.8, nsentences=28, sample_size=867.8, sample_size_v1=0, sample_size_v2=0, ppl=254.31, wps=1026.8, ups=1.18, wpb=867.8, bsz=28, num_updates=5080, lr=1.3827e-05, gnorm=0.127, clip=0, loss_scale=2, train_wall=8, gb_free=16.9, wall=4352
2023-08-08 17:20:29 - progress_bar.py[line:272] - INFO: epoch 003:    621 / 2241 loss=8.63, loss_v1=0, loss_v2=0, nll_loss=8.027, ntokens=852.3, nsentences=28, sample_size=852.3, sample_size_v1=0, sample_size_v2=0, ppl=260.85, wps=1012, ups=1.19, wpb=852.3, bsz=28, num_updates=5090, lr=1.37914e-05, gnorm=0.124, clip=0, loss_scale=2, train_wall=8, gb_free=17.4, wall=4360
2023-08-08 17:20:38 - progress_bar.py[line:272] - INFO: epoch 003:    631 / 2241 loss=8.483, loss_v1=0, loss_v2=0, nll_loss=7.865, ntokens=805.3, nsentences=28, sample_size=805.3, sample_size_v1=0, sample_size_v2=0, ppl=233.14, wps=962.3, ups=1.19, wpb=805.3, bsz=28, num_updates=5100, lr=1.37558e-05, gnorm=0.127, clip=0, loss_scale=2, train_wall=8, gb_free=17.5, wall=4369
2023-08-08 17:20:46 - progress_bar.py[line:272] - INFO: epoch 003:    641 / 2241 loss=8.526, loss_v1=0, loss_v2=0, nll_loss=7.915, ntokens=755.2, nsentences=28, sample_size=755.2, sample_size_v1=0, sample_size_v2=0, ppl=241.36, wps=899.6, ups=1.19, wpb=755.2, bsz=28, num_updates=5110, lr=1.37202e-05, gnorm=0.13, clip=0, loss_scale=2, train_wall=8, gb_free=16.4, wall=4377
2023-08-08 17:20:54 - progress_bar.py[line:272] - INFO: epoch 003:    651 / 2241 loss=8.52, loss_v1=0, loss_v2=0, nll_loss=7.905, ntokens=722, nsentences=28, sample_size=722, sample_size_v1=0, sample_size_v2=0, ppl=239.69, wps=858.7, ups=1.19, wpb=722, bsz=28, num_updates=5120, lr=1.36846e-05, gnorm=0.129, clip=0, loss_scale=2, train_wall=8, gb_free=17.8, wall=4386
2023-08-08 17:21:03 - progress_bar.py[line:272] - INFO: epoch 003:    661 / 2241 loss=8.534, loss_v1=0, loss_v2=0, nll_loss=7.918, ntokens=785.5, nsentences=28, sample_size=785.5, sample_size_v1=0, sample_size_v2=0, ppl=241.85, wps=938, ups=1.19, wpb=785.5, bsz=28, num_updates=5130, lr=1.3649e-05, gnorm=0.124, clip=0, loss_scale=2, train_wall=8, gb_free=17.5, wall=4394
2023-08-08 17:21:11 - progress_bar.py[line:272] - INFO: epoch 003:    671 / 2241 loss=8.626, loss_v1=0, loss_v2=0, nll_loss=8.015, ntokens=885.4, nsentences=28, sample_size=885.4, sample_size_v1=0, sample_size_v2=0, ppl=258.71, wps=1045, ups=1.18, wpb=885.4, bsz=28, num_updates=5140, lr=1.36134e-05, gnorm=0.131, clip=0, loss_scale=2, train_wall=8, gb_free=17.6, wall=4402
2023-08-08 17:21:20 - progress_bar.py[line:272] - INFO: epoch 003:    681 / 2241 loss=8.597, loss_v1=0, loss_v2=0, nll_loss=7.991, ntokens=797.4, nsentences=28, sample_size=797.4, sample_size_v1=0, sample_size_v2=0, ppl=254.36, wps=946.9, ups=1.19, wpb=797.4, bsz=28, num_updates=5150, lr=1.35778e-05, gnorm=0.13, clip=0, loss_scale=2, train_wall=8, gb_free=17.4, wall=4411
2023-08-08 17:21:28 - progress_bar.py[line:272] - INFO: epoch 003:    691 / 2241 loss=8.448, loss_v1=0, loss_v2=0, nll_loss=7.833, ntokens=761.7, nsentences=28, sample_size=761.7, sample_size_v1=0, sample_size_v2=0, ppl=228.03, wps=909.5, ups=1.19, wpb=761.7, bsz=28, num_updates=5160, lr=1.35422e-05, gnorm=0.127, clip=0, loss_scale=2, train_wall=8, gb_free=17.3, wall=4419
2023-08-08 17:21:36 - progress_bar.py[line:272] - INFO: epoch 003:    701 / 2241 loss=8.456, loss_v1=0, loss_v2=0, nll_loss=7.839, ntokens=804.3, nsentences=28, sample_size=804.3, sample_size_v1=0, sample_size_v2=0, ppl=228.95, wps=954.7, ups=1.19, wpb=804.3, bsz=28, num_updates=5170, lr=1.35066e-05, gnorm=0.125, clip=0, loss_scale=2, train_wall=8, gb_free=17.6, wall=4428
2023-08-08 17:21:45 - progress_bar.py[line:272] - INFO: epoch 003:    711 / 2241 loss=8.581, loss_v1=0, loss_v2=0, nll_loss=7.975, ntokens=807.1, nsentences=28, sample_size=807.1, sample_size_v1=0, sample_size_v2=0, ppl=251.56, wps=958.3, ups=1.19, wpb=807.1, bsz=28, num_updates=5180, lr=1.3471e-05, gnorm=0.124, clip=0, loss_scale=2, train_wall=8, gb_free=17, wall=4436
2023-08-08 17:21:53 - progress_bar.py[line:272] - INFO: epoch 003:    721 / 2241 loss=8.652, loss_v1=0, loss_v2=0, nll_loss=8.042, ntokens=831.4, nsentences=28, sample_size=831.4, sample_size_v1=0, sample_size_v2=0, ppl=263.65, wps=981, ups=1.18, wpb=831.4, bsz=28, num_updates=5190, lr=1.34354e-05, gnorm=0.131, clip=0, loss_scale=2, train_wall=8, gb_free=17.7, wall=4445
2023-08-08 17:22:02 - progress_bar.py[line:272] - INFO: epoch 003:    731 / 2241 loss=8.514, loss_v1=0, loss_v2=0, nll_loss=7.898, ntokens=845.3, nsentences=28, sample_size=845.3, sample_size_v1=0, sample_size_v2=0, ppl=238.45, wps=994.6, ups=1.18, wpb=845.3, bsz=28, num_updates=5200, lr=1.33998e-05, gnorm=0.125, clip=0, loss_scale=2, train_wall=8, gb_free=17.1, wall=4453
2023-08-08 17:22:10 - progress_bar.py[line:272] - INFO: epoch 003:    741 / 2241 loss=8.59, loss_v1=0, loss_v2=0, nll_loss=7.981, ntokens=820.1, nsentences=28, sample_size=820.1, sample_size_v1=0, sample_size_v2=0, ppl=252.64, wps=972.5, ups=1.19, wpb=820.1, bsz=28, num_updates=5210, lr=1.33642e-05, gnorm=0.127, clip=0, loss_scale=2, train_wall=8, gb_free=17.2, wall=4461
2023-08-08 17:22:19 - progress_bar.py[line:272] - INFO: epoch 003:    751 / 2241 loss=8.575, loss_v1=0, loss_v2=0, nll_loss=7.96, ntokens=805.6, nsentences=28, sample_size=805.6, sample_size_v1=0, sample_size_v2=0, ppl=248.97, wps=950, ups=1.18, wpb=805.6, bsz=28, num_updates=5220, lr=1.33286e-05, gnorm=0.13, clip=0, loss_scale=2, train_wall=8, gb_free=17.8, wall=4470
2023-08-08 17:22:27 - progress_bar.py[line:272] - INFO: epoch 003:    761 / 2241 loss=8.587, loss_v1=0, loss_v2=0, nll_loss=7.972, ntokens=807.7, nsentences=28, sample_size=807.7, sample_size_v1=0, sample_size_v2=0, ppl=251.13, wps=955, ups=1.18, wpb=807.7, bsz=28, num_updates=5230, lr=1.3293e-05, gnorm=0.127, clip=0, loss_scale=2, train_wall=8, gb_free=16.7, wall=4478
2023-08-08 17:22:36 - progress_bar.py[line:272] - INFO: epoch 003:    771 / 2241 loss=8.658, loss_v1=0, loss_v2=0, nll_loss=8.045, ntokens=748.2, nsentences=28, sample_size=748.2, sample_size_v1=0, sample_size_v2=0, ppl=264.03, wps=886.2, ups=1.18, wpb=748.2, bsz=28, num_updates=5240, lr=1.32574e-05, gnorm=0.129, clip=0, loss_scale=2, train_wall=8, gb_free=17.2, wall=4487
2023-08-08 17:22:44 - progress_bar.py[line:272] - INFO: epoch 003:    781 / 2241 loss=8.327, loss_v1=0, loss_v2=0, nll_loss=7.7, ntokens=710.9, nsentences=28, sample_size=710.9, sample_size_v1=0, sample_size_v2=0, ppl=207.99, wps=849.7, ups=1.2, wpb=710.9, bsz=28, num_updates=5250, lr=1.32218e-05, gnorm=0.13, clip=0, loss_scale=2, train_wall=8, gb_free=17.3, wall=4495
2023-08-08 17:22:52 - progress_bar.py[line:272] - INFO: epoch 003:    791 / 2241 loss=8.455, loss_v1=0, loss_v2=0, nll_loss=7.834, ntokens=705.2, nsentences=28, sample_size=705.2, sample_size_v1=0, sample_size_v2=0, ppl=228.1, wps=846.5, ups=1.2, wpb=705.2, bsz=28, num_updates=5260, lr=1.31862e-05, gnorm=0.131, clip=0, loss_scale=2, train_wall=8, gb_free=16.8, wall=4504
2023-08-08 17:23:01 - progress_bar.py[line:272] - INFO: epoch 003:    801 / 2241 loss=8.54, loss_v1=0, loss_v2=0, nll_loss=7.91, ntokens=698, nsentences=28, sample_size=698, sample_size_v1=0, sample_size_v2=0, ppl=240.59, wps=834.3, ups=1.2, wpb=698, bsz=28, num_updates=5270, lr=1.31506e-05, gnorm=0.147, clip=0, loss_scale=2, train_wall=8, gb_free=17.2, wall=4512
2023-08-08 17:23:09 - progress_bar.py[line:272] - INFO: epoch 003:    811 / 2241 loss=8.474, loss_v1=0, loss_v2=0, nll_loss=7.857, ntokens=668, nsentences=28, sample_size=668, sample_size_v1=0, sample_size_v2=0, ppl=231.9, wps=805.8, ups=1.21, wpb=668, bsz=28, num_updates=5280, lr=1.3115e-05, gnorm=0.134, clip=0, loss_scale=2, train_wall=8, gb_free=17.2, wall=4520
2023-08-08 17:23:17 - progress_bar.py[line:272] - INFO: epoch 003:    821 / 2241 loss=8.424, loss_v1=0, loss_v2=0, nll_loss=7.799, ntokens=739.7, nsentences=28, sample_size=739.7, sample_size_v1=0, sample_size_v2=0, ppl=222.68, wps=882.8, ups=1.19, wpb=739.7, bsz=28, num_updates=5290, lr=1.30794e-05, gnorm=0.126, clip=0, loss_scale=2, train_wall=8, gb_free=17.1, wall=4529
2023-08-08 17:23:26 - progress_bar.py[line:272] - INFO: epoch 003:    831 / 2241 loss=8.402, loss_v1=0, loss_v2=0, nll_loss=7.778, ntokens=732.1, nsentences=28, sample_size=732.1, sample_size_v1=0, sample_size_v2=0, ppl=219.46, wps=874.3, ups=1.19, wpb=732.1, bsz=28, num_updates=5300, lr=1.30438e-05, gnorm=0.125, clip=0, loss_scale=2, train_wall=8, gb_free=17.3, wall=4537
2023-08-08 17:23:34 - progress_bar.py[line:272] - INFO: epoch 003:    841 / 2241 loss=8.636, loss_v1=0, loss_v2=0, nll_loss=8.013, ntokens=782.6, nsentences=28, sample_size=782.6, sample_size_v1=0, sample_size_v2=0, ppl=258.27, wps=925, ups=1.18, wpb=782.6, bsz=28, num_updates=5310, lr=1.30082e-05, gnorm=0.143, clip=0, loss_scale=2, train_wall=8, gb_free=17.4, wall=4545
2023-08-08 17:23:43 - progress_bar.py[line:272] - INFO: epoch 003:    851 / 2241 loss=8.532, loss_v1=0, loss_v2=0, nll_loss=7.921, ntokens=753.6, nsentences=28, sample_size=753.6, sample_size_v1=0, sample_size_v2=0, ppl=242.38, wps=895.9, ups=1.19, wpb=753.6, bsz=28, num_updates=5320, lr=1.29726e-05, gnorm=0.129, clip=0, loss_scale=2, train_wall=8, gb_free=17.8, wall=4554
2023-08-08 17:23:51 - progress_bar.py[line:272] - INFO: epoch 003:    861 / 2241 loss=8.414, loss_v1=0, loss_v2=0, nll_loss=7.793, ntokens=664.4, nsentences=28, sample_size=664.4, sample_size_v1=0, sample_size_v2=0, ppl=221.78, wps=799.4, ups=1.2, wpb=664.4, bsz=28, num_updates=5330, lr=1.2937e-05, gnorm=0.133, clip=0, loss_scale=2, train_wall=8, gb_free=17.7, wall=4562
2023-08-08 17:23:59 - progress_bar.py[line:272] - INFO: epoch 003:    871 / 2241 loss=8.334, loss_v1=0, loss_v2=0, nll_loss=7.703, ntokens=718.8, nsentences=28, sample_size=718.8, sample_size_v1=0, sample_size_v2=0, ppl=208.37, wps=866.6, ups=1.21, wpb=718.8, bsz=28, num_updates=5340, lr=1.29014e-05, gnorm=0.13, clip=0, loss_scale=2, train_wall=8, gb_free=17.7, wall=4571
2023-08-08 17:24:08 - progress_bar.py[line:272] - INFO: epoch 003:    881 / 2241 loss=8.499, loss_v1=0, loss_v2=0, nll_loss=7.882, ntokens=762.5, nsentences=28, sample_size=762.5, sample_size_v1=0, sample_size_v2=0, ppl=235.96, wps=910.1, ups=1.19, wpb=762.5, bsz=28, num_updates=5350, lr=1.28658e-05, gnorm=0.127, clip=0, loss_scale=2, train_wall=8, gb_free=16.8, wall=4579
2023-08-08 17:24:16 - progress_bar.py[line:272] - INFO: epoch 003:    891 / 2241 loss=8.485, loss_v1=0, loss_v2=0, nll_loss=7.87, ntokens=763.7, nsentences=28, sample_size=763.7, sample_size_v1=0, sample_size_v2=0, ppl=233.99, wps=915, ups=1.2, wpb=763.7, bsz=28, num_updates=5360, lr=1.28302e-05, gnorm=0.128, clip=0, loss_scale=2, train_wall=8, gb_free=17.6, wall=4587
2023-08-08 17:24:25 - progress_bar.py[line:272] - INFO: epoch 003:    901 / 2241 loss=8.494, loss_v1=0, loss_v2=0, nll_loss=7.878, ntokens=758.1, nsentences=28, sample_size=758.1, sample_size_v1=0, sample_size_v2=0, ppl=235.2, wps=898.7, ups=1.19, wpb=758.1, bsz=28, num_updates=5370, lr=1.27946e-05, gnorm=0.132, clip=0, loss_scale=2, train_wall=8, gb_free=16.5, wall=4596
2023-08-08 17:24:33 - progress_bar.py[line:272] - INFO: epoch 003:    911 / 2241 loss=8.467, loss_v1=0, loss_v2=0, nll_loss=7.846, ntokens=803.8, nsentences=28, sample_size=803.8, sample_size_v1=0, sample_size_v2=0, ppl=230.13, wps=955.1, ups=1.19, wpb=803.8, bsz=28, num_updates=5380, lr=1.2759e-05, gnorm=0.127, clip=0, loss_scale=2, train_wall=8, gb_free=17.4, wall=4604
2023-08-08 17:24:41 - progress_bar.py[line:272] - INFO: epoch 003:    921 / 2241 loss=8.331, loss_v1=0, loss_v2=0, nll_loss=7.705, ntokens=704, nsentences=28, sample_size=704, sample_size_v1=0, sample_size_v2=0, ppl=208.59, wps=849.4, ups=1.21, wpb=704, bsz=28, num_updates=5390, lr=1.27234e-05, gnorm=0.127, clip=0, loss_scale=2, train_wall=8, gb_free=17.6, wall=4612
2023-08-08 17:24:50 - progress_bar.py[line:272] - INFO: epoch 003:    931 / 2241 loss=8.449, loss_v1=0, loss_v2=0, nll_loss=7.829, ntokens=714.3, nsentences=28, sample_size=714.3, sample_size_v1=0, sample_size_v2=0, ppl=227.4, wps=859.2, ups=1.2, wpb=714.3, bsz=28, num_updates=5400, lr=1.26878e-05, gnorm=0.137, clip=0, loss_scale=2, train_wall=8, gb_free=17.5, wall=4621
2023-08-08 17:24:58 - progress_bar.py[line:272] - INFO: epoch 003:    941 / 2241 loss=8.326, loss_v1=0, loss_v2=0, nll_loss=7.702, ntokens=658.7, nsentences=28, sample_size=658.7, sample_size_v1=0, sample_size_v2=0, ppl=208.25, wps=797.3, ups=1.21, wpb=658.7, bsz=28, num_updates=5410, lr=1.26522e-05, gnorm=0.127, clip=0, loss_scale=2, train_wall=8, gb_free=17.1, wall=4629
2023-08-08 17:25:06 - progress_bar.py[line:272] - INFO: epoch 003:    951 / 2241 loss=8.394, loss_v1=0, loss_v2=0, nll_loss=7.763, ntokens=743.9, nsentences=28, sample_size=743.9, sample_size_v1=0, sample_size_v2=0, ppl=217.23, wps=890.7, ups=1.2, wpb=743.9, bsz=28, num_updates=5420, lr=1.26166e-05, gnorm=0.13, clip=0, loss_scale=4, train_wall=8, gb_free=17.5, wall=4637
2023-08-08 17:25:15 - progress_bar.py[line:272] - INFO: epoch 003:    961 / 2241 loss=8.496, loss_v1=0, loss_v2=0, nll_loss=7.865, ntokens=786.5, nsentences=28, sample_size=786.5, sample_size_v1=0, sample_size_v2=0, ppl=233.09, wps=937.1, ups=1.19, wpb=786.5, bsz=28, num_updates=5430, lr=1.2581e-05, gnorm=0.131, clip=0, loss_scale=4, train_wall=8, gb_free=17.2, wall=4646
2023-08-08 17:25:23 - progress_bar.py[line:272] - INFO: epoch 003:    971 / 2241 loss=8.44, loss_v1=0, loss_v2=0, nll_loss=7.815, ntokens=795.4, nsentences=28, sample_size=795.4, sample_size_v1=0, sample_size_v2=0, ppl=225.18, wps=946.6, ups=1.19, wpb=795.4, bsz=28, num_updates=5440, lr=1.25454e-05, gnorm=0.13, clip=0, loss_scale=4, train_wall=8, gb_free=17.5, wall=4654
2023-08-08 17:25:31 - progress_bar.py[line:272] - INFO: epoch 003:    981 / 2241 loss=8.515, loss_v1=0, loss_v2=0, nll_loss=7.9, ntokens=783.8, nsentences=28, sample_size=783.8, sample_size_v1=0, sample_size_v2=0, ppl=238.94, wps=929.4, ups=1.19, wpb=783.8, bsz=28, num_updates=5450, lr=1.25098e-05, gnorm=0.126, clip=0, loss_scale=4, train_wall=8, gb_free=16.8, wall=4663
2023-08-08 17:25:40 - progress_bar.py[line:272] - INFO: epoch 003:    991 / 2241 loss=8.461, loss_v1=0, loss_v2=0, nll_loss=7.835, ntokens=753.4, nsentences=28, sample_size=753.4, sample_size_v1=0, sample_size_v2=0, ppl=228.41, wps=904.1, ups=1.2, wpb=753.4, bsz=28, num_updates=5460, lr=1.24742e-05, gnorm=0.128, clip=0, loss_scale=4, train_wall=8, gb_free=17.3, wall=4671
2023-08-08 17:25:48 - progress_bar.py[line:272] - INFO: epoch 003:   1001 / 2241 loss=8.44, loss_v1=0, loss_v2=0, nll_loss=7.812, ntokens=753.7, nsentences=28, sample_size=753.7, sample_size_v1=0, sample_size_v2=0, ppl=224.76, wps=902.8, ups=1.2, wpb=753.7, bsz=28, num_updates=5470, lr=1.24386e-05, gnorm=0.131, clip=0, loss_scale=4, train_wall=8, gb_free=17.5, wall=4679
2023-08-08 17:25:57 - progress_bar.py[line:272] - INFO: epoch 003:   1011 / 2241 loss=8.478, loss_v1=0, loss_v2=0, nll_loss=7.861, ntokens=855.8, nsentences=28, sample_size=855.8, sample_size_v1=0, sample_size_v2=0, ppl=232.52, wps=1014.4, ups=1.19, wpb=855.8, bsz=28, num_updates=5480, lr=1.2403e-05, gnorm=0.124, clip=0, loss_scale=4, train_wall=8, gb_free=17.2, wall=4688
2023-08-08 17:26:05 - progress_bar.py[line:272] - INFO: epoch 003:   1021 / 2241 loss=8.542, loss_v1=0, loss_v2=0, nll_loss=7.923, ntokens=795.8, nsentences=28, sample_size=795.8, sample_size_v1=0, sample_size_v2=0, ppl=242.75, wps=943.3, ups=1.19, wpb=795.8, bsz=28, num_updates=5490, lr=1.23674e-05, gnorm=0.128, clip=0, loss_scale=4, train_wall=8, gb_free=17.5, wall=4696
2023-08-08 17:26:13 - progress_bar.py[line:272] - INFO: epoch 003:   1031 / 2241 loss=8.498, loss_v1=0, loss_v2=0, nll_loss=7.877, ntokens=813, nsentences=28, sample_size=813, sample_size_v1=0, sample_size_v2=0, ppl=235.15, wps=967, ups=1.19, wpb=813, bsz=28, num_updates=5500, lr=1.23318e-05, gnorm=0.127, clip=0, loss_scale=4, train_wall=8, gb_free=16.7, wall=4705
2023-08-08 17:26:22 - progress_bar.py[line:272] - INFO: epoch 003:   1041 / 2241 loss=8.48, loss_v1=0, loss_v2=0, nll_loss=7.866, ntokens=837.6, nsentences=28, sample_size=837.6, sample_size_v1=0, sample_size_v2=0, ppl=233.35, wps=999, ups=1.19, wpb=837.6, bsz=28, num_updates=5510, lr=1.22962e-05, gnorm=0.122, clip=0, loss_scale=4, train_wall=8, gb_free=17.5, wall=4713
2023-08-08 17:26:30 - progress_bar.py[line:272] - INFO: epoch 003:   1051 / 2241 loss=8.531, loss_v1=0, loss_v2=0, nll_loss=7.909, ntokens=691.2, nsentences=28, sample_size=691.2, sample_size_v1=0, sample_size_v2=0, ppl=240.29, wps=829, ups=1.2, wpb=691.2, bsz=28, num_updates=5520, lr=1.22606e-05, gnorm=0.13, clip=0, loss_scale=4, train_wall=8, gb_free=17.2, wall=4721
2023-08-08 17:26:39 - progress_bar.py[line:272] - INFO: epoch 003:   1061 / 2241 loss=8.463, loss_v1=0, loss_v2=0, nll_loss=7.841, ntokens=747.7, nsentences=28, sample_size=747.7, sample_size_v1=0, sample_size_v2=0, ppl=229.24, wps=887.9, ups=1.19, wpb=747.7, bsz=28, num_updates=5530, lr=1.2225e-05, gnorm=0.134, clip=0, loss_scale=4, train_wall=8, gb_free=15.9, wall=4730
2023-08-08 17:26:47 - progress_bar.py[line:272] - INFO: epoch 003:   1071 / 2241 loss=8.323, loss_v1=0, loss_v2=0, nll_loss=7.692, ntokens=704.1, nsentences=28, sample_size=704.1, sample_size_v1=0, sample_size_v2=0, ppl=206.73, wps=844.4, ups=1.2, wpb=704.1, bsz=28, num_updates=5540, lr=1.21894e-05, gnorm=0.131, clip=0, loss_scale=4, train_wall=8, gb_free=17.6, wall=4738
2023-08-08 17:26:55 - progress_bar.py[line:272] - INFO: epoch 003:   1081 / 2241 loss=8.424, loss_v1=0, loss_v2=0, nll_loss=7.796, ntokens=735.1, nsentences=28, sample_size=735.1, sample_size_v1=0, sample_size_v2=0, ppl=222.26, wps=871, ups=1.18, wpb=735.1, bsz=28, num_updates=5550, lr=1.21538e-05, gnorm=0.135, clip=0, loss_scale=4, train_wall=8, gb_free=17.5, wall=4746
2023-08-08 17:27:04 - progress_bar.py[line:272] - INFO: epoch 003:   1091 / 2241 loss=8.331, loss_v1=0, loss_v2=0, nll_loss=7.696, ntokens=717.9, nsentences=28, sample_size=717.9, sample_size_v1=0, sample_size_v2=0, ppl=207.42, wps=859.6, ups=1.2, wpb=717.9, bsz=28, num_updates=5560, lr=1.21182e-05, gnorm=0.129, clip=0, loss_scale=4, train_wall=8, gb_free=17.6, wall=4755
2023-08-08 17:27:12 - progress_bar.py[line:272] - INFO: epoch 003:   1101 / 2241 loss=8.427, loss_v1=0, loss_v2=0, nll_loss=7.797, ntokens=750.4, nsentences=28, sample_size=750.4, sample_size_v1=0, sample_size_v2=0, ppl=222.44, wps=899.9, ups=1.2, wpb=750.4, bsz=28, num_updates=5570, lr=1.20826e-05, gnorm=0.128, clip=0, loss_scale=4, train_wall=8, gb_free=17.6, wall=4763
2023-08-08 17:27:20 - progress_bar.py[line:272] - INFO: epoch 003:   1111 / 2241 loss=8.556, loss_v1=0, loss_v2=0, nll_loss=7.93, ntokens=806.2, nsentences=28, sample_size=806.2, sample_size_v1=0, sample_size_v2=0, ppl=243.91, wps=954.9, ups=1.18, wpb=806.2, bsz=28, num_updates=5580, lr=1.2047e-05, gnorm=0.137, clip=0, loss_scale=4, train_wall=8, gb_free=17.6, wall=4772
2023-08-08 17:27:29 - progress_bar.py[line:272] - INFO: epoch 003:   1121 / 2241 loss=8.381, loss_v1=0, loss_v2=0, nll_loss=7.755, ntokens=731.1, nsentences=28, sample_size=731.1, sample_size_v1=0, sample_size_v2=0, ppl=215.96, wps=874.3, ups=1.2, wpb=731.1, bsz=28, num_updates=5590, lr=1.20114e-05, gnorm=0.126, clip=0, loss_scale=4, train_wall=8, gb_free=17.3, wall=4780
2023-08-08 17:27:37 - progress_bar.py[line:272] - INFO: epoch 003:   1131 / 2241 loss=8.461, loss_v1=0, loss_v2=0, nll_loss=7.838, ntokens=779.1, nsentences=28, sample_size=779.1, sample_size_v1=0, sample_size_v2=0, ppl=228.89, wps=927.8, ups=1.19, wpb=779.1, bsz=28, num_updates=5600, lr=1.19758e-05, gnorm=0.129, clip=0, loss_scale=4, train_wall=8, gb_free=17, wall=4788
2023-08-08 17:27:46 - progress_bar.py[line:272] - INFO: epoch 003:   1141 / 2241 loss=8.486, loss_v1=0, loss_v2=0, nll_loss=7.86, ntokens=811.8, nsentences=28, sample_size=811.8, sample_size_v1=0, sample_size_v2=0, ppl=232.26, wps=967.9, ups=1.19, wpb=811.8, bsz=28, num_updates=5610, lr=1.19402e-05, gnorm=0.127, clip=0, loss_scale=4, train_wall=8, gb_free=16.6, wall=4797
2023-08-08 17:27:54 - progress_bar.py[line:272] - INFO: epoch 003:   1151 / 2241 loss=8.404, loss_v1=0, loss_v2=0, nll_loss=7.772, ntokens=786.3, nsentences=28, sample_size=786.3, sample_size_v1=0, sample_size_v2=0, ppl=218.58, wps=929.6, ups=1.18, wpb=786.3, bsz=28, num_updates=5620, lr=1.19046e-05, gnorm=0.129, clip=0, loss_scale=4, train_wall=8, gb_free=17.1, wall=4805
2023-08-08 17:28:02 - progress_bar.py[line:272] - INFO: epoch 003:   1161 / 2241 loss=8.506, loss_v1=0, loss_v2=0, nll_loss=7.882, ntokens=799.9, nsentences=28, sample_size=799.9, sample_size_v1=0, sample_size_v2=0, ppl=235.82, wps=951.1, ups=1.19, wpb=799.9, bsz=28, num_updates=5630, lr=1.1869e-05, gnorm=0.134, clip=0, loss_scale=4, train_wall=8, gb_free=17.2, wall=4814
2023-08-08 17:28:11 - progress_bar.py[line:272] - INFO: epoch 003:   1171 / 2241 loss=8.555, loss_v1=0, loss_v2=0, nll_loss=7.926, ntokens=816.2, nsentences=28, sample_size=816.2, sample_size_v1=0, sample_size_v2=0, ppl=243.16, wps=961.3, ups=1.18, wpb=816.2, bsz=28, num_updates=5640, lr=1.18334e-05, gnorm=0.131, clip=0, loss_scale=4, train_wall=8, gb_free=17.1, wall=4822
2023-08-08 17:28:20 - progress_bar.py[line:272] - INFO: epoch 003:   1181 / 2241 loss=8.512, loss_v1=0, loss_v2=0, nll_loss=7.901, ntokens=797.5, nsentences=28, sample_size=797.5, sample_size_v1=0, sample_size_v2=0, ppl=239, wps=933.3, ups=1.17, wpb=797.5, bsz=28, num_updates=5650, lr=1.17978e-05, gnorm=0.126, clip=0, loss_scale=4, train_wall=9, gb_free=16.9, wall=4831
2023-08-08 17:28:28 - progress_bar.py[line:272] - INFO: epoch 003:   1191 / 2241 loss=8.357, loss_v1=0, loss_v2=0, nll_loss=7.727, ntokens=752.4, nsentences=28, sample_size=752.4, sample_size_v1=0, sample_size_v2=0, ppl=211.91, wps=891.9, ups=1.19, wpb=752.4, bsz=28, num_updates=5660, lr=1.17622e-05, gnorm=0.127, clip=0, loss_scale=4, train_wall=8, gb_free=16.8, wall=4839
2023-08-08 17:28:36 - progress_bar.py[line:272] - INFO: epoch 003:   1201 / 2241 loss=8.429, loss_v1=0, loss_v2=0, nll_loss=7.799, ntokens=792.3, nsentences=28, sample_size=792.3, sample_size_v1=0, sample_size_v2=0, ppl=222.66, wps=939.2, ups=1.19, wpb=792.3, bsz=28, num_updates=5670, lr=1.17266e-05, gnorm=0.134, clip=0, loss_scale=4, train_wall=8, gb_free=17.8, wall=4848
2023-08-08 17:28:45 - progress_bar.py[line:272] - INFO: epoch 003:   1211 / 2241 loss=8.499, loss_v1=0, loss_v2=0, nll_loss=7.868, ntokens=861.8, nsentences=28, sample_size=861.8, sample_size_v1=0, sample_size_v2=0, ppl=233.59, wps=1016.3, ups=1.18, wpb=861.8, bsz=28, num_updates=5680, lr=1.1691e-05, gnorm=0.131, clip=0, loss_scale=4, train_wall=8, gb_free=16.6, wall=4856
2023-08-08 17:28:53 - progress_bar.py[line:272] - INFO: epoch 003:   1221 / 2241 loss=8.354, loss_v1=0, loss_v2=0, nll_loss=7.727, ntokens=856.7, nsentences=28, sample_size=856.7, sample_size_v1=0, sample_size_v2=0, ppl=211.83, wps=1005.4, ups=1.17, wpb=856.7, bsz=28, num_updates=5690, lr=1.16554e-05, gnorm=0.128, clip=0, loss_scale=4, train_wall=9, gb_free=17.1, wall=4865
2023-08-08 17:29:02 - progress_bar.py[line:272] - INFO: epoch 003:   1231 / 2241 loss=8.431, loss_v1=0, loss_v2=0, nll_loss=7.804, ntokens=848.3, nsentences=28, sample_size=848.3, sample_size_v1=0, sample_size_v2=0, ppl=223.5, wps=997.9, ups=1.18, wpb=848.3, bsz=28, num_updates=5700, lr=1.16198e-05, gnorm=0.128, clip=0, loss_scale=4, train_wall=8, gb_free=17.4, wall=4873
2023-08-08 17:29:10 - progress_bar.py[line:272] - INFO: epoch 003:   1241 / 2241 loss=8.294, loss_v1=0, loss_v2=0, nll_loss=7.655, ntokens=822.3, nsentences=28, sample_size=822.3, sample_size_v1=0, sample_size_v2=0, ppl=201.58, wps=975.6, ups=1.19, wpb=822.3, bsz=28, num_updates=5710, lr=1.15842e-05, gnorm=0.127, clip=0, loss_scale=4, train_wall=8, gb_free=17.4, wall=4882
2023-08-08 17:29:19 - progress_bar.py[line:272] - INFO: epoch 003:   1251 / 2241 loss=8.486, loss_v1=0, loss_v2=0, nll_loss=7.855, ntokens=861.7, nsentences=28, sample_size=861.7, sample_size_v1=0, sample_size_v2=0, ppl=231.53, wps=1016.6, ups=1.18, wpb=861.7, bsz=28, num_updates=5720, lr=1.15486e-05, gnorm=0.126, clip=0, loss_scale=4, train_wall=8, gb_free=17.5, wall=4890
2023-08-08 17:29:27 - progress_bar.py[line:272] - INFO: epoch 003:   1261 / 2241 loss=8.403, loss_v1=0, loss_v2=0, nll_loss=7.773, ntokens=841.3, nsentences=28, sample_size=841.3, sample_size_v1=0, sample_size_v2=0, ppl=218.76, wps=998.7, ups=1.19, wpb=841.3, bsz=28, num_updates=5730, lr=1.1513e-05, gnorm=0.124, clip=0, loss_scale=4, train_wall=8, gb_free=17.7, wall=4898
2023-08-08 17:29:36 - progress_bar.py[line:272] - INFO: epoch 003:   1271 / 2241 loss=8.489, loss_v1=0, loss_v2=0, nll_loss=7.866, ntokens=843.4, nsentences=28, sample_size=843.4, sample_size_v1=0, sample_size_v2=0, ppl=233.35, wps=1001.2, ups=1.19, wpb=843.4, bsz=28, num_updates=5740, lr=1.14774e-05, gnorm=0.128, clip=0, loss_scale=4, train_wall=8, gb_free=17.2, wall=4907
2023-08-08 17:29:44 - progress_bar.py[line:272] - INFO: epoch 003:   1281 / 2241 loss=8.498, loss_v1=0, loss_v2=0, nll_loss=7.869, ntokens=829.8, nsentences=28, sample_size=829.8, sample_size_v1=0, sample_size_v2=0, ppl=233.78, wps=985.9, ups=1.19, wpb=829.8, bsz=28, num_updates=5750, lr=1.14418e-05, gnorm=0.13, clip=0, loss_scale=4, train_wall=8, gb_free=17.2, wall=4915
2023-08-08 17:29:53 - progress_bar.py[line:272] - INFO: epoch 003:   1291 / 2241 loss=8.359, loss_v1=0, loss_v2=0, nll_loss=7.726, ntokens=824, nsentences=28, sample_size=824, sample_size_v1=0, sample_size_v2=0, ppl=211.78, wps=974.8, ups=1.18, wpb=824, bsz=28, num_updates=5760, lr=1.14062e-05, gnorm=0.128, clip=0, loss_scale=4, train_wall=8, gb_free=17.4, wall=4924
2023-08-08 17:30:01 - progress_bar.py[line:272] - INFO: epoch 003:   1301 / 2241 loss=8.413, loss_v1=0, loss_v2=0, nll_loss=7.778, ntokens=818.5, nsentences=28, sample_size=818.5, sample_size_v1=0, sample_size_v2=0, ppl=219.44, wps=954.9, ups=1.17, wpb=818.5, bsz=28, num_updates=5770, lr=1.13706e-05, gnorm=0.153, clip=0, loss_scale=4, train_wall=9, gb_free=17.5, wall=4932
2023-08-08 17:30:10 - progress_bar.py[line:272] - INFO: epoch 003:   1311 / 2241 loss=8.294, loss_v1=0, loss_v2=0, nll_loss=7.658, ntokens=817.5, nsentences=28, sample_size=817.5, sample_size_v1=0, sample_size_v2=0, ppl=201.99, wps=970.9, ups=1.19, wpb=817.5, bsz=28, num_updates=5780, lr=1.1335e-05, gnorm=0.133, clip=0, loss_scale=4, train_wall=8, gb_free=17.3, wall=4941
2023-08-08 17:30:18 - progress_bar.py[line:272] - INFO: epoch 003:   1321 / 2241 loss=8.366, loss_v1=0, loss_v2=0, nll_loss=7.736, ntokens=782.3, nsentences=28, sample_size=782.3, sample_size_v1=0, sample_size_v2=0, ppl=213.17, wps=931.8, ups=1.19, wpb=782.3, bsz=28, num_updates=5790, lr=1.12994e-05, gnorm=0.126, clip=0, loss_scale=4, train_wall=8, gb_free=17.6, wall=4949
2023-08-08 17:30:26 - progress_bar.py[line:272] - INFO: epoch 003:   1331 / 2241 loss=8.409, loss_v1=0, loss_v2=0, nll_loss=7.774, ntokens=863.6, nsentences=28, sample_size=863.6, sample_size_v1=0, sample_size_v2=0, ppl=218.89, wps=1020.6, ups=1.18, wpb=863.6, bsz=28, num_updates=5800, lr=1.12638e-05, gnorm=0.126, clip=0, loss_scale=4, train_wall=8, gb_free=16.6, wall=4958
2023-08-08 17:30:35 - progress_bar.py[line:272] - INFO: epoch 003:   1341 / 2241 loss=8.51, loss_v1=0, loss_v2=0, nll_loss=7.863, ntokens=890.1, nsentences=28, sample_size=890.1, sample_size_v1=0, sample_size_v2=0, ppl=232.75, wps=1036.5, ups=1.16, wpb=890.1, bsz=28, num_updates=5810, lr=1.12282e-05, gnorm=0.135, clip=0, loss_scale=4, train_wall=9, gb_free=17.5, wall=4966
2023-08-08 17:30:43 - progress_bar.py[line:272] - INFO: epoch 003:   1351 / 2241 loss=8.371, loss_v1=0, loss_v2=0, nll_loss=7.732, ntokens=885, nsentences=28, sample_size=885, sample_size_v1=0, sample_size_v2=0, ppl=212.58, wps=1048.7, ups=1.18, wpb=885, bsz=28, num_updates=5820, lr=1.11926e-05, gnorm=0.126, clip=0, loss_scale=4, train_wall=8, gb_free=17.3, wall=4975
2023-08-08 17:30:52 - progress_bar.py[line:272] - INFO: epoch 003:   1361 / 2241 loss=8.384, loss_v1=0, loss_v2=0, nll_loss=7.75, ntokens=796.4, nsentences=28, sample_size=796.4, sample_size_v1=0, sample_size_v2=0, ppl=215.25, wps=941.5, ups=1.18, wpb=796.4, bsz=28, num_updates=5830, lr=1.1157e-05, gnorm=0.132, clip=0, loss_scale=4, train_wall=8, gb_free=17.5, wall=4983
2023-08-08 17:31:00 - progress_bar.py[line:272] - INFO: epoch 003:   1371 / 2241 loss=8.307, loss_v1=0, loss_v2=0, nll_loss=7.67, ntokens=879.7, nsentences=28, sample_size=879.7, sample_size_v1=0, sample_size_v2=0, ppl=203.67, wps=1041.7, ups=1.18, wpb=879.7, bsz=28, num_updates=5840, lr=1.11214e-05, gnorm=0.126, clip=0, loss_scale=4, train_wall=8, gb_free=17, wall=4992
2023-08-08 17:31:09 - progress_bar.py[line:272] - INFO: epoch 003:   1381 / 2241 loss=8.329, loss_v1=0, loss_v2=0, nll_loss=7.677, ntokens=833.8, nsentences=28, sample_size=833.8, sample_size_v1=0, sample_size_v2=0, ppl=204.62, wps=983.5, ups=1.18, wpb=833.8, bsz=28, num_updates=5850, lr=1.10858e-05, gnorm=0.136, clip=0, loss_scale=4, train_wall=8, gb_free=16.5, wall=5000
2023-08-08 17:31:17 - progress_bar.py[line:272] - INFO: epoch 003:   1391 / 2241 loss=8.266, loss_v1=0, loss_v2=0, nll_loss=7.624, ntokens=783.7, nsentences=28, sample_size=783.7, sample_size_v1=0, sample_size_v2=0, ppl=197.31, wps=936.2, ups=1.19, wpb=783.7, bsz=28, num_updates=5860, lr=1.10502e-05, gnorm=0.126, clip=0, loss_scale=4, train_wall=8, gb_free=17.6, wall=5008
2023-08-08 17:31:26 - progress_bar.py[line:272] - INFO: epoch 003:   1401 / 2241 loss=8.474, loss_v1=0, loss_v2=0, nll_loss=7.836, ntokens=829.1, nsentences=28, sample_size=829.1, sample_size_v1=0, sample_size_v2=0, ppl=228.46, wps=979.8, ups=1.18, wpb=829.1, bsz=28, num_updates=5870, lr=1.10146e-05, gnorm=0.132, clip=0, loss_scale=4, train_wall=8, gb_free=17.7, wall=5017
2023-08-08 17:31:34 - progress_bar.py[line:272] - INFO: epoch 003:   1411 / 2241 loss=8.46, loss_v1=0, loss_v2=0, nll_loss=7.83, ntokens=881.2, nsentences=28, sample_size=881.2, sample_size_v1=0, sample_size_v2=0, ppl=227.6, wps=1037.5, ups=1.18, wpb=881.2, bsz=28, num_updates=5880, lr=1.0979e-05, gnorm=0.134, clip=0, loss_scale=4, train_wall=8, gb_free=17.7, wall=5025
2023-08-08 17:31:43 - progress_bar.py[line:272] - INFO: epoch 003:   1421 / 2241 loss=8.349, loss_v1=0, loss_v2=0, nll_loss=7.711, ntokens=846.1, nsentences=28, sample_size=846.1, sample_size_v1=0, sample_size_v2=0, ppl=209.58, wps=1001.8, ups=1.18, wpb=846.1, bsz=28, num_updates=5890, lr=1.09434e-05, gnorm=0.133, clip=0, loss_scale=4, train_wall=8, gb_free=17.2, wall=5034
2023-08-08 17:31:51 - progress_bar.py[line:272] - INFO: epoch 003:   1431 / 2241 loss=8.338, loss_v1=0, loss_v2=0, nll_loss=7.696, ntokens=798.9, nsentences=28, sample_size=798.9, sample_size_v1=0, sample_size_v2=0, ppl=207.42, wps=948.5, ups=1.19, wpb=798.9, bsz=28, num_updates=5900, lr=1.09078e-05, gnorm=0.127, clip=0, loss_scale=4, train_wall=8, gb_free=17.5, wall=5042
2023-08-08 17:32:00 - progress_bar.py[line:272] - INFO: epoch 003:   1441 / 2241 loss=8.418, loss_v1=0, loss_v2=0, nll_loss=7.776, ntokens=868, nsentences=28, sample_size=868, sample_size_v1=0, sample_size_v2=0, ppl=219.17, wps=1024.8, ups=1.18, wpb=868, bsz=28, num_updates=5910, lr=1.08722e-05, gnorm=0.13, clip=0, loss_scale=4, train_wall=8, gb_free=17.8, wall=5051
2023-08-08 17:32:08 - progress_bar.py[line:272] - INFO: epoch 003:   1451 / 2241 loss=8.314, loss_v1=0, loss_v2=0, nll_loss=7.682, ntokens=723.1, nsentences=28, sample_size=723.1, sample_size_v1=0, sample_size_v2=0, ppl=205.37, wps=867.8, ups=1.2, wpb=723.1, bsz=28, num_updates=5920, lr=1.08366e-05, gnorm=0.134, clip=0, loss_scale=4, train_wall=8, gb_free=17.4, wall=5059
2023-08-08 17:32:16 - progress_bar.py[line:272] - INFO: epoch 003:   1461 / 2241 loss=8.377, loss_v1=0, loss_v2=0, nll_loss=7.733, ntokens=779.8, nsentences=28, sample_size=779.8, sample_size_v1=0, sample_size_v2=0, ppl=212.81, wps=926.6, ups=1.19, wpb=779.8, bsz=28, num_updates=5930, lr=1.0801e-05, gnorm=0.133, clip=0, loss_scale=8, train_wall=8, gb_free=16.7, wall=5067
2023-08-08 17:32:25 - progress_bar.py[line:272] - INFO: epoch 003:   1471 / 2241 loss=8.46, loss_v1=0, loss_v2=0, nll_loss=7.82, ntokens=777.2, nsentences=28, sample_size=777.2, sample_size_v1=0, sample_size_v2=0, ppl=225.9, wps=922, ups=1.19, wpb=777.2, bsz=28, num_updates=5940, lr=1.07654e-05, gnorm=0.131, clip=0, loss_scale=8, train_wall=8, gb_free=17.7, wall=5076
2023-08-08 17:32:33 - progress_bar.py[line:272] - INFO: epoch 003:   1481 / 2241 loss=8.432, loss_v1=0, loss_v2=0, nll_loss=7.788, ntokens=785.7, nsentences=28, sample_size=785.7, sample_size_v1=0, sample_size_v2=0, ppl=220.98, wps=932.6, ups=1.19, wpb=785.7, bsz=28, num_updates=5950, lr=1.07298e-05, gnorm=0.137, clip=0, loss_scale=8, train_wall=8, gb_free=17.5, wall=5084
2023-08-08 17:32:42 - progress_bar.py[line:272] - INFO: epoch 003:   1491 / 2241 loss=8.449, loss_v1=0, loss_v2=0, nll_loss=7.819, ntokens=810.4, nsentences=28, sample_size=810.4, sample_size_v1=0, sample_size_v2=0, ppl=225.82, wps=955.9, ups=1.18, wpb=810.4, bsz=28, num_updates=5960, lr=1.06942e-05, gnorm=0.132, clip=0, loss_scale=8, train_wall=8, gb_free=16.8, wall=5093
2023-08-08 17:32:50 - progress_bar.py[line:272] - INFO: epoch 003:   1501 / 2241 loss=8.355, loss_v1=0, loss_v2=0, nll_loss=7.723, ntokens=804.6, nsentences=27.6, sample_size=804.6, sample_size_v1=0, sample_size_v2=0, ppl=211.35, wps=963.9, ups=1.2, wpb=804.6, bsz=27.6, num_updates=5970, lr=1.06586e-05, gnorm=0.128, clip=0, loss_scale=8, train_wall=8, gb_free=17.5, wall=5101
2023-08-08 17:32:58 - progress_bar.py[line:272] - INFO: epoch 003:   1511 / 2241 loss=8.352, loss_v1=0, loss_v2=0, nll_loss=7.713, ntokens=782.1, nsentences=28, sample_size=782.1, sample_size_v1=0, sample_size_v2=0, ppl=209.8, wps=925.9, ups=1.18, wpb=782.1, bsz=28, num_updates=5980, lr=1.0623e-05, gnorm=0.124, clip=0, loss_scale=8, train_wall=8, gb_free=17.5, wall=5110
2023-08-08 17:33:07 - progress_bar.py[line:272] - INFO: epoch 003:   1521 / 2241 loss=8.359, loss_v1=0, loss_v2=0, nll_loss=7.724, ntokens=792.1, nsentences=28, sample_size=792.1, sample_size_v1=0, sample_size_v2=0, ppl=211.46, wps=937, ups=1.18, wpb=792.1, bsz=28, num_updates=5990, lr=1.05874e-05, gnorm=0.135, clip=0, loss_scale=8, train_wall=8, gb_free=17, wall=5118
2023-08-08 17:33:15 - progress_bar.py[line:272] - INFO: epoch 003:   1531 / 2241 loss=8.481, loss_v1=0, loss_v2=0, nll_loss=7.846, ntokens=855, nsentences=28, sample_size=855, sample_size_v1=0, sample_size_v2=0, ppl=230.1, wps=996.7, ups=1.17, wpb=855, bsz=28, num_updates=6000, lr=1.05518e-05, gnorm=0.127, clip=0, loss_scale=8, train_wall=9, gb_free=16.8, wall=5127
2023-08-08 17:33:24 - progress_bar.py[line:272] - INFO: epoch 003:   1541 / 2241 loss=8.367, loss_v1=0, loss_v2=0, nll_loss=7.731, ntokens=757.7, nsentences=28, sample_size=757.7, sample_size_v1=0, sample_size_v2=0, ppl=212.39, wps=894, ups=1.18, wpb=757.7, bsz=28, num_updates=6010, lr=1.05162e-05, gnorm=0.129, clip=0, loss_scale=8, train_wall=8, gb_free=17.7, wall=5135
2023-08-08 17:33:32 - progress_bar.py[line:272] - INFO: epoch 003:   1551 / 2241 loss=8.417, loss_v1=0, loss_v2=0, nll_loss=7.792, ntokens=782.4, nsentences=28, sample_size=782.4, sample_size_v1=0, sample_size_v2=0, ppl=221.67, wps=928.5, ups=1.19, wpb=782.4, bsz=28, num_updates=6020, lr=1.04806e-05, gnorm=0.13, clip=0, loss_scale=8, train_wall=8, gb_free=17.4, wall=5144
2023-08-08 17:33:41 - progress_bar.py[line:272] - INFO: epoch 003:   1561 / 2241 loss=8.489, loss_v1=0, loss_v2=0, nll_loss=7.846, ntokens=932.8, nsentences=28, sample_size=932.8, sample_size_v1=0, sample_size_v2=0, ppl=230.09, wps=1090.4, ups=1.17, wpb=932.8, bsz=28, num_updates=6030, lr=1.0445e-05, gnorm=0.133, clip=0, loss_scale=8, train_wall=9, gb_free=17.2, wall=5152
2023-08-08 17:33:49 - progress_bar.py[line:272] - INFO: epoch 003:   1571 / 2241 loss=8.517, loss_v1=0, loss_v2=0, nll_loss=7.883, ntokens=868.4, nsentences=28, sample_size=868.4, sample_size_v1=0, sample_size_v2=0, ppl=236.1, wps=1015.7, ups=1.17, wpb=868.4, bsz=28, num_updates=6040, lr=1.04094e-05, gnorm=0.129, clip=0, loss_scale=8, train_wall=9, gb_free=17, wall=5161
2023-08-08 17:33:58 - progress_bar.py[line:272] - INFO: epoch 003:   1581 / 2241 loss=8.24, loss_v1=0, loss_v2=0, nll_loss=7.595, ntokens=794.2, nsentences=28, sample_size=794.2, sample_size_v1=0, sample_size_v2=0, ppl=193.34, wps=939.1, ups=1.18, wpb=794.2, bsz=28, num_updates=6050, lr=1.03738e-05, gnorm=0.124, clip=0, loss_scale=8, train_wall=8, gb_free=17.5, wall=5169
2023-08-08 17:34:06 - progress_bar.py[line:272] - INFO: epoch 003:   1591 / 2241 loss=8.445, loss_v1=0, loss_v2=0, nll_loss=7.814, ntokens=825.2, nsentences=28, sample_size=825.2, sample_size_v1=0, sample_size_v2=0, ppl=225.06, wps=979.7, ups=1.19, wpb=825.2, bsz=28, num_updates=6060, lr=1.03382e-05, gnorm=0.128, clip=0, loss_scale=8, train_wall=8, gb_free=16.9, wall=5178
2023-08-08 17:34:15 - progress_bar.py[line:272] - INFO: epoch 003:   1601 / 2241 loss=8.427, loss_v1=0, loss_v2=0, nll_loss=7.784, ntokens=841.9, nsentences=28, sample_size=841.9, sample_size_v1=0, sample_size_v2=0, ppl=220.37, wps=994.2, ups=1.18, wpb=841.9, bsz=28, num_updates=6070, lr=1.03026e-05, gnorm=0.124, clip=0, loss_scale=8, train_wall=8, gb_free=17, wall=5186
2023-08-08 17:34:23 - progress_bar.py[line:272] - INFO: epoch 003:   1611 / 2241 loss=8.4, loss_v1=0, loss_v2=0, nll_loss=7.773, ntokens=823.4, nsentences=28, sample_size=823.4, sample_size_v1=0, sample_size_v2=0, ppl=218.69, wps=969.3, ups=1.18, wpb=823.4, bsz=28, num_updates=6080, lr=1.0267e-05, gnorm=0.132, clip=0, loss_scale=8, train_wall=8, gb_free=17.4, wall=5194
2023-08-08 17:34:32 - progress_bar.py[line:272] - INFO: epoch 003:   1621 / 2241 loss=8.302, loss_v1=0, loss_v2=0, nll_loss=7.647, ntokens=898.7, nsentences=28, sample_size=898.7, sample_size_v1=0, sample_size_v2=0, ppl=200.43, wps=1065.6, ups=1.19, wpb=898.7, bsz=28, num_updates=6090, lr=1.02314e-05, gnorm=0.126, clip=0, loss_scale=8, train_wall=8, gb_free=17.1, wall=5203
2023-08-08 17:34:40 - progress_bar.py[line:272] - INFO: epoch 003:   1631 / 2241 loss=8.385, loss_v1=0, loss_v2=0, nll_loss=7.744, ntokens=867.4, nsentences=28, sample_size=867.4, sample_size_v1=0, sample_size_v2=0, ppl=214.35, wps=1024.4, ups=1.18, wpb=867.4, bsz=28, num_updates=6100, lr=1.01958e-05, gnorm=0.125, clip=0, loss_scale=8, train_wall=8, gb_free=17.1, wall=5211
2023-08-08 17:34:49 - progress_bar.py[line:272] - INFO: epoch 003:   1641 / 2241 loss=8.479, loss_v1=0, loss_v2=0, nll_loss=7.847, ntokens=832.9, nsentences=28, sample_size=832.9, sample_size_v1=0, sample_size_v2=0, ppl=230.23, wps=981.7, ups=1.18, wpb=832.9, bsz=28, num_updates=6110, lr=1.01602e-05, gnorm=0.13, clip=0, loss_scale=8, train_wall=8, gb_free=16.9, wall=5220
2023-08-08 17:34:57 - progress_bar.py[line:272] - INFO: epoch 003:   1651 / 2241 loss=8.364, loss_v1=0, loss_v2=0, nll_loss=7.728, ntokens=828.7, nsentences=28, sample_size=828.7, sample_size_v1=0, sample_size_v2=0, ppl=212.01, wps=983.2, ups=1.19, wpb=828.7, bsz=28, num_updates=6120, lr=1.01246e-05, gnorm=0.128, clip=0, loss_scale=8, train_wall=8, gb_free=17.5, wall=5228
2023-08-08 17:35:06 - progress_bar.py[line:272] - INFO: epoch 003:   1661 / 2241 loss=8.379, loss_v1=0, loss_v2=0, nll_loss=7.749, ntokens=864.4, nsentences=28, sample_size=864.4, sample_size_v1=0, sample_size_v2=0, ppl=215.16, wps=1015.5, ups=1.17, wpb=864.4, bsz=28, num_updates=6130, lr=1.0089e-05, gnorm=0.123, clip=0, loss_scale=8, train_wall=8, gb_free=16.8, wall=5237
2023-08-08 17:35:14 - progress_bar.py[line:272] - INFO: epoch 003:   1671 / 2241 loss=8.337, loss_v1=0, loss_v2=0, nll_loss=7.696, ntokens=834.3, nsentences=28, sample_size=834.3, sample_size_v1=0, sample_size_v2=0, ppl=207.4, wps=986.3, ups=1.18, wpb=834.3, bsz=28, num_updates=6140, lr=1.00534e-05, gnorm=0.13, clip=0, loss_scale=8, train_wall=8, gb_free=17.4, wall=5245
2023-08-08 17:35:23 - progress_bar.py[line:272] - INFO: epoch 003:   1681 / 2241 loss=8.421, loss_v1=0, loss_v2=0, nll_loss=7.785, ntokens=915, nsentences=28, sample_size=915, sample_size_v1=0, sample_size_v2=0, ppl=220.52, wps=1077, ups=1.18, wpb=915, bsz=28, num_updates=6150, lr=1.00178e-05, gnorm=0.126, clip=0, loss_scale=8, train_wall=8, gb_free=17, wall=5254
2023-08-08 17:35:31 - progress_bar.py[line:272] - INFO: epoch 003:   1691 / 2241 loss=8.411, loss_v1=0, loss_v2=0, nll_loss=7.78, ntokens=864.2, nsentences=28, sample_size=864.2, sample_size_v1=0, sample_size_v2=0, ppl=219.85, wps=1021.1, ups=1.18, wpb=864.2, bsz=28, num_updates=6160, lr=9.9822e-06, gnorm=0.13, clip=0, loss_scale=8, train_wall=8, gb_free=17.2, wall=5262
2023-08-08 17:35:40 - progress_bar.py[line:272] - INFO: epoch 003:   1701 / 2241 loss=8.364, loss_v1=0, loss_v2=0, nll_loss=7.735, ntokens=879.6, nsentences=28, sample_size=879.6, sample_size_v1=0, sample_size_v2=0, ppl=213.01, wps=1034.2, ups=1.18, wpb=879.6, bsz=28, num_updates=6170, lr=9.9466e-06, gnorm=0.121, clip=0, loss_scale=8, train_wall=8, gb_free=17.3, wall=5271
2023-08-08 17:35:48 - progress_bar.py[line:272] - INFO: epoch 003:   1711 / 2241 loss=8.379, loss_v1=0, loss_v2=0, nll_loss=7.732, ntokens=820.6, nsentences=28, sample_size=820.6, sample_size_v1=0, sample_size_v2=0, ppl=212.62, wps=968, ups=1.18, wpb=820.6, bsz=28, num_updates=6180, lr=9.911e-06, gnorm=0.131, clip=0, loss_scale=8, train_wall=8, gb_free=17.1, wall=5279
2023-08-08 17:35:57 - progress_bar.py[line:272] - INFO: epoch 003:   1721 / 2241 loss=8.4, loss_v1=0, loss_v2=0, nll_loss=7.773, ntokens=917.7, nsentences=28, sample_size=917.7, sample_size_v1=0, sample_size_v2=0, ppl=218.69, wps=1071.2, ups=1.17, wpb=917.7, bsz=28, num_updates=6190, lr=9.8754e-06, gnorm=0.127, clip=0, loss_scale=8, train_wall=9, gb_free=17.5, wall=5288
2023-08-08 17:36:05 - progress_bar.py[line:272] - INFO: epoch 003:   1731 / 2241 loss=8.307, loss_v1=0, loss_v2=0, nll_loss=7.649, ntokens=849.6, nsentences=28, sample_size=849.6, sample_size_v1=0, sample_size_v2=0, ppl=200.74, wps=1000.5, ups=1.18, wpb=849.6, bsz=28, num_updates=6200, lr=9.8398e-06, gnorm=0.127, clip=0, loss_scale=8, train_wall=8, gb_free=17.5, wall=5296
2023-08-08 17:36:14 - progress_bar.py[line:272] - INFO: epoch 003:   1741 / 2241 loss=8.476, loss_v1=0, loss_v2=0, nll_loss=7.848, ntokens=943.7, nsentences=28, sample_size=943.7, sample_size_v1=0, sample_size_v2=0, ppl=230.34, wps=1102.7, ups=1.17, wpb=943.7, bsz=28, num_updates=6210, lr=9.8042e-06, gnorm=0.134, clip=0, loss_scale=8, train_wall=9, gb_free=16.8, wall=5305
2023-08-08 17:36:22 - progress_bar.py[line:272] - INFO: epoch 003:   1751 / 2241 loss=8.596, loss_v1=0, loss_v2=0, nll_loss=7.968, ntokens=956.6, nsentences=28, sample_size=956.6, sample_size_v1=0, sample_size_v2=0, ppl=250.35, wps=1109.5, ups=1.16, wpb=956.6, bsz=28, num_updates=6220, lr=9.7686e-06, gnorm=0.133, clip=0, loss_scale=8, train_wall=9, gb_free=17.2, wall=5313
2023-08-08 17:36:31 - progress_bar.py[line:272] - INFO: epoch 003:   1761 / 2241 loss=8.38, loss_v1=0, loss_v2=0, nll_loss=7.741, ntokens=938.8, nsentences=28, sample_size=938.8, sample_size_v1=0, sample_size_v2=0, ppl=213.89, wps=1094.7, ups=1.17, wpb=938.8, bsz=28, num_updates=6230, lr=9.733e-06, gnorm=0.123, clip=0, loss_scale=8, train_wall=9, gb_free=17.2, wall=5322
2023-08-08 17:36:39 - progress_bar.py[line:272] - INFO: epoch 003:   1771 / 2241 loss=8.367, loss_v1=0, loss_v2=0, nll_loss=7.727, ntokens=902.6, nsentences=28, sample_size=902.6, sample_size_v1=0, sample_size_v2=0, ppl=211.88, wps=1062.7, ups=1.18, wpb=902.6, bsz=28, num_updates=6240, lr=9.6974e-06, gnorm=0.133, clip=0, loss_scale=8, train_wall=8, gb_free=17.3, wall=5331
2023-08-08 17:36:48 - progress_bar.py[line:272] - INFO: epoch 003:   1781 / 2241 loss=8.446, loss_v1=0, loss_v2=0, nll_loss=7.811, ntokens=895, nsentences=28, sample_size=895, sample_size_v1=0, sample_size_v2=0, ppl=224.56, wps=1057.5, ups=1.18, wpb=895, bsz=28, num_updates=6250, lr=9.6618e-06, gnorm=0.128, clip=0, loss_scale=8, train_wall=8, gb_free=17.3, wall=5339
2023-08-08 17:36:56 - progress_bar.py[line:272] - INFO: epoch 003:   1791 / 2241 loss=8.502, loss_v1=0, loss_v2=0, nll_loss=7.877, ntokens=912.1, nsentences=28, sample_size=912.1, sample_size_v1=0, sample_size_v2=0, ppl=235.01, wps=1071.2, ups=1.17, wpb=912.1, bsz=28, num_updates=6260, lr=9.6262e-06, gnorm=0.129, clip=0, loss_scale=8, train_wall=8, gb_free=17.2, wall=5348
2023-08-08 17:37:05 - progress_bar.py[line:272] - INFO: epoch 003:   1801 / 2241 loss=8.458, loss_v1=0, loss_v2=0, nll_loss=7.831, ntokens=969.3, nsentences=28, sample_size=969.3, sample_size_v1=0, sample_size_v2=0, ppl=227.74, wps=1133.9, ups=1.17, wpb=969.3, bsz=28, num_updates=6270, lr=9.5906e-06, gnorm=0.128, clip=0, loss_scale=8, train_wall=9, gb_free=17.5, wall=5356
2023-08-08 17:37:13 - progress_bar.py[line:272] - INFO: epoch 003:   1811 / 2241 loss=8.385, loss_v1=0, loss_v2=0, nll_loss=7.754, ntokens=823, nsentences=28, sample_size=823, sample_size_v1=0, sample_size_v2=0, ppl=215.81, wps=967.6, ups=1.18, wpb=823, bsz=28, num_updates=6280, lr=9.555e-06, gnorm=0.129, clip=0, loss_scale=8, train_wall=8, gb_free=16.7, wall=5365
2023-08-08 17:37:22 - progress_bar.py[line:272] - INFO: epoch 003:   1821 / 2241 loss=8.424, loss_v1=0, loss_v2=0, nll_loss=7.787, ntokens=957.2, nsentences=28, sample_size=957.2, sample_size_v1=0, sample_size_v2=0, ppl=220.89, wps=1118.3, ups=1.17, wpb=957.2, bsz=28, num_updates=6290, lr=9.5194e-06, gnorm=0.13, clip=0, loss_scale=8, train_wall=9, gb_free=17.5, wall=5373
2023-08-08 17:37:31 - progress_bar.py[line:272] - INFO: epoch 003:   1831 / 2241 loss=8.586, loss_v1=0, loss_v2=0, nll_loss=7.957, ntokens=1038.1, nsentences=28, sample_size=1038.1, sample_size_v1=0, sample_size_v2=0, ppl=248.45, wps=1199.4, ups=1.16, wpb=1038.1, bsz=28, num_updates=6300, lr=9.4838e-06, gnorm=0.126, clip=0, loss_scale=8, train_wall=9, gb_free=16.3, wall=5382
2023-08-08 17:37:39 - progress_bar.py[line:272] - INFO: epoch 003:   1841 / 2241 loss=8.671, loss_v1=0, loss_v2=0, nll_loss=8.04, ntokens=1041.3, nsentences=28, sample_size=1041.3, sample_size_v1=0, sample_size_v2=0, ppl=263.24, wps=1205.3, ups=1.16, wpb=1041.3, bsz=28, num_updates=6310, lr=9.4482e-06, gnorm=0.134, clip=0, loss_scale=8, train_wall=9, gb_free=16.3, wall=5390
2023-08-08 17:37:48 - progress_bar.py[line:272] - INFO: epoch 003:   1851 / 2241 loss=8.553, loss_v1=0, loss_v2=0, nll_loss=7.905, ntokens=1013.6, nsentences=28, sample_size=1013.6, sample_size_v1=0, sample_size_v2=0, ppl=239.69, wps=1171, ups=1.16, wpb=1013.6, bsz=28, num_updates=6320, lr=9.4126e-06, gnorm=0.131, clip=0, loss_scale=8, train_wall=9, gb_free=17.5, wall=5399
2023-08-08 17:37:57 - progress_bar.py[line:272] - INFO: epoch 003:   1861 / 2241 loss=8.432, loss_v1=0, loss_v2=0, nll_loss=7.797, ntokens=1025.7, nsentences=28, sample_size=1025.7, sample_size_v1=0, sample_size_v2=0, ppl=222.35, wps=1197.4, ups=1.17, wpb=1025.7, bsz=28, num_updates=6330, lr=9.377e-06, gnorm=0.131, clip=0, loss_scale=8, train_wall=9, gb_free=17.1, wall=5408
2023-08-08 17:38:05 - progress_bar.py[line:272] - INFO: epoch 003:   1871 / 2241 loss=8.45, loss_v1=0, loss_v2=0, nll_loss=7.812, ntokens=918.5, nsentences=28, sample_size=918.5, sample_size_v1=0, sample_size_v2=0, ppl=224.72, wps=1075.5, ups=1.17, wpb=918.5, bsz=28, num_updates=6340, lr=9.3414e-06, gnorm=0.124, clip=0, loss_scale=8, train_wall=9, gb_free=17.1, wall=5416
2023-08-08 17:38:14 - progress_bar.py[line:272] - INFO: epoch 003:   1881 / 2241 loss=8.524, loss_v1=0, loss_v2=0, nll_loss=7.896, ntokens=949, nsentences=28, sample_size=949, sample_size_v1=0, sample_size_v2=0, ppl=238.2, wps=1110.7, ups=1.17, wpb=949, bsz=28, num_updates=6350, lr=9.3058e-06, gnorm=0.133, clip=0, loss_scale=8, train_wall=9, gb_free=16.8, wall=5425
2023-08-08 17:38:22 - progress_bar.py[line:272] - INFO: epoch 003:   1891 / 2241 loss=8.409, loss_v1=0, loss_v2=0, nll_loss=7.758, ntokens=911, nsentences=28, sample_size=911, sample_size_v1=0, sample_size_v2=0, ppl=216.51, wps=1066.8, ups=1.17, wpb=911, bsz=28, num_updates=6360, lr=9.2702e-06, gnorm=0.131, clip=0, loss_scale=8, train_wall=9, gb_free=16.8, wall=5433
2023-08-08 17:38:28 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-08-08 17:38:32 - progress_bar.py[line:272] - INFO: epoch 003:   1902 / 2241 loss=8.617, loss_v1=0, loss_v2=0, nll_loss=7.996, ntokens=982.6, nsentences=28, sample_size=982.6, sample_size_v1=0, sample_size_v2=0, ppl=255.25, wps=1036, ups=1.05, wpb=982.6, bsz=28, num_updates=6370, lr=9.2346e-06, gnorm=0.132, clip=0, loss_scale=4, train_wall=9, gb_free=16.8, wall=5443
2023-08-08 17:38:40 - progress_bar.py[line:272] - INFO: epoch 003:   1912 / 2241 loss=8.353, loss_v1=0, loss_v2=0, nll_loss=7.716, ntokens=804.9, nsentences=28, sample_size=804.9, sample_size_v1=0, sample_size_v2=0, ppl=210.21, wps=947.1, ups=1.18, wpb=804.9, bsz=28, num_updates=6380, lr=9.199e-06, gnorm=0.127, clip=0, loss_scale=4, train_wall=8, gb_free=16.9, wall=5451
2023-08-08 17:38:49 - progress_bar.py[line:272] - INFO: epoch 003:   1922 / 2241 loss=8.33, loss_v1=0, loss_v2=0, nll_loss=7.692, ntokens=887, nsentences=28, sample_size=887, sample_size_v1=0, sample_size_v2=0, ppl=206.72, wps=1037.4, ups=1.17, wpb=887, bsz=28, num_updates=6390, lr=9.1634e-06, gnorm=0.125, clip=0, loss_scale=4, train_wall=9, gb_free=17.4, wall=5460
2023-08-08 17:38:57 - progress_bar.py[line:272] - INFO: epoch 003:   1932 / 2241 loss=8.403, loss_v1=0, loss_v2=0, nll_loss=7.757, ntokens=913.3, nsentences=28, sample_size=913.3, sample_size_v1=0, sample_size_v2=0, ppl=216.34, wps=1069.3, ups=1.17, wpb=913.3, bsz=28, num_updates=6400, lr=9.1278e-06, gnorm=0.13, clip=0, loss_scale=4, train_wall=9, gb_free=17.1, wall=5468
2023-08-08 17:39:06 - progress_bar.py[line:272] - INFO: epoch 003:   1942 / 2241 loss=8.36, loss_v1=0, loss_v2=0, nll_loss=7.724, ntokens=856.9, nsentences=28, sample_size=856.9, sample_size_v1=0, sample_size_v2=0, ppl=211.48, wps=1007.8, ups=1.18, wpb=856.9, bsz=28, num_updates=6410, lr=9.0922e-06, gnorm=0.125, clip=0, loss_scale=4, train_wall=8, gb_free=16.7, wall=5477
2023-08-08 17:39:14 - progress_bar.py[line:272] - INFO: epoch 003:   1952 / 2241 loss=8.451, loss_v1=0, loss_v2=0, nll_loss=7.809, ntokens=917.7, nsentences=28, sample_size=917.7, sample_size_v1=0, sample_size_v2=0, ppl=224.24, wps=1078.3, ups=1.17, wpb=917.7, bsz=28, num_updates=6420, lr=9.0566e-06, gnorm=0.127, clip=0, loss_scale=4, train_wall=8, gb_free=17.3, wall=5485
2023-08-08 17:39:23 - progress_bar.py[line:272] - INFO: epoch 003:   1962 / 2241 loss=8.48, loss_v1=0, loss_v2=0, nll_loss=7.852, ntokens=831.6, nsentences=28, sample_size=831.6, sample_size_v1=0, sample_size_v2=0, ppl=231.01, wps=978.5, ups=1.18, wpb=831.6, bsz=28, num_updates=6430, lr=9.021e-06, gnorm=0.127, clip=0, loss_scale=4, train_wall=8, gb_free=17.6, wall=5494
2023-08-08 17:39:31 - progress_bar.py[line:272] - INFO: epoch 003:   1972 / 2241 loss=8.321, loss_v1=0, loss_v2=0, nll_loss=7.682, ntokens=811.2, nsentences=28, sample_size=811.2, sample_size_v1=0, sample_size_v2=0, ppl=205.41, wps=962.2, ups=1.19, wpb=811.2, bsz=28, num_updates=6440, lr=8.9854e-06, gnorm=0.124, clip=0, loss_scale=4, train_wall=8, gb_free=17.7, wall=5502
2023-08-08 17:39:40 - progress_bar.py[line:272] - INFO: epoch 003:   1982 / 2241 loss=8.367, loss_v1=0, loss_v2=0, nll_loss=7.726, ntokens=865.3, nsentences=28, sample_size=865.3, sample_size_v1=0, sample_size_v2=0, ppl=211.74, wps=1019.7, ups=1.18, wpb=865.3, bsz=28, num_updates=6450, lr=8.9498e-06, gnorm=0.133, clip=0, loss_scale=4, train_wall=8, gb_free=17.3, wall=5511
2023-08-08 17:39:48 - progress_bar.py[line:272] - INFO: epoch 003:   1992 / 2241 loss=8.357, loss_v1=0, loss_v2=0, nll_loss=7.713, ntokens=861, nsentences=28, sample_size=861, sample_size_v1=0, sample_size_v2=0, ppl=209.82, wps=1008.4, ups=1.17, wpb=861, bsz=28, num_updates=6460, lr=8.9142e-06, gnorm=0.128, clip=0, loss_scale=4, train_wall=9, gb_free=16.5, wall=5519
2023-08-08 17:39:57 - progress_bar.py[line:272] - INFO: epoch 003:   2002 / 2241 loss=8.384, loss_v1=0, loss_v2=0, nll_loss=7.738, ntokens=893.6, nsentences=28, sample_size=893.6, sample_size_v1=0, sample_size_v2=0, ppl=213.43, wps=1040.7, ups=1.16, wpb=893.6, bsz=28, num_updates=6470, lr=8.8786e-06, gnorm=0.126, clip=0, loss_scale=4, train_wall=9, gb_free=17.6, wall=5528
2023-08-08 17:40:05 - progress_bar.py[line:272] - INFO: epoch 003:   2012 / 2241 loss=8.51, loss_v1=0, loss_v2=0, nll_loss=7.873, ntokens=842.1, nsentences=28, sample_size=842.1, sample_size_v1=0, sample_size_v2=0, ppl=234.51, wps=998.6, ups=1.19, wpb=842.1, bsz=28, num_updates=6480, lr=8.843e-06, gnorm=0.13, clip=0, loss_scale=4, train_wall=8, gb_free=17.5, wall=5536
2023-08-08 17:40:14 - progress_bar.py[line:272] - INFO: epoch 003:   2022 / 2241 loss=8.498, loss_v1=0, loss_v2=0, nll_loss=7.856, ntokens=892.6, nsentences=28, sample_size=892.6, sample_size_v1=0, sample_size_v2=0, ppl=231.6, wps=1048.2, ups=1.17, wpb=892.6, bsz=28, num_updates=6490, lr=8.8074e-06, gnorm=0.143, clip=0, loss_scale=4, train_wall=8, gb_free=17.5, wall=5545
2023-08-08 17:40:22 - progress_bar.py[line:272] - INFO: epoch 003:   2032 / 2241 loss=8.377, loss_v1=0, loss_v2=0, nll_loss=7.739, ntokens=845.7, nsentences=28, sample_size=845.7, sample_size_v1=0, sample_size_v2=0, ppl=213.68, wps=988.9, ups=1.17, wpb=845.7, bsz=28, num_updates=6500, lr=8.7718e-06, gnorm=0.127, clip=0, loss_scale=4, train_wall=9, gb_free=16.6, wall=5554
2023-08-08 17:40:31 - progress_bar.py[line:272] - INFO: epoch 003:   2042 / 2241 loss=8.51, loss_v1=0, loss_v2=0, nll_loss=7.876, ntokens=788.7, nsentences=28, sample_size=788.7, sample_size_v1=0, sample_size_v2=0, ppl=234.96, wps=930.6, ups=1.18, wpb=788.7, bsz=28, num_updates=6510, lr=8.73621e-06, gnorm=0.132, clip=0, loss_scale=4, train_wall=8, gb_free=17.8, wall=5562
2023-08-08 17:40:39 - progress_bar.py[line:272] - INFO: epoch 003:   2052 / 2241 loss=8.417, loss_v1=0, loss_v2=0, nll_loss=7.776, ntokens=830.4, nsentences=28, sample_size=830.4, sample_size_v1=0, sample_size_v2=0, ppl=219.2, wps=979.9, ups=1.18, wpb=830.4, bsz=28, num_updates=6520, lr=8.70061e-06, gnorm=0.13, clip=0, loss_scale=4, train_wall=8, gb_free=17.3, wall=5570
2023-08-08 17:40:48 - progress_bar.py[line:272] - INFO: epoch 003:   2062 / 2241 loss=8.323, loss_v1=0, loss_v2=0, nll_loss=7.674, ntokens=882.4, nsentences=28, sample_size=882.4, sample_size_v1=0, sample_size_v2=0, ppl=204.28, wps=1031.8, ups=1.17, wpb=882.4, bsz=28, num_updates=6530, lr=8.66501e-06, gnorm=0.126, clip=0, loss_scale=4, train_wall=9, gb_free=17, wall=5579
2023-08-08 17:40:56 - progress_bar.py[line:272] - INFO: epoch 003:   2072 / 2241 loss=8.321, loss_v1=0, loss_v2=0, nll_loss=7.672, ntokens=868, nsentences=28, sample_size=868, sample_size_v1=0, sample_size_v2=0, ppl=203.98, wps=1023.9, ups=1.18, wpb=868, bsz=28, num_updates=6540, lr=8.62941e-06, gnorm=0.127, clip=0, loss_scale=4, train_wall=8, gb_free=17.4, wall=5588
2023-08-08 17:41:05 - progress_bar.py[line:272] - INFO: epoch 003:   2082 / 2241 loss=8.461, loss_v1=0, loss_v2=0, nll_loss=7.817, ntokens=920.8, nsentences=28, sample_size=920.8, sample_size_v1=0, sample_size_v2=0, ppl=225.52, wps=1072.9, ups=1.17, wpb=920.8, bsz=28, num_updates=6550, lr=8.59381e-06, gnorm=0.127, clip=0, loss_scale=4, train_wall=9, gb_free=17.2, wall=5596
2023-08-08 17:41:13 - progress_bar.py[line:272] - INFO: epoch 003:   2092 / 2241 loss=8.554, loss_v1=0, loss_v2=0, nll_loss=7.903, ntokens=931.2, nsentences=28, sample_size=931.2, sample_size_v1=0, sample_size_v2=0, ppl=239.36, wps=1088.5, ups=1.17, wpb=931.2, bsz=28, num_updates=6560, lr=8.55821e-06, gnorm=0.138, clip=0, loss_scale=4, train_wall=9, gb_free=17, wall=5605
2023-08-08 17:41:22 - progress_bar.py[line:272] - INFO: epoch 003:   2102 / 2241 loss=8.396, loss_v1=0, loss_v2=0, nll_loss=7.755, ntokens=894.1, nsentences=28, sample_size=894.1, sample_size_v1=0, sample_size_v2=0, ppl=215.96, wps=1051, ups=1.18, wpb=894.1, bsz=28, num_updates=6570, lr=8.52261e-06, gnorm=0.127, clip=0, loss_scale=4, train_wall=8, gb_free=16.7, wall=5613
2023-08-08 17:41:31 - progress_bar.py[line:272] - INFO: epoch 003:   2112 / 2241 loss=8.476, loss_v1=0, loss_v2=0, nll_loss=7.841, ntokens=950.4, nsentences=28, sample_size=950.4, sample_size_v1=0, sample_size_v2=0, ppl=229.27, wps=1102.5, ups=1.16, wpb=950.4, bsz=28, num_updates=6580, lr=8.48701e-06, gnorm=0.126, clip=0, loss_scale=4, train_wall=9, gb_free=17.5, wall=5622
2023-08-08 17:41:39 - progress_bar.py[line:272] - INFO: epoch 003:   2122 / 2241 loss=8.425, loss_v1=0, loss_v2=0, nll_loss=7.785, ntokens=898.7, nsentences=28, sample_size=898.7, sample_size_v1=0, sample_size_v2=0, ppl=220.53, wps=1050.4, ups=1.17, wpb=898.7, bsz=28, num_updates=6590, lr=8.45141e-06, gnorm=0.129, clip=0, loss_scale=4, train_wall=9, gb_free=16.9, wall=5630
2023-08-08 17:41:48 - progress_bar.py[line:272] - INFO: epoch 003:   2132 / 2241 loss=8.609, loss_v1=0, loss_v2=0, nll_loss=7.974, ntokens=1092.4, nsentences=28, sample_size=1092.4, sample_size_v1=0, sample_size_v2=0, ppl=251.5, wps=1259.9, ups=1.15, wpb=1092.4, bsz=28, num_updates=6600, lr=8.41581e-06, gnorm=0.138, clip=0, loss_scale=4, train_wall=9, gb_free=16.6, wall=5639
2023-08-08 17:41:56 - progress_bar.py[line:272] - INFO: epoch 003:   2142 / 2241 loss=8.28, loss_v1=0, loss_v2=0, nll_loss=7.632, ntokens=726.8, nsentences=28, sample_size=726.8, sample_size_v1=0, sample_size_v2=0, ppl=198.33, wps=870.7, ups=1.2, wpb=726.8, bsz=28, num_updates=6610, lr=8.38021e-06, gnorm=0.134, clip=0, loss_scale=4, train_wall=8, gb_free=17.6, wall=5647
2023-08-08 17:42:05 - progress_bar.py[line:272] - INFO: epoch 003:   2152 / 2241 loss=8.295, loss_v1=0, loss_v2=0, nll_loss=7.649, ntokens=812.5, nsentences=28, sample_size=812.5, sample_size_v1=0, sample_size_v2=0, ppl=200.77, wps=962.8, ups=1.18, wpb=812.5, bsz=28, num_updates=6620, lr=8.34461e-06, gnorm=0.13, clip=0, loss_scale=4, train_wall=8, gb_free=17.6, wall=5656
2023-08-08 17:42:13 - progress_bar.py[line:272] - INFO: epoch 003:   2162 / 2241 loss=8.276, loss_v1=0, loss_v2=0, nll_loss=7.638, ntokens=785.8, nsentences=28, sample_size=785.8, sample_size_v1=0, sample_size_v2=0, ppl=199.14, wps=930.4, ups=1.18, wpb=785.8, bsz=28, num_updates=6630, lr=8.30901e-06, gnorm=0.128, clip=0, loss_scale=4, train_wall=8, gb_free=17.5, wall=5664
2023-08-08 17:42:22 - progress_bar.py[line:272] - INFO: epoch 003:   2172 / 2241 loss=8.522, loss_v1=0, loss_v2=0, nll_loss=7.873, ntokens=963.2, nsentences=28, sample_size=963.2, sample_size_v1=0, sample_size_v2=0, ppl=234.47, wps=1116.6, ups=1.16, wpb=963.2, bsz=28, num_updates=6640, lr=8.27341e-06, gnorm=0.139, clip=0, loss_scale=4, train_wall=9, gb_free=16.7, wall=5673
2023-08-08 17:42:30 - progress_bar.py[line:272] - INFO: epoch 003:   2182 / 2241 loss=8.378, loss_v1=0, loss_v2=0, nll_loss=7.731, ntokens=901.4, nsentences=28, sample_size=901.4, sample_size_v1=0, sample_size_v2=0, ppl=212.4, wps=1059, ups=1.17, wpb=901.4, bsz=28, num_updates=6650, lr=8.23781e-06, gnorm=0.134, clip=0, loss_scale=4, train_wall=8, gb_free=17.3, wall=5681
2023-08-08 17:42:39 - progress_bar.py[line:272] - INFO: epoch 003:   2192 / 2241 loss=8.546, loss_v1=0, loss_v2=0, nll_loss=7.919, ntokens=1040.5, nsentences=28, sample_size=1040.5, sample_size_v1=0, sample_size_v2=0, ppl=242.1, wps=1191.5, ups=1.15, wpb=1040.5, bsz=28, num_updates=6660, lr=8.20221e-06, gnorm=0.132, clip=0, loss_scale=4, train_wall=9, gb_free=17, wall=5690
2023-08-08 17:42:48 - progress_bar.py[line:272] - INFO: epoch 003:   2202 / 2241 loss=8.553, loss_v1=0, loss_v2=0, nll_loss=7.919, ntokens=1034.6, nsentences=28, sample_size=1034.6, sample_size_v1=0, sample_size_v2=0, ppl=241.95, wps=1189.8, ups=1.15, wpb=1034.6, bsz=28, num_updates=6670, lr=8.16661e-06, gnorm=0.131, clip=0, loss_scale=4, train_wall=9, gb_free=17.3, wall=5699
2023-08-08 17:42:56 - progress_bar.py[line:272] - INFO: epoch 003:   2212 / 2241 loss=8.542, loss_v1=0, loss_v2=0, nll_loss=7.914, ntokens=966.7, nsentences=28, sample_size=966.7, sample_size_v1=0, sample_size_v2=0, ppl=241.11, wps=1124.9, ups=1.16, wpb=966.7, bsz=28, num_updates=6680, lr=8.13101e-06, gnorm=0.137, clip=0, loss_scale=4, train_wall=9, gb_free=16.3, wall=5707
2023-08-08 17:43:05 - progress_bar.py[line:272] - INFO: epoch 003:   2222 / 2241 loss=8.427, loss_v1=0, loss_v2=0, nll_loss=7.793, ntokens=994.2, nsentences=28, sample_size=994.2, sample_size_v1=0, sample_size_v2=0, ppl=221.72, wps=1147.2, ups=1.15, wpb=994.2, bsz=28, num_updates=6690, lr=8.09541e-06, gnorm=0.128, clip=0, loss_scale=4, train_wall=9, gb_free=17, wall=5716
2023-08-08 17:43:13 - progress_bar.py[line:272] - INFO: epoch 003:   2232 / 2241 loss=8.434, loss_v1=0, loss_v2=0, nll_loss=7.799, ntokens=868.1, nsentences=28, sample_size=868.1, sample_size_v1=0, sample_size_v2=0, ppl=222.72, wps=1018.6, ups=1.17, wpb=868.1, bsz=28, num_updates=6700, lr=8.05981e-06, gnorm=0.134, clip=0, loss_scale=4, train_wall=9, gb_free=17.7, wall=5725
2023-08-08 17:43:21 - train.py[line:332] - INFO: end of epoch 3 (average epoch stats below)
2023-08-08 17:43:21 - progress_bar.py[line:282] - INFO: epoch 003 | loss 8.475 | loss_v1 0 | loss_v2 0 | nll_loss 7.845 | ntokens 840.529 | nsentences 27.989 | sample_size 840.529 | sample_size_v1 0 | sample_size_v2 0 | ppl 229.88 | wps 989.4 | ups 1.18 | wpb 840.5 | bsz 28 | num_updates 6709 | lr 8.02777e-06 | gnorm 0.132 | clip 0 | loss_scale 4 | train_wall 1895 | gb_free 17.4 | wall 5732
2023-08-08 17:43:21 - trainer.py[line:639] - INFO: loading train data for epoch 4
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/sgcls/vg_train_full.tsv slice_id 0 row count 62723 total row count 62723
slice_id 0 seek offset 0
2023-08-08 17:43:23 - trainer.py[line:703] - INFO: begin training epoch 4
2023-08-08 17:43:23 - train.py[line:305] - INFO: Start iterating over samples
2023-08-08 17:43:24 - progress_bar.py[line:272] - INFO: epoch 004:      1 / 2241 loss=8.491, loss_v1=0, loss_v2=0, nll_loss=7.857, ntokens=905.6, nsentences=25.9, sample_size=905.6, sample_size_v1=0, sample_size_v2=0, ppl=231.82, wps=885.9, ups=0.98, wpb=905.6, bsz=25.9, num_updates=6710, lr=8.02421e-06, gnorm=0.135, clip=0, loss_scale=4, train_wall=8, gb_free=17, wall=5735
2023-08-08 17:43:32 - progress_bar.py[line:272] - INFO: epoch 004:     11 / 2241 loss=8.51, loss_v1=0, loss_v2=0, nll_loss=7.85, ntokens=899, nsentences=28, sample_size=899, sample_size_v1=0, sample_size_v2=0, ppl=230.74, wps=1044.2, ups=1.16, wpb=899, bsz=28, num_updates=6720, lr=7.98861e-06, gnorm=0.167, clip=0, loss_scale=4, train_wall=9, gb_free=17, wall=5743
2023-08-08 17:43:41 - progress_bar.py[line:272] - INFO: epoch 004:     21 / 2241 loss=8.483, loss_v1=0, loss_v2=0, nll_loss=7.83, ntokens=845.2, nsentences=28, sample_size=845.2, sample_size_v1=0, sample_size_v2=0, ppl=227.49, wps=987.7, ups=1.17, wpb=845.2, bsz=28, num_updates=6730, lr=7.95301e-06, gnorm=0.151, clip=0, loss_scale=4, train_wall=9, gb_free=17.2, wall=5752
2023-08-08 17:43:49 - progress_bar.py[line:272] - INFO: epoch 004:     31 / 2241 loss=8.358, loss_v1=0, loss_v2=0, nll_loss=7.697, ntokens=777.2, nsentences=28, sample_size=777.2, sample_size_v1=0, sample_size_v2=0, ppl=207.56, wps=917.9, ups=1.18, wpb=777.2, bsz=28, num_updates=6740, lr=7.91741e-06, gnorm=0.156, clip=0, loss_scale=4, train_wall=8, gb_free=17.7, wall=5761
2023-08-08 17:43:58 - progress_bar.py[line:272] - INFO: epoch 004:     41 / 2241 loss=8.213, loss_v1=0, loss_v2=0, nll_loss=7.533, ntokens=729.9, nsentences=28, sample_size=729.9, sample_size_v1=0, sample_size_v2=0, ppl=185.18, wps=868.9, ups=1.19, wpb=729.9, bsz=28, num_updates=6750, lr=7.88181e-06, gnorm=0.15, clip=0, loss_scale=4, train_wall=8, gb_free=17.2, wall=5769
2023-08-08 17:44:06 - progress_bar.py[line:272] - INFO: epoch 004:     51 / 2241 loss=8.317, loss_v1=0, loss_v2=0, nll_loss=7.62, ntokens=931.7, nsentences=28, sample_size=931.7, sample_size_v1=0, sample_size_v2=0, ppl=196.75, wps=1077.5, ups=1.16, wpb=931.7, bsz=28, num_updates=6760, lr=7.84621e-06, gnorm=0.176, clip=0, loss_scale=4, train_wall=9, gb_free=17.2, wall=5778
2023-08-08 17:44:15 - progress_bar.py[line:272] - INFO: epoch 004:     61 / 2241 loss=8.316, loss_v1=0, loss_v2=0, nll_loss=7.629, ntokens=807.8, nsentences=28, sample_size=807.8, sample_size_v1=0, sample_size_v2=0, ppl=197.99, wps=941.8, ups=1.17, wpb=807.8, bsz=28, num_updates=6770, lr=7.81061e-06, gnorm=0.187, clip=0, loss_scale=4, train_wall=9, gb_free=17, wall=5786
2023-08-08 17:44:24 - progress_bar.py[line:272] - INFO: epoch 004:     71 / 2241 loss=8.286, loss_v1=0, loss_v2=0, nll_loss=7.607, ntokens=770.5, nsentences=28, sample_size=770.5, sample_size_v1=0, sample_size_v2=0, ppl=194.92, wps=900.9, ups=1.17, wpb=770.5, bsz=28, num_updates=6780, lr=7.77501e-06, gnorm=0.171, clip=0, loss_scale=4, train_wall=9, gb_free=17.2, wall=5795
2023-08-08 17:44:32 - progress_bar.py[line:272] - INFO: epoch 004:     81 / 2241 loss=8.098, loss_v1=0, loss_v2=0, nll_loss=7.402, ntokens=722.6, nsentences=28, sample_size=722.6, sample_size_v1=0, sample_size_v2=0, ppl=169.12, wps=856.6, ups=1.19, wpb=722.6, bsz=28, num_updates=6790, lr=7.73941e-06, gnorm=0.177, clip=0, loss_scale=4, train_wall=8, gb_free=17.3, wall=5803
2023-08-08 17:44:35 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-08-08 17:44:42 - progress_bar.py[line:272] - INFO: epoch 004:     92 / 2241 loss=8.559, loss_v1=0, loss_v2=0, nll_loss=7.875, ntokens=1098.1, nsentences=28, sample_size=1098.1, sample_size_v1=0, sample_size_v2=0, ppl=234.79, wps=1129, ups=1.03, wpb=1098.1, bsz=28, num_updates=6800, lr=7.70381e-06, gnorm=0.184, clip=0, loss_scale=2, train_wall=10, gb_free=16, wall=5813
2023-08-08 17:44:51 - progress_bar.py[line:272] - INFO: epoch 004:    102 / 2241 loss=8.745, loss_v1=0, loss_v2=0, nll_loss=8.086, ntokens=1106.2, nsentences=28, sample_size=1106.2, sample_size_v1=0, sample_size_v2=0, ppl=271.81, wps=1249.7, ups=1.13, wpb=1106.2, bsz=28, num_updates=6810, lr=7.66821e-06, gnorm=0.158, clip=0, loss_scale=2, train_wall=9, gb_free=16, wall=5822
2023-08-08 17:44:59 - progress_bar.py[line:272] - INFO: epoch 004:    112 / 2241 loss=8.496, loss_v1=0, loss_v2=0, nll_loss=7.831, ntokens=939.7, nsentences=28, sample_size=939.7, sample_size_v1=0, sample_size_v2=0, ppl=227.66, wps=1079.7, ups=1.15, wpb=939.7, bsz=28, num_updates=6820, lr=7.63261e-06, gnorm=0.166, clip=0, loss_scale=2, train_wall=9, gb_free=17.5, wall=5830
2023-08-08 17:45:08 - progress_bar.py[line:272] - INFO: epoch 004:    122 / 2241 loss=8.362, loss_v1=0, loss_v2=0, nll_loss=7.697, ntokens=826, nsentences=28, sample_size=826, sample_size_v1=0, sample_size_v2=0, ppl=207.49, wps=965.6, ups=1.17, wpb=826, bsz=28, num_updates=6830, lr=7.59701e-06, gnorm=0.159, clip=0, loss_scale=2, train_wall=9, gb_free=16.7, wall=5839
2023-08-08 17:45:16 - progress_bar.py[line:272] - INFO: epoch 004:    132 / 2241 loss=8.103, loss_v1=0, loss_v2=0, nll_loss=7.41, ntokens=821.5, nsentences=28, sample_size=821.5, sample_size_v1=0, sample_size_v2=0, ppl=170.02, wps=967.8, ups=1.18, wpb=821.5, bsz=28, num_updates=6840, lr=7.56141e-06, gnorm=0.168, clip=0, loss_scale=2, train_wall=8, gb_free=16.8, wall=5847
2023-08-08 17:45:25 - progress_bar.py[line:272] - INFO: epoch 004:    142 / 2241 loss=8.424, loss_v1=0, loss_v2=0, nll_loss=7.789, ntokens=794.4, nsentences=28, sample_size=794.4, sample_size_v1=0, sample_size_v2=0, ppl=221.18, wps=937, ups=1.18, wpb=794.4, bsz=28, num_updates=6850, lr=7.52581e-06, gnorm=0.133, clip=0, loss_scale=2, train_wall=8, gb_free=17.2, wall=5856
2023-08-08 17:45:33 - progress_bar.py[line:272] - INFO: epoch 004:    152 / 2241 loss=8.383, loss_v1=0, loss_v2=0, nll_loss=7.732, ntokens=807, nsentences=28, sample_size=807, sample_size_v1=0, sample_size_v2=0, ppl=212.62, wps=950.6, ups=1.18, wpb=807, bsz=28, num_updates=6860, lr=7.49021e-06, gnorm=0.129, clip=0, loss_scale=2, train_wall=8, gb_free=17.1, wall=5864
2023-08-08 17:45:42 - progress_bar.py[line:272] - INFO: epoch 004:    162 / 2241 loss=8.261, loss_v1=0, loss_v2=0, nll_loss=7.6, ntokens=803.1, nsentences=28, sample_size=803.1, sample_size_v1=0, sample_size_v2=0, ppl=194.01, wps=935.4, ups=1.16, wpb=803.1, bsz=28, num_updates=6870, lr=7.45461e-06, gnorm=0.127, clip=0, loss_scale=2, train_wall=9, gb_free=17, wall=5873
2023-08-08 17:45:51 - progress_bar.py[line:272] - INFO: epoch 004:    172 / 2241 loss=8.451, loss_v1=0, loss_v2=0, nll_loss=7.814, ntokens=963.4, nsentences=28, sample_size=963.4, sample_size_v1=0, sample_size_v2=0, ppl=225.04, wps=1103.1, ups=1.14, wpb=963.4, bsz=28, num_updates=6880, lr=7.41901e-06, gnorm=0.15, clip=0, loss_scale=2, train_wall=9, gb_free=17.2, wall=5882
2023-08-08 17:45:59 - progress_bar.py[line:272] - INFO: epoch 004:    182 / 2241 loss=8.328, loss_v1=0, loss_v2=0, nll_loss=7.674, ntokens=924, nsentences=28, sample_size=924, sample_size_v1=0, sample_size_v2=0, ppl=204.24, wps=1068.3, ups=1.16, wpb=924, bsz=28, num_updates=6890, lr=7.38341e-06, gnorm=0.137, clip=0, loss_scale=2, train_wall=9, gb_free=16.2, wall=5890
2023-08-08 17:46:08 - progress_bar.py[line:272] - INFO: epoch 004:    192 / 2241 loss=8.431, loss_v1=0, loss_v2=0, nll_loss=7.784, ntokens=975.5, nsentences=28, sample_size=975.5, sample_size_v1=0, sample_size_v2=0, ppl=220.47, wps=1126.3, ups=1.15, wpb=975.5, bsz=28, num_updates=6900, lr=7.34781e-06, gnorm=0.143, clip=0, loss_scale=2, train_wall=9, gb_free=17.2, wall=5899
2023-08-08 17:46:17 - progress_bar.py[line:272] - INFO: epoch 004:    202 / 2241 loss=8.388, loss_v1=0, loss_v2=0, nll_loss=7.731, ntokens=938.7, nsentences=28, sample_size=938.7, sample_size_v1=0, sample_size_v2=0, ppl=212.41, wps=1077, ups=1.15, wpb=938.7, bsz=28, num_updates=6910, lr=7.31221e-06, gnorm=0.133, clip=0, loss_scale=2, train_wall=9, gb_free=16.8, wall=5908
2023-08-08 17:46:25 - progress_bar.py[line:272] - INFO: epoch 004:    212 / 2241 loss=8.388, loss_v1=0, loss_v2=0, nll_loss=7.746, ntokens=886.1, nsentences=28, sample_size=886.1, sample_size_v1=0, sample_size_v2=0, ppl=214.74, wps=1015.6, ups=1.15, wpb=886.1, bsz=28, num_updates=6920, lr=7.27661e-06, gnorm=0.132, clip=0, loss_scale=2, train_wall=9, gb_free=16.8, wall=5917
2023-08-08 17:46:34 - progress_bar.py[line:272] - INFO: epoch 004:    222 / 2241 loss=8.159, loss_v1=0, loss_v2=0, nll_loss=7.487, ntokens=744.6, nsentences=28, sample_size=744.6, sample_size_v1=0, sample_size_v2=0, ppl=179.39, wps=876.6, ups=1.18, wpb=744.6, bsz=28, num_updates=6930, lr=7.24101e-06, gnorm=0.131, clip=0, loss_scale=2, train_wall=8, gb_free=17.4, wall=5925
2023-08-08 17:46:42 - progress_bar.py[line:272] - INFO: epoch 004:    232 / 2241 loss=8.198, loss_v1=0, loss_v2=0, nll_loss=7.536, ntokens=735.2, nsentences=28, sample_size=735.2, sample_size_v1=0, sample_size_v2=0, ppl=185.62, wps=863.4, ups=1.17, wpb=735.2, bsz=28, num_updates=6940, lr=7.20541e-06, gnorm=0.134, clip=0, loss_scale=2, train_wall=8, gb_free=17.5, wall=5934
2023-08-08 17:46:51 - progress_bar.py[line:272] - INFO: epoch 004:    242 / 2241 loss=8.326, loss_v1=0, loss_v2=0, nll_loss=7.659, ntokens=758.7, nsentences=28, sample_size=758.7, sample_size_v1=0, sample_size_v2=0, ppl=202.08, wps=885.9, ups=1.17, wpb=758.7, bsz=28, num_updates=6950, lr=7.16981e-06, gnorm=0.139, clip=0, loss_scale=2, train_wall=9, gb_free=17.4, wall=5942
2023-08-08 17:47:00 - progress_bar.py[line:272] - INFO: epoch 004:    252 / 2241 loss=8.316, loss_v1=0, loss_v2=0, nll_loss=7.646, ntokens=879.8, nsentences=28, sample_size=879.8, sample_size_v1=0, sample_size_v2=0, ppl=200.23, wps=1015.6, ups=1.15, wpb=879.8, bsz=28, num_updates=6960, lr=7.13421e-06, gnorm=0.148, clip=0, loss_scale=2, train_wall=9, gb_free=17.2, wall=5951
2023-08-08 17:47:08 - progress_bar.py[line:272] - INFO: epoch 004:    262 / 2241 loss=8.215, loss_v1=0, loss_v2=0, nll_loss=7.556, ntokens=858.5, nsentences=28, sample_size=858.5, sample_size_v1=0, sample_size_v2=0, ppl=188.22, wps=993.8, ups=1.16, wpb=858.5, bsz=28, num_updates=6970, lr=7.09861e-06, gnorm=0.14, clip=0, loss_scale=2, train_wall=9, gb_free=16.9, wall=5959
2023-08-08 17:47:17 - progress_bar.py[line:272] - INFO: epoch 004:    272 / 2241 loss=8.266, loss_v1=0, loss_v2=0, nll_loss=7.613, ntokens=883.3, nsentences=28, sample_size=883.3, sample_size_v1=0, sample_size_v2=0, ppl=195.74, wps=1026.2, ups=1.16, wpb=883.3, bsz=28, num_updates=6980, lr=7.06301e-06, gnorm=0.134, clip=0, loss_scale=2, train_wall=9, gb_free=17, wall=5968
2023-08-08 17:47:25 - progress_bar.py[line:272] - INFO: epoch 004:    282 / 2241 loss=8.418, loss_v1=0, loss_v2=0, nll_loss=7.784, ntokens=766.3, nsentences=28, sample_size=766.3, sample_size_v1=0, sample_size_v2=0, ppl=220.4, wps=903.7, ups=1.18, wpb=766.3, bsz=28, num_updates=6990, lr=7.02741e-06, gnorm=0.149, clip=0, loss_scale=2, train_wall=8, gb_free=17.3, wall=5977
2023-08-08 17:47:34 - progress_bar.py[line:272] - INFO: epoch 004:    292 / 2241 loss=8.606, loss_v1=0, loss_v2=0, nll_loss=7.989, ntokens=918.7, nsentences=28, sample_size=918.7, sample_size_v1=0, sample_size_v2=0, ppl=254.04, wps=1071.3, ups=1.17, wpb=918.7, bsz=28, num_updates=7000, lr=6.99181e-06, gnorm=0.139, clip=0, loss_scale=2, train_wall=9, gb_free=16.9, wall=5985
2023-08-08 17:47:42 - progress_bar.py[line:272] - INFO: epoch 004:    302 / 2241 loss=8.566, loss_v1=0, loss_v2=0, nll_loss=7.941, ntokens=897.6, nsentences=28, sample_size=897.6, sample_size_v1=0, sample_size_v2=0, ppl=245.71, wps=1055.1, ups=1.18, wpb=897.6, bsz=28, num_updates=7010, lr=6.95621e-06, gnorm=0.132, clip=0, loss_scale=2, train_wall=8, gb_free=17.2, wall=5994
2023-08-08 17:47:51 - progress_bar.py[line:272] - INFO: epoch 004:    312 / 2241 loss=8.572, loss_v1=0, loss_v2=0, nll_loss=7.942, ntokens=897.5, nsentences=28, sample_size=897.5, sample_size_v1=0, sample_size_v2=0, ppl=245.94, wps=1054.4, ups=1.17, wpb=897.5, bsz=28, num_updates=7020, lr=6.92061e-06, gnorm=0.13, clip=0, loss_scale=2, train_wall=8, gb_free=16.8, wall=6002
2023-08-08 17:48:00 - progress_bar.py[line:272] - INFO: epoch 004:    322 / 2241 loss=8.766, loss_v1=0, loss_v2=0, nll_loss=8.155, ntokens=905.5, nsentences=28, sample_size=905.5, sample_size_v1=0, sample_size_v2=0, ppl=284.97, wps=1058.2, ups=1.17, wpb=905.5, bsz=28, num_updates=7030, lr=6.88501e-06, gnorm=0.137, clip=0, loss_scale=2, train_wall=9, gb_free=17.2, wall=6011
2023-08-08 17:48:08 - progress_bar.py[line:272] - INFO: epoch 004:    332 / 2241 loss=8.691, loss_v1=0, loss_v2=0, nll_loss=8.082, ntokens=958.1, nsentences=28, sample_size=958.1, sample_size_v1=0, sample_size_v2=0, ppl=270.97, wps=1116.1, ups=1.16, wpb=958.1, bsz=28, num_updates=7040, lr=6.84941e-06, gnorm=0.133, clip=0, loss_scale=2, train_wall=9, gb_free=17.6, wall=6019
/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.elastic.agent.server.api:Received 2 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3215507 closing signal SIGINT
Error in sys.excepthook:
Traceback (most recent call last):
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/linecache.py", line 47, in getlines
    return updatecache(filename, module_globals)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/linecache.py", line 137, in updatecache
    lines = fp.readlines()
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/codecs.py", line 319, in decode
    def decode(self, input, final=False):
KeyboardInterrupt

Original exception was:
Traceback (most recent call last):
  File "../../train.py", line 539, in <module>
    cli_main()
  File "../../train.py", line 532, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/zcai75/Github/OFA/fairseq/fairseq/distributed/utils.py", line 374, in call_main
    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
  File "/home/zcai75/Github/OFA/fairseq/fairseq/distributed/utils.py", line 348, in distributed_main
    main(cfg, **kwargs)
  File "../../train.py", line 199, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "../../train.py", line 310, in train
    log_output = trainer.train_step(samples)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zcai75/Github/OFA_forked/trainer.py", line 773, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/home/zcai75/Github/OFA_forked/tasks/ofa_task.py", line 334, in train_step
    loss, sample_size, logging_output = criterion(model, sample, update_num=update_num)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zcai75/Github/OFA_forked/criterions/label_smoothed_cross_entropy.py", line 199, in forward
    net_output = model(**sample["net_input"])
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zcai75/Github/OFA_forked/models/ofa/ofa.py", line 99, in forward
    x, extra = self.decoder(
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zcai75/Github/OFA_forked/models/ofa/unify_transformer.py", line 1526, in forward
    x, extra = self.extract_features(
  File "/home/zcai75/Github/OFA_forked/models/ofa/unify_transformer.py", line 1550, in extract_features
    return self.extract_features_scriptable(
  File "/home/zcai75/Github/OFA_forked/models/ofa/unify_transformer.py", line 1734, in extract_features_scriptable
    x, layer_attn, _ = layer(
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zcai75/Github/OFA_forked/models/ofa/unify_transformer_layer.py", line 501, in forward
    x, attn = self.self_attn(
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zcai75/Github/OFA_forked/models/ofa/unify_multihead_attention.py", line 378, in forward
    attn_weights_float = utils.softmax(
  File "/home/zcai75/Github/OFA/fairseq/fairseq/utils.py", line 514, in softmax
    return F.softmax(x, dim=dim, dtype=torch.float32)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/functional.py", line 1836, in softmax
    ret = input.softmax(dim, dtype=dtype)
KeyboardInterrupt
wandb: Waiting for W&B process to finish... (failed 255). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:                  train/bsz ▁▁▁
wandb:                 train/clip ▁▁▁
wandb:              train/gb_free ▁▁▁
wandb:                train/gnorm █▁▁
wandb:                 train/loss █▂▁
wandb:           train/loss_scale ▁▁█
wandb:              train/loss_v1 ▁▁▁
wandb:              train/loss_v2 ▁▁▁
wandb:                   train/lr █▅▁
wandb:             train/nll_loss █▂▁
wandb:           train/nsentences ▁▁▁
wandb:              train/ntokens ▁▄█
wandb:                  train/ppl █▁▁
wandb:          train/sample_size ▁▄█
wandb:       train/sample_size_v1 ▁▁▁
wandb:       train/sample_size_v2 ▁▁▁
wandb:           train/train_wall █▄▁
wandb:                  train/ups ▁▁█
wandb:                 train/wall ▁▅█
wandb:                  train/wpb ▁▅█
wandb:                  train/wps ▁▃█
wandb:            train_inner/bsz ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:           train_inner/clip ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        train_inner/gb_free ▆▇▆▇▄▇▇▆▇▇▁▇▆▆▃▄▅█▄▆▇▅▆▇▇▆▆▇▇▆█▆▇▇▇▇▇▅▂█
wandb:          train_inner/gnorm █▇▆▄▄▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁
wandb:           train_inner/loss █▇▇▆▆▅▅▄▄▃▃▃▃▂▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_inner/loss_scale ▁▁▁▁▁▁▁▂▂▂▂▂▁▁▁▁▁▁▁▂▂▂▄▂▂▁▁▁▂▂▂▄▄▄██▄▄▂▂
wandb:        train_inner/loss_v1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        train_inner/loss_v2 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:             train_inner/lr ▁▃▆███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂
wandb:       train_inner/nll_loss ██▇▇▆▅▅▄▄▃▃▃▃▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂
wandb:     train_inner/nsentences ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        train_inner/ntokens ▆▅▅▄▄▃▃▄▂▅▇▄▅▆▆▃▂▁▂▄▄▄▅▅▅▅▄▄▄▄▃▃▄▂▄▇▄▆█▆
wandb:            train_inner/ppl █▆▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    train_inner/sample_size ▆▅▅▄▄▃▃▄▂▅▇▄▅▆▆▃▂▁▂▄▄▄▅▅▅▅▄▄▄▄▃▃▄▂▄▇▄▆█▆
wandb: train_inner/sample_size_v1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_inner/sample_size_v2 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_inner/train_wall ▅▅▁▁▁▁▁▁▁▁▅▅▅▅▅▁▁▁▁▁▁▅▁▁▅▅▅▁▁▁▁▁▁▁▁▅▁▅█▅
wandb:            train_inner/ups ▆▆▇▇▇█▇▇█▇▆▇▇▆▆▇███▇▇▇▇▇▇▇▆▇█▇██▇█▇▆█▆▁▆
wandb:           train_inner/wall ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            train_inner/wpb ▆▅▅▄▄▃▃▄▂▅▇▄▅▆▆▃▂▁▂▄▄▄▅▅▅▅▄▄▄▄▃▃▄▂▄▇▄▆█▆
wandb:            train_inner/wps ▆▆▆▅▄▄▃▅▃▆█▅▆▆▇▃▃▁▂▅▅▅▆▆▆▆▅▅▅▅▃▃▄▂▄█▅▇▇▇
wandb: 
wandb: Run summary:
wandb:                  train/bsz 28.0
wandb:                 train/clip 0.0
wandb:              train/gb_free 17.4
wandb:                train/gnorm 0.132
wandb:                 train/loss 8.475
wandb:           train/loss_scale 4.0
wandb:              train/loss_v1 0.0
wandb:              train/loss_v2 0.0
wandb:                   train/lr 1e-05
wandb:             train/nll_loss 7.845
wandb:           train/nsentences 27.989
wandb:              train/ntokens 840.529
wandb:                  train/ppl 229.88
wandb:          train/sample_size 840.529
wandb:       train/sample_size_v1 0.0
wandb:       train/sample_size_v2 0.0
wandb:           train/train_wall 1895.0
wandb:                  train/ups 1.18
wandb:                 train/wall 5732.0
wandb:                  train/wpb 840.5
wandb:                  train/wps 989.4
wandb:            train_inner/bsz 28.0
wandb:           train_inner/clip 0.0
wandb:        train_inner/gb_free 17.6
wandb:          train_inner/gnorm 0.133
wandb:           train_inner/loss 8.691
wandb:     train_inner/loss_scale 2.0
wandb:        train_inner/loss_v1 0.0
wandb:        train_inner/loss_v2 0.0
wandb:             train_inner/lr 1e-05
wandb:       train_inner/nll_loss 8.082
wandb:     train_inner/nsentences 28.0
wandb:        train_inner/ntokens 958.1
wandb:            train_inner/ppl 270.97
wandb:    train_inner/sample_size 958.1
wandb: train_inner/sample_size_v1 0.0
wandb: train_inner/sample_size_v2 0.0
wandb:     train_inner/train_wall 9.0
wandb:            train_inner/ups 1.16
wandb:           train_inner/wall 6019.0
wandb:            train_inner/wpb 958.1
wandb:            train_inner/wps 1116.1
wandb: 
wandb: 🚀 View run _4_3e-5_512 at: https://wandb.ai/jackcai1206/OFA-VG/runs/303vy8x0
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230808_160753-303vy8x0/logs
Traceback (most recent call last):
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 850, in _invoke_run
    time.sleep(monitor_interval)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 60, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3215464 got signal: 2

/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-08-02 12:00:42 - utils.py[line:255] - INFO: distributed init (rank 2): env://
2023-08-02 12:00:42 - utils.py[line:261] - INFO: Start init
2023-08-02 12:00:42 - distributed_c10d.py[line:228] - INFO: Added key: store_based_barrier_key:1 to store for rank: 2
2023-08-02 12:00:42 - utils.py[line:255] - INFO: distributed init (rank 0): env://
2023-08-02 12:00:42 - utils.py[line:261] - INFO: Start init
2023-08-02 12:00:42 - distributed_c10d.py[line:228] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-08-02 12:00:42 - utils.py[line:255] - INFO: distributed init (rank 1): env://
2023-08-02 12:00:42 - utils.py[line:261] - INFO: Start init
2023-08-02 12:00:42 - distributed_c10d.py[line:228] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-08-02 12:00:42 - distributed_c10d.py[line:262] - INFO: Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 3 nodes.
2023-08-02 12:00:42 - utils.py[line:271] - INFO: initialized host AMD4RTX3090GPU14 as rank 1
single-machine distributed training is initialized.
2023-08-02 12:00:42 - distributed_c10d.py[line:262] - INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 3 nodes.
2023-08-02 12:00:42 - utils.py[line:271] - INFO: initialized host AMD4RTX3090GPU14 as rank 0
single-machine distributed training is initialized.
2023-08-02 12:00:42 - distributed_c10d.py[line:262] - INFO: Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 3 nodes.
2023-08-02 12:00:42 - utils.py[line:271] - INFO: initialized host AMD4RTX3090GPU14 as rank 2
single-machine distributed training is initialized.
2023-08-02 12:00:43 - train.py[line:77] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './tensorboard/_30_3e-5_512', 'wandb_project': 'OFA-VG', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 2097152, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 3, 'distributed_num_procs': 3, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 3, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 12, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 12, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 30, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [4], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '../../checkpoints/OFA/vrd2_checkpoints/_30_3e-5_512_balanced', 'restore_file': '../../checkpoints/OFA/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 3}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_type_embedding=True, all_gather_list_size=2097152, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=12, batch_size_valid=12, best_checkpoint_metric='loss', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='../../dataset/OFA_data/vrd2_balanced/vg_train_full.tsv,../../dataset/OFA_data/vrd2_balanced/vg_val_full.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=3, distributed_port=-1, distributed_rank=0, distributed_world_size=3, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"max_len_a":0,"max_len_b":200}', eval_print_samples=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=False, freeze_encoder_embedding=False, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=30, max_source_positions=1024, max_src_length=100, max_target_positions=1024, max_tgt_length=100, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=3, num_bins=1000, num_shards=1, num_workers=0, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=512, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='../../checkpoints/OFA/ofa_base.pt', sample_patch_num=196, save_dir='../../checkpoints/OFA/vrd2_checkpoints/_30_3e-5_512_balanced', save_interval=1, save_interval_updates=0, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols=None, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, simul_type=None, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, sync_bn=False, task='vrd2', tensorboard_logdir='./tensorboard/_30_3e-5_512', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[4], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=0, vg_json_dir=None, wandb_project='OFA-VG', warmup_ratio=0.06, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vrd2', 'data': '../../dataset/OFA_data/vrd2_balanced/vg_train_full.tsv,../../dataset/OFA_data/vrd2_balanced/vg_val_full.tsv', 'selected_cols': None, 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 100, 'max_tgt_length': 100, 'code_dict_size': 8192, 'patch_image_size': 512, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'eval_args': '{"beam":5,"max_len_a":0,"max_len_b":200}', 'eval_print_samples': False, 'vg_json_dir': None}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.06, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2023-08-02 12:00:43 - vrd2.py[line:90] - INFO: vrd setup: source dictionary: 59461 types
2023-08-02 12:00:43 - vrd2.py[line:91] - INFO: vrd setup: target dictionary: 59461 types
/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
2023-08-02 12:00:46 - train.py[line:110] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59461, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59461, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59461, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-08-02 12:00:46 - train.py[line:111] - INFO: task: VRD2Task
2023-08-02 12:00:46 - train.py[line:112] - INFO: model: OFAModel
2023-08-02 12:00:46 - train.py[line:113] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-08-02 12:00:46 - train.py[line:114] - INFO: num. shared model params: 182,241,608 (num. trained: 182,241,608)
2023-08-02 12:00:46 - train.py[line:121] - INFO: num. expert model params: 0 (num. trained: 0)
local datafile ../../dataset/OFA_data/vrd2_balanced/vg_val_full.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/vrd2_balanced/vg_val_full.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/vrd2_balanced/vg_val_full.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/vrd2_balanced/vg_val_full.tsv slice_id 0 row count 36215 total row count 108643
/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
local datafile ../../dataset/OFA_data/vrd2_balanced/vg_val_full.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/vrd2_balanced/vg_val_full.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/vrd2_balanced/vg_val_full.tsv slice_id 1 row count 36214 total row count 108643
/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
local datafile ../../dataset/OFA_data/vrd2_balanced/vg_val_full.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/vrd2_balanced/vg_val_full.tsv slice_id 2 row count 36214 total row count 108643
/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
2023-08-02 12:00:46 - distributed_c10d.py[line:228] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2023-08-02 12:00:46 - distributed_c10d.py[line:262] - INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 3 nodes.
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-08-02 12:00:46 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-08-02 12:00:46 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 3 workers***********************
2023-08-02 12:00:46 - utils.py[line:761] - INFO: rank   0: capabilities =  8.6  ; total memory = 23.688 GB ; name = NVIDIA GeForce RTX 3090 Ti              
2023-08-02 12:00:46 - utils.py[line:761] - INFO: rank   1: capabilities =  8.6  ; total memory = 23.688 GB ; name = NVIDIA GeForce RTX 3090 Ti              
2023-08-02 12:00:46 - utils.py[line:761] - INFO: rank   2: capabilities =  8.6  ; total memory = 23.688 GB ; name = NVIDIA GeForce RTX 3090 Ti              
2023-08-02 12:00:46 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 3 workers***********************
2023-08-02 12:00:46 - train.py[line:152] - INFO: training on 3 devices (GPUs/TPUs)
2023-08-02 12:00:46 - train.py[line:157] - INFO: max tokens per device = None and max sentences per device = 12
2023-08-02 12:00:46 - trainer.py[line:458] - INFO: Preparing to load checkpoint ../../checkpoints/OFA/ofa_base.pt
2023-08-02 12:00:48 - trainer.py[line:617] - INFO: Loaded checkpoint ../../checkpoints/OFA/ofa_base.pt (epoch 48 @ 0 updates)
2023-08-02 12:00:48 - trainer.py[line:639] - INFO: loading train data for epoch 1
local datafile ../../dataset/OFA_data/vrd2_balanced/vg_train_full.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/vrd2_balanced/vg_train_full.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/vrd2_balanced/vg_train_full.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/vrd2_balanced/vg_train_full.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/vrd2_balanced/vg_train_full.tsv slice_id 0 row count 268254 total row count 804762
local datafile ../../dataset/OFA_data/vrd2_balanced/vg_train_full.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/vrd2_balanced/vg_train_full.tsv slice_id 1 row count 268254 total row count 804762
local datafile ../../dataset/OFA_data/vrd2_balanced/vg_train_full.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/vrd2_balanced/vg_train_full.tsv slice_id 2 row count 268254 total row count 804762
slice_id 1 seek offset 268254
Total steps 167670, warmup steps 10060, warmup_factor 9.940357852882704e-05
slice_id 0 seek offset 0
slice_id 2 seek offset 536508
Total steps 167670, warmup steps 10060, warmup_factor 9.940357852882704e-05
Total steps 167670, warmup steps 10060, warmup_factor 9.940357852882704e-05
wandb: Currently logged in as: jackcai1206. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /home/zcai75/Github/OFA_forked/run_scripts/vrd_2/wandb/run-20230802_120050-17tr21jj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run _30_3e-5_512_balanced
wandb: ⭐️ View project at https://wandb.ai/jackcai1206/OFA-VG
wandb: 🚀 View run at https://wandb.ai/jackcai1206/OFA-VG/runs/17tr21jj
2023-08-02 12:00:56 - trainer.py[line:703] - INFO: begin training epoch 1
2023-08-02 12:00:56 - train.py[line:305] - INFO: Start iterating over samples
2023-08-02 12:01:00 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-02 12:01:03 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-02 12:01:05 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-02 12:01:08 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-02 12:01:33 - progress_bar.py[line:272] - INFO: epoch 001:     14 / 5589 loss=12.442, loss_v1=0, loss_v2=0, nll_loss=11.772, ntokens=1058.8, nsentences=144, sample_size=1058.8, sample_size_v1=0, sample_size_v2=0, ppl=3496.47, wps=430.8, ups=0.41, wpb=1058.8, bsz=144, num_updates=10, lr=2.98211e-08, gnorm=41.465, clip=100, loss_scale=8, train_wall=37, gb_free=6.6, wall=47
2023-08-02 12:01:57 - progress_bar.py[line:272] - INFO: epoch 001:     24 / 5589 loss=12.538, loss_v1=0, loss_v2=0, nll_loss=11.876, ntokens=1055.1, nsentences=144, sample_size=1055.1, sample_size_v1=0, sample_size_v2=0, ppl=3758.26, wps=428.3, ups=0.41, wpb=1055.1, bsz=144, num_updates=20, lr=5.96421e-08, gnorm=41.847, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=72
2023-08-02 12:02:22 - progress_bar.py[line:272] - INFO: epoch 001:     34 / 5589 loss=12.494, loss_v1=0, loss_v2=0, nll_loss=11.83, ntokens=1053.9, nsentences=144, sample_size=1053.9, sample_size_v1=0, sample_size_v2=0, ppl=3641.39, wps=427.8, ups=0.41, wpb=1053.9, bsz=144, num_updates=30, lr=8.94632e-08, gnorm=41.246, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=96
2023-08-02 12:02:47 - progress_bar.py[line:272] - INFO: epoch 001:     44 / 5589 loss=12.45, loss_v1=0, loss_v2=0, nll_loss=11.776, ntokens=1058.3, nsentences=144, sample_size=1058.3, sample_size_v1=0, sample_size_v2=0, ppl=3507.39, wps=429.2, ups=0.41, wpb=1058.3, bsz=144, num_updates=40, lr=1.19284e-07, gnorm=40.787, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=121
2023-08-02 12:03:11 - progress_bar.py[line:272] - INFO: epoch 001:     54 / 5589 loss=12.445, loss_v1=0, loss_v2=0, nll_loss=11.772, ntokens=1056.2, nsentences=144, sample_size=1056.2, sample_size_v1=0, sample_size_v2=0, ppl=3497.88, wps=428, ups=0.41, wpb=1056.2, bsz=144, num_updates=50, lr=1.49105e-07, gnorm=41.504, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=146
2023-08-02 12:03:36 - progress_bar.py[line:272] - INFO: epoch 001:     64 / 5589 loss=12.472, loss_v1=0, loss_v2=0, nll_loss=11.803, ntokens=1053.5, nsentences=144, sample_size=1053.5, sample_size_v1=0, sample_size_v2=0, ppl=3574.15, wps=426.5, ups=0.4, wpb=1053.5, bsz=144, num_updates=60, lr=1.78926e-07, gnorm=41.148, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=170
2023-08-02 12:04:01 - progress_bar.py[line:272] - INFO: epoch 001:     74 / 5589 loss=12.428, loss_v1=0, loss_v2=0, nll_loss=11.762, ntokens=1052.7, nsentences=144, sample_size=1052.7, sample_size_v1=0, sample_size_v2=0, ppl=3473.73, wps=426.4, ups=0.41, wpb=1052.7, bsz=144, num_updates=70, lr=2.08748e-07, gnorm=40.191, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=195
2023-08-02 12:04:25 - progress_bar.py[line:272] - INFO: epoch 001:     84 / 5589 loss=12.284, loss_v1=0, loss_v2=0, nll_loss=11.614, ntokens=1056.2, nsentences=144, sample_size=1056.2, sample_size_v1=0, sample_size_v2=0, ppl=3134.84, wps=428, ups=0.41, wpb=1056.2, bsz=144, num_updates=80, lr=2.38569e-07, gnorm=39.265, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=220
2023-08-02 12:04:50 - progress_bar.py[line:272] - INFO: epoch 001:     94 / 5589 loss=12.162, loss_v1=0, loss_v2=0, nll_loss=11.493, ntokens=1059.6, nsentences=144, sample_size=1059.6, sample_size_v1=0, sample_size_v2=0, ppl=2881.93, wps=429.7, ups=0.41, wpb=1059.6, bsz=144, num_updates=90, lr=2.6839e-07, gnorm=38.246, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=244
2023-08-02 12:05:15 - progress_bar.py[line:272] - INFO: epoch 001:    104 / 5589 loss=12.14, loss_v1=0, loss_v2=0, nll_loss=11.479, ntokens=1054.2, nsentences=144, sample_size=1054.2, sample_size_v1=0, sample_size_v2=0, ppl=2853.71, wps=427.5, ups=0.41, wpb=1054.2, bsz=144, num_updates=100, lr=2.98211e-07, gnorm=37.911, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=269
2023-08-02 12:05:39 - progress_bar.py[line:272] - INFO: epoch 001:    114 / 5589 loss=11.808, loss_v1=0, loss_v2=0, nll_loss=11.134, ntokens=1053.8, nsentences=144, sample_size=1053.8, sample_size_v1=0, sample_size_v2=0, ppl=2247.98, wps=427.6, ups=0.41, wpb=1053.8, bsz=144, num_updates=110, lr=3.28032e-07, gnorm=34.537, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=294
2023-08-02 12:06:04 - progress_bar.py[line:272] - INFO: epoch 001:    124 / 5589 loss=11.607, loss_v1=0, loss_v2=0, nll_loss=10.933, ntokens=1057.8, nsentences=144, sample_size=1057.8, sample_size_v1=0, sample_size_v2=0, ppl=1954.9, wps=429.2, ups=0.41, wpb=1057.8, bsz=144, num_updates=120, lr=3.57853e-07, gnorm=33.046, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=318
2023-08-02 12:06:29 - progress_bar.py[line:272] - INFO: epoch 001:    134 / 5589 loss=11.47, loss_v1=0, loss_v2=0, nll_loss=10.798, ntokens=1053.8, nsentences=144, sample_size=1053.8, sample_size_v1=0, sample_size_v2=0, ppl=1780.96, wps=427.6, ups=0.41, wpb=1053.8, bsz=144, num_updates=130, lr=3.87674e-07, gnorm=31.861, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=343
2023-08-02 12:06:53 - progress_bar.py[line:272] - INFO: epoch 001:    144 / 5589 loss=11.272, loss_v1=0, loss_v2=0, nll_loss=10.593, ntokens=1060.5, nsentences=144, sample_size=1060.5, sample_size_v1=0, sample_size_v2=0, ppl=1544.31, wps=430.1, ups=0.41, wpb=1060.5, bsz=144, num_updates=140, lr=4.17495e-07, gnorm=30.756, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=368
2023-08-02 12:07:18 - progress_bar.py[line:272] - INFO: epoch 001:    154 / 5589 loss=10.944, loss_v1=0, loss_v2=0, nll_loss=10.269, ntokens=1056.4, nsentences=144, sample_size=1056.4, sample_size_v1=0, sample_size_v2=0, ppl=1233.74, wps=428.6, ups=0.41, wpb=1056.4, bsz=144, num_updates=150, lr=4.47316e-07, gnorm=28.471, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=392
2023-08-02 12:07:43 - progress_bar.py[line:272] - INFO: epoch 001:    164 / 5589 loss=10.556, loss_v1=0, loss_v2=0, nll_loss=9.88, ntokens=1054.7, nsentences=144, sample_size=1054.7, sample_size_v1=0, sample_size_v2=0, ppl=942.19, wps=427.8, ups=0.41, wpb=1054.7, bsz=144, num_updates=160, lr=4.77137e-07, gnorm=25.743, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=417
2023-08-02 12:08:07 - progress_bar.py[line:272] - INFO: epoch 001:    174 / 5589 loss=10.311, loss_v1=0, loss_v2=0, nll_loss=9.634, ntokens=1057.5, nsentences=144, sample_size=1057.5, sample_size_v1=0, sample_size_v2=0, ppl=794.34, wps=428.1, ups=0.4, wpb=1057.5, bsz=144, num_updates=170, lr=5.06958e-07, gnorm=24.283, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=442
2023-08-02 12:08:32 - progress_bar.py[line:272] - INFO: epoch 001:    184 / 5589 loss=10.085, loss_v1=0, loss_v2=0, nll_loss=9.408, ntokens=1055, nsentences=144, sample_size=1055, sample_size_v1=0, sample_size_v2=0, ppl=679.37, wps=427.9, ups=0.41, wpb=1055, bsz=144, num_updates=180, lr=5.36779e-07, gnorm=22.944, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=466
2023-08-02 12:08:57 - progress_bar.py[line:272] - INFO: epoch 001:    194 / 5589 loss=9.835, loss_v1=0, loss_v2=0, nll_loss=9.151, ntokens=1056, nsentences=144, sample_size=1056, sample_size_v1=0, sample_size_v2=0, ppl=568.66, wps=428, ups=0.41, wpb=1056, bsz=144, num_updates=190, lr=5.666e-07, gnorm=21.392, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=491
2023-08-02 12:09:21 - progress_bar.py[line:272] - INFO: epoch 001:    204 / 5589 loss=9.596, loss_v1=0, loss_v2=0, nll_loss=8.907, ntokens=1057.1, nsentences=144, sample_size=1057.1, sample_size_v1=0, sample_size_v2=0, ppl=479.89, wps=429, ups=0.41, wpb=1057.1, bsz=144, num_updates=200, lr=5.96421e-07, gnorm=20.424, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=516
2023-08-02 12:09:46 - progress_bar.py[line:272] - INFO: epoch 001:    214 / 5589 loss=9.356, loss_v1=0, loss_v2=0, nll_loss=8.674, ntokens=1054.6, nsentences=144, sample_size=1054.6, sample_size_v1=0, sample_size_v2=0, ppl=408.39, wps=428.1, ups=0.41, wpb=1054.6, bsz=144, num_updates=210, lr=6.26243e-07, gnorm=19.537, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=540
2023-08-02 12:10:11 - progress_bar.py[line:272] - INFO: epoch 001:    224 / 5589 loss=9.083, loss_v1=0, loss_v2=0, nll_loss=8.389, ntokens=1057.2, nsentences=144, sample_size=1057.2, sample_size_v1=0, sample_size_v2=0, ppl=335.34, wps=429.3, ups=0.41, wpb=1057.2, bsz=144, num_updates=220, lr=6.56064e-07, gnorm=18.632, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=565
2023-08-02 12:10:35 - progress_bar.py[line:272] - INFO: epoch 001:    234 / 5589 loss=8.756, loss_v1=0, loss_v2=0, nll_loss=8.054, ntokens=1054.9, nsentences=144, sample_size=1054.9, sample_size_v1=0, sample_size_v2=0, ppl=265.74, wps=428.2, ups=0.41, wpb=1054.9, bsz=144, num_updates=230, lr=6.85885e-07, gnorm=18.003, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=589
2023-08-02 12:11:00 - progress_bar.py[line:272] - INFO: epoch 001:    244 / 5589 loss=8.468, loss_v1=0, loss_v2=0, nll_loss=7.756, ntokens=1054.3, nsentences=144, sample_size=1054.3, sample_size_v1=0, sample_size_v2=0, ppl=216.16, wps=428, ups=0.41, wpb=1054.3, bsz=144, num_updates=240, lr=7.15706e-07, gnorm=17.693, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=614
2023-08-02 12:11:25 - progress_bar.py[line:272] - INFO: epoch 001:    254 / 5589 loss=8.156, loss_v1=0, loss_v2=0, nll_loss=7.427, ntokens=1056.3, nsentences=144, sample_size=1056.3, sample_size_v1=0, sample_size_v2=0, ppl=172.05, wps=428.2, ups=0.41, wpb=1056.3, bsz=144, num_updates=250, lr=7.45527e-07, gnorm=16.934, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=639
2023-08-02 12:11:49 - progress_bar.py[line:272] - INFO: epoch 001:    264 / 5589 loss=7.912, loss_v1=0, loss_v2=0, nll_loss=7.164, ntokens=1055.4, nsentences=144, sample_size=1055.4, sample_size_v1=0, sample_size_v2=0, ppl=143.45, wps=427.3, ups=0.4, wpb=1055.4, bsz=144, num_updates=260, lr=7.75348e-07, gnorm=16.147, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=663
2023-08-02 12:12:14 - progress_bar.py[line:272] - INFO: epoch 001:    274 / 5589 loss=7.687, loss_v1=0, loss_v2=0, nll_loss=6.926, ntokens=1053, nsentences=144, sample_size=1053, sample_size_v1=0, sample_size_v2=0, ppl=121.64, wps=426.1, ups=0.4, wpb=1053, bsz=144, num_updates=270, lr=8.05169e-07, gnorm=15.063, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=688
2023-08-02 12:12:39 - progress_bar.py[line:272] - INFO: epoch 001:    284 / 5589 loss=7.412, loss_v1=0, loss_v2=0, nll_loss=6.638, ntokens=1055.9, nsentences=144, sample_size=1055.9, sample_size_v1=0, sample_size_v2=0, ppl=99.57, wps=427.8, ups=0.41, wpb=1055.9, bsz=144, num_updates=280, lr=8.3499e-07, gnorm=13.67, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=713
2023-08-02 12:13:03 - progress_bar.py[line:272] - INFO: epoch 001:    294 / 5589 loss=7.182, loss_v1=0, loss_v2=0, nll_loss=6.39, ntokens=1051.9, nsentences=144, sample_size=1051.9, sample_size_v1=0, sample_size_v2=0, ppl=83.85, wps=426.1, ups=0.41, wpb=1051.9, bsz=144, num_updates=290, lr=8.64811e-07, gnorm=12.357, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=738
2023-08-02 12:13:28 - progress_bar.py[line:272] - INFO: epoch 001:    304 / 5589 loss=6.925, loss_v1=0, loss_v2=0, nll_loss=6.115, ntokens=1055.7, nsentences=144, sample_size=1055.7, sample_size_v1=0, sample_size_v2=0, ppl=69.33, wps=427.5, ups=0.4, wpb=1055.7, bsz=144, num_updates=300, lr=8.94632e-07, gnorm=11.615, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=762
2023-08-02 12:13:53 - progress_bar.py[line:272] - INFO: epoch 001:    314 / 5589 loss=6.694, loss_v1=0, loss_v2=0, nll_loss=5.87, ntokens=1056.5, nsentences=144, sample_size=1056.5, sample_size_v1=0, sample_size_v2=0, ppl=58.48, wps=428, ups=0.41, wpb=1056.5, bsz=144, num_updates=310, lr=9.24453e-07, gnorm=11.142, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=787
2023-08-02 12:14:17 - progress_bar.py[line:272] - INFO: epoch 001:    324 / 5589 loss=6.458, loss_v1=0, loss_v2=0, nll_loss=5.619, ntokens=1055.5, nsentences=144, sample_size=1055.5, sample_size_v1=0, sample_size_v2=0, ppl=49.15, wps=427.4, ups=0.4, wpb=1055.5, bsz=144, num_updates=320, lr=9.54274e-07, gnorm=10.164, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=812
2023-08-02 12:14:42 - progress_bar.py[line:272] - INFO: epoch 001:    334 / 5589 loss=6.222, loss_v1=0, loss_v2=0, nll_loss=5.353, ntokens=1061.1, nsentences=144, sample_size=1061.1, sample_size_v1=0, sample_size_v2=0, ppl=40.88, wps=430.8, ups=0.41, wpb=1061.1, bsz=144, num_updates=330, lr=9.84095e-07, gnorm=9.525, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=836
2023-08-02 12:15:07 - progress_bar.py[line:272] - INFO: epoch 001:    344 / 5589 loss=6.022, loss_v1=0, loss_v2=0, nll_loss=5.127, ntokens=1054.5, nsentences=144, sample_size=1054.5, sample_size_v1=0, sample_size_v2=0, ppl=34.93, wps=428.1, ups=0.41, wpb=1054.5, bsz=144, num_updates=340, lr=1.01392e-06, gnorm=8.911, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=861
2023-08-02 12:15:31 - progress_bar.py[line:272] - INFO: epoch 001:    354 / 5589 loss=5.796, loss_v1=0, loss_v2=0, nll_loss=4.87, ntokens=1057.1, nsentences=144, sample_size=1057.1, sample_size_v1=0, sample_size_v2=0, ppl=29.24, wps=429.4, ups=0.41, wpb=1057.1, bsz=144, num_updates=350, lr=1.04374e-06, gnorm=8.04, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=886
2023-08-02 12:15:56 - progress_bar.py[line:272] - INFO: epoch 001:    364 / 5589 loss=5.639, loss_v1=0, loss_v2=0, nll_loss=4.676, ntokens=1053.8, nsentences=144, sample_size=1053.8, sample_size_v1=0, sample_size_v2=0, ppl=25.56, wps=427.7, ups=0.41, wpb=1053.8, bsz=144, num_updates=360, lr=1.07356e-06, gnorm=7.365, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=910
2023-08-02 12:16:21 - progress_bar.py[line:272] - INFO: epoch 001:    374 / 5589 loss=5.479, loss_v1=0, loss_v2=0, nll_loss=4.476, ntokens=1054, nsentences=144, sample_size=1054, sample_size_v1=0, sample_size_v2=0, ppl=22.25, wps=428, ups=0.41, wpb=1054, bsz=144, num_updates=370, lr=1.10338e-06, gnorm=7.061, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=935
2023-08-02 12:16:45 - progress_bar.py[line:272] - INFO: epoch 001:    384 / 5589 loss=5.33, loss_v1=0, loss_v2=0, nll_loss=4.302, ntokens=1055.4, nsentences=144, sample_size=1055.4, sample_size_v1=0, sample_size_v2=0, ppl=19.72, wps=428.6, ups=0.41, wpb=1055.4, bsz=144, num_updates=380, lr=1.1332e-06, gnorm=6.639, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=959
2023-08-02 12:17:10 - progress_bar.py[line:272] - INFO: epoch 001:    394 / 5589 loss=5.219, loss_v1=0, loss_v2=0, nll_loss=4.158, ntokens=1057.8, nsentences=144, sample_size=1057.8, sample_size_v1=0, sample_size_v2=0, ppl=17.85, wps=428.5, ups=0.41, wpb=1057.8, bsz=144, num_updates=390, lr=1.16302e-06, gnorm=6.231, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=984
2023-08-02 12:17:35 - progress_bar.py[line:272] - INFO: epoch 001:    404 / 5589 loss=5.112, loss_v1=0, loss_v2=0, nll_loss=4.024, ntokens=1058.4, nsentences=144, sample_size=1058.4, sample_size_v1=0, sample_size_v2=0, ppl=16.26, wps=428.9, ups=0.41, wpb=1058.4, bsz=144, num_updates=400, lr=1.19284e-06, gnorm=5.995, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=1009
2023-08-02 12:17:59 - progress_bar.py[line:272] - INFO: epoch 001:    414 / 5589 loss=5.01, loss_v1=0, loss_v2=0, nll_loss=3.904, ntokens=1055.5, nsentences=144, sample_size=1055.5, sample_size_v1=0, sample_size_v2=0, ppl=14.97, wps=428.1, ups=0.41, wpb=1055.5, bsz=144, num_updates=410, lr=1.22266e-06, gnorm=5.905, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=1033
2023-08-02 12:18:24 - progress_bar.py[line:272] - INFO: epoch 001:    424 / 5589 loss=4.972, loss_v1=0, loss_v2=0, nll_loss=3.848, ntokens=1055.7, nsentences=144, sample_size=1055.7, sample_size_v1=0, sample_size_v2=0, ppl=14.4, wps=427.5, ups=0.4, wpb=1055.7, bsz=144, num_updates=420, lr=1.25249e-06, gnorm=5.864, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=1058
2023-08-02 12:18:49 - progress_bar.py[line:272] - INFO: epoch 001:    434 / 5589 loss=4.851, loss_v1=0, loss_v2=0, nll_loss=3.693, ntokens=1055.1, nsentences=144, sample_size=1055.1, sample_size_v1=0, sample_size_v2=0, ppl=12.93, wps=427.5, ups=0.41, wpb=1055.1, bsz=144, num_updates=430, lr=1.28231e-06, gnorm=5.864, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=1083
2023-08-02 12:19:13 - progress_bar.py[line:272] - INFO: epoch 001:    444 / 5589 loss=4.781, loss_v1=0, loss_v2=0, nll_loss=3.606, ntokens=1055.4, nsentences=144, sample_size=1055.4, sample_size_v1=0, sample_size_v2=0, ppl=12.17, wps=427.7, ups=0.41, wpb=1055.4, bsz=144, num_updates=440, lr=1.31213e-06, gnorm=5.831, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=1108
2023-08-02 12:19:38 - progress_bar.py[line:272] - INFO: epoch 001:    454 / 5589 loss=4.722, loss_v1=0, loss_v2=0, nll_loss=3.545, ntokens=1050.4, nsentences=144, sample_size=1050.4, sample_size_v1=0, sample_size_v2=0, ppl=11.67, wps=425.4, ups=0.4, wpb=1050.4, bsz=144, num_updates=450, lr=1.34195e-06, gnorm=5.898, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=1132
2023-08-02 12:20:03 - progress_bar.py[line:272] - INFO: epoch 001:    464 / 5589 loss=4.65, loss_v1=0, loss_v2=0, nll_loss=3.443, ntokens=1055.6, nsentences=144, sample_size=1055.6, sample_size_v1=0, sample_size_v2=0, ppl=10.87, wps=427.5, ups=0.4, wpb=1055.6, bsz=144, num_updates=460, lr=1.37177e-06, gnorm=5.891, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=1157
2023-08-02 12:20:27 - progress_bar.py[line:272] - INFO: epoch 001:    474 / 5589 loss=4.619, loss_v1=0, loss_v2=0, nll_loss=3.404, ntokens=1060, nsentences=144, sample_size=1060, sample_size_v1=0, sample_size_v2=0, ppl=10.59, wps=429.2, ups=0.4, wpb=1060, bsz=144, num_updates=470, lr=1.40159e-06, gnorm=6.054, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=1182
2023-08-02 12:20:52 - progress_bar.py[line:272] - INFO: epoch 001:    484 / 5589 loss=4.526, loss_v1=0, loss_v2=0, nll_loss=3.289, ntokens=1053.2, nsentences=144, sample_size=1053.2, sample_size_v1=0, sample_size_v2=0, ppl=9.77, wps=426.4, ups=0.4, wpb=1053.2, bsz=144, num_updates=480, lr=1.43141e-06, gnorm=6.183, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=1206
2023-08-02 12:21:17 - progress_bar.py[line:272] - INFO: epoch 001:    494 / 5589 loss=4.478, loss_v1=0, loss_v2=0, nll_loss=3.238, ntokens=1055.4, nsentences=144, sample_size=1055.4, sample_size_v1=0, sample_size_v2=0, ppl=9.44, wps=427.3, ups=0.4, wpb=1055.4, bsz=144, num_updates=490, lr=1.46123e-06, gnorm=6.285, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=1231
2023-08-02 12:21:41 - progress_bar.py[line:272] - INFO: epoch 001:    504 / 5589 loss=4.435, loss_v1=0, loss_v2=0, nll_loss=3.165, ntokens=1056.8, nsentences=144, sample_size=1056.8, sample_size_v1=0, sample_size_v2=0, ppl=8.97, wps=427.8, ups=0.4, wpb=1056.8, bsz=144, num_updates=500, lr=1.49105e-06, gnorm=6.653, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=1256
2023-08-02 12:22:06 - progress_bar.py[line:272] - INFO: epoch 001:    514 / 5589 loss=4.369, loss_v1=0, loss_v2=0, nll_loss=3.097, ntokens=1053.9, nsentences=144, sample_size=1053.9, sample_size_v1=0, sample_size_v2=0, ppl=8.56, wps=426.8, ups=0.4, wpb=1053.9, bsz=144, num_updates=510, lr=1.52087e-06, gnorm=6.634, clip=100, loss_scale=8, train_wall=25, gb_free=6.6, wall=1280
2023-08-02 12:22:31 - progress_bar.py[line:272] - INFO: epoch 001:    524 / 5589 loss=4.325, loss_v1=0, loss_v2=0, nll_loss=3.046, ntokens=1058.6, nsentences=144, sample_size=1058.6, sample_size_v1=0, sample_size_v2=0, ppl=8.26, wps=429.4, ups=0.41, wpb=1058.6, bsz=144, num_updates=520, lr=1.5507e-06, gnorm=6.65, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=1305
2023-08-02 12:22:55 - progress_bar.py[line:272] - INFO: epoch 001:    534 / 5589 loss=4.278, loss_v1=0, loss_v2=0, nll_loss=2.982, ntokens=1056.4, nsentences=144, sample_size=1056.4, sample_size_v1=0, sample_size_v2=0, ppl=7.9, wps=428.3, ups=0.41, wpb=1056.4, bsz=144, num_updates=530, lr=1.58052e-06, gnorm=6.955, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=1330
2023-08-02 12:23:20 - progress_bar.py[line:272] - INFO: epoch 001:    544 / 5589 loss=4.218, loss_v1=0, loss_v2=0, nll_loss=2.91, ntokens=1051.8, nsentences=144, sample_size=1051.8, sample_size_v1=0, sample_size_v2=0, ppl=7.51, wps=426.1, ups=0.41, wpb=1051.8, bsz=144, num_updates=540, lr=1.61034e-06, gnorm=7.145, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=1354
2023-08-02 12:23:45 - progress_bar.py[line:272] - INFO: epoch 001:    554 / 5589 loss=4.182, loss_v1=0, loss_v2=0, nll_loss=2.875, ntokens=1060, nsentences=144, sample_size=1060, sample_size_v1=0, sample_size_v2=0, ppl=7.33, wps=429.5, ups=0.41, wpb=1060, bsz=144, num_updates=550, lr=1.64016e-06, gnorm=7.029, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=1379
2023-08-02 12:24:10 - progress_bar.py[line:272] - INFO: epoch 001:    564 / 5589 loss=4.122, loss_v1=0, loss_v2=0, nll_loss=2.806, ntokens=1058, nsentences=144, sample_size=1058, sample_size_v1=0, sample_size_v2=0, ppl=6.99, wps=429.3, ups=0.41, wpb=1058, bsz=144, num_updates=560, lr=1.66998e-06, gnorm=7.149, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=1404
2023-08-02 12:24:34 - progress_bar.py[line:272] - INFO: epoch 001:    574 / 5589 loss=4.093, loss_v1=0, loss_v2=0, nll_loss=2.775, ntokens=1054.4, nsentences=144, sample_size=1054.4, sample_size_v1=0, sample_size_v2=0, ppl=6.85, wps=427.7, ups=0.41, wpb=1054.4, bsz=144, num_updates=570, lr=1.6998e-06, gnorm=7.241, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=1428
2023-08-02 12:24:59 - progress_bar.py[line:272] - INFO: epoch 001:    584 / 5589 loss=4.034, loss_v1=0, loss_v2=0, nll_loss=2.717, ntokens=1055.6, nsentences=144, sample_size=1055.6, sample_size_v1=0, sample_size_v2=0, ppl=6.58, wps=428.4, ups=0.41, wpb=1055.6, bsz=144, num_updates=580, lr=1.72962e-06, gnorm=7.29, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=1453
2023-08-02 12:25:23 - progress_bar.py[line:272] - INFO: epoch 001:    594 / 5589 loss=3.995, loss_v1=0, loss_v2=0, nll_loss=2.67, ntokens=1057, nsentences=144, sample_size=1057, sample_size_v1=0, sample_size_v2=0, ppl=6.37, wps=428.8, ups=0.41, wpb=1057, bsz=144, num_updates=590, lr=1.75944e-06, gnorm=7.274, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=1478
2023-08-02 12:25:48 - progress_bar.py[line:272] - INFO: epoch 001:    604 / 5589 loss=3.971, loss_v1=0, loss_v2=0, nll_loss=2.645, ntokens=1053.7, nsentences=144, sample_size=1053.7, sample_size_v1=0, sample_size_v2=0, ppl=6.25, wps=427.4, ups=0.41, wpb=1053.7, bsz=144, num_updates=600, lr=1.78926e-06, gnorm=7.391, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=1502
2023-08-02 12:26:13 - progress_bar.py[line:272] - INFO: epoch 001:    614 / 5589 loss=3.914, loss_v1=0, loss_v2=0, nll_loss=2.586, ntokens=1059.6, nsentences=144, sample_size=1059.6, sample_size_v1=0, sample_size_v2=0, ppl=6, wps=430, ups=0.41, wpb=1059.6, bsz=144, num_updates=610, lr=1.81909e-06, gnorm=7.324, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=1527
2023-08-02 12:26:37 - progress_bar.py[line:272] - INFO: epoch 001:    624 / 5589 loss=3.883, loss_v1=0, loss_v2=0, nll_loss=2.554, ntokens=1056.2, nsentences=144, sample_size=1056.2, sample_size_v1=0, sample_size_v2=0, ppl=5.87, wps=428.7, ups=0.41, wpb=1056.2, bsz=144, num_updates=620, lr=1.84891e-06, gnorm=7.389, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=1552
2023-08-02 12:27:02 - progress_bar.py[line:272] - INFO: epoch 001:    634 / 5589 loss=3.842, loss_v1=0, loss_v2=0, nll_loss=2.514, ntokens=1057.6, nsentences=144, sample_size=1057.6, sample_size_v1=0, sample_size_v2=0, ppl=5.71, wps=429.1, ups=0.41, wpb=1057.6, bsz=144, num_updates=630, lr=1.87873e-06, gnorm=7.36, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=1576
2023-08-02 12:27:27 - progress_bar.py[line:272] - INFO: epoch 001:    644 / 5589 loss=3.811, loss_v1=0, loss_v2=0, nll_loss=2.479, ntokens=1053, nsentences=144, sample_size=1053, sample_size_v1=0, sample_size_v2=0, ppl=5.58, wps=427.2, ups=0.41, wpb=1053, bsz=144, num_updates=640, lr=1.90855e-06, gnorm=7.393, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=1601
2023-08-02 12:27:51 - progress_bar.py[line:272] - INFO: epoch 001:    654 / 5589 loss=3.774, loss_v1=0, loss_v2=0, nll_loss=2.447, ntokens=1056.1, nsentences=144, sample_size=1056.1, sample_size_v1=0, sample_size_v2=0, ppl=5.45, wps=428.6, ups=0.41, wpb=1056.1, bsz=144, num_updates=650, lr=1.93837e-06, gnorm=7.417, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=1626
2023-08-02 12:28:16 - progress_bar.py[line:272] - INFO: epoch 001:    664 / 5589 loss=3.732, loss_v1=0, loss_v2=0, nll_loss=2.402, ntokens=1057.6, nsentences=144, sample_size=1057.6, sample_size_v1=0, sample_size_v2=0, ppl=5.29, wps=429.1, ups=0.41, wpb=1057.6, bsz=144, num_updates=660, lr=1.96819e-06, gnorm=7.355, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=1650
2023-08-02 12:28:41 - progress_bar.py[line:272] - INFO: epoch 001:    674 / 5589 loss=3.688, loss_v1=0, loss_v2=0, nll_loss=2.354, ntokens=1057.5, nsentences=144, sample_size=1057.5, sample_size_v1=0, sample_size_v2=0, ppl=5.11, wps=429.1, ups=0.41, wpb=1057.5, bsz=144, num_updates=670, lr=1.99801e-06, gnorm=7.384, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=1675
2023-08-02 12:29:05 - progress_bar.py[line:272] - INFO: epoch 001:    684 / 5589 loss=3.635, loss_v1=0, loss_v2=0, nll_loss=2.29, ntokens=1058.9, nsentences=144, sample_size=1058.9, sample_size_v1=0, sample_size_v2=0, ppl=4.89, wps=429.1, ups=0.41, wpb=1058.9, bsz=144, num_updates=680, lr=2.02783e-06, gnorm=7.358, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=1700
2023-08-02 12:29:30 - progress_bar.py[line:272] - INFO: epoch 001:    694 / 5589 loss=3.623, loss_v1=0, loss_v2=0, nll_loss=2.293, ntokens=1058.3, nsentences=144, sample_size=1058.3, sample_size_v1=0, sample_size_v2=0, ppl=4.9, wps=428.6, ups=0.41, wpb=1058.3, bsz=144, num_updates=690, lr=2.05765e-06, gnorm=7.378, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=1724
2023-08-02 12:29:55 - progress_bar.py[line:272] - INFO: epoch 001:    704 / 5589 loss=3.585, loss_v1=0, loss_v2=0, nll_loss=2.251, ntokens=1052.1, nsentences=144, sample_size=1052.1, sample_size_v1=0, sample_size_v2=0, ppl=4.76, wps=426.8, ups=0.41, wpb=1052.1, bsz=144, num_updates=700, lr=2.08748e-06, gnorm=7.391, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=1749
2023-08-02 12:30:19 - progress_bar.py[line:272] - INFO: epoch 001:    714 / 5589 loss=3.572, loss_v1=0, loss_v2=0, nll_loss=2.237, ntokens=1053.4, nsentences=144, sample_size=1053.4, sample_size_v1=0, sample_size_v2=0, ppl=4.71, wps=427.4, ups=0.41, wpb=1053.4, bsz=144, num_updates=710, lr=2.1173e-06, gnorm=7.357, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=1774
2023-08-02 12:30:44 - progress_bar.py[line:272] - INFO: epoch 001:    724 / 5589 loss=3.526, loss_v1=0, loss_v2=0, nll_loss=2.188, ntokens=1056.8, nsentences=144, sample_size=1056.8, sample_size_v1=0, sample_size_v2=0, ppl=4.56, wps=428.7, ups=0.41, wpb=1056.8, bsz=144, num_updates=720, lr=2.14712e-06, gnorm=7.34, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=1798
2023-08-02 12:31:09 - progress_bar.py[line:272] - INFO: epoch 001:    734 / 5589 loss=3.505, loss_v1=0, loss_v2=0, nll_loss=2.177, ntokens=1054.1, nsentences=144, sample_size=1054.1, sample_size_v1=0, sample_size_v2=0, ppl=4.52, wps=427.8, ups=0.41, wpb=1054.1, bsz=144, num_updates=730, lr=2.17694e-06, gnorm=7.337, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=1823
2023-08-02 12:31:33 - progress_bar.py[line:272] - INFO: epoch 001:    744 / 5589 loss=3.448, loss_v1=0, loss_v2=0, nll_loss=2.109, ntokens=1054.9, nsentences=144, sample_size=1054.9, sample_size_v1=0, sample_size_v2=0, ppl=4.31, wps=427.8, ups=0.41, wpb=1054.9, bsz=144, num_updates=740, lr=2.20676e-06, gnorm=7.342, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=1848
2023-08-02 12:31:58 - progress_bar.py[line:272] - INFO: epoch 001:    754 / 5589 loss=3.428, loss_v1=0, loss_v2=0, nll_loss=2.091, ntokens=1056, nsentences=144, sample_size=1056, sample_size_v1=0, sample_size_v2=0, ppl=4.26, wps=428, ups=0.41, wpb=1056, bsz=144, num_updates=750, lr=2.23658e-06, gnorm=7.247, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=1872
2023-08-02 12:32:23 - progress_bar.py[line:272] - INFO: epoch 001:    764 / 5589 loss=3.4, loss_v1=0, loss_v2=0, nll_loss=2.067, ntokens=1057.8, nsentences=144, sample_size=1057.8, sample_size_v1=0, sample_size_v2=0, ppl=4.19, wps=428, ups=0.4, wpb=1057.8, bsz=144, num_updates=760, lr=2.2664e-06, gnorm=7.199, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=1897
2023-08-02 12:32:47 - progress_bar.py[line:272] - INFO: epoch 001:    774 / 5589 loss=3.35, loss_v1=0, loss_v2=0, nll_loss=2.012, ntokens=1054.9, nsentences=144, sample_size=1054.9, sample_size_v1=0, sample_size_v2=0, ppl=4.03, wps=427.2, ups=0.4, wpb=1054.9, bsz=144, num_updates=770, lr=2.29622e-06, gnorm=7.161, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=1922
2023-08-02 12:33:12 - progress_bar.py[line:272] - INFO: epoch 001:    784 / 5589 loss=3.343, loss_v1=0, loss_v2=0, nll_loss=2.008, ntokens=1055.6, nsentences=144, sample_size=1055.6, sample_size_v1=0, sample_size_v2=0, ppl=4.02, wps=427.4, ups=0.4, wpb=1055.6, bsz=144, num_updates=780, lr=2.32604e-06, gnorm=7.157, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=1946
2023-08-02 12:33:37 - progress_bar.py[line:272] - INFO: epoch 001:    794 / 5589 loss=3.313, loss_v1=0, loss_v2=0, nll_loss=1.977, ntokens=1052.8, nsentences=144, sample_size=1052.8, sample_size_v1=0, sample_size_v2=0, ppl=3.94, wps=425.5, ups=0.4, wpb=1052.8, bsz=144, num_updates=790, lr=2.35586e-06, gnorm=7.097, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=1971
2023-08-02 12:34:02 - progress_bar.py[line:272] - INFO: epoch 001:    804 / 5589 loss=3.274, loss_v1=0, loss_v2=0, nll_loss=1.936, ntokens=1058.2, nsentences=144, sample_size=1058.2, sample_size_v1=0, sample_size_v2=0, ppl=3.83, wps=428.6, ups=0.41, wpb=1058.2, bsz=144, num_updates=800, lr=2.38569e-06, gnorm=7.065, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=1996
2023-08-02 12:34:26 - progress_bar.py[line:272] - INFO: epoch 001:    814 / 5589 loss=3.249, loss_v1=0, loss_v2=0, nll_loss=1.913, ntokens=1057.3, nsentences=144, sample_size=1057.3, sample_size_v1=0, sample_size_v2=0, ppl=3.77, wps=428.2, ups=0.4, wpb=1057.3, bsz=144, num_updates=810, lr=2.41551e-06, gnorm=6.993, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=2021
2023-08-02 12:34:51 - progress_bar.py[line:272] - INFO: epoch 001:    824 / 5589 loss=3.201, loss_v1=0, loss_v2=0, nll_loss=1.859, ntokens=1057.7, nsentences=144, sample_size=1057.7, sample_size_v1=0, sample_size_v2=0, ppl=3.63, wps=424.7, ups=0.4, wpb=1057.7, bsz=144, num_updates=820, lr=2.44533e-06, gnorm=6.91, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=2045
2023-08-02 12:35:16 - progress_bar.py[line:272] - INFO: epoch 001:    834 / 5589 loss=3.195, loss_v1=0, loss_v2=0, nll_loss=1.852, ntokens=1059.2, nsentences=144, sample_size=1059.2, sample_size_v1=0, sample_size_v2=0, ppl=3.61, wps=429.3, ups=0.41, wpb=1059.2, bsz=144, num_updates=830, lr=2.47515e-06, gnorm=6.842, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=2070
2023-08-02 12:35:40 - progress_bar.py[line:272] - INFO: epoch 001:    844 / 5589 loss=3.148, loss_v1=0, loss_v2=0, nll_loss=1.808, ntokens=1055.8, nsentences=144, sample_size=1055.8, sample_size_v1=0, sample_size_v2=0, ppl=3.5, wps=427.9, ups=0.41, wpb=1055.8, bsz=144, num_updates=840, lr=2.50497e-06, gnorm=6.786, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=2095
2023-08-02 12:36:05 - progress_bar.py[line:272] - INFO: epoch 001:    854 / 5589 loss=3.149, loss_v1=0, loss_v2=0, nll_loss=1.814, ntokens=1052.1, nsentences=144, sample_size=1052.1, sample_size_v1=0, sample_size_v2=0, ppl=3.52, wps=426.4, ups=0.41, wpb=1052.1, bsz=144, num_updates=850, lr=2.53479e-06, gnorm=6.752, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=2119
2023-08-02 12:36:30 - progress_bar.py[line:272] - INFO: epoch 001:    864 / 5589 loss=3.085, loss_v1=0, loss_v2=0, nll_loss=1.743, ntokens=1053.8, nsentences=144, sample_size=1053.8, sample_size_v1=0, sample_size_v2=0, ppl=3.35, wps=427.4, ups=0.41, wpb=1053.8, bsz=144, num_updates=860, lr=2.56461e-06, gnorm=6.669, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=2144
2023-08-02 12:36:55 - progress_bar.py[line:272] - INFO: epoch 001:    874 / 5589 loss=3.066, loss_v1=0, loss_v2=0, nll_loss=1.723, ntokens=1058.3, nsentences=144, sample_size=1058.3, sample_size_v1=0, sample_size_v2=0, ppl=3.3, wps=428.6, ups=0.41, wpb=1058.3, bsz=144, num_updates=870, lr=2.59443e-06, gnorm=6.559, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=2169
2023-08-02 12:37:19 - progress_bar.py[line:272] - INFO: epoch 001:    884 / 5589 loss=3.025, loss_v1=0, loss_v2=0, nll_loss=1.683, ntokens=1055.6, nsentences=144, sample_size=1055.6, sample_size_v1=0, sample_size_v2=0, ppl=3.21, wps=427.6, ups=0.41, wpb=1055.6, bsz=144, num_updates=880, lr=2.62425e-06, gnorm=6.506, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=2193
2023-08-02 12:37:44 - progress_bar.py[line:272] - INFO: epoch 001:    894 / 5589 loss=3.019, loss_v1=0, loss_v2=0, nll_loss=1.678, ntokens=1050.6, nsentences=144, sample_size=1050.6, sample_size_v1=0, sample_size_v2=0, ppl=3.2, wps=425.5, ups=0.41, wpb=1050.6, bsz=144, num_updates=890, lr=2.65408e-06, gnorm=6.446, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=2218
2023-08-02 12:38:09 - progress_bar.py[line:272] - INFO: epoch 001:    904 / 5589 loss=3.005, loss_v1=0, loss_v2=0, nll_loss=1.668, ntokens=1055.2, nsentences=144, sample_size=1055.2, sample_size_v1=0, sample_size_v2=0, ppl=3.18, wps=427.4, ups=0.41, wpb=1055.2, bsz=144, num_updates=900, lr=2.6839e-06, gnorm=6.29, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=2243
2023-08-02 12:38:33 - progress_bar.py[line:272] - INFO: epoch 001:    914 / 5589 loss=2.978, loss_v1=0, loss_v2=0, nll_loss=1.638, ntokens=1055.4, nsentences=144, sample_size=1055.4, sample_size_v1=0, sample_size_v2=0, ppl=3.11, wps=427.8, ups=0.41, wpb=1055.4, bsz=144, num_updates=910, lr=2.71372e-06, gnorm=6.218, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=2268
2023-08-02 12:38:58 - progress_bar.py[line:272] - INFO: epoch 001:    924 / 5589 loss=2.924, loss_v1=0, loss_v2=0, nll_loss=1.58, ntokens=1055.1, nsentences=144, sample_size=1055.1, sample_size_v1=0, sample_size_v2=0, ppl=2.99, wps=427.9, ups=0.41, wpb=1055.1, bsz=144, num_updates=920, lr=2.74354e-06, gnorm=6.025, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=2292
2023-08-02 12:39:23 - progress_bar.py[line:272] - INFO: epoch 001:    934 / 5589 loss=2.922, loss_v1=0, loss_v2=0, nll_loss=1.576, ntokens=1056.5, nsentences=144, sample_size=1056.5, sample_size_v1=0, sample_size_v2=0, ppl=2.98, wps=428.1, ups=0.41, wpb=1056.5, bsz=144, num_updates=930, lr=2.77336e-06, gnorm=5.993, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=2317
2023-08-02 12:39:47 - progress_bar.py[line:272] - INFO: epoch 001:    944 / 5589 loss=2.886, loss_v1=0, loss_v2=0, nll_loss=1.542, ntokens=1054.5, nsentences=144, sample_size=1054.5, sample_size_v1=0, sample_size_v2=0, ppl=2.91, wps=427.2, ups=0.41, wpb=1054.5, bsz=144, num_updates=940, lr=2.80318e-06, gnorm=5.819, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=2342
2023-08-02 12:40:12 - progress_bar.py[line:272] - INFO: epoch 001:    954 / 5589 loss=2.877, loss_v1=0, loss_v2=0, nll_loss=1.539, ntokens=1056.2, nsentences=144, sample_size=1056.2, sample_size_v1=0, sample_size_v2=0, ppl=2.91, wps=427.9, ups=0.41, wpb=1056.2, bsz=144, num_updates=950, lr=2.833e-06, gnorm=5.637, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=2366
2023-08-02 12:40:37 - progress_bar.py[line:272] - INFO: epoch 001:    964 / 5589 loss=2.818, loss_v1=0, loss_v2=0, nll_loss=1.472, ntokens=1053.8, nsentences=144, sample_size=1053.8, sample_size_v1=0, sample_size_v2=0, ppl=2.77, wps=426.9, ups=0.41, wpb=1053.8, bsz=144, num_updates=960, lr=2.86282e-06, gnorm=5.508, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=2391
2023-08-02 12:41:02 - progress_bar.py[line:272] - INFO: epoch 001:    974 / 5589 loss=2.835, loss_v1=0, loss_v2=0, nll_loss=1.491, ntokens=1050.4, nsentences=144, sample_size=1050.4, sample_size_v1=0, sample_size_v2=0, ppl=2.81, wps=421.7, ups=0.4, wpb=1050.4, bsz=144, num_updates=970, lr=2.89264e-06, gnorm=5.451, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=2416
2023-08-02 12:41:26 - progress_bar.py[line:272] - INFO: epoch 001:    984 / 5589 loss=2.775, loss_v1=0, loss_v2=0, nll_loss=1.433, ntokens=1054.9, nsentences=144, sample_size=1054.9, sample_size_v1=0, sample_size_v2=0, ppl=2.7, wps=428.3, ups=0.41, wpb=1054.9, bsz=144, num_updates=980, lr=2.92247e-06, gnorm=5.237, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=2440
2023-08-02 12:41:51 - progress_bar.py[line:272] - INFO: epoch 001:    994 / 5589 loss=2.763, loss_v1=0, loss_v2=0, nll_loss=1.415, ntokens=1054.9, nsentences=144, sample_size=1054.9, sample_size_v1=0, sample_size_v2=0, ppl=2.67, wps=428, ups=0.41, wpb=1054.9, bsz=144, num_updates=990, lr=2.95229e-06, gnorm=5.036, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=2465
2023-08-02 12:42:16 - progress_bar.py[line:272] - INFO: epoch 001:   1004 / 5589 loss=2.751, loss_v1=0, loss_v2=0, nll_loss=1.405, ntokens=1058.1, nsentences=144, sample_size=1058.1, sample_size_v1=0, sample_size_v2=0, ppl=2.65, wps=428.3, ups=0.4, wpb=1058.1, bsz=144, num_updates=1000, lr=2.98211e-06, gnorm=4.948, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=2490
2023-08-02 12:42:40 - progress_bar.py[line:272] - INFO: epoch 001:   1014 / 5589 loss=2.75, loss_v1=0, loss_v2=0, nll_loss=1.413, ntokens=1055.7, nsentences=144, sample_size=1055.7, sample_size_v1=0, sample_size_v2=0, ppl=2.66, wps=427.9, ups=0.41, wpb=1055.7, bsz=144, num_updates=1010, lr=3.01193e-06, gnorm=4.853, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=2515
2023-08-02 12:43:05 - progress_bar.py[line:272] - INFO: epoch 001:   1024 / 5589 loss=2.709, loss_v1=0, loss_v2=0, nll_loss=1.366, ntokens=1057.3, nsentences=144, sample_size=1057.3, sample_size_v1=0, sample_size_v2=0, ppl=2.58, wps=428.8, ups=0.41, wpb=1057.3, bsz=144, num_updates=1020, lr=3.04175e-06, gnorm=4.634, clip=100, loss_scale=16, train_wall=25, gb_free=6.6, wall=2539
2023-08-02 12:43:30 - progress_bar.py[line:272] - INFO: epoch 001:   1034 / 5589 loss=2.69, loss_v1=0, loss_v2=0, nll_loss=1.344, ntokens=1053.9, nsentences=144, sample_size=1053.9, sample_size_v1=0, sample_size_v2=0, ppl=2.54, wps=427.7, ups=0.41, wpb=1053.9, bsz=144, num_updates=1030, lr=3.07157e-06, gnorm=4.555, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=2564
2023-08-02 12:43:54 - progress_bar.py[line:272] - INFO: epoch 001:   1044 / 5589 loss=2.688, loss_v1=0, loss_v2=0, nll_loss=1.349, ntokens=1053.8, nsentences=144, sample_size=1053.8, sample_size_v1=0, sample_size_v2=0, ppl=2.55, wps=427.5, ups=0.41, wpb=1053.8, bsz=144, num_updates=1040, lr=3.10139e-06, gnorm=4.355, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=2588
2023-08-02 12:44:19 - progress_bar.py[line:272] - INFO: epoch 001:   1054 / 5589 loss=2.636, loss_v1=0, loss_v2=0, nll_loss=1.29, ntokens=1056.5, nsentences=144, sample_size=1056.5, sample_size_v1=0, sample_size_v2=0, ppl=2.45, wps=428.5, ups=0.41, wpb=1056.5, bsz=144, num_updates=1050, lr=3.13121e-06, gnorm=4.277, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=2613
2023-08-02 12:44:44 - progress_bar.py[line:272] - INFO: epoch 001:   1064 / 5589 loss=2.636, loss_v1=0, loss_v2=0, nll_loss=1.294, ntokens=1057.2, nsentences=144, sample_size=1057.2, sample_size_v1=0, sample_size_v2=0, ppl=2.45, wps=428.2, ups=0.41, wpb=1057.2, bsz=144, num_updates=1060, lr=3.16103e-06, gnorm=4.093, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=2638
2023-08-02 12:45:08 - progress_bar.py[line:272] - INFO: epoch 001:   1074 / 5589 loss=2.598, loss_v1=0, loss_v2=0, nll_loss=1.247, ntokens=1054.3, nsentences=144, sample_size=1054.3, sample_size_v1=0, sample_size_v2=0, ppl=2.37, wps=427.1, ups=0.41, wpb=1054.3, bsz=144, num_updates=1070, lr=3.19085e-06, gnorm=3.961, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=2663
2023-08-02 12:45:33 - progress_bar.py[line:272] - INFO: epoch 001:   1084 / 5589 loss=2.584, loss_v1=0, loss_v2=0, nll_loss=1.242, ntokens=1058.6, nsentences=144, sample_size=1058.6, sample_size_v1=0, sample_size_v2=0, ppl=2.36, wps=428.9, ups=0.41, wpb=1058.6, bsz=144, num_updates=1080, lr=3.22068e-06, gnorm=3.808, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=2687
2023-08-02 12:45:58 - progress_bar.py[line:272] - INFO: epoch 001:   1094 / 5589 loss=2.57, loss_v1=0, loss_v2=0, nll_loss=1.229, ntokens=1056.3, nsentences=144, sample_size=1056.3, sample_size_v1=0, sample_size_v2=0, ppl=2.34, wps=428.1, ups=0.41, wpb=1056.3, bsz=144, num_updates=1090, lr=3.2505e-06, gnorm=3.627, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=2712
2023-08-02 12:46:22 - progress_bar.py[line:272] - INFO: epoch 001:   1104 / 5589 loss=2.535, loss_v1=0, loss_v2=0, nll_loss=1.187, ntokens=1058.8, nsentences=144, sample_size=1058.8, sample_size_v1=0, sample_size_v2=0, ppl=2.28, wps=428.9, ups=0.41, wpb=1058.8, bsz=144, num_updates=1100, lr=3.28032e-06, gnorm=3.52, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=2737
2023-08-02 12:46:47 - progress_bar.py[line:272] - INFO: epoch 001:   1114 / 5589 loss=2.538, loss_v1=0, loss_v2=0, nll_loss=1.193, ntokens=1057, nsentences=144, sample_size=1057, sample_size_v1=0, sample_size_v2=0, ppl=2.29, wps=428.4, ups=0.41, wpb=1057, bsz=144, num_updates=1110, lr=3.31014e-06, gnorm=3.432, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=2761
2023-08-02 12:47:12 - progress_bar.py[line:272] - INFO: epoch 001:   1124 / 5589 loss=2.519, loss_v1=0, loss_v2=0, nll_loss=1.175, ntokens=1055.2, nsentences=144, sample_size=1055.2, sample_size_v1=0, sample_size_v2=0, ppl=2.26, wps=427.1, ups=0.4, wpb=1055.2, bsz=144, num_updates=1120, lr=3.33996e-06, gnorm=3.276, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=2786
2023-08-02 12:47:36 - progress_bar.py[line:272] - INFO: epoch 001:   1134 / 5589 loss=2.503, loss_v1=0, loss_v2=0, nll_loss=1.159, ntokens=1056, nsentences=144, sample_size=1056, sample_size_v1=0, sample_size_v2=0, ppl=2.23, wps=427.7, ups=0.41, wpb=1056, bsz=144, num_updates=1130, lr=3.36978e-06, gnorm=3.292, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=2811
2023-08-02 12:48:01 - progress_bar.py[line:272] - INFO: epoch 001:   1144 / 5589 loss=2.495, loss_v1=0, loss_v2=0, nll_loss=1.154, ntokens=1058, nsentences=144, sample_size=1058, sample_size_v1=0, sample_size_v2=0, ppl=2.23, wps=428.6, ups=0.41, wpb=1058, bsz=144, num_updates=1140, lr=3.3996e-06, gnorm=3.071, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=2835
2023-08-02 12:48:26 - progress_bar.py[line:272] - INFO: epoch 001:   1154 / 5589 loss=2.468, loss_v1=0, loss_v2=0, nll_loss=1.123, ntokens=1057.2, nsentences=144, sample_size=1057.2, sample_size_v1=0, sample_size_v2=0, ppl=2.18, wps=428.2, ups=0.41, wpb=1057.2, bsz=144, num_updates=1150, lr=3.42942e-06, gnorm=3.005, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=2860
2023-08-02 12:48:50 - progress_bar.py[line:272] - INFO: epoch 001:   1164 / 5589 loss=2.47, loss_v1=0, loss_v2=0, nll_loss=1.125, ntokens=1057.4, nsentences=144, sample_size=1057.4, sample_size_v1=0, sample_size_v2=0, ppl=2.18, wps=428.4, ups=0.41, wpb=1057.4, bsz=144, num_updates=1160, lr=3.45924e-06, gnorm=2.938, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=2885
2023-08-02 12:49:15 - progress_bar.py[line:272] - INFO: epoch 001:   1174 / 5589 loss=2.458, loss_v1=0, loss_v2=0, nll_loss=1.12, ntokens=1055, nsentences=144, sample_size=1055, sample_size_v1=0, sample_size_v2=0, ppl=2.17, wps=427.4, ups=0.41, wpb=1055, bsz=144, num_updates=1170, lr=3.48907e-06, gnorm=2.85, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=2909
2023-08-02 12:49:40 - progress_bar.py[line:272] - INFO: epoch 001:   1184 / 5589 loss=2.436, loss_v1=0, loss_v2=0, nll_loss=1.09, ntokens=1061.2, nsentences=144, sample_size=1061.2, sample_size_v1=0, sample_size_v2=0, ppl=2.13, wps=429.7, ups=0.4, wpb=1061.2, bsz=144, num_updates=1180, lr=3.51889e-06, gnorm=2.693, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=2934
2023-08-02 12:50:05 - progress_bar.py[line:272] - INFO: epoch 001:   1194 / 5589 loss=2.416, loss_v1=0, loss_v2=0, nll_loss=1.077, ntokens=1054, nsentences=144, sample_size=1054, sample_size_v1=0, sample_size_v2=0, ppl=2.11, wps=427, ups=0.41, wpb=1054, bsz=144, num_updates=1190, lr=3.54871e-06, gnorm=2.609, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=2959
2023-08-02 12:50:29 - progress_bar.py[line:272] - INFO: epoch 001:   1204 / 5589 loss=2.408, loss_v1=0, loss_v2=0, nll_loss=1.063, ntokens=1057.9, nsentences=144, sample_size=1057.9, sample_size_v1=0, sample_size_v2=0, ppl=2.09, wps=429.2, ups=0.41, wpb=1057.9, bsz=144, num_updates=1200, lr=3.57853e-06, gnorm=2.498, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=2983
2023-08-02 12:50:54 - progress_bar.py[line:272] - INFO: epoch 001:   1214 / 5589 loss=2.4, loss_v1=0, loss_v2=0, nll_loss=1.062, ntokens=1054.4, nsentences=144, sample_size=1054.4, sample_size_v1=0, sample_size_v2=0, ppl=2.09, wps=428.1, ups=0.41, wpb=1054.4, bsz=144, num_updates=1210, lr=3.60835e-06, gnorm=2.459, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3008
2023-08-02 12:51:18 - progress_bar.py[line:272] - INFO: epoch 001:   1224 / 5589 loss=2.392, loss_v1=0, loss_v2=0, nll_loss=1.051, ntokens=1055.5, nsentences=144, sample_size=1055.5, sample_size_v1=0, sample_size_v2=0, ppl=2.07, wps=428.6, ups=0.41, wpb=1055.5, bsz=144, num_updates=1220, lr=3.63817e-06, gnorm=2.415, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3033
2023-08-02 12:51:43 - progress_bar.py[line:272] - INFO: epoch 001:   1234 / 5589 loss=2.389, loss_v1=0, loss_v2=0, nll_loss=1.054, ntokens=1055.1, nsentences=144, sample_size=1055.1, sample_size_v1=0, sample_size_v2=0, ppl=2.08, wps=428.6, ups=0.41, wpb=1055.1, bsz=144, num_updates=1230, lr=3.66799e-06, gnorm=2.315, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3057
2023-08-02 12:52:08 - progress_bar.py[line:272] - INFO: epoch 001:   1244 / 5589 loss=2.368, loss_v1=0, loss_v2=0, nll_loss=1.027, ntokens=1055.9, nsentences=144, sample_size=1055.9, sample_size_v1=0, sample_size_v2=0, ppl=2.04, wps=428.7, ups=0.41, wpb=1055.9, bsz=144, num_updates=1240, lr=3.69781e-06, gnorm=2.256, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3082
2023-08-02 12:52:32 - progress_bar.py[line:272] - INFO: epoch 001:   1254 / 5589 loss=2.352, loss_v1=0, loss_v2=0, nll_loss=1.013, ntokens=1052.2, nsentences=144, sample_size=1052.2, sample_size_v1=0, sample_size_v2=0, ppl=2.02, wps=427.4, ups=0.41, wpb=1052.2, bsz=144, num_updates=1250, lr=3.72763e-06, gnorm=2.263, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3107
2023-08-02 12:52:57 - progress_bar.py[line:272] - INFO: epoch 001:   1264 / 5589 loss=2.315, loss_v1=0, loss_v2=0, nll_loss=0.968, ntokens=1058.8, nsentences=144, sample_size=1058.8, sample_size_v1=0, sample_size_v2=0, ppl=1.96, wps=430, ups=0.41, wpb=1058.8, bsz=144, num_updates=1260, lr=3.75746e-06, gnorm=2.082, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3131
2023-08-02 12:53:22 - progress_bar.py[line:272] - INFO: epoch 001:   1274 / 5589 loss=2.338, loss_v1=0, loss_v2=0, nll_loss=1, ntokens=1056.6, nsentences=144, sample_size=1056.6, sample_size_v1=0, sample_size_v2=0, ppl=2, wps=428.5, ups=0.41, wpb=1056.6, bsz=144, num_updates=1270, lr=3.78728e-06, gnorm=2.177, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3156
2023-08-02 12:53:46 - progress_bar.py[line:272] - INFO: epoch 001:   1284 / 5589 loss=2.333, loss_v1=0, loss_v2=0, nll_loss=0.998, ntokens=1054.3, nsentences=144, sample_size=1054.3, sample_size_v1=0, sample_size_v2=0, ppl=2, wps=427.7, ups=0.41, wpb=1054.3, bsz=144, num_updates=1280, lr=3.8171e-06, gnorm=2.113, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3181
2023-08-02 12:54:11 - progress_bar.py[line:272] - INFO: epoch 001:   1294 / 5589 loss=2.318, loss_v1=0, loss_v2=0, nll_loss=0.978, ntokens=1057.7, nsentences=144, sample_size=1057.7, sample_size_v1=0, sample_size_v2=0, ppl=1.97, wps=429.1, ups=0.41, wpb=1057.7, bsz=144, num_updates=1290, lr=3.84692e-06, gnorm=1.989, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3205
2023-08-02 12:54:36 - progress_bar.py[line:272] - INFO: epoch 001:   1304 / 5589 loss=2.301, loss_v1=0, loss_v2=0, nll_loss=0.961, ntokens=1057.1, nsentences=144, sample_size=1057.1, sample_size_v1=0, sample_size_v2=0, ppl=1.95, wps=428.5, ups=0.41, wpb=1057.1, bsz=144, num_updates=1300, lr=3.87674e-06, gnorm=1.984, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3230
2023-08-02 12:55:00 - progress_bar.py[line:272] - INFO: epoch 001:   1314 / 5589 loss=2.298, loss_v1=0, loss_v2=0, nll_loss=0.962, ntokens=1059.2, nsentences=144, sample_size=1059.2, sample_size_v1=0, sample_size_v2=0, ppl=1.95, wps=429.4, ups=0.41, wpb=1059.2, bsz=144, num_updates=1310, lr=3.90656e-06, gnorm=1.963, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3255
2023-08-02 12:55:25 - progress_bar.py[line:272] - INFO: epoch 001:   1324 / 5589 loss=2.286, loss_v1=0, loss_v2=0, nll_loss=0.946, ntokens=1057, nsentences=144, sample_size=1057, sample_size_v1=0, sample_size_v2=0, ppl=1.93, wps=428.7, ups=0.41, wpb=1057, bsz=144, num_updates=1320, lr=3.93638e-06, gnorm=1.924, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3279
2023-08-02 12:55:50 - progress_bar.py[line:272] - INFO: epoch 001:   1334 / 5589 loss=2.283, loss_v1=0, loss_v2=0, nll_loss=0.948, ntokens=1058.5, nsentences=144, sample_size=1058.5, sample_size_v1=0, sample_size_v2=0, ppl=1.93, wps=429.6, ups=0.41, wpb=1058.5, bsz=144, num_updates=1330, lr=3.9662e-06, gnorm=1.829, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3304
2023-08-02 12:56:14 - progress_bar.py[line:272] - INFO: epoch 001:   1344 / 5589 loss=2.289, loss_v1=0, loss_v2=0, nll_loss=0.955, ntokens=1052.2, nsentences=144, sample_size=1052.2, sample_size_v1=0, sample_size_v2=0, ppl=1.94, wps=426.9, ups=0.41, wpb=1052.2, bsz=144, num_updates=1340, lr=3.99602e-06, gnorm=1.895, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3328
2023-08-02 12:56:39 - progress_bar.py[line:272] - INFO: epoch 001:   1354 / 5589 loss=2.265, loss_v1=0, loss_v2=0, nll_loss=0.931, ntokens=1054.9, nsentences=144, sample_size=1054.9, sample_size_v1=0, sample_size_v2=0, ppl=1.91, wps=427.2, ups=0.4, wpb=1054.9, bsz=144, num_updates=1350, lr=4.02584e-06, gnorm=1.865, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3353
2023-08-02 12:57:04 - progress_bar.py[line:272] - INFO: epoch 001:   1364 / 5589 loss=2.263, loss_v1=0, loss_v2=0, nll_loss=0.929, ntokens=1057.1, nsentences=144, sample_size=1057.1, sample_size_v1=0, sample_size_v2=0, ppl=1.9, wps=428.1, ups=0.4, wpb=1057.1, bsz=144, num_updates=1360, lr=4.05567e-06, gnorm=1.768, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3378
2023-08-02 12:57:28 - progress_bar.py[line:272] - INFO: epoch 001:   1374 / 5589 loss=2.251, loss_v1=0, loss_v2=0, nll_loss=0.912, ntokens=1053.1, nsentences=144, sample_size=1053.1, sample_size_v1=0, sample_size_v2=0, ppl=1.88, wps=426.7, ups=0.41, wpb=1053.1, bsz=144, num_updates=1370, lr=4.08549e-06, gnorm=1.746, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3403
2023-08-02 12:57:53 - progress_bar.py[line:272] - INFO: epoch 001:   1384 / 5589 loss=2.258, loss_v1=0, loss_v2=0, nll_loss=0.929, ntokens=1054.9, nsentences=144, sample_size=1054.9, sample_size_v1=0, sample_size_v2=0, ppl=1.9, wps=426.6, ups=0.4, wpb=1054.9, bsz=144, num_updates=1380, lr=4.11531e-06, gnorm=1.768, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3427
2023-08-02 12:58:18 - progress_bar.py[line:272] - INFO: epoch 001:   1394 / 5589 loss=2.252, loss_v1=0, loss_v2=0, nll_loss=0.918, ntokens=1055.6, nsentences=144, sample_size=1055.6, sample_size_v1=0, sample_size_v2=0, ppl=1.89, wps=427.5, ups=0.4, wpb=1055.6, bsz=144, num_updates=1390, lr=4.14513e-06, gnorm=1.756, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3452
2023-08-02 12:58:42 - progress_bar.py[line:272] - INFO: epoch 001:   1404 / 5589 loss=2.243, loss_v1=0, loss_v2=0, nll_loss=0.911, ntokens=1055, nsentences=144, sample_size=1055, sample_size_v1=0, sample_size_v2=0, ppl=1.88, wps=427.4, ups=0.41, wpb=1055, bsz=144, num_updates=1400, lr=4.17495e-06, gnorm=1.682, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3477
2023-08-02 12:59:07 - progress_bar.py[line:272] - INFO: epoch 001:   1414 / 5589 loss=2.225, loss_v1=0, loss_v2=0, nll_loss=0.891, ntokens=1057.6, nsentences=144, sample_size=1057.6, sample_size_v1=0, sample_size_v2=0, ppl=1.85, wps=428.4, ups=0.41, wpb=1057.6, bsz=144, num_updates=1410, lr=4.20477e-06, gnorm=1.758, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3501
2023-08-02 12:59:32 - progress_bar.py[line:272] - INFO: epoch 001:   1424 / 5589 loss=2.225, loss_v1=0, loss_v2=0, nll_loss=0.894, ntokens=1051.9, nsentences=144, sample_size=1051.9, sample_size_v1=0, sample_size_v2=0, ppl=1.86, wps=426.3, ups=0.41, wpb=1051.9, bsz=144, num_updates=1420, lr=4.23459e-06, gnorm=1.669, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3526
2023-08-02 12:59:56 - progress_bar.py[line:272] - INFO: epoch 001:   1434 / 5589 loss=2.226, loss_v1=0, loss_v2=0, nll_loss=0.895, ntokens=1053.6, nsentences=144, sample_size=1053.6, sample_size_v1=0, sample_size_v2=0, ppl=1.86, wps=426.8, ups=0.41, wpb=1053.6, bsz=144, num_updates=1430, lr=4.26441e-06, gnorm=1.679, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3551
2023-08-02 13:00:21 - progress_bar.py[line:272] - INFO: epoch 001:   1444 / 5589 loss=2.214, loss_v1=0, loss_v2=0, nll_loss=0.884, ntokens=1056, nsentences=144, sample_size=1056, sample_size_v1=0, sample_size_v2=0, ppl=1.85, wps=428.4, ups=0.41, wpb=1056, bsz=144, num_updates=1440, lr=4.29423e-06, gnorm=1.668, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3575
2023-08-02 13:00:46 - progress_bar.py[line:272] - INFO: epoch 001:   1454 / 5589 loss=2.207, loss_v1=0, loss_v2=0, nll_loss=0.877, ntokens=1059.6, nsentences=144, sample_size=1059.6, sample_size_v1=0, sample_size_v2=0, ppl=1.84, wps=429.9, ups=0.41, wpb=1059.6, bsz=144, num_updates=1450, lr=4.32406e-06, gnorm=1.617, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3600
2023-08-02 13:01:10 - progress_bar.py[line:272] - INFO: epoch 001:   1464 / 5589 loss=2.214, loss_v1=0, loss_v2=0, nll_loss=0.884, ntokens=1054.4, nsentences=144, sample_size=1054.4, sample_size_v1=0, sample_size_v2=0, ppl=1.85, wps=427.8, ups=0.41, wpb=1054.4, bsz=144, num_updates=1460, lr=4.35388e-06, gnorm=1.708, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3625
2023-08-02 13:01:35 - progress_bar.py[line:272] - INFO: epoch 001:   1474 / 5589 loss=2.197, loss_v1=0, loss_v2=0, nll_loss=0.867, ntokens=1057.1, nsentences=144, sample_size=1057.1, sample_size_v1=0, sample_size_v2=0, ppl=1.82, wps=428.7, ups=0.41, wpb=1057.1, bsz=144, num_updates=1470, lr=4.3837e-06, gnorm=1.616, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3649
2023-08-02 13:02:00 - progress_bar.py[line:272] - INFO: epoch 001:   1484 / 5589 loss=2.195, loss_v1=0, loss_v2=0, nll_loss=0.865, ntokens=1054.3, nsentences=144, sample_size=1054.3, sample_size_v1=0, sample_size_v2=0, ppl=1.82, wps=427.8, ups=0.41, wpb=1054.3, bsz=144, num_updates=1480, lr=4.41352e-06, gnorm=1.639, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3674
2023-08-02 13:02:24 - progress_bar.py[line:272] - INFO: epoch 001:   1494 / 5589 loss=2.191, loss_v1=0, loss_v2=0, nll_loss=0.862, ntokens=1056.6, nsentences=144, sample_size=1056.6, sample_size_v1=0, sample_size_v2=0, ppl=1.82, wps=428.7, ups=0.41, wpb=1056.6, bsz=144, num_updates=1490, lr=4.44334e-06, gnorm=1.617, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3699
2023-08-02 13:02:49 - progress_bar.py[line:272] - INFO: epoch 001:   1504 / 5589 loss=2.169, loss_v1=0, loss_v2=0, nll_loss=0.836, ntokens=1055.1, nsentences=144, sample_size=1055.1, sample_size_v1=0, sample_size_v2=0, ppl=1.79, wps=428.2, ups=0.41, wpb=1055.1, bsz=144, num_updates=1500, lr=4.47316e-06, gnorm=1.589, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3723
2023-08-02 13:03:14 - progress_bar.py[line:272] - INFO: epoch 001:   1514 / 5589 loss=2.181, loss_v1=0, loss_v2=0, nll_loss=0.85, ntokens=1053.8, nsentences=144, sample_size=1053.8, sample_size_v1=0, sample_size_v2=0, ppl=1.8, wps=427.5, ups=0.41, wpb=1053.8, bsz=144, num_updates=1510, lr=4.50298e-06, gnorm=1.644, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3748
2023-08-02 13:03:38 - progress_bar.py[line:272] - INFO: epoch 001:   1524 / 5589 loss=2.187, loss_v1=0, loss_v2=0, nll_loss=0.863, ntokens=1052.6, nsentences=144, sample_size=1052.6, sample_size_v1=0, sample_size_v2=0, ppl=1.82, wps=427.1, ups=0.41, wpb=1052.6, bsz=144, num_updates=1520, lr=4.5328e-06, gnorm=1.686, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3773
2023-08-02 13:04:03 - progress_bar.py[line:272] - INFO: epoch 001:   1534 / 5589 loss=2.173, loss_v1=0, loss_v2=0, nll_loss=0.843, ntokens=1057, nsentences=144, sample_size=1057, sample_size_v1=0, sample_size_v2=0, ppl=1.79, wps=428.9, ups=0.41, wpb=1057, bsz=144, num_updates=1530, lr=4.56262e-06, gnorm=1.639, clip=100, loss_scale=32, train_wall=25, gb_free=6.6, wall=3797
2023-08-02 13:04:28 - progress_bar.py[line:272] - INFO: epoch 001:   1544 / 5589 loss=2.175, loss_v1=0, loss_v2=0, nll_loss=0.851, ntokens=1055, nsentences=144, sample_size=1055, sample_size_v1=0, sample_size_v2=0, ppl=1.8, wps=428.1, ups=0.41, wpb=1055, bsz=144, num_updates=1540, lr=4.59245e-06, gnorm=1.669, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=3822
2023-08-02 13:04:52 - progress_bar.py[line:272] - INFO: epoch 001:   1554 / 5589 loss=2.176, loss_v1=0, loss_v2=0, nll_loss=0.846, ntokens=1057.9, nsentences=144, sample_size=1057.9, sample_size_v1=0, sample_size_v2=0, ppl=1.8, wps=429.1, ups=0.41, wpb=1057.9, bsz=144, num_updates=1550, lr=4.62227e-06, gnorm=1.626, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=3847
2023-08-02 13:05:17 - progress_bar.py[line:272] - INFO: epoch 001:   1564 / 5589 loss=2.162, loss_v1=0, loss_v2=0, nll_loss=0.836, ntokens=1054.2, nsentences=144, sample_size=1054.2, sample_size_v1=0, sample_size_v2=0, ppl=1.78, wps=427.8, ups=0.41, wpb=1054.2, bsz=144, num_updates=1560, lr=4.65209e-06, gnorm=1.54, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=3871
2023-08-02 13:05:42 - progress_bar.py[line:272] - INFO: epoch 001:   1574 / 5589 loss=2.157, loss_v1=0, loss_v2=0, nll_loss=0.829, ntokens=1057.5, nsentences=144, sample_size=1057.5, sample_size_v1=0, sample_size_v2=0, ppl=1.78, wps=429, ups=0.41, wpb=1057.5, bsz=144, num_updates=1570, lr=4.68191e-06, gnorm=1.559, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=3896
2023-08-02 13:06:06 - progress_bar.py[line:272] - INFO: epoch 001:   1584 / 5589 loss=2.177, loss_v1=0, loss_v2=0, nll_loss=0.851, ntokens=1057.3, nsentences=144, sample_size=1057.3, sample_size_v1=0, sample_size_v2=0, ppl=1.8, wps=428.7, ups=0.41, wpb=1057.3, bsz=144, num_updates=1580, lr=4.71173e-06, gnorm=1.701, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=3920
2023-08-02 13:06:31 - progress_bar.py[line:272] - INFO: epoch 001:   1594 / 5589 loss=2.144, loss_v1=0, loss_v2=0, nll_loss=0.82, ntokens=1058.8, nsentences=144, sample_size=1058.8, sample_size_v1=0, sample_size_v2=0, ppl=1.77, wps=429.2, ups=0.41, wpb=1058.8, bsz=144, num_updates=1590, lr=4.74155e-06, gnorm=1.595, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=3945
2023-08-02 13:06:56 - progress_bar.py[line:272] - INFO: epoch 001:   1604 / 5589 loss=2.153, loss_v1=0, loss_v2=0, nll_loss=0.827, ntokens=1057.3, nsentences=144, sample_size=1057.3, sample_size_v1=0, sample_size_v2=0, ppl=1.77, wps=428.8, ups=0.41, wpb=1057.3, bsz=144, num_updates=1600, lr=4.77137e-06, gnorm=1.536, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=3970
2023-08-02 13:07:20 - progress_bar.py[line:272] - INFO: epoch 001:   1614 / 5589 loss=2.139, loss_v1=0, loss_v2=0, nll_loss=0.815, ntokens=1054.4, nsentences=144, sample_size=1054.4, sample_size_v1=0, sample_size_v2=0, ppl=1.76, wps=427.7, ups=0.41, wpb=1054.4, bsz=144, num_updates=1610, lr=4.80119e-06, gnorm=1.664, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=3994
2023-08-02 13:07:45 - progress_bar.py[line:272] - INFO: epoch 001:   1624 / 5589 loss=2.141, loss_v1=0, loss_v2=0, nll_loss=0.814, ntokens=1056.6, nsentences=144, sample_size=1056.6, sample_size_v1=0, sample_size_v2=0, ppl=1.76, wps=428.4, ups=0.41, wpb=1056.6, bsz=144, num_updates=1620, lr=4.83101e-06, gnorm=1.624, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4019
2023-08-02 13:08:10 - progress_bar.py[line:272] - INFO: epoch 001:   1634 / 5589 loss=2.126, loss_v1=0, loss_v2=0, nll_loss=0.798, ntokens=1057, nsentences=144, sample_size=1057, sample_size_v1=0, sample_size_v2=0, ppl=1.74, wps=428.8, ups=0.41, wpb=1057, bsz=144, num_updates=1630, lr=4.86083e-06, gnorm=1.608, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4044
2023-08-02 13:08:34 - progress_bar.py[line:272] - INFO: epoch 001:   1644 / 5589 loss=2.135, loss_v1=0, loss_v2=0, nll_loss=0.812, ntokens=1054.5, nsentences=144, sample_size=1054.5, sample_size_v1=0, sample_size_v2=0, ppl=1.76, wps=427.4, ups=0.41, wpb=1054.5, bsz=144, num_updates=1640, lr=4.89066e-06, gnorm=1.587, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4068
2023-08-02 13:08:59 - progress_bar.py[line:272] - INFO: epoch 001:   1654 / 5589 loss=2.132, loss_v1=0, loss_v2=0, nll_loss=0.809, ntokens=1053.8, nsentences=144, sample_size=1053.8, sample_size_v1=0, sample_size_v2=0, ppl=1.75, wps=427.3, ups=0.41, wpb=1053.8, bsz=144, num_updates=1650, lr=4.92048e-06, gnorm=1.6, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4093
2023-08-02 13:09:24 - progress_bar.py[line:272] - INFO: epoch 001:   1664 / 5589 loss=2.12, loss_v1=0, loss_v2=0, nll_loss=0.795, ntokens=1055.4, nsentences=144, sample_size=1055.4, sample_size_v1=0, sample_size_v2=0, ppl=1.73, wps=428.1, ups=0.41, wpb=1055.4, bsz=144, num_updates=1660, lr=4.9503e-06, gnorm=1.596, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4118
2023-08-02 13:09:48 - progress_bar.py[line:272] - INFO: epoch 001:   1674 / 5589 loss=2.119, loss_v1=0, loss_v2=0, nll_loss=0.794, ntokens=1056.7, nsentences=144, sample_size=1056.7, sample_size_v1=0, sample_size_v2=0, ppl=1.73, wps=428.4, ups=0.41, wpb=1056.7, bsz=144, num_updates=1670, lr=4.98012e-06, gnorm=1.582, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4142
2023-08-02 13:10:13 - progress_bar.py[line:272] - INFO: epoch 001:   1684 / 5589 loss=2.125, loss_v1=0, loss_v2=0, nll_loss=0.803, ntokens=1054.3, nsentences=144, sample_size=1054.3, sample_size_v1=0, sample_size_v2=0, ppl=1.74, wps=427.1, ups=0.41, wpb=1054.3, bsz=144, num_updates=1680, lr=5.00994e-06, gnorm=1.615, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4167
2023-08-02 13:10:38 - progress_bar.py[line:272] - INFO: epoch 001:   1694 / 5589 loss=2.111, loss_v1=0, loss_v2=0, nll_loss=0.786, ntokens=1057.8, nsentences=144, sample_size=1057.8, sample_size_v1=0, sample_size_v2=0, ppl=1.72, wps=429.2, ups=0.41, wpb=1057.8, bsz=144, num_updates=1690, lr=5.03976e-06, gnorm=1.574, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4192
2023-08-02 13:11:02 - progress_bar.py[line:272] - INFO: epoch 001:   1704 / 5589 loss=2.09, loss_v1=0, loss_v2=0, nll_loss=0.762, ntokens=1052.2, nsentences=144, sample_size=1052.2, sample_size_v1=0, sample_size_v2=0, ppl=1.7, wps=426.7, ups=0.41, wpb=1052.2, bsz=144, num_updates=1700, lr=5.06958e-06, gnorm=1.628, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4216
2023-08-02 13:11:27 - progress_bar.py[line:272] - INFO: epoch 001:   1714 / 5589 loss=2.108, loss_v1=0, loss_v2=0, nll_loss=0.786, ntokens=1053, nsentences=144, sample_size=1053, sample_size_v1=0, sample_size_v2=0, ppl=1.72, wps=427.1, ups=0.41, wpb=1053, bsz=144, num_updates=1710, lr=5.0994e-06, gnorm=1.569, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4241
2023-08-02 13:11:52 - progress_bar.py[line:272] - INFO: epoch 001:   1724 / 5589 loss=2.118, loss_v1=0, loss_v2=0, nll_loss=0.795, ntokens=1056.7, nsentences=144, sample_size=1056.7, sample_size_v1=0, sample_size_v2=0, ppl=1.73, wps=428.5, ups=0.41, wpb=1056.7, bsz=144, num_updates=1720, lr=5.12922e-06, gnorm=1.642, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4266
2023-08-02 13:12:16 - progress_bar.py[line:272] - INFO: epoch 001:   1734 / 5589 loss=2.083, loss_v1=0, loss_v2=0, nll_loss=0.758, ntokens=1051.9, nsentences=144, sample_size=1051.9, sample_size_v1=0, sample_size_v2=0, ppl=1.69, wps=426.5, ups=0.41, wpb=1051.9, bsz=144, num_updates=1730, lr=5.15905e-06, gnorm=1.506, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4290
2023-08-02 13:12:41 - progress_bar.py[line:272] - INFO: epoch 001:   1744 / 5589 loss=2.087, loss_v1=0, loss_v2=0, nll_loss=0.763, ntokens=1055.5, nsentences=144, sample_size=1055.5, sample_size_v1=0, sample_size_v2=0, ppl=1.7, wps=428.1, ups=0.41, wpb=1055.5, bsz=144, num_updates=1740, lr=5.18887e-06, gnorm=1.534, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4315
2023-08-02 13:13:06 - progress_bar.py[line:272] - INFO: epoch 001:   1754 / 5589 loss=2.098, loss_v1=0, loss_v2=0, nll_loss=0.775, ntokens=1056.2, nsentences=144, sample_size=1056.2, sample_size_v1=0, sample_size_v2=0, ppl=1.71, wps=428.1, ups=0.41, wpb=1056.2, bsz=144, num_updates=1750, lr=5.21869e-06, gnorm=1.585, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4340
2023-08-02 13:13:30 - progress_bar.py[line:272] - INFO: epoch 001:   1764 / 5589 loss=2.083, loss_v1=0, loss_v2=0, nll_loss=0.756, ntokens=1054.7, nsentences=144, sample_size=1054.7, sample_size_v1=0, sample_size_v2=0, ppl=1.69, wps=427.6, ups=0.41, wpb=1054.7, bsz=144, num_updates=1760, lr=5.24851e-06, gnorm=1.592, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4364
2023-08-02 13:13:55 - progress_bar.py[line:272] - INFO: epoch 001:   1774 / 5589 loss=2.083, loss_v1=0, loss_v2=0, nll_loss=0.761, ntokens=1057.3, nsentences=144, sample_size=1057.3, sample_size_v1=0, sample_size_v2=0, ppl=1.7, wps=428.8, ups=0.41, wpb=1057.3, bsz=144, num_updates=1770, lr=5.27833e-06, gnorm=1.64, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4389
2023-08-02 13:14:20 - progress_bar.py[line:272] - INFO: epoch 001:   1784 / 5589 loss=2.089, loss_v1=0, loss_v2=0, nll_loss=0.765, ntokens=1053.1, nsentences=144, sample_size=1053.1, sample_size_v1=0, sample_size_v2=0, ppl=1.7, wps=426.8, ups=0.41, wpb=1053.1, bsz=144, num_updates=1780, lr=5.30815e-06, gnorm=1.597, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4414
2023-08-02 13:14:44 - progress_bar.py[line:272] - INFO: epoch 001:   1794 / 5589 loss=2.075, loss_v1=0, loss_v2=0, nll_loss=0.753, ntokens=1058.8, nsentences=144, sample_size=1058.8, sample_size_v1=0, sample_size_v2=0, ppl=1.69, wps=429.3, ups=0.41, wpb=1058.8, bsz=144, num_updates=1790, lr=5.33797e-06, gnorm=1.606, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4438
2023-08-02 13:15:09 - progress_bar.py[line:272] - INFO: epoch 001:   1804 / 5589 loss=2.085, loss_v1=0, loss_v2=0, nll_loss=0.762, ntokens=1054.6, nsentences=144, sample_size=1054.6, sample_size_v1=0, sample_size_v2=0, ppl=1.7, wps=427.6, ups=0.41, wpb=1054.6, bsz=144, num_updates=1800, lr=5.36779e-06, gnorm=1.656, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4463
2023-08-02 13:15:34 - progress_bar.py[line:272] - INFO: epoch 001:   1814 / 5589 loss=2.056, loss_v1=0, loss_v2=0, nll_loss=0.733, ntokens=1056, nsentences=144, sample_size=1056, sample_size_v1=0, sample_size_v2=0, ppl=1.66, wps=427.1, ups=0.4, wpb=1056, bsz=144, num_updates=1810, lr=5.39761e-06, gnorm=1.526, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4488
2023-08-02 13:15:58 - progress_bar.py[line:272] - INFO: epoch 001:   1824 / 5589 loss=2.067, loss_v1=0, loss_v2=0, nll_loss=0.744, ntokens=1053.9, nsentences=144, sample_size=1053.9, sample_size_v1=0, sample_size_v2=0, ppl=1.67, wps=426.6, ups=0.4, wpb=1053.9, bsz=144, num_updates=1820, lr=5.42744e-06, gnorm=1.658, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4513
2023-08-02 13:16:23 - progress_bar.py[line:272] - INFO: epoch 001:   1834 / 5589 loss=2.056, loss_v1=0, loss_v2=0, nll_loss=0.733, ntokens=1056.6, nsentences=144, sample_size=1056.6, sample_size_v1=0, sample_size_v2=0, ppl=1.66, wps=427.7, ups=0.4, wpb=1056.6, bsz=144, num_updates=1830, lr=5.45726e-06, gnorm=1.648, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4537
2023-08-02 13:16:48 - progress_bar.py[line:272] - INFO: epoch 001:   1844 / 5589 loss=2.05, loss_v1=0, loss_v2=0, nll_loss=0.722, ntokens=1053.2, nsentences=144, sample_size=1053.2, sample_size_v1=0, sample_size_v2=0, ppl=1.65, wps=426.3, ups=0.4, wpb=1053.2, bsz=144, num_updates=1840, lr=5.48708e-06, gnorm=1.645, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4562
2023-08-02 13:17:12 - progress_bar.py[line:272] - INFO: epoch 001:   1854 / 5589 loss=2.068, loss_v1=0, loss_v2=0, nll_loss=0.748, ntokens=1054.1, nsentences=144, sample_size=1054.1, sample_size_v1=0, sample_size_v2=0, ppl=1.68, wps=426.8, ups=0.4, wpb=1054.1, bsz=144, num_updates=1850, lr=5.5169e-06, gnorm=1.651, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4587
2023-08-02 13:17:37 - progress_bar.py[line:272] - INFO: epoch 001:   1864 / 5589 loss=2.074, loss_v1=0, loss_v2=0, nll_loss=0.756, ntokens=1052.1, nsentences=144, sample_size=1052.1, sample_size_v1=0, sample_size_v2=0, ppl=1.69, wps=426.1, ups=0.4, wpb=1052.1, bsz=144, num_updates=1860, lr=5.54672e-06, gnorm=1.652, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4611
2023-08-02 13:18:02 - progress_bar.py[line:272] - INFO: epoch 001:   1874 / 5589 loss=2.065, loss_v1=0, loss_v2=0, nll_loss=0.743, ntokens=1055.3, nsentences=144, sample_size=1055.3, sample_size_v1=0, sample_size_v2=0, ppl=1.67, wps=427.6, ups=0.41, wpb=1055.3, bsz=144, num_updates=1870, lr=5.57654e-06, gnorm=1.672, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4636
2023-08-02 13:18:27 - progress_bar.py[line:272] - INFO: epoch 001:   1884 / 5589 loss=2.047, loss_v1=0, loss_v2=0, nll_loss=0.723, ntokens=1058.8, nsentences=144, sample_size=1058.8, sample_size_v1=0, sample_size_v2=0, ppl=1.65, wps=428.2, ups=0.4, wpb=1058.8, bsz=144, num_updates=1880, lr=5.60636e-06, gnorm=1.559, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4661
2023-08-02 13:18:51 - progress_bar.py[line:272] - INFO: epoch 001:   1894 / 5589 loss=2.049, loss_v1=0, loss_v2=0, nll_loss=0.725, ntokens=1043.3, nsentences=142.2, sample_size=1043.3, sample_size_v1=0, sample_size_v2=0, ppl=1.65, wps=426.8, ups=0.41, wpb=1043.3, bsz=142.2, num_updates=1890, lr=5.63618e-06, gnorm=1.639, clip=100, loss_scale=64, train_wall=24, gb_free=6.6, wall=4685
2023-08-02 13:19:16 - progress_bar.py[line:272] - INFO: epoch 001:   1904 / 5589 loss=2.056, loss_v1=0, loss_v2=0, nll_loss=0.736, ntokens=1053.7, nsentences=144, sample_size=1053.7, sample_size_v1=0, sample_size_v2=0, ppl=1.67, wps=426.5, ups=0.4, wpb=1053.7, bsz=144, num_updates=1900, lr=5.666e-06, gnorm=1.544, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4710
2023-08-02 13:19:40 - progress_bar.py[line:272] - INFO: epoch 001:   1914 / 5589 loss=2.042, loss_v1=0, loss_v2=0, nll_loss=0.719, ntokens=1056.8, nsentences=144, sample_size=1056.8, sample_size_v1=0, sample_size_v2=0, ppl=1.65, wps=427.7, ups=0.4, wpb=1056.8, bsz=144, num_updates=1910, lr=5.69583e-06, gnorm=1.691, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4735
2023-08-02 13:20:05 - progress_bar.py[line:272] - INFO: epoch 001:   1924 / 5589 loss=2.045, loss_v1=0, loss_v2=0, nll_loss=0.724, ntokens=1058.2, nsentences=144, sample_size=1058.2, sample_size_v1=0, sample_size_v2=0, ppl=1.65, wps=428.3, ups=0.4, wpb=1058.2, bsz=144, num_updates=1920, lr=5.72565e-06, gnorm=1.571, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4759
2023-08-02 13:20:30 - progress_bar.py[line:272] - INFO: epoch 001:   1934 / 5589 loss=2.039, loss_v1=0, loss_v2=0, nll_loss=0.717, ntokens=1056.7, nsentences=144, sample_size=1056.7, sample_size_v1=0, sample_size_v2=0, ppl=1.64, wps=427.7, ups=0.4, wpb=1056.7, bsz=144, num_updates=1930, lr=5.75547e-06, gnorm=1.596, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4784
2023-08-02 13:20:54 - progress_bar.py[line:272] - INFO: epoch 001:   1944 / 5589 loss=2.029, loss_v1=0, loss_v2=0, nll_loss=0.705, ntokens=1061.1, nsentences=144, sample_size=1061.1, sample_size_v1=0, sample_size_v2=0, ppl=1.63, wps=429.8, ups=0.41, wpb=1061.1, bsz=144, num_updates=1940, lr=5.78529e-06, gnorm=1.568, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4809
2023-08-02 13:21:19 - progress_bar.py[line:272] - INFO: epoch 001:   1954 / 5589 loss=2.04, loss_v1=0, loss_v2=0, nll_loss=0.72, ntokens=1055.9, nsentences=144, sample_size=1055.9, sample_size_v1=0, sample_size_v2=0, ppl=1.65, wps=427.6, ups=0.4, wpb=1055.9, bsz=144, num_updates=1950, lr=5.81511e-06, gnorm=1.714, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4833
2023-08-02 13:21:44 - progress_bar.py[line:272] - INFO: epoch 001:   1964 / 5589 loss=2.033, loss_v1=0, loss_v2=0, nll_loss=0.712, ntokens=1055.6, nsentences=144, sample_size=1055.6, sample_size_v1=0, sample_size_v2=0, ppl=1.64, wps=427.6, ups=0.41, wpb=1055.6, bsz=144, num_updates=1960, lr=5.84493e-06, gnorm=1.617, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4858
2023-08-02 13:22:09 - progress_bar.py[line:272] - INFO: epoch 001:   1974 / 5589 loss=2.028, loss_v1=0, loss_v2=0, nll_loss=0.707, ntokens=1056.3, nsentences=144, sample_size=1056.3, sample_size_v1=0, sample_size_v2=0, ppl=1.63, wps=428.4, ups=0.41, wpb=1056.3, bsz=144, num_updates=1970, lr=5.87475e-06, gnorm=1.585, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4883
2023-08-02 13:22:33 - progress_bar.py[line:272] - INFO: epoch 001:   1984 / 5589 loss=2.032, loss_v1=0, loss_v2=0, nll_loss=0.712, ntokens=1056.6, nsentences=144, sample_size=1056.6, sample_size_v1=0, sample_size_v2=0, ppl=1.64, wps=428.4, ups=0.41, wpb=1056.6, bsz=144, num_updates=1980, lr=5.90457e-06, gnorm=1.594, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4907
2023-08-02 13:22:58 - progress_bar.py[line:272] - INFO: epoch 001:   1994 / 5589 loss=2.026, loss_v1=0, loss_v2=0, nll_loss=0.706, ntokens=1054.5, nsentences=144, sample_size=1054.5, sample_size_v1=0, sample_size_v2=0, ppl=1.63, wps=427.7, ups=0.41, wpb=1054.5, bsz=144, num_updates=1990, lr=5.93439e-06, gnorm=1.551, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4932
2023-08-02 13:23:23 - progress_bar.py[line:272] - INFO: epoch 001:   2004 / 5589 loss=2.037, loss_v1=0, loss_v2=0, nll_loss=0.718, ntokens=1053.4, nsentences=144, sample_size=1053.4, sample_size_v1=0, sample_size_v2=0, ppl=1.64, wps=427.4, ups=0.41, wpb=1053.4, bsz=144, num_updates=2000, lr=5.96421e-06, gnorm=1.592, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4957
2023-08-02 13:23:47 - progress_bar.py[line:272] - INFO: epoch 001:   2014 / 5589 loss=2.015, loss_v1=0, loss_v2=0, nll_loss=0.692, ntokens=1059.8, nsentences=144, sample_size=1059.8, sample_size_v1=0, sample_size_v2=0, ppl=1.62, wps=429.6, ups=0.41, wpb=1059.8, bsz=144, num_updates=2010, lr=5.99404e-06, gnorm=1.553, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=4981
2023-08-02 13:24:12 - progress_bar.py[line:272] - INFO: epoch 001:   2024 / 5589 loss=2.004, loss_v1=0, loss_v2=0, nll_loss=0.68, ntokens=1055.3, nsentences=144, sample_size=1055.3, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=427.4, ups=0.41, wpb=1055.3, bsz=144, num_updates=2020, lr=6.02386e-06, gnorm=1.635, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=5006
2023-08-02 13:24:37 - progress_bar.py[line:272] - INFO: epoch 001:   2034 / 5589 loss=2.028, loss_v1=0, loss_v2=0, nll_loss=0.712, ntokens=1053.3, nsentences=144, sample_size=1053.3, sample_size_v1=0, sample_size_v2=0, ppl=1.64, wps=426.2, ups=0.4, wpb=1053.3, bsz=144, num_updates=2030, lr=6.05368e-06, gnorm=1.611, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=5031
2023-08-02 13:25:01 - progress_bar.py[line:272] - INFO: epoch 001:   2044 / 5589 loss=2.029, loss_v1=0, loss_v2=0, nll_loss=0.709, ntokens=1053.6, nsentences=144, sample_size=1053.6, sample_size_v1=0, sample_size_v2=0, ppl=1.63, wps=426.7, ups=0.4, wpb=1053.6, bsz=144, num_updates=2040, lr=6.0835e-06, gnorm=1.6, clip=100, loss_scale=64, train_wall=25, gb_free=6.6, wall=5056
2023-08-02 13:25:26 - progress_bar.py[line:272] - INFO: epoch 001:   2054 / 5589 loss=2.027, loss_v1=0, loss_v2=0, nll_loss=0.71, ntokens=1053.9, nsentences=144, sample_size=1053.9, sample_size_v1=0, sample_size_v2=0, ppl=1.64, wps=426.7, ups=0.4, wpb=1053.9, bsz=144, num_updates=2050, lr=6.11332e-06, gnorm=1.666, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5080
2023-08-02 13:25:51 - progress_bar.py[line:272] - INFO: epoch 001:   2064 / 5589 loss=2.018, loss_v1=0, loss_v2=0, nll_loss=0.701, ntokens=1054.8, nsentences=144, sample_size=1054.8, sample_size_v1=0, sample_size_v2=0, ppl=1.63, wps=427.1, ups=0.4, wpb=1054.8, bsz=144, num_updates=2060, lr=6.14314e-06, gnorm=1.613, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5105
2023-08-02 13:26:15 - progress_bar.py[line:272] - INFO: epoch 001:   2074 / 5589 loss=2.005, loss_v1=0, loss_v2=0, nll_loss=0.684, ntokens=1054.7, nsentences=144, sample_size=1054.7, sample_size_v1=0, sample_size_v2=0, ppl=1.61, wps=427.1, ups=0.4, wpb=1054.7, bsz=144, num_updates=2070, lr=6.17296e-06, gnorm=1.55, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5130
2023-08-02 13:26:40 - progress_bar.py[line:272] - INFO: epoch 001:   2084 / 5589 loss=2.02, loss_v1=0, loss_v2=0, nll_loss=0.702, ntokens=1052.9, nsentences=144, sample_size=1052.9, sample_size_v1=0, sample_size_v2=0, ppl=1.63, wps=427.1, ups=0.41, wpb=1052.9, bsz=144, num_updates=2080, lr=6.20278e-06, gnorm=1.655, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5154
2023-08-02 13:27:05 - progress_bar.py[line:272] - INFO: epoch 001:   2094 / 5589 loss=2.017, loss_v1=0, loss_v2=0, nll_loss=0.696, ntokens=1056.8, nsentences=144, sample_size=1056.8, sample_size_v1=0, sample_size_v2=0, ppl=1.62, wps=428.7, ups=0.41, wpb=1056.8, bsz=144, num_updates=2090, lr=6.2326e-06, gnorm=1.613, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5179
2023-08-02 13:27:29 - progress_bar.py[line:272] - INFO: epoch 001:   2104 / 5589 loss=2.022, loss_v1=0, loss_v2=0, nll_loss=0.708, ntokens=1054.8, nsentences=144, sample_size=1054.8, sample_size_v1=0, sample_size_v2=0, ppl=1.63, wps=427.9, ups=0.41, wpb=1054.8, bsz=144, num_updates=2100, lr=6.26243e-06, gnorm=1.59, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5204
2023-08-02 13:27:54 - progress_bar.py[line:272] - INFO: epoch 001:   2114 / 5589 loss=1.997, loss_v1=0, loss_v2=0, nll_loss=0.674, ntokens=1054.5, nsentences=144, sample_size=1054.5, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=428, ups=0.41, wpb=1054.5, bsz=144, num_updates=2110, lr=6.29225e-06, gnorm=1.502, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5228
2023-08-02 13:28:19 - progress_bar.py[line:272] - INFO: epoch 001:   2124 / 5589 loss=2.008, loss_v1=0, loss_v2=0, nll_loss=0.689, ntokens=1060.3, nsentences=144, sample_size=1060.3, sample_size_v1=0, sample_size_v2=0, ppl=1.61, wps=430.1, ups=0.41, wpb=1060.3, bsz=144, num_updates=2120, lr=6.32207e-06, gnorm=1.599, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5253
2023-08-02 13:28:43 - progress_bar.py[line:272] - INFO: epoch 001:   2134 / 5589 loss=1.995, loss_v1=0, loss_v2=0, nll_loss=0.676, ntokens=1057.7, nsentences=144, sample_size=1057.7, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=429.1, ups=0.41, wpb=1057.7, bsz=144, num_updates=2130, lr=6.35189e-06, gnorm=1.622, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5278
2023-08-02 13:29:08 - progress_bar.py[line:272] - INFO: epoch 001:   2144 / 5589 loss=1.995, loss_v1=0, loss_v2=0, nll_loss=0.677, ntokens=1052.7, nsentences=144, sample_size=1052.7, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=427.2, ups=0.41, wpb=1052.7, bsz=144, num_updates=2140, lr=6.38171e-06, gnorm=1.627, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5302
2023-08-02 13:29:33 - progress_bar.py[line:272] - INFO: epoch 001:   2154 / 5589 loss=1.997, loss_v1=0, loss_v2=0, nll_loss=0.676, ntokens=1060.6, nsentences=144, sample_size=1060.6, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=430.2, ups=0.41, wpb=1060.6, bsz=144, num_updates=2150, lr=6.41153e-06, gnorm=1.576, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5327
2023-08-02 13:29:57 - progress_bar.py[line:272] - INFO: epoch 001:   2164 / 5589 loss=1.988, loss_v1=0, loss_v2=0, nll_loss=0.672, ntokens=1052.7, nsentences=144, sample_size=1052.7, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=427.1, ups=0.41, wpb=1052.7, bsz=144, num_updates=2160, lr=6.44135e-06, gnorm=1.577, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5352
2023-08-02 13:30:22 - progress_bar.py[line:272] - INFO: epoch 001:   2174 / 5589 loss=1.996, loss_v1=0, loss_v2=0, nll_loss=0.679, ntokens=1055.1, nsentences=144, sample_size=1055.1, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=427.6, ups=0.41, wpb=1055.1, bsz=144, num_updates=2170, lr=6.47117e-06, gnorm=1.65, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5376
2023-08-02 13:30:47 - progress_bar.py[line:272] - INFO: epoch 001:   2184 / 5589 loss=1.982, loss_v1=0, loss_v2=0, nll_loss=0.662, ntokens=1054.2, nsentences=144, sample_size=1054.2, sample_size_v1=0, sample_size_v2=0, ppl=1.58, wps=427.7, ups=0.41, wpb=1054.2, bsz=144, num_updates=2180, lr=6.50099e-06, gnorm=1.553, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5401
2023-08-02 13:31:11 - progress_bar.py[line:272] - INFO: epoch 001:   2194 / 5589 loss=1.993, loss_v1=0, loss_v2=0, nll_loss=0.676, ntokens=1056, nsentences=144, sample_size=1056, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=427.9, ups=0.41, wpb=1056, bsz=144, num_updates=2190, lr=6.53082e-06, gnorm=1.556, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5426
2023-08-02 13:31:36 - progress_bar.py[line:272] - INFO: epoch 001:   2204 / 5589 loss=1.994, loss_v1=0, loss_v2=0, nll_loss=0.678, ntokens=1057.1, nsentences=144, sample_size=1057.1, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=428.1, ups=0.4, wpb=1057.1, bsz=144, num_updates=2200, lr=6.56064e-06, gnorm=1.625, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5450
2023-08-02 13:32:01 - progress_bar.py[line:272] - INFO: epoch 001:   2214 / 5589 loss=1.993, loss_v1=0, loss_v2=0, nll_loss=0.675, ntokens=1055.9, nsentences=144, sample_size=1055.9, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=427.8, ups=0.41, wpb=1055.9, bsz=144, num_updates=2210, lr=6.59046e-06, gnorm=1.56, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5475
2023-08-02 13:32:25 - progress_bar.py[line:272] - INFO: epoch 001:   2224 / 5589 loss=1.987, loss_v1=0, loss_v2=0, nll_loss=0.67, ntokens=1056.4, nsentences=144, sample_size=1056.4, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=427.8, ups=0.4, wpb=1056.4, bsz=144, num_updates=2220, lr=6.62028e-06, gnorm=1.529, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5500
2023-08-02 13:32:50 - progress_bar.py[line:272] - INFO: epoch 001:   2234 / 5589 loss=1.967, loss_v1=0, loss_v2=0, nll_loss=0.647, ntokens=1058.3, nsentences=144, sample_size=1058.3, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=428.7, ups=0.41, wpb=1058.3, bsz=144, num_updates=2230, lr=6.6501e-06, gnorm=1.588, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5524
2023-08-02 13:33:15 - progress_bar.py[line:272] - INFO: epoch 001:   2244 / 5589 loss=1.966, loss_v1=0, loss_v2=0, nll_loss=0.646, ntokens=1049.2, nsentences=144, sample_size=1049.2, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=425.2, ups=0.41, wpb=1049.2, bsz=144, num_updates=2240, lr=6.67992e-06, gnorm=1.603, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5549
2023-08-02 13:33:39 - progress_bar.py[line:272] - INFO: epoch 001:   2254 / 5589 loss=1.962, loss_v1=0, loss_v2=0, nll_loss=0.641, ntokens=1060.1, nsentences=144, sample_size=1060.1, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=429.6, ups=0.41, wpb=1060.1, bsz=144, num_updates=2250, lr=6.70974e-06, gnorm=1.58, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5574
2023-08-02 13:34:04 - progress_bar.py[line:272] - INFO: epoch 001:   2264 / 5589 loss=1.983, loss_v1=0, loss_v2=0, nll_loss=0.667, ntokens=1054.4, nsentences=144, sample_size=1054.4, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=427.6, ups=0.41, wpb=1054.4, bsz=144, num_updates=2260, lr=6.73956e-06, gnorm=1.731, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5598
2023-08-02 13:34:29 - progress_bar.py[line:272] - INFO: epoch 001:   2274 / 5589 loss=1.974, loss_v1=0, loss_v2=0, nll_loss=0.655, ntokens=1055.1, nsentences=144, sample_size=1055.1, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=428.1, ups=0.41, wpb=1055.1, bsz=144, num_updates=2270, lr=6.76938e-06, gnorm=1.616, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5623
2023-08-02 13:34:53 - progress_bar.py[line:272] - INFO: epoch 001:   2284 / 5589 loss=1.973, loss_v1=0, loss_v2=0, nll_loss=0.655, ntokens=1058.4, nsentences=144, sample_size=1058.4, sample_size_v1=0, sample_size_v2=0, ppl=1.58, wps=429.3, ups=0.41, wpb=1058.4, bsz=144, num_updates=2280, lr=6.7992e-06, gnorm=1.629, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5648
2023-08-02 13:35:18 - progress_bar.py[line:272] - INFO: epoch 001:   2294 / 5589 loss=1.971, loss_v1=0, loss_v2=0, nll_loss=0.655, ntokens=1056.3, nsentences=144, sample_size=1056.3, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=427.8, ups=0.41, wpb=1056.3, bsz=144, num_updates=2290, lr=6.82903e-06, gnorm=1.6, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5672
2023-08-02 13:35:43 - progress_bar.py[line:272] - INFO: epoch 001:   2304 / 5589 loss=1.957, loss_v1=0, loss_v2=0, nll_loss=0.637, ntokens=1056.8, nsentences=144, sample_size=1056.8, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=428, ups=0.4, wpb=1056.8, bsz=144, num_updates=2300, lr=6.85885e-06, gnorm=1.644, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5697
2023-08-02 13:36:07 - progress_bar.py[line:272] - INFO: epoch 001:   2314 / 5589 loss=1.975, loss_v1=0, loss_v2=0, nll_loss=0.657, ntokens=1052.7, nsentences=144, sample_size=1052.7, sample_size_v1=0, sample_size_v2=0, ppl=1.58, wps=426.2, ups=0.4, wpb=1052.7, bsz=144, num_updates=2310, lr=6.88867e-06, gnorm=1.795, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5722
2023-08-02 13:36:32 - progress_bar.py[line:272] - INFO: epoch 001:   2324 / 5589 loss=1.969, loss_v1=0, loss_v2=0, nll_loss=0.652, ntokens=1055.9, nsentences=144, sample_size=1055.9, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=427.5, ups=0.4, wpb=1055.9, bsz=144, num_updates=2320, lr=6.91849e-06, gnorm=1.599, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5746
2023-08-02 13:36:57 - progress_bar.py[line:272] - INFO: epoch 001:   2334 / 5589 loss=1.957, loss_v1=0, loss_v2=0, nll_loss=0.639, ntokens=1050.3, nsentences=144, sample_size=1050.3, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=425.4, ups=0.4, wpb=1050.3, bsz=144, num_updates=2330, lr=6.94831e-06, gnorm=1.661, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5771
2023-08-02 13:37:22 - progress_bar.py[line:272] - INFO: epoch 001:   2344 / 5589 loss=1.964, loss_v1=0, loss_v2=0, nll_loss=0.643, ntokens=1054, nsentences=144, sample_size=1054, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=427, ups=0.41, wpb=1054, bsz=144, num_updates=2340, lr=6.97813e-06, gnorm=1.634, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5796
2023-08-02 13:37:46 - progress_bar.py[line:272] - INFO: epoch 001:   2354 / 5589 loss=1.96, loss_v1=0, loss_v2=0, nll_loss=0.643, ntokens=1054.9, nsentences=144, sample_size=1054.9, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=427.1, ups=0.4, wpb=1054.9, bsz=144, num_updates=2350, lr=7.00795e-06, gnorm=1.562, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5820
2023-08-02 13:38:11 - progress_bar.py[line:272] - INFO: epoch 001:   2364 / 5589 loss=1.954, loss_v1=0, loss_v2=0, nll_loss=0.639, ntokens=1059.6, nsentences=144, sample_size=1059.6, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=430, ups=0.41, wpb=1059.6, bsz=144, num_updates=2360, lr=7.03777e-06, gnorm=1.634, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5845
2023-08-02 13:38:36 - progress_bar.py[line:272] - INFO: epoch 001:   2374 / 5589 loss=1.95, loss_v1=0, loss_v2=0, nll_loss=0.63, ntokens=1051.8, nsentences=144, sample_size=1051.8, sample_size_v1=0, sample_size_v2=0, ppl=1.55, wps=426.9, ups=0.41, wpb=1051.8, bsz=144, num_updates=2370, lr=7.06759e-06, gnorm=1.577, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5870
2023-08-02 13:39:00 - progress_bar.py[line:272] - INFO: epoch 001:   2384 / 5589 loss=1.948, loss_v1=0, loss_v2=0, nll_loss=0.632, ntokens=1060.8, nsentences=144, sample_size=1060.8, sample_size_v1=0, sample_size_v2=0, ppl=1.55, wps=430.6, ups=0.41, wpb=1060.8, bsz=144, num_updates=2380, lr=7.09742e-06, gnorm=1.546, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5894
2023-08-02 13:39:25 - progress_bar.py[line:272] - INFO: epoch 001:   2394 / 5589 loss=1.965, loss_v1=0, loss_v2=0, nll_loss=0.649, ntokens=1058.7, nsentences=144, sample_size=1058.7, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=429.3, ups=0.41, wpb=1058.7, bsz=144, num_updates=2390, lr=7.12724e-06, gnorm=1.549, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5919
2023-08-02 13:39:49 - progress_bar.py[line:272] - INFO: epoch 001:   2404 / 5589 loss=1.955, loss_v1=0, loss_v2=0, nll_loss=0.64, ntokens=1055.6, nsentences=144, sample_size=1055.6, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=428.3, ups=0.41, wpb=1055.6, bsz=144, num_updates=2400, lr=7.15706e-06, gnorm=1.606, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5944
2023-08-02 13:40:14 - progress_bar.py[line:272] - INFO: epoch 001:   2414 / 5589 loss=1.947, loss_v1=0, loss_v2=0, nll_loss=0.63, ntokens=1056.4, nsentences=144, sample_size=1056.4, sample_size_v1=0, sample_size_v2=0, ppl=1.55, wps=428.7, ups=0.41, wpb=1056.4, bsz=144, num_updates=2410, lr=7.18688e-06, gnorm=1.498, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5968
2023-08-02 13:40:39 - progress_bar.py[line:272] - INFO: epoch 001:   2424 / 5589 loss=1.95, loss_v1=0, loss_v2=0, nll_loss=0.633, ntokens=1057.5, nsentences=144, sample_size=1057.5, sample_size_v1=0, sample_size_v2=0, ppl=1.55, wps=429, ups=0.41, wpb=1057.5, bsz=144, num_updates=2420, lr=7.2167e-06, gnorm=1.58, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=5993
2023-08-02 13:41:03 - progress_bar.py[line:272] - INFO: epoch 001:   2434 / 5589 loss=1.946, loss_v1=0, loss_v2=0, nll_loss=0.628, ntokens=1055.2, nsentences=144, sample_size=1055.2, sample_size_v1=0, sample_size_v2=0, ppl=1.55, wps=428.2, ups=0.41, wpb=1055.2, bsz=144, num_updates=2430, lr=7.24652e-06, gnorm=1.555, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6018
2023-08-02 13:41:28 - progress_bar.py[line:272] - INFO: epoch 001:   2444 / 5589 loss=1.94, loss_v1=0, loss_v2=0, nll_loss=0.621, ntokens=1052.2, nsentences=144, sample_size=1052.2, sample_size_v1=0, sample_size_v2=0, ppl=1.54, wps=426.9, ups=0.41, wpb=1052.2, bsz=144, num_updates=2440, lr=7.27634e-06, gnorm=1.587, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6042
2023-08-02 13:41:53 - progress_bar.py[line:272] - INFO: epoch 001:   2454 / 5589 loss=1.952, loss_v1=0, loss_v2=0, nll_loss=0.638, ntokens=1057.6, nsentences=144, sample_size=1057.6, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=429.1, ups=0.41, wpb=1057.6, bsz=144, num_updates=2450, lr=7.30616e-06, gnorm=1.593, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6067
2023-08-02 13:42:17 - progress_bar.py[line:272] - INFO: epoch 001:   2464 / 5589 loss=1.946, loss_v1=0, loss_v2=0, nll_loss=0.632, ntokens=1058.6, nsentences=144, sample_size=1058.6, sample_size_v1=0, sample_size_v2=0, ppl=1.55, wps=429.4, ups=0.41, wpb=1058.6, bsz=144, num_updates=2460, lr=7.33598e-06, gnorm=1.629, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6092
2023-08-02 13:42:42 - progress_bar.py[line:272] - INFO: epoch 001:   2474 / 5589 loss=1.94, loss_v1=0, loss_v2=0, nll_loss=0.623, ntokens=1053.7, nsentences=144, sample_size=1053.7, sample_size_v1=0, sample_size_v2=0, ppl=1.54, wps=427.5, ups=0.41, wpb=1053.7, bsz=144, num_updates=2470, lr=7.36581e-06, gnorm=1.521, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6116
2023-08-02 13:43:07 - progress_bar.py[line:272] - INFO: epoch 001:   2484 / 5589 loss=1.955, loss_v1=0, loss_v2=0, nll_loss=0.64, ntokens=1051, nsentences=144, sample_size=1051, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=426.4, ups=0.41, wpb=1051, bsz=144, num_updates=2480, lr=7.39563e-06, gnorm=1.594, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6141
2023-08-02 13:43:31 - progress_bar.py[line:272] - INFO: epoch 001:   2494 / 5589 loss=1.946, loss_v1=0, loss_v2=0, nll_loss=0.631, ntokens=1053, nsentences=144, sample_size=1053, sample_size_v1=0, sample_size_v2=0, ppl=1.55, wps=427.2, ups=0.41, wpb=1053, bsz=144, num_updates=2490, lr=7.42545e-06, gnorm=1.531, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6166
2023-08-02 13:43:56 - progress_bar.py[line:272] - INFO: epoch 001:   2504 / 5589 loss=1.939, loss_v1=0, loss_v2=0, nll_loss=0.622, ntokens=1058.9, nsentences=144, sample_size=1058.9, sample_size_v1=0, sample_size_v2=0, ppl=1.54, wps=429.5, ups=0.41, wpb=1058.9, bsz=144, num_updates=2500, lr=7.45527e-06, gnorm=1.523, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6190
2023-08-02 13:44:21 - progress_bar.py[line:272] - INFO: epoch 001:   2514 / 5589 loss=1.932, loss_v1=0, loss_v2=0, nll_loss=0.614, ntokens=1057.4, nsentences=144, sample_size=1057.4, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=428.9, ups=0.41, wpb=1057.4, bsz=144, num_updates=2510, lr=7.48509e-06, gnorm=1.563, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6215
2023-08-02 13:44:45 - progress_bar.py[line:272] - INFO: epoch 001:   2524 / 5589 loss=1.958, loss_v1=0, loss_v2=0, nll_loss=0.646, ntokens=1057.2, nsentences=144, sample_size=1057.2, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=429, ups=0.41, wpb=1057.2, bsz=144, num_updates=2520, lr=7.51491e-06, gnorm=1.637, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6240
2023-08-02 13:45:10 - progress_bar.py[line:272] - INFO: epoch 001:   2534 / 5589 loss=1.945, loss_v1=0, loss_v2=0, nll_loss=0.628, ntokens=1054.8, nsentences=144, sample_size=1054.8, sample_size_v1=0, sample_size_v2=0, ppl=1.55, wps=428.3, ups=0.41, wpb=1054.8, bsz=144, num_updates=2530, lr=7.54473e-06, gnorm=1.603, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6264
2023-08-02 13:45:35 - progress_bar.py[line:272] - INFO: epoch 001:   2544 / 5589 loss=1.937, loss_v1=0, loss_v2=0, nll_loss=0.623, ntokens=1058.8, nsentences=144, sample_size=1058.8, sample_size_v1=0, sample_size_v2=0, ppl=1.54, wps=429.8, ups=0.41, wpb=1058.8, bsz=144, num_updates=2540, lr=7.57455e-06, gnorm=1.615, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6289
2023-08-02 13:45:59 - progress_bar.py[line:272] - INFO: epoch 001:   2554 / 5589 loss=1.92, loss_v1=0, loss_v2=0, nll_loss=0.601, ntokens=1054, nsentences=144, sample_size=1054, sample_size_v1=0, sample_size_v2=0, ppl=1.52, wps=427.7, ups=0.41, wpb=1054, bsz=144, num_updates=2550, lr=7.60437e-06, gnorm=1.476, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6313
2023-08-02 13:46:24 - progress_bar.py[line:272] - INFO: epoch 001:   2564 / 5589 loss=1.935, loss_v1=0, loss_v2=0, nll_loss=0.619, ntokens=1052.9, nsentences=144, sample_size=1052.9, sample_size_v1=0, sample_size_v2=0, ppl=1.54, wps=427.2, ups=0.41, wpb=1052.9, bsz=144, num_updates=2560, lr=7.63419e-06, gnorm=1.503, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=6338
2023-08-02 13:46:36 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-08-02 13:46:51 - progress_bar.py[line:272] - INFO: epoch 001:   2575 / 5589 loss=1.924, loss_v1=0, loss_v2=0, nll_loss=0.606, ntokens=1059.3, nsentences=144, sample_size=1059.3, sample_size_v1=0, sample_size_v2=0, ppl=1.52, wps=391.2, ups=0.37, wpb=1059.3, bsz=144, num_updates=2570, lr=7.66402e-06, gnorm=1.601, clip=100, loss_scale=128, train_wall=27, gb_free=6.6, wall=6365
2023-08-02 13:47:16 - progress_bar.py[line:272] - INFO: epoch 001:   2585 / 5589 loss=1.927, loss_v1=0, loss_v2=0, nll_loss=0.609, ntokens=1054.8, nsentences=144, sample_size=1054.8, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=428, ups=0.41, wpb=1054.8, bsz=144, num_updates=2580, lr=7.69384e-06, gnorm=1.641, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6390
2023-08-02 13:47:40 - progress_bar.py[line:272] - INFO: epoch 001:   2595 / 5589 loss=1.954, loss_v1=0, loss_v2=0, nll_loss=0.643, ntokens=1061.4, nsentences=144, sample_size=1061.4, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=430.8, ups=0.41, wpb=1061.4, bsz=144, num_updates=2590, lr=7.72366e-06, gnorm=1.674, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6414
2023-08-02 13:48:05 - progress_bar.py[line:272] - INFO: epoch 001:   2605 / 5589 loss=1.931, loss_v1=0, loss_v2=0, nll_loss=0.614, ntokens=1058.7, nsentences=144, sample_size=1058.7, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=429.6, ups=0.41, wpb=1058.7, bsz=144, num_updates=2600, lr=7.75348e-06, gnorm=1.589, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6439
2023-08-02 13:48:29 - progress_bar.py[line:272] - INFO: epoch 001:   2615 / 5589 loss=1.914, loss_v1=0, loss_v2=0, nll_loss=0.596, ntokens=1053.3, nsentences=144, sample_size=1053.3, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=427.5, ups=0.41, wpb=1053.3, bsz=144, num_updates=2610, lr=7.7833e-06, gnorm=1.574, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6464
2023-08-02 13:48:54 - progress_bar.py[line:272] - INFO: epoch 001:   2625 / 5589 loss=1.922, loss_v1=0, loss_v2=0, nll_loss=0.606, ntokens=1056.5, nsentences=144, sample_size=1056.5, sample_size_v1=0, sample_size_v2=0, ppl=1.52, wps=428.9, ups=0.41, wpb=1056.5, bsz=144, num_updates=2620, lr=7.81312e-06, gnorm=1.556, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6488
2023-08-02 13:49:19 - progress_bar.py[line:272] - INFO: epoch 001:   2635 / 5589 loss=1.921, loss_v1=0, loss_v2=0, nll_loss=0.607, ntokens=1060.9, nsentences=144, sample_size=1060.9, sample_size_v1=0, sample_size_v2=0, ppl=1.52, wps=430.6, ups=0.41, wpb=1060.9, bsz=144, num_updates=2630, lr=7.84294e-06, gnorm=1.579, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6513
2023-08-02 13:49:43 - progress_bar.py[line:272] - INFO: epoch 001:   2645 / 5589 loss=1.939, loss_v1=0, loss_v2=0, nll_loss=0.627, ntokens=1057.2, nsentences=144, sample_size=1057.2, sample_size_v1=0, sample_size_v2=0, ppl=1.54, wps=429.2, ups=0.41, wpb=1057.2, bsz=144, num_updates=2640, lr=7.87276e-06, gnorm=1.575, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6538
2023-08-02 13:50:08 - progress_bar.py[line:272] - INFO: epoch 001:   2655 / 5589 loss=1.935, loss_v1=0, loss_v2=0, nll_loss=0.622, ntokens=1057.2, nsentences=144, sample_size=1057.2, sample_size_v1=0, sample_size_v2=0, ppl=1.54, wps=429.1, ups=0.41, wpb=1057.2, bsz=144, num_updates=2650, lr=7.90258e-06, gnorm=1.61, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6562
2023-08-02 13:50:33 - progress_bar.py[line:272] - INFO: epoch 001:   2665 / 5589 loss=1.931, loss_v1=0, loss_v2=0, nll_loss=0.617, ntokens=1056.9, nsentences=144, sample_size=1056.9, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=428.8, ups=0.41, wpb=1056.9, bsz=144, num_updates=2660, lr=7.93241e-06, gnorm=1.585, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6587
2023-08-02 13:50:57 - progress_bar.py[line:272] - INFO: epoch 001:   2675 / 5589 loss=1.907, loss_v1=0, loss_v2=0, nll_loss=0.591, ntokens=1058, nsentences=144, sample_size=1058, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=429.6, ups=0.41, wpb=1058, bsz=144, num_updates=2670, lr=7.96223e-06, gnorm=1.537, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6612
2023-08-02 13:51:22 - progress_bar.py[line:272] - INFO: epoch 001:   2685 / 5589 loss=1.906, loss_v1=0, loss_v2=0, nll_loss=0.589, ntokens=1057.3, nsentences=144, sample_size=1057.3, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=429.2, ups=0.41, wpb=1057.3, bsz=144, num_updates=2680, lr=7.99205e-06, gnorm=1.684, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6636
2023-08-02 13:51:47 - progress_bar.py[line:272] - INFO: epoch 001:   2695 / 5589 loss=1.926, loss_v1=0, loss_v2=0, nll_loss=0.61, ntokens=1057.3, nsentences=144, sample_size=1057.3, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=429.1, ups=0.41, wpb=1057.3, bsz=144, num_updates=2690, lr=8.02187e-06, gnorm=1.547, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6661
2023-08-02 13:52:11 - progress_bar.py[line:272] - INFO: epoch 001:   2705 / 5589 loss=1.906, loss_v1=0, loss_v2=0, nll_loss=0.592, ntokens=1055.6, nsentences=144, sample_size=1055.6, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=428.4, ups=0.41, wpb=1055.6, bsz=144, num_updates=2700, lr=8.05169e-06, gnorm=1.471, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6686
2023-08-02 13:52:36 - progress_bar.py[line:272] - INFO: epoch 001:   2715 / 5589 loss=1.915, loss_v1=0, loss_v2=0, nll_loss=0.597, ntokens=1055.7, nsentences=144, sample_size=1055.7, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=428.6, ups=0.41, wpb=1055.7, bsz=144, num_updates=2710, lr=8.08151e-06, gnorm=1.525, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6710
2023-08-02 13:53:01 - progress_bar.py[line:272] - INFO: epoch 001:   2725 / 5589 loss=1.914, loss_v1=0, loss_v2=0, nll_loss=0.603, ntokens=1053, nsentences=144, sample_size=1053, sample_size_v1=0, sample_size_v2=0, ppl=1.52, wps=427.4, ups=0.41, wpb=1053, bsz=144, num_updates=2720, lr=8.11133e-06, gnorm=1.56, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6735
2023-08-02 13:53:25 - progress_bar.py[line:272] - INFO: epoch 001:   2735 / 5589 loss=1.898, loss_v1=0, loss_v2=0, nll_loss=0.578, ntokens=1056.4, nsentences=144, sample_size=1056.4, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=428.8, ups=0.41, wpb=1056.4, bsz=144, num_updates=2730, lr=8.14115e-06, gnorm=1.521, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6759
2023-08-02 13:53:50 - progress_bar.py[line:272] - INFO: epoch 001:   2745 / 5589 loss=1.912, loss_v1=0, loss_v2=0, nll_loss=0.596, ntokens=1057, nsentences=144, sample_size=1057, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=429.2, ups=0.41, wpb=1057, bsz=144, num_updates=2740, lr=8.17097e-06, gnorm=1.484, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6784
2023-08-02 13:54:14 - progress_bar.py[line:272] - INFO: epoch 001:   2755 / 5589 loss=1.911, loss_v1=0, loss_v2=0, nll_loss=0.597, ntokens=1056.9, nsentences=144, sample_size=1056.9, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=428.8, ups=0.41, wpb=1056.9, bsz=144, num_updates=2750, lr=8.2008e-06, gnorm=1.557, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6809
2023-08-02 13:54:39 - progress_bar.py[line:272] - INFO: epoch 001:   2765 / 5589 loss=1.924, loss_v1=0, loss_v2=0, nll_loss=0.611, ntokens=1056.5, nsentences=144, sample_size=1056.5, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=428.7, ups=0.41, wpb=1056.5, bsz=144, num_updates=2760, lr=8.23062e-06, gnorm=1.543, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6833
2023-08-02 13:55:04 - progress_bar.py[line:272] - INFO: epoch 001:   2775 / 5589 loss=1.912, loss_v1=0, loss_v2=0, nll_loss=0.596, ntokens=1056.3, nsentences=144, sample_size=1056.3, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=428.5, ups=0.41, wpb=1056.3, bsz=144, num_updates=2770, lr=8.26044e-06, gnorm=1.528, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6858
2023-08-02 13:55:28 - progress_bar.py[line:272] - INFO: epoch 001:   2785 / 5589 loss=1.911, loss_v1=0, loss_v2=0, nll_loss=0.598, ntokens=1056.6, nsentences=144, sample_size=1056.6, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=428.6, ups=0.41, wpb=1056.6, bsz=144, num_updates=2780, lr=8.29026e-06, gnorm=1.495, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6883
2023-08-02 13:55:53 - progress_bar.py[line:272] - INFO: epoch 001:   2795 / 5589 loss=1.91, loss_v1=0, loss_v2=0, nll_loss=0.594, ntokens=1058.1, nsentences=144, sample_size=1058.1, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=429.1, ups=0.41, wpb=1058.1, bsz=144, num_updates=2790, lr=8.32008e-06, gnorm=1.601, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6907
2023-08-02 13:56:18 - progress_bar.py[line:272] - INFO: epoch 001:   2805 / 5589 loss=1.926, loss_v1=0, loss_v2=0, nll_loss=0.617, ntokens=1057.6, nsentences=144, sample_size=1057.6, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=429.2, ups=0.41, wpb=1057.6, bsz=144, num_updates=2800, lr=8.3499e-06, gnorm=1.646, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6932
2023-08-02 13:56:42 - progress_bar.py[line:272] - INFO: epoch 001:   2815 / 5589 loss=1.915, loss_v1=0, loss_v2=0, nll_loss=0.602, ntokens=1054.4, nsentences=144, sample_size=1054.4, sample_size_v1=0, sample_size_v2=0, ppl=1.52, wps=427.8, ups=0.41, wpb=1054.4, bsz=144, num_updates=2810, lr=8.37972e-06, gnorm=1.594, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6957
2023-08-02 13:57:07 - progress_bar.py[line:272] - INFO: epoch 001:   2825 / 5589 loss=1.898, loss_v1=0, loss_v2=0, nll_loss=0.581, ntokens=1056.8, nsentences=144, sample_size=1056.8, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=428.6, ups=0.41, wpb=1056.8, bsz=144, num_updates=2820, lr=8.40954e-06, gnorm=1.504, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=6981
2023-08-02 13:57:32 - progress_bar.py[line:272] - INFO: epoch 001:   2835 / 5589 loss=1.911, loss_v1=0, loss_v2=0, nll_loss=0.597, ntokens=1056.4, nsentences=144, sample_size=1056.4, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=427.5, ups=0.4, wpb=1056.4, bsz=144, num_updates=2830, lr=8.43936e-06, gnorm=1.622, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7006
2023-08-02 13:57:56 - progress_bar.py[line:272] - INFO: epoch 001:   2845 / 5589 loss=1.898, loss_v1=0, loss_v2=0, nll_loss=0.584, ntokens=1058.4, nsentences=144, sample_size=1058.4, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=428.7, ups=0.41, wpb=1058.4, bsz=144, num_updates=2840, lr=8.46918e-06, gnorm=1.476, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7031
2023-08-02 13:58:21 - progress_bar.py[line:272] - INFO: epoch 001:   2855 / 5589 loss=1.917, loss_v1=0, loss_v2=0, nll_loss=0.605, ntokens=1058.3, nsentences=144, sample_size=1058.3, sample_size_v1=0, sample_size_v2=0, ppl=1.52, wps=429.3, ups=0.41, wpb=1058.3, bsz=144, num_updates=2850, lr=8.49901e-06, gnorm=1.573, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7055
2023-08-02 13:58:46 - progress_bar.py[line:272] - INFO: epoch 001:   2865 / 5589 loss=1.909, loss_v1=0, loss_v2=0, nll_loss=0.596, ntokens=1052, nsentences=144, sample_size=1052, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=426.7, ups=0.41, wpb=1052, bsz=144, num_updates=2860, lr=8.52883e-06, gnorm=1.566, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7080
2023-08-02 13:59:10 - progress_bar.py[line:272] - INFO: epoch 001:   2875 / 5589 loss=1.893, loss_v1=0, loss_v2=0, nll_loss=0.58, ntokens=1055.9, nsentences=144, sample_size=1055.9, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=428.3, ups=0.41, wpb=1055.9, bsz=144, num_updates=2870, lr=8.55865e-06, gnorm=1.501, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7105
2023-08-02 13:59:35 - progress_bar.py[line:272] - INFO: epoch 001:   2885 / 5589 loss=1.889, loss_v1=0, loss_v2=0, nll_loss=0.574, ntokens=1054.7, nsentences=144, sample_size=1054.7, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=427.8, ups=0.41, wpb=1054.7, bsz=144, num_updates=2880, lr=8.58847e-06, gnorm=1.573, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7129
2023-08-02 14:00:00 - progress_bar.py[line:272] - INFO: epoch 001:   2895 / 5589 loss=1.898, loss_v1=0, loss_v2=0, nll_loss=0.584, ntokens=1053.9, nsentences=144, sample_size=1053.9, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=427.7, ups=0.41, wpb=1053.9, bsz=144, num_updates=2890, lr=8.61829e-06, gnorm=1.555, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7154
2023-08-02 14:00:24 - progress_bar.py[line:272] - INFO: epoch 001:   2905 / 5589 loss=1.911, loss_v1=0, loss_v2=0, nll_loss=0.6, ntokens=1057.6, nsentences=144, sample_size=1057.6, sample_size_v1=0, sample_size_v2=0, ppl=1.52, wps=429.5, ups=0.41, wpb=1057.6, bsz=144, num_updates=2900, lr=8.64811e-06, gnorm=1.683, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7179
2023-08-02 14:00:49 - progress_bar.py[line:272] - INFO: epoch 001:   2915 / 5589 loss=1.876, loss_v1=0, loss_v2=0, nll_loss=0.56, ntokens=1058.5, nsentences=144, sample_size=1058.5, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=429.9, ups=0.41, wpb=1058.5, bsz=144, num_updates=2910, lr=8.67793e-06, gnorm=1.463, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7203
2023-08-02 14:01:14 - progress_bar.py[line:272] - INFO: epoch 001:   2925 / 5589 loss=1.91, loss_v1=0, loss_v2=0, nll_loss=0.599, ntokens=1054, nsentences=144, sample_size=1054, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=427.7, ups=0.41, wpb=1054, bsz=144, num_updates=2920, lr=8.70775e-06, gnorm=1.554, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7228
2023-08-02 14:01:38 - progress_bar.py[line:272] - INFO: epoch 001:   2935 / 5589 loss=1.881, loss_v1=0, loss_v2=0, nll_loss=0.565, ntokens=1055.5, nsentences=144, sample_size=1055.5, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=428.7, ups=0.41, wpb=1055.5, bsz=144, num_updates=2930, lr=8.73757e-06, gnorm=1.511, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7253
2023-08-02 14:02:03 - progress_bar.py[line:272] - INFO: epoch 001:   2945 / 5589 loss=1.9, loss_v1=0, loss_v2=0, nll_loss=0.588, ntokens=1057.5, nsentences=144, sample_size=1057.5, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=429.5, ups=0.41, wpb=1057.5, bsz=144, num_updates=2940, lr=8.7674e-06, gnorm=1.575, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7277
2023-08-02 14:02:28 - progress_bar.py[line:272] - INFO: epoch 001:   2955 / 5589 loss=1.901, loss_v1=0, loss_v2=0, nll_loss=0.59, ntokens=1056.7, nsentences=144, sample_size=1056.7, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=428.7, ups=0.41, wpb=1056.7, bsz=144, num_updates=2950, lr=8.79722e-06, gnorm=1.545, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7302
2023-08-02 14:02:52 - progress_bar.py[line:272] - INFO: epoch 001:   2965 / 5589 loss=1.888, loss_v1=0, loss_v2=0, nll_loss=0.575, ntokens=1055.7, nsentences=144, sample_size=1055.7, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=428.7, ups=0.41, wpb=1055.7, bsz=144, num_updates=2960, lr=8.82704e-06, gnorm=1.508, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7326
2023-08-02 14:03:17 - progress_bar.py[line:272] - INFO: epoch 001:   2975 / 5589 loss=1.908, loss_v1=0, loss_v2=0, nll_loss=0.599, ntokens=1056.7, nsentences=144, sample_size=1056.7, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=429, ups=0.41, wpb=1056.7, bsz=144, num_updates=2970, lr=8.85686e-06, gnorm=1.537, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7351
2023-08-02 14:03:41 - progress_bar.py[line:272] - INFO: epoch 001:   2985 / 5589 loss=1.883, loss_v1=0, loss_v2=0, nll_loss=0.568, ntokens=1056.3, nsentences=144, sample_size=1056.3, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=428.8, ups=0.41, wpb=1056.3, bsz=144, num_updates=2980, lr=8.88668e-06, gnorm=1.456, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7376
2023-08-02 14:04:06 - progress_bar.py[line:272] - INFO: epoch 001:   2995 / 5589 loss=1.886, loss_v1=0, loss_v2=0, nll_loss=0.574, ntokens=1056.1, nsentences=144, sample_size=1056.1, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=427.4, ups=0.4, wpb=1056.1, bsz=144, num_updates=2990, lr=8.9165e-06, gnorm=1.522, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7400
2023-08-02 14:04:31 - progress_bar.py[line:272] - INFO: epoch 001:   3005 / 5589 loss=1.891, loss_v1=0, loss_v2=0, nll_loss=0.578, ntokens=1060.9, nsentences=144, sample_size=1060.9, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=429.7, ups=0.41, wpb=1060.9, bsz=144, num_updates=3000, lr=8.94632e-06, gnorm=1.537, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7425
2023-08-02 14:04:56 - progress_bar.py[line:272] - INFO: epoch 001:   3015 / 5589 loss=1.889, loss_v1=0, loss_v2=0, nll_loss=0.577, ntokens=1055.5, nsentences=144, sample_size=1055.5, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=427.2, ups=0.4, wpb=1055.5, bsz=144, num_updates=3010, lr=8.97614e-06, gnorm=1.43, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7450
2023-08-02 14:05:20 - progress_bar.py[line:272] - INFO: epoch 001:   3025 / 5589 loss=1.879, loss_v1=0, loss_v2=0, nll_loss=0.564, ntokens=1058.5, nsentences=144, sample_size=1058.5, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=428.6, ups=0.4, wpb=1058.5, bsz=144, num_updates=3020, lr=9.00596e-06, gnorm=1.509, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7475
2023-08-02 14:05:45 - progress_bar.py[line:272] - INFO: epoch 001:   3035 / 5589 loss=1.888, loss_v1=0, loss_v2=0, nll_loss=0.575, ntokens=1055.9, nsentences=144, sample_size=1055.9, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=427.7, ups=0.41, wpb=1055.9, bsz=144, num_updates=3030, lr=9.03579e-06, gnorm=1.557, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7499
2023-08-02 14:06:10 - progress_bar.py[line:272] - INFO: epoch 001:   3045 / 5589 loss=1.877, loss_v1=0, loss_v2=0, nll_loss=0.565, ntokens=1055.5, nsentences=144, sample_size=1055.5, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=427.5, ups=0.41, wpb=1055.5, bsz=144, num_updates=3040, lr=9.06561e-06, gnorm=1.463, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7524
2023-08-02 14:06:34 - progress_bar.py[line:272] - INFO: epoch 001:   3055 / 5589 loss=1.894, loss_v1=0, loss_v2=0, nll_loss=0.58, ntokens=1057.2, nsentences=144, sample_size=1057.2, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=428.8, ups=0.41, wpb=1057.2, bsz=144, num_updates=3050, lr=9.09543e-06, gnorm=1.488, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7549
2023-08-02 14:06:59 - progress_bar.py[line:272] - INFO: epoch 001:   3065 / 5589 loss=1.893, loss_v1=0, loss_v2=0, nll_loss=0.584, ntokens=1056.2, nsentences=144, sample_size=1056.2, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=428.3, ups=0.41, wpb=1056.2, bsz=144, num_updates=3060, lr=9.12525e-06, gnorm=1.577, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7573
2023-08-02 14:07:24 - progress_bar.py[line:272] - INFO: epoch 001:   3075 / 5589 loss=1.884, loss_v1=0, loss_v2=0, nll_loss=0.569, ntokens=1053.6, nsentences=144, sample_size=1053.6, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=427.5, ups=0.41, wpb=1053.6, bsz=144, num_updates=3070, lr=9.15507e-06, gnorm=1.571, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7598
2023-08-02 14:07:48 - progress_bar.py[line:272] - INFO: epoch 001:   3085 / 5589 loss=1.881, loss_v1=0, loss_v2=0, nll_loss=0.568, ntokens=1059.3, nsentences=144, sample_size=1059.3, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=429.7, ups=0.41, wpb=1059.3, bsz=144, num_updates=3080, lr=9.18489e-06, gnorm=1.494, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=7623
2023-08-02 14:08:13 - progress_bar.py[line:272] - INFO: epoch 001:   3095 / 5589 loss=1.89, loss_v1=0, loss_v2=0, nll_loss=0.578, ntokens=1053.7, nsentences=144, sample_size=1053.7, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=427.3, ups=0.41, wpb=1053.7, bsz=144, num_updates=3090, lr=9.21471e-06, gnorm=1.526, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=7647
2023-08-02 14:08:38 - progress_bar.py[line:272] - INFO: epoch 001:   3105 / 5589 loss=1.892, loss_v1=0, loss_v2=0, nll_loss=0.58, ntokens=1052.8, nsentences=144, sample_size=1052.8, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=427.1, ups=0.41, wpb=1052.8, bsz=144, num_updates=3100, lr=9.24453e-06, gnorm=1.526, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=7672
2023-08-02 14:09:02 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-08-02 14:09:05 - progress_bar.py[line:272] - INFO: epoch 001:   3116 / 5589 loss=1.869, loss_v1=0, loss_v2=0, nll_loss=0.555, ntokens=1057.3, nsentences=144, sample_size=1057.3, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=390.3, ups=0.37, wpb=1057.3, bsz=144, num_updates=3110, lr=9.27435e-06, gnorm=1.551, clip=100, loss_scale=128, train_wall=27, gb_free=6.6, wall=7699
2023-08-02 14:09:29 - progress_bar.py[line:272] - INFO: epoch 001:   3126 / 5589 loss=1.882, loss_v1=0, loss_v2=0, nll_loss=0.569, ntokens=1052.9, nsentences=144, sample_size=1052.9, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=427.2, ups=0.41, wpb=1052.9, bsz=144, num_updates=3120, lr=9.30417e-06, gnorm=1.487, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7724
2023-08-02 14:09:54 - progress_bar.py[line:272] - INFO: epoch 001:   3136 / 5589 loss=1.862, loss_v1=0, loss_v2=0, nll_loss=0.547, ntokens=1059.8, nsentences=144, sample_size=1059.8, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=429.6, ups=0.41, wpb=1059.8, bsz=144, num_updates=3130, lr=9.334e-06, gnorm=1.509, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7748
2023-08-02 14:10:19 - progress_bar.py[line:272] - INFO: epoch 001:   3146 / 5589 loss=1.881, loss_v1=0, loss_v2=0, nll_loss=0.57, ntokens=1051.3, nsentences=144, sample_size=1051.3, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=426.5, ups=0.41, wpb=1051.3, bsz=144, num_updates=3140, lr=9.36382e-06, gnorm=1.502, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7773
2023-08-02 14:10:43 - progress_bar.py[line:272] - INFO: epoch 001:   3156 / 5589 loss=1.872, loss_v1=0, loss_v2=0, nll_loss=0.559, ntokens=1053.9, nsentences=144, sample_size=1053.9, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=427.8, ups=0.41, wpb=1053.9, bsz=144, num_updates=3150, lr=9.39364e-06, gnorm=1.488, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7798
2023-08-02 14:11:08 - progress_bar.py[line:272] - INFO: epoch 001:   3166 / 5589 loss=1.876, loss_v1=0, loss_v2=0, nll_loss=0.565, ntokens=1051.7, nsentences=144, sample_size=1051.7, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=426.7, ups=0.41, wpb=1051.7, bsz=144, num_updates=3160, lr=9.42346e-06, gnorm=1.587, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7822
2023-08-02 14:11:33 - progress_bar.py[line:272] - INFO: epoch 001:   3176 / 5589 loss=1.874, loss_v1=0, loss_v2=0, nll_loss=0.562, ntokens=1055.6, nsentences=144, sample_size=1055.6, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=428.3, ups=0.41, wpb=1055.6, bsz=144, num_updates=3170, lr=9.45328e-06, gnorm=1.524, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7847
2023-08-02 14:11:57 - progress_bar.py[line:272] - INFO: epoch 001:   3186 / 5589 loss=1.866, loss_v1=0, loss_v2=0, nll_loss=0.553, ntokens=1053.9, nsentences=144, sample_size=1053.9, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=427.7, ups=0.41, wpb=1053.9, bsz=144, num_updates=3180, lr=9.4831e-06, gnorm=1.477, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7871
2023-08-02 14:12:22 - progress_bar.py[line:272] - INFO: epoch 001:   3196 / 5589 loss=1.873, loss_v1=0, loss_v2=0, nll_loss=0.56, ntokens=1053.9, nsentences=144, sample_size=1053.9, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=427.7, ups=0.41, wpb=1053.9, bsz=144, num_updates=3190, lr=9.51292e-06, gnorm=1.502, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7896
2023-08-02 14:12:46 - progress_bar.py[line:272] - INFO: epoch 001:   3206 / 5589 loss=1.88, loss_v1=0, loss_v2=0, nll_loss=0.57, ntokens=1052, nsentences=144, sample_size=1052, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=426.8, ups=0.41, wpb=1052, bsz=144, num_updates=3200, lr=9.54274e-06, gnorm=1.513, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7921
2023-08-02 14:13:11 - progress_bar.py[line:272] - INFO: epoch 001:   3216 / 5589 loss=1.863, loss_v1=0, loss_v2=0, nll_loss=0.55, ntokens=1055.5, nsentences=144, sample_size=1055.5, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=428.1, ups=0.41, wpb=1055.5, bsz=144, num_updates=3210, lr=9.57256e-06, gnorm=1.419, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7945
2023-08-02 14:13:36 - progress_bar.py[line:272] - INFO: epoch 001:   3226 / 5589 loss=1.875, loss_v1=0, loss_v2=0, nll_loss=0.565, ntokens=1058, nsentences=144, sample_size=1058, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=428.2, ups=0.4, wpb=1058, bsz=144, num_updates=3220, lr=9.60239e-06, gnorm=1.571, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7970
2023-08-02 14:14:01 - progress_bar.py[line:272] - INFO: epoch 001:   3236 / 5589 loss=1.868, loss_v1=0, loss_v2=0, nll_loss=0.554, ntokens=1053.1, nsentences=144, sample_size=1053.1, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=426.1, ups=0.4, wpb=1053.1, bsz=144, num_updates=3230, lr=9.63221e-06, gnorm=1.474, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=7995
2023-08-02 14:14:25 - progress_bar.py[line:272] - INFO: epoch 001:   3246 / 5589 loss=1.859, loss_v1=0, loss_v2=0, nll_loss=0.546, ntokens=1053.6, nsentences=144, sample_size=1053.6, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=427.4, ups=0.41, wpb=1053.6, bsz=144, num_updates=3240, lr=9.66203e-06, gnorm=1.475, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8020
2023-08-02 14:14:50 - progress_bar.py[line:272] - INFO: epoch 001:   3256 / 5589 loss=1.877, loss_v1=0, loss_v2=0, nll_loss=0.568, ntokens=1052.5, nsentences=144, sample_size=1052.5, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=427, ups=0.41, wpb=1052.5, bsz=144, num_updates=3250, lr=9.69185e-06, gnorm=1.536, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8044
2023-08-02 14:15:15 - progress_bar.py[line:272] - INFO: epoch 001:   3266 / 5589 loss=1.875, loss_v1=0, loss_v2=0, nll_loss=0.564, ntokens=1056.4, nsentences=144, sample_size=1056.4, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=428.3, ups=0.41, wpb=1056.4, bsz=144, num_updates=3260, lr=9.72167e-06, gnorm=1.542, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8069
2023-08-02 14:15:39 - progress_bar.py[line:272] - INFO: epoch 001:   3276 / 5589 loss=1.866, loss_v1=0, loss_v2=0, nll_loss=0.556, ntokens=1060.5, nsentences=144, sample_size=1060.5, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=430.5, ups=0.41, wpb=1060.5, bsz=144, num_updates=3270, lr=9.75149e-06, gnorm=1.552, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8093
2023-08-02 14:16:04 - progress_bar.py[line:272] - INFO: epoch 001:   3286 / 5589 loss=1.877, loss_v1=0, loss_v2=0, nll_loss=0.566, ntokens=1054.7, nsentences=144, sample_size=1054.7, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=427.9, ups=0.41, wpb=1054.7, bsz=144, num_updates=3280, lr=9.78131e-06, gnorm=1.429, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8118
2023-08-02 14:16:29 - progress_bar.py[line:272] - INFO: epoch 001:   3296 / 5589 loss=1.86, loss_v1=0, loss_v2=0, nll_loss=0.548, ntokens=1056.4, nsentences=144, sample_size=1056.4, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=428.1, ups=0.41, wpb=1056.4, bsz=144, num_updates=3290, lr=9.81113e-06, gnorm=1.636, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8143
2023-08-02 14:16:53 - progress_bar.py[line:272] - INFO: epoch 001:   3306 / 5589 loss=1.866, loss_v1=0, loss_v2=0, nll_loss=0.553, ntokens=1060.5, nsentences=144, sample_size=1060.5, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=430.3, ups=0.41, wpb=1060.5, bsz=144, num_updates=3300, lr=9.84095e-06, gnorm=1.514, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8167
2023-08-02 14:17:18 - progress_bar.py[line:272] - INFO: epoch 001:   3316 / 5589 loss=1.866, loss_v1=0, loss_v2=0, nll_loss=0.557, ntokens=1052.8, nsentences=144, sample_size=1052.8, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=426.7, ups=0.41, wpb=1052.8, bsz=144, num_updates=3310, lr=9.87078e-06, gnorm=1.584, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8192
2023-08-02 14:17:43 - progress_bar.py[line:272] - INFO: epoch 001:   3326 / 5589 loss=1.861, loss_v1=0, loss_v2=0, nll_loss=0.548, ntokens=1056.3, nsentences=144, sample_size=1056.3, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=428.4, ups=0.41, wpb=1056.3, bsz=144, num_updates=3320, lr=9.9006e-06, gnorm=1.511, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8217
2023-08-02 14:18:07 - progress_bar.py[line:272] - INFO: epoch 001:   3336 / 5589 loss=1.863, loss_v1=0, loss_v2=0, nll_loss=0.551, ntokens=1052.1, nsentences=144, sample_size=1052.1, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=426.8, ups=0.41, wpb=1052.1, bsz=144, num_updates=3330, lr=9.93042e-06, gnorm=1.494, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8241
2023-08-02 14:18:32 - progress_bar.py[line:272] - INFO: epoch 001:   3346 / 5589 loss=1.866, loss_v1=0, loss_v2=0, nll_loss=0.556, ntokens=1056.4, nsentences=144, sample_size=1056.4, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=428.3, ups=0.41, wpb=1056.4, bsz=144, num_updates=3340, lr=9.96024e-06, gnorm=1.513, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8266
2023-08-02 14:18:56 - progress_bar.py[line:272] - INFO: epoch 001:   3356 / 5589 loss=1.846, loss_v1=0, loss_v2=0, nll_loss=0.531, ntokens=1055.2, nsentences=144, sample_size=1055.2, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=428.2, ups=0.41, wpb=1055.2, bsz=144, num_updates=3350, lr=9.99006e-06, gnorm=1.473, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8291
2023-08-02 14:19:21 - progress_bar.py[line:272] - INFO: epoch 001:   3366 / 5589 loss=1.855, loss_v1=0, loss_v2=0, nll_loss=0.543, ntokens=1054.1, nsentences=144, sample_size=1054.1, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=427.6, ups=0.41, wpb=1054.1, bsz=144, num_updates=3360, lr=1.00199e-05, gnorm=1.415, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8315
2023-08-02 14:19:46 - progress_bar.py[line:272] - INFO: epoch 001:   3376 / 5589 loss=1.847, loss_v1=0, loss_v2=0, nll_loss=0.533, ntokens=1058.9, nsentences=144, sample_size=1058.9, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=429.6, ups=0.41, wpb=1058.9, bsz=144, num_updates=3370, lr=1.00497e-05, gnorm=1.447, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8340
2023-08-02 14:20:10 - progress_bar.py[line:272] - INFO: epoch 001:   3386 / 5589 loss=1.857, loss_v1=0, loss_v2=0, nll_loss=0.546, ntokens=1056.5, nsentences=144, sample_size=1056.5, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=428.6, ups=0.41, wpb=1056.5, bsz=144, num_updates=3380, lr=1.00795e-05, gnorm=1.479, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8365
2023-08-02 14:20:35 - progress_bar.py[line:272] - INFO: epoch 001:   3396 / 5589 loss=1.862, loss_v1=0, loss_v2=0, nll_loss=0.551, ntokens=1057.6, nsentences=144, sample_size=1057.6, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=429.3, ups=0.41, wpb=1057.6, bsz=144, num_updates=3390, lr=1.01093e-05, gnorm=1.471, clip=90, loss_scale=128, train_wall=25, gb_free=6.6, wall=8389
2023-08-02 14:21:00 - progress_bar.py[line:272] - INFO: epoch 001:   3406 / 5589 loss=1.847, loss_v1=0, loss_v2=0, nll_loss=0.537, ntokens=1052.4, nsentences=144, sample_size=1052.4, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=427, ups=0.41, wpb=1052.4, bsz=144, num_updates=3400, lr=1.01392e-05, gnorm=1.434, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8414
2023-08-02 14:21:24 - progress_bar.py[line:272] - INFO: epoch 001:   3416 / 5589 loss=1.855, loss_v1=0, loss_v2=0, nll_loss=0.541, ntokens=1053.6, nsentences=144, sample_size=1053.6, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=427.4, ups=0.41, wpb=1053.6, bsz=144, num_updates=3410, lr=1.0169e-05, gnorm=1.505, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8439
2023-08-02 14:21:49 - progress_bar.py[line:272] - INFO: epoch 001:   3426 / 5589 loss=1.855, loss_v1=0, loss_v2=0, nll_loss=0.545, ntokens=1053.3, nsentences=144, sample_size=1053.3, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=427.2, ups=0.41, wpb=1053.3, bsz=144, num_updates=3420, lr=1.01988e-05, gnorm=1.499, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8463
2023-08-02 14:22:14 - progress_bar.py[line:272] - INFO: epoch 001:   3436 / 5589 loss=1.853, loss_v1=0, loss_v2=0, nll_loss=0.541, ntokens=1055.9, nsentences=144, sample_size=1055.9, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=428.7, ups=0.41, wpb=1055.9, bsz=144, num_updates=3430, lr=1.02286e-05, gnorm=1.507, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8488
2023-08-02 14:22:38 - progress_bar.py[line:272] - INFO: epoch 001:   3446 / 5589 loss=1.856, loss_v1=0, loss_v2=0, nll_loss=0.545, ntokens=1057.9, nsentences=144, sample_size=1057.9, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=429.2, ups=0.41, wpb=1057.9, bsz=144, num_updates=3440, lr=1.02584e-05, gnorm=1.498, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8513
2023-08-02 14:23:03 - progress_bar.py[line:272] - INFO: epoch 001:   3456 / 5589 loss=1.852, loss_v1=0, loss_v2=0, nll_loss=0.542, ntokens=1051.7, nsentences=144, sample_size=1051.7, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=426.6, ups=0.41, wpb=1051.7, bsz=144, num_updates=3450, lr=1.02883e-05, gnorm=1.721, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8537
2023-08-02 14:23:28 - progress_bar.py[line:272] - INFO: epoch 001:   3466 / 5589 loss=1.847, loss_v1=0, loss_v2=0, nll_loss=0.533, ntokens=1051.9, nsentences=144, sample_size=1051.9, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=427, ups=0.41, wpb=1051.9, bsz=144, num_updates=3460, lr=1.03181e-05, gnorm=1.526, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8562
2023-08-02 14:23:52 - progress_bar.py[line:272] - INFO: epoch 001:   3476 / 5589 loss=1.846, loss_v1=0, loss_v2=0, nll_loss=0.535, ntokens=1054.3, nsentences=144, sample_size=1054.3, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=427.9, ups=0.41, wpb=1054.3, bsz=144, num_updates=3470, lr=1.03479e-05, gnorm=1.516, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8587
2023-08-02 14:24:17 - progress_bar.py[line:272] - INFO: epoch 001:   3486 / 5589 loss=1.865, loss_v1=0, loss_v2=0, nll_loss=0.555, ntokens=1056.9, nsentences=144, sample_size=1056.9, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=429, ups=0.41, wpb=1056.9, bsz=144, num_updates=3480, lr=1.03777e-05, gnorm=1.656, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8611
2023-08-02 14:24:42 - progress_bar.py[line:272] - INFO: epoch 001:   3496 / 5589 loss=1.862, loss_v1=0, loss_v2=0, nll_loss=0.554, ntokens=1055.6, nsentences=144, sample_size=1055.6, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=428.1, ups=0.41, wpb=1055.6, bsz=144, num_updates=3490, lr=1.04076e-05, gnorm=1.554, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8636
2023-08-02 14:25:06 - progress_bar.py[line:272] - INFO: epoch 001:   3506 / 5589 loss=1.844, loss_v1=0, loss_v2=0, nll_loss=0.531, ntokens=1053.7, nsentences=144, sample_size=1053.7, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=427.3, ups=0.41, wpb=1053.7, bsz=144, num_updates=3500, lr=1.04374e-05, gnorm=1.425, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8660
2023-08-02 14:25:31 - progress_bar.py[line:272] - INFO: epoch 001:   3516 / 5589 loss=1.853, loss_v1=0, loss_v2=0, nll_loss=0.543, ntokens=1059.9, nsentences=144, sample_size=1059.9, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=430.1, ups=0.41, wpb=1059.9, bsz=144, num_updates=3510, lr=1.04672e-05, gnorm=1.514, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8685
2023-08-02 14:25:56 - progress_bar.py[line:272] - INFO: epoch 001:   3526 / 5589 loss=1.859, loss_v1=0, loss_v2=0, nll_loss=0.551, ntokens=1054.4, nsentences=144, sample_size=1054.4, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=427.8, ups=0.41, wpb=1054.4, bsz=144, num_updates=3520, lr=1.0497e-05, gnorm=1.58, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8710
2023-08-02 14:26:20 - progress_bar.py[line:272] - INFO: epoch 001:   3536 / 5589 loss=1.846, loss_v1=0, loss_v2=0, nll_loss=0.533, ntokens=1057.3, nsentences=144, sample_size=1057.3, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=428.9, ups=0.41, wpb=1057.3, bsz=144, num_updates=3530, lr=1.05268e-05, gnorm=1.542, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8734
2023-08-02 14:26:45 - progress_bar.py[line:272] - INFO: epoch 001:   3546 / 5589 loss=1.849, loss_v1=0, loss_v2=0, nll_loss=0.538, ntokens=1057.3, nsentences=144, sample_size=1057.3, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=429, ups=0.41, wpb=1057.3, bsz=144, num_updates=3540, lr=1.05567e-05, gnorm=1.442, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8759
2023-08-02 14:27:09 - progress_bar.py[line:272] - INFO: epoch 001:   3556 / 5589 loss=1.843, loss_v1=0, loss_v2=0, nll_loss=0.531, ntokens=1060.2, nsentences=144, sample_size=1060.2, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=430.3, ups=0.41, wpb=1060.2, bsz=144, num_updates=3550, lr=1.05865e-05, gnorm=1.547, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8784
2023-08-02 14:27:34 - progress_bar.py[line:272] - INFO: epoch 001:   3566 / 5589 loss=1.845, loss_v1=0, loss_v2=0, nll_loss=0.535, ntokens=1061.8, nsentences=144, sample_size=1061.8, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=430.9, ups=0.41, wpb=1061.8, bsz=144, num_updates=3560, lr=1.06163e-05, gnorm=1.487, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8808
2023-08-02 14:27:59 - progress_bar.py[line:272] - INFO: epoch 001:   3576 / 5589 loss=1.854, loss_v1=0, loss_v2=0, nll_loss=0.546, ntokens=1053.4, nsentences=144, sample_size=1053.4, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=427.7, ups=0.41, wpb=1053.4, bsz=144, num_updates=3570, lr=1.06461e-05, gnorm=1.412, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8833
2023-08-02 14:28:23 - progress_bar.py[line:272] - INFO: epoch 001:   3586 / 5589 loss=1.844, loss_v1=0, loss_v2=0, nll_loss=0.533, ntokens=1057.3, nsentences=144, sample_size=1057.3, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=429.1, ups=0.41, wpb=1057.3, bsz=144, num_updates=3580, lr=1.06759e-05, gnorm=1.466, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8858
2023-08-02 14:28:48 - progress_bar.py[line:272] - INFO: epoch 001:   3596 / 5589 loss=1.858, loss_v1=0, loss_v2=0, nll_loss=0.549, ntokens=1057.3, nsentences=144, sample_size=1057.3, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=429.1, ups=0.41, wpb=1057.3, bsz=144, num_updates=3590, lr=1.07058e-05, gnorm=1.469, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8882
2023-08-02 14:29:13 - progress_bar.py[line:272] - INFO: epoch 001:   3606 / 5589 loss=1.838, loss_v1=0, loss_v2=0, nll_loss=0.526, ntokens=1052.7, nsentences=144, sample_size=1052.7, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=427.5, ups=0.41, wpb=1052.7, bsz=144, num_updates=3600, lr=1.07356e-05, gnorm=1.399, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8907
2023-08-02 14:29:37 - progress_bar.py[line:272] - INFO: epoch 001:   3616 / 5589 loss=1.849, loss_v1=0, loss_v2=0, nll_loss=0.54, ntokens=1058.3, nsentences=144, sample_size=1058.3, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=429.5, ups=0.41, wpb=1058.3, bsz=144, num_updates=3610, lr=1.07654e-05, gnorm=1.447, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8932
2023-08-02 14:30:02 - progress_bar.py[line:272] - INFO: epoch 001:   3626 / 5589 loss=1.85, loss_v1=0, loss_v2=0, nll_loss=0.54, ntokens=1059.4, nsentences=144, sample_size=1059.4, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=429.8, ups=0.41, wpb=1059.4, bsz=144, num_updates=3620, lr=1.07952e-05, gnorm=1.5, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=8956
2023-08-02 14:30:27 - progress_bar.py[line:272] - INFO: epoch 001:   3636 / 5589 loss=1.83, loss_v1=0, loss_v2=0, nll_loss=0.517, ntokens=1055.9, nsentences=144, sample_size=1055.9, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=428.6, ups=0.41, wpb=1055.9, bsz=144, num_updates=3630, lr=1.0825e-05, gnorm=1.39, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=8981
2023-08-02 14:30:51 - progress_bar.py[line:272] - INFO: epoch 001:   3646 / 5589 loss=1.832, loss_v1=0, loss_v2=0, nll_loss=0.52, ntokens=1057.4, nsentences=144, sample_size=1057.4, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=429.2, ups=0.41, wpb=1057.4, bsz=144, num_updates=3640, lr=1.08549e-05, gnorm=1.478, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=9006
2023-08-02 14:31:16 - progress_bar.py[line:272] - INFO: epoch 001:   3656 / 5589 loss=1.838, loss_v1=0, loss_v2=0, nll_loss=0.526, ntokens=1054, nsentences=144, sample_size=1054, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=427.7, ups=0.41, wpb=1054, bsz=144, num_updates=3650, lr=1.08847e-05, gnorm=1.469, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=9030
2023-08-02 14:31:36 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-08-02 14:31:43 - progress_bar.py[line:272] - INFO: epoch 001:   3667 / 5589 loss=1.834, loss_v1=0, loss_v2=0, nll_loss=0.524, ntokens=1056.6, nsentences=144, sample_size=1056.6, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=390.1, ups=0.37, wpb=1056.6, bsz=144, num_updates=3660, lr=1.09145e-05, gnorm=1.467, clip=100, loss_scale=128, train_wall=27, gb_free=6.6, wall=9057
2023-08-02 14:32:08 - progress_bar.py[line:272] - INFO: epoch 001:   3677 / 5589 loss=1.845, loss_v1=0, loss_v2=0, nll_loss=0.535, ntokens=1055.9, nsentences=144, sample_size=1055.9, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=428.4, ups=0.41, wpb=1055.9, bsz=144, num_updates=3670, lr=1.09443e-05, gnorm=1.552, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9082
2023-08-02 14:32:32 - progress_bar.py[line:272] - INFO: epoch 001:   3687 / 5589 loss=1.843, loss_v1=0, loss_v2=0, nll_loss=0.535, ntokens=1051.4, nsentences=144, sample_size=1051.4, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=426.5, ups=0.41, wpb=1051.4, bsz=144, num_updates=3680, lr=1.09742e-05, gnorm=1.529, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9107
2023-08-02 14:32:57 - progress_bar.py[line:272] - INFO: epoch 001:   3697 / 5589 loss=1.832, loss_v1=0, loss_v2=0, nll_loss=0.52, ntokens=1049.8, nsentences=144, sample_size=1049.8, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=425.5, ups=0.41, wpb=1049.8, bsz=144, num_updates=3690, lr=1.1004e-05, gnorm=1.438, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9131
2023-08-02 14:33:22 - progress_bar.py[line:272] - INFO: epoch 001:   3707 / 5589 loss=1.847, loss_v1=0, loss_v2=0, nll_loss=0.538, ntokens=1055.2, nsentences=144, sample_size=1055.2, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=427.1, ups=0.4, wpb=1055.2, bsz=144, num_updates=3700, lr=1.10338e-05, gnorm=1.462, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9156
2023-08-02 14:33:46 - progress_bar.py[line:272] - INFO: epoch 001:   3717 / 5589 loss=1.845, loss_v1=0, loss_v2=0, nll_loss=0.535, ntokens=1057.1, nsentences=144, sample_size=1057.1, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=427.6, ups=0.4, wpb=1057.1, bsz=144, num_updates=3710, lr=1.10636e-05, gnorm=1.406, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9181
2023-08-02 14:34:11 - progress_bar.py[line:272] - INFO: epoch 001:   3727 / 5589 loss=1.824, loss_v1=0, loss_v2=0, nll_loss=0.512, ntokens=1054.5, nsentences=144, sample_size=1054.5, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=426.4, ups=0.4, wpb=1054.5, bsz=144, num_updates=3720, lr=1.10934e-05, gnorm=1.532, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9205
2023-08-02 14:34:36 - progress_bar.py[line:272] - INFO: epoch 001:   3737 / 5589 loss=1.843, loss_v1=0, loss_v2=0, nll_loss=0.534, ntokens=1053.1, nsentences=144, sample_size=1053.1, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=423.6, ups=0.4, wpb=1053.1, bsz=144, num_updates=3730, lr=1.11233e-05, gnorm=1.412, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9230
2023-08-02 14:35:01 - progress_bar.py[line:272] - INFO: epoch 001:   3747 / 5589 loss=1.815, loss_v1=0, loss_v2=0, nll_loss=0.501, ntokens=1057.1, nsentences=144, sample_size=1057.1, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=427.3, ups=0.4, wpb=1057.1, bsz=144, num_updates=3740, lr=1.11531e-05, gnorm=1.44, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9255
2023-08-02 14:35:25 - progress_bar.py[line:272] - INFO: epoch 001:   3757 / 5589 loss=1.839, loss_v1=0, loss_v2=0, nll_loss=0.53, ntokens=1056.5, nsentences=144, sample_size=1056.5, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=428.3, ups=0.41, wpb=1056.5, bsz=144, num_updates=3750, lr=1.11829e-05, gnorm=1.513, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9280
2023-08-02 14:35:50 - progress_bar.py[line:272] - INFO: epoch 001:   3767 / 5589 loss=1.835, loss_v1=0, loss_v2=0, nll_loss=0.525, ntokens=1054.6, nsentences=144, sample_size=1054.6, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=427.6, ups=0.41, wpb=1054.6, bsz=144, num_updates=3760, lr=1.12127e-05, gnorm=1.509, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9304
2023-08-02 14:36:15 - progress_bar.py[line:272] - INFO: epoch 001:   3777 / 5589 loss=1.822, loss_v1=0, loss_v2=0, nll_loss=0.509, ntokens=1054.2, nsentences=144, sample_size=1054.2, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=427.8, ups=0.41, wpb=1054.2, bsz=144, num_updates=3770, lr=1.12425e-05, gnorm=1.51, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9329
2023-08-02 14:36:39 - progress_bar.py[line:272] - INFO: epoch 001:   3787 / 5589 loss=1.837, loss_v1=0, loss_v2=0, nll_loss=0.529, ntokens=1058, nsentences=144, sample_size=1058, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=429.2, ups=0.41, wpb=1058, bsz=144, num_updates=3780, lr=1.12724e-05, gnorm=1.499, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9354
2023-08-02 14:37:04 - progress_bar.py[line:272] - INFO: epoch 001:   3797 / 5589 loss=1.839, loss_v1=0, loss_v2=0, nll_loss=0.529, ntokens=1053.8, nsentences=144, sample_size=1053.8, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=427.2, ups=0.41, wpb=1053.8, bsz=144, num_updates=3790, lr=1.13022e-05, gnorm=1.496, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9378
2023-08-02 14:37:29 - progress_bar.py[line:272] - INFO: epoch 001:   3807 / 5589 loss=1.849, loss_v1=0, loss_v2=0, nll_loss=0.543, ntokens=1050.8, nsentences=144, sample_size=1050.8, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=425, ups=0.4, wpb=1050.8, bsz=144, num_updates=3800, lr=1.1332e-05, gnorm=1.474, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9403
2023-08-02 14:37:53 - progress_bar.py[line:272] - INFO: epoch 001:   3817 / 5589 loss=1.826, loss_v1=0, loss_v2=0, nll_loss=0.514, ntokens=1053.6, nsentences=144, sample_size=1053.6, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=426.5, ups=0.4, wpb=1053.6, bsz=144, num_updates=3810, lr=1.13618e-05, gnorm=1.419, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9428
2023-08-02 14:38:18 - progress_bar.py[line:272] - INFO: epoch 001:   3827 / 5589 loss=1.843, loss_v1=0, loss_v2=0, nll_loss=0.535, ntokens=1052.4, nsentences=144, sample_size=1052.4, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=426.2, ups=0.4, wpb=1052.4, bsz=144, num_updates=3820, lr=1.13917e-05, gnorm=1.448, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9452
2023-08-02 14:38:43 - progress_bar.py[line:272] - INFO: epoch 001:   3837 / 5589 loss=1.827, loss_v1=0, loss_v2=0, nll_loss=0.518, ntokens=1055.1, nsentences=144, sample_size=1055.1, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=427, ups=0.4, wpb=1055.1, bsz=144, num_updates=3830, lr=1.14215e-05, gnorm=1.464, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9477
2023-08-02 14:39:08 - progress_bar.py[line:272] - INFO: epoch 001:   3847 / 5589 loss=1.822, loss_v1=0, loss_v2=0, nll_loss=0.509, ntokens=1057.1, nsentences=144, sample_size=1057.1, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=427.8, ups=0.4, wpb=1057.1, bsz=144, num_updates=3840, lr=1.14513e-05, gnorm=1.496, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9502
2023-08-02 14:39:32 - progress_bar.py[line:272] - INFO: epoch 001:   3857 / 5589 loss=1.821, loss_v1=0, loss_v2=0, nll_loss=0.512, ntokens=1054.8, nsentences=144, sample_size=1054.8, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=426.7, ups=0.4, wpb=1054.8, bsz=144, num_updates=3850, lr=1.14811e-05, gnorm=1.473, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9527
2023-08-02 14:39:57 - progress_bar.py[line:272] - INFO: epoch 001:   3867 / 5589 loss=1.83, loss_v1=0, loss_v2=0, nll_loss=0.52, ntokens=1055.6, nsentences=144, sample_size=1055.6, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=427.3, ups=0.4, wpb=1055.6, bsz=144, num_updates=3860, lr=1.15109e-05, gnorm=1.471, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9551
2023-08-02 14:40:22 - progress_bar.py[line:272] - INFO: epoch 001:   3877 / 5589 loss=1.812, loss_v1=0, loss_v2=0, nll_loss=0.498, ntokens=1056, nsentences=144, sample_size=1056, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=427.5, ups=0.4, wpb=1056, bsz=144, num_updates=3870, lr=1.15408e-05, gnorm=1.547, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9576
2023-08-02 14:40:46 - progress_bar.py[line:272] - INFO: epoch 001:   3887 / 5589 loss=1.83, loss_v1=0, loss_v2=0, nll_loss=0.523, ntokens=1057.6, nsentences=144, sample_size=1057.6, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=428.1, ups=0.4, wpb=1057.6, bsz=144, num_updates=3880, lr=1.15706e-05, gnorm=1.466, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9601
2023-08-02 14:41:11 - progress_bar.py[line:272] - INFO: epoch 001:   3897 / 5589 loss=1.833, loss_v1=0, loss_v2=0, nll_loss=0.524, ntokens=1059.4, nsentences=144, sample_size=1059.4, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=429.1, ups=0.41, wpb=1059.4, bsz=144, num_updates=3890, lr=1.16004e-05, gnorm=1.503, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9625
2023-08-02 14:41:36 - progress_bar.py[line:272] - INFO: epoch 001:   3907 / 5589 loss=1.83, loss_v1=0, loss_v2=0, nll_loss=0.52, ntokens=1058.4, nsentences=144, sample_size=1058.4, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=428.7, ups=0.41, wpb=1058.4, bsz=144, num_updates=3900, lr=1.16302e-05, gnorm=1.37, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9650
2023-08-02 14:42:01 - progress_bar.py[line:272] - INFO: epoch 001:   3917 / 5589 loss=1.828, loss_v1=0, loss_v2=0, nll_loss=0.52, ntokens=1057.4, nsentences=144, sample_size=1057.4, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=427.9, ups=0.4, wpb=1057.4, bsz=144, num_updates=3910, lr=1.166e-05, gnorm=1.436, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9675
2023-08-02 14:42:25 - progress_bar.py[line:272] - INFO: epoch 001:   3927 / 5589 loss=1.819, loss_v1=0, loss_v2=0, nll_loss=0.506, ntokens=1053.5, nsentences=144, sample_size=1053.5, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=426.7, ups=0.41, wpb=1053.5, bsz=144, num_updates=3920, lr=1.16899e-05, gnorm=1.515, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9699
2023-08-02 14:42:50 - progress_bar.py[line:272] - INFO: epoch 001:   3937 / 5589 loss=1.828, loss_v1=0, loss_v2=0, nll_loss=0.521, ntokens=1056, nsentences=144, sample_size=1056, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=427.4, ups=0.4, wpb=1056, bsz=144, num_updates=3930, lr=1.17197e-05, gnorm=1.447, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9724
2023-08-02 14:43:15 - progress_bar.py[line:272] - INFO: epoch 001:   3947 / 5589 loss=1.824, loss_v1=0, loss_v2=0, nll_loss=0.513, ntokens=1056.6, nsentences=144, sample_size=1056.6, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=427.5, ups=0.4, wpb=1056.6, bsz=144, num_updates=3940, lr=1.17495e-05, gnorm=1.421, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9749
2023-08-02 14:43:39 - progress_bar.py[line:272] - INFO: epoch 001:   3957 / 5589 loss=1.819, loss_v1=0, loss_v2=0, nll_loss=0.509, ntokens=1055.5, nsentences=144, sample_size=1055.5, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=427.4, ups=0.4, wpb=1055.5, bsz=144, num_updates=3950, lr=1.17793e-05, gnorm=1.471, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9774
2023-08-02 14:44:04 - progress_bar.py[line:272] - INFO: epoch 001:   3967 / 5589 loss=1.824, loss_v1=0, loss_v2=0, nll_loss=0.514, ntokens=1059.1, nsentences=144, sample_size=1059.1, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=428.3, ups=0.4, wpb=1059.1, bsz=144, num_updates=3960, lr=1.18091e-05, gnorm=1.426, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9798
2023-08-02 14:44:29 - progress_bar.py[line:272] - INFO: epoch 001:   3977 / 5589 loss=1.808, loss_v1=0, loss_v2=0, nll_loss=0.497, ntokens=1056.6, nsentences=144, sample_size=1056.6, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=427.2, ups=0.4, wpb=1056.6, bsz=144, num_updates=3970, lr=1.1839e-05, gnorm=1.414, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9823
2023-08-02 14:44:53 - progress_bar.py[line:272] - INFO: epoch 001:   3987 / 5589 loss=1.826, loss_v1=0, loss_v2=0, nll_loss=0.517, ntokens=1052, nsentences=144, sample_size=1052, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=426, ups=0.4, wpb=1052, bsz=144, num_updates=3980, lr=1.18688e-05, gnorm=1.402, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9848
2023-08-02 14:45:18 - progress_bar.py[line:272] - INFO: epoch 001:   3997 / 5589 loss=1.813, loss_v1=0, loss_v2=0, nll_loss=0.502, ntokens=1054.7, nsentences=144, sample_size=1054.7, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=427, ups=0.4, wpb=1054.7, bsz=144, num_updates=3990, lr=1.18986e-05, gnorm=1.389, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9872
2023-08-02 14:45:43 - progress_bar.py[line:272] - INFO: epoch 001:   4007 / 5589 loss=1.809, loss_v1=0, loss_v2=0, nll_loss=0.497, ntokens=1050.4, nsentences=144, sample_size=1050.4, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=424.3, ups=0.4, wpb=1050.4, bsz=144, num_updates=4000, lr=1.19284e-05, gnorm=1.383, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9897
2023-08-02 14:46:08 - progress_bar.py[line:272] - INFO: epoch 001:   4017 / 5589 loss=1.806, loss_v1=0, loss_v2=0, nll_loss=0.496, ntokens=1053.3, nsentences=144, sample_size=1053.3, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=426.3, ups=0.4, wpb=1053.3, bsz=144, num_updates=4010, lr=1.19583e-05, gnorm=1.457, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9922
2023-08-02 14:46:32 - progress_bar.py[line:272] - INFO: epoch 001:   4027 / 5589 loss=1.821, loss_v1=0, loss_v2=0, nll_loss=0.51, ntokens=1060.8, nsentences=144, sample_size=1060.8, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=429.7, ups=0.41, wpb=1060.8, bsz=144, num_updates=4020, lr=1.19881e-05, gnorm=1.48, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9947
2023-08-02 14:46:57 - progress_bar.py[line:272] - INFO: epoch 001:   4037 / 5589 loss=1.827, loss_v1=0, loss_v2=0, nll_loss=0.52, ntokens=1052.2, nsentences=144, sample_size=1052.2, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=426, ups=0.4, wpb=1052.2, bsz=144, num_updates=4030, lr=1.20179e-05, gnorm=1.505, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9971
2023-08-02 14:47:22 - progress_bar.py[line:272] - INFO: epoch 001:   4047 / 5589 loss=1.819, loss_v1=0, loss_v2=0, nll_loss=0.508, ntokens=1057.9, nsentences=144, sample_size=1057.9, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=428.3, ups=0.4, wpb=1057.9, bsz=144, num_updates=4040, lr=1.20477e-05, gnorm=1.426, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=9996
2023-08-02 14:47:46 - progress_bar.py[line:272] - INFO: epoch 001:   4057 / 5589 loss=1.84, loss_v1=0, loss_v2=0, nll_loss=0.535, ntokens=1051.6, nsentences=144, sample_size=1051.6, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=425.7, ups=0.4, wpb=1051.6, bsz=144, num_updates=4050, lr=1.20775e-05, gnorm=1.472, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10021
2023-08-02 14:48:11 - progress_bar.py[line:272] - INFO: epoch 001:   4067 / 5589 loss=1.82, loss_v1=0, loss_v2=0, nll_loss=0.51, ntokens=1052.4, nsentences=144, sample_size=1052.4, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=422.3, ups=0.4, wpb=1052.4, bsz=144, num_updates=4060, lr=1.21074e-05, gnorm=1.427, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10046
2023-08-02 14:48:36 - progress_bar.py[line:272] - INFO: epoch 001:   4077 / 5589 loss=1.817, loss_v1=0, loss_v2=0, nll_loss=0.508, ntokens=1057.2, nsentences=144, sample_size=1057.2, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=427.8, ups=0.4, wpb=1057.2, bsz=144, num_updates=4070, lr=1.21372e-05, gnorm=1.454, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10070
2023-08-02 14:49:01 - progress_bar.py[line:272] - INFO: epoch 001:   4087 / 5589 loss=1.817, loss_v1=0, loss_v2=0, nll_loss=0.508, ntokens=1053.6, nsentences=144, sample_size=1053.6, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=426.4, ups=0.4, wpb=1053.6, bsz=144, num_updates=4080, lr=1.2167e-05, gnorm=1.392, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10095
2023-08-02 14:49:26 - progress_bar.py[line:272] - INFO: epoch 001:   4097 / 5589 loss=1.817, loss_v1=0, loss_v2=0, nll_loss=0.509, ntokens=1056.5, nsentences=144, sample_size=1056.5, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=428, ups=0.41, wpb=1056.5, bsz=144, num_updates=4090, lr=1.21968e-05, gnorm=1.411, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10120
2023-08-02 14:49:50 - progress_bar.py[line:272] - INFO: epoch 001:   4107 / 5589 loss=1.819, loss_v1=0, loss_v2=0, nll_loss=0.508, ntokens=1053.5, nsentences=144, sample_size=1053.5, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=427.2, ups=0.41, wpb=1053.5, bsz=144, num_updates=4100, lr=1.22266e-05, gnorm=1.415, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10144
2023-08-02 14:50:15 - progress_bar.py[line:272] - INFO: epoch 001:   4117 / 5589 loss=1.804, loss_v1=0, loss_v2=0, nll_loss=0.496, ntokens=1057.7, nsentences=144, sample_size=1057.7, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=429, ups=0.41, wpb=1057.7, bsz=144, num_updates=4110, lr=1.22565e-05, gnorm=1.389, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10169
2023-08-02 14:50:39 - progress_bar.py[line:272] - INFO: epoch 001:   4127 / 5589 loss=1.8, loss_v1=0, loss_v2=0, nll_loss=0.487, ntokens=1058, nsentences=144, sample_size=1058, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=429.1, ups=0.41, wpb=1058, bsz=144, num_updates=4120, lr=1.22863e-05, gnorm=1.461, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10194
2023-08-02 14:51:04 - progress_bar.py[line:272] - INFO: epoch 001:   4137 / 5589 loss=1.808, loss_v1=0, loss_v2=0, nll_loss=0.499, ntokens=1055.5, nsentences=144, sample_size=1055.5, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=428, ups=0.41, wpb=1055.5, bsz=144, num_updates=4130, lr=1.23161e-05, gnorm=1.538, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10218
2023-08-02 14:51:29 - progress_bar.py[line:272] - INFO: epoch 001:   4147 / 5589 loss=1.808, loss_v1=0, loss_v2=0, nll_loss=0.499, ntokens=1052.9, nsentences=144, sample_size=1052.9, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=426.4, ups=0.4, wpb=1052.9, bsz=144, num_updates=4140, lr=1.23459e-05, gnorm=1.529, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10243
2023-08-02 14:51:54 - progress_bar.py[line:272] - INFO: epoch 001:   4157 / 5589 loss=1.812, loss_v1=0, loss_v2=0, nll_loss=0.502, ntokens=1056, nsentences=144, sample_size=1056, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=427.8, ups=0.41, wpb=1056, bsz=144, num_updates=4150, lr=1.23757e-05, gnorm=1.533, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10268
2023-08-02 14:52:18 - progress_bar.py[line:272] - INFO: epoch 001:   4167 / 5589 loss=1.802, loss_v1=0, loss_v2=0, nll_loss=0.49, ntokens=1056.8, nsentences=144, sample_size=1056.8, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=427.7, ups=0.4, wpb=1056.8, bsz=144, num_updates=4160, lr=1.24056e-05, gnorm=1.466, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10293
2023-08-02 14:52:43 - progress_bar.py[line:272] - INFO: epoch 001:   4177 / 5589 loss=1.815, loss_v1=0, loss_v2=0, nll_loss=0.507, ntokens=1059.5, nsentences=144, sample_size=1059.5, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=428.6, ups=0.4, wpb=1059.5, bsz=144, num_updates=4170, lr=1.24354e-05, gnorm=1.493, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=10317
2023-08-02 14:53:08 - progress_bar.py[line:272] - INFO: epoch 001:   4187 / 5589 loss=1.807, loss_v1=0, loss_v2=0, nll_loss=0.497, ntokens=1057, nsentences=144, sample_size=1057, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=428.7, ups=0.41, wpb=1057, bsz=144, num_updates=4180, lr=1.24652e-05, gnorm=1.611, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=10342
2023-08-02 14:53:32 - progress_bar.py[line:272] - INFO: epoch 001:   4197 / 5589 loss=1.812, loss_v1=0, loss_v2=0, nll_loss=0.502, ntokens=1055.2, nsentences=144, sample_size=1055.2, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=428.6, ups=0.41, wpb=1055.2, bsz=144, num_updates=4190, lr=1.2495e-05, gnorm=1.515, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=10367
2023-08-02 14:53:57 - progress_bar.py[line:272] - INFO: epoch 001:   4207 / 5589 loss=1.813, loss_v1=0, loss_v2=0, nll_loss=0.505, ntokens=1054.1, nsentences=144, sample_size=1054.1, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=427.8, ups=0.41, wpb=1054.1, bsz=144, num_updates=4200, lr=1.25249e-05, gnorm=1.439, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=10391
2023-08-02 14:54:22 - progress_bar.py[line:272] - INFO: epoch 001:   4217 / 5589 loss=1.817, loss_v1=0, loss_v2=0, nll_loss=0.509, ntokens=1056.9, nsentences=144, sample_size=1056.9, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=428.6, ups=0.41, wpb=1056.9, bsz=144, num_updates=4210, lr=1.25547e-05, gnorm=1.568, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=10416
2023-08-02 14:54:46 - progress_bar.py[line:272] - INFO: epoch 001:   4227 / 5589 loss=1.822, loss_v1=0, loss_v2=0, nll_loss=0.514, ntokens=1055.6, nsentences=144, sample_size=1055.6, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=428.3, ups=0.41, wpb=1055.6, bsz=144, num_updates=4220, lr=1.25845e-05, gnorm=1.416, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=10440
2023-08-02 14:55:11 - progress_bar.py[line:272] - INFO: epoch 001:   4237 / 5589 loss=1.815, loss_v1=0, loss_v2=0, nll_loss=0.506, ntokens=1054.3, nsentences=144, sample_size=1054.3, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=424.4, ups=0.4, wpb=1054.3, bsz=144, num_updates=4230, lr=1.26143e-05, gnorm=1.447, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=10465
2023-08-02 14:55:36 - progress_bar.py[line:272] - INFO: epoch 001:   4247 / 5589 loss=1.804, loss_v1=0, loss_v2=0, nll_loss=0.493, ntokens=1056.1, nsentences=144, sample_size=1056.1, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=428.6, ups=0.41, wpb=1056.1, bsz=144, num_updates=4240, lr=1.26441e-05, gnorm=1.518, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=10490
2023-08-02 14:55:50 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-08-02 14:56:03 - progress_bar.py[line:272] - INFO: epoch 001:   4258 / 5589 loss=1.811, loss_v1=0, loss_v2=0, nll_loss=0.504, ntokens=1051.6, nsentences=144, sample_size=1051.6, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=388.3, ups=0.37, wpb=1051.6, bsz=144, num_updates=4250, lr=1.2674e-05, gnorm=1.467, clip=100, loss_scale=128, train_wall=27, gb_free=6.6, wall=10517
2023-08-02 14:56:27 - progress_bar.py[line:272] - INFO: epoch 001:   4268 / 5589 loss=1.816, loss_v1=0, loss_v2=0, nll_loss=0.506, ntokens=1060.5, nsentences=144, sample_size=1060.5, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=429.9, ups=0.41, wpb=1060.5, bsz=144, num_updates=4260, lr=1.27038e-05, gnorm=1.423, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10542
2023-08-02 14:56:52 - progress_bar.py[line:272] - INFO: epoch 001:   4278 / 5589 loss=1.811, loss_v1=0, loss_v2=0, nll_loss=0.505, ntokens=1053.5, nsentences=144, sample_size=1053.5, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=427.5, ups=0.41, wpb=1053.5, bsz=144, num_updates=4270, lr=1.27336e-05, gnorm=1.516, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10566
2023-08-02 14:57:17 - progress_bar.py[line:272] - INFO: epoch 001:   4288 / 5589 loss=1.813, loss_v1=0, loss_v2=0, nll_loss=0.505, ntokens=1055.7, nsentences=144, sample_size=1055.7, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=428, ups=0.41, wpb=1055.7, bsz=144, num_updates=4280, lr=1.27634e-05, gnorm=1.485, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10591
2023-08-02 14:57:41 - progress_bar.py[line:272] - INFO: epoch 001:   4298 / 5589 loss=1.798, loss_v1=0, loss_v2=0, nll_loss=0.486, ntokens=1055.9, nsentences=144, sample_size=1055.9, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=428.2, ups=0.41, wpb=1055.9, bsz=144, num_updates=4290, lr=1.27932e-05, gnorm=1.479, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10616
2023-08-02 14:58:06 - progress_bar.py[line:272] - INFO: epoch 001:   4308 / 5589 loss=1.789, loss_v1=0, loss_v2=0, nll_loss=0.481, ntokens=1056.6, nsentences=144, sample_size=1056.6, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=429.2, ups=0.41, wpb=1056.6, bsz=144, num_updates=4300, lr=1.28231e-05, gnorm=1.414, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10640
2023-08-02 14:58:31 - progress_bar.py[line:272] - INFO: epoch 001:   4318 / 5589 loss=1.803, loss_v1=0, loss_v2=0, nll_loss=0.491, ntokens=1054.2, nsentences=144, sample_size=1054.2, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=427.9, ups=0.41, wpb=1054.2, bsz=144, num_updates=4310, lr=1.28529e-05, gnorm=1.491, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10665
2023-08-02 14:58:55 - progress_bar.py[line:272] - INFO: epoch 001:   4328 / 5589 loss=1.808, loss_v1=0, loss_v2=0, nll_loss=0.499, ntokens=1053, nsentences=144, sample_size=1053, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=427.1, ups=0.41, wpb=1053, bsz=144, num_updates=4320, lr=1.28827e-05, gnorm=1.465, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10690
2023-08-02 14:59:20 - progress_bar.py[line:272] - INFO: epoch 001:   4338 / 5589 loss=1.798, loss_v1=0, loss_v2=0, nll_loss=0.489, ntokens=1053.8, nsentences=144, sample_size=1053.8, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=426.9, ups=0.41, wpb=1053.8, bsz=144, num_updates=4330, lr=1.29125e-05, gnorm=1.423, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10714
2023-08-02 14:59:45 - progress_bar.py[line:272] - INFO: epoch 001:   4348 / 5589 loss=1.796, loss_v1=0, loss_v2=0, nll_loss=0.484, ntokens=1051.9, nsentences=144, sample_size=1051.9, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=426.1, ups=0.41, wpb=1051.9, bsz=144, num_updates=4340, lr=1.29423e-05, gnorm=1.489, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10739
2023-08-02 15:00:09 - progress_bar.py[line:272] - INFO: epoch 001:   4358 / 5589 loss=1.803, loss_v1=0, loss_v2=0, nll_loss=0.494, ntokens=1054.7, nsentences=144, sample_size=1054.7, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=427.1, ups=0.4, wpb=1054.7, bsz=144, num_updates=4350, lr=1.29722e-05, gnorm=1.458, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10764
2023-08-02 15:00:34 - progress_bar.py[line:272] - INFO: epoch 001:   4368 / 5589 loss=1.807, loss_v1=0, loss_v2=0, nll_loss=0.5, ntokens=1052.4, nsentences=144, sample_size=1052.4, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=426.2, ups=0.41, wpb=1052.4, bsz=144, num_updates=4360, lr=1.3002e-05, gnorm=1.458, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10788
2023-08-02 15:00:59 - progress_bar.py[line:272] - INFO: epoch 001:   4378 / 5589 loss=1.807, loss_v1=0, loss_v2=0, nll_loss=0.499, ntokens=1053.6, nsentences=144, sample_size=1053.6, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=426.5, ups=0.4, wpb=1053.6, bsz=144, num_updates=4370, lr=1.30318e-05, gnorm=1.403, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10813
2023-08-02 15:01:24 - progress_bar.py[line:272] - INFO: epoch 001:   4388 / 5589 loss=1.793, loss_v1=0, loss_v2=0, nll_loss=0.483, ntokens=1057.7, nsentences=144, sample_size=1057.7, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=428.4, ups=0.41, wpb=1057.7, bsz=144, num_updates=4380, lr=1.30616e-05, gnorm=1.441, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10838
2023-08-02 15:01:48 - progress_bar.py[line:272] - INFO: epoch 001:   4398 / 5589 loss=1.796, loss_v1=0, loss_v2=0, nll_loss=0.488, ntokens=1056.7, nsentences=144, sample_size=1056.7, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=427.8, ups=0.4, wpb=1056.7, bsz=144, num_updates=4390, lr=1.30915e-05, gnorm=1.428, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10863
2023-08-02 15:02:13 - progress_bar.py[line:272] - INFO: epoch 001:   4408 / 5589 loss=1.78, loss_v1=0, loss_v2=0, nll_loss=0.466, ntokens=1057.8, nsentences=144, sample_size=1057.8, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=428.3, ups=0.4, wpb=1057.8, bsz=144, num_updates=4400, lr=1.31213e-05, gnorm=1.426, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10887
2023-08-02 15:02:38 - progress_bar.py[line:272] - INFO: epoch 001:   4418 / 5589 loss=1.802, loss_v1=0, loss_v2=0, nll_loss=0.497, ntokens=1054.1, nsentences=144, sample_size=1054.1, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=426.8, ups=0.4, wpb=1054.1, bsz=144, num_updates=4410, lr=1.31511e-05, gnorm=1.472, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10912
2023-08-02 15:03:02 - progress_bar.py[line:272] - INFO: epoch 001:   4428 / 5589 loss=1.805, loss_v1=0, loss_v2=0, nll_loss=0.496, ntokens=1053.7, nsentences=144, sample_size=1053.7, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=426.4, ups=0.4, wpb=1053.7, bsz=144, num_updates=4420, lr=1.31809e-05, gnorm=1.388, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10937
2023-08-02 15:03:27 - progress_bar.py[line:272] - INFO: epoch 001:   4438 / 5589 loss=1.799, loss_v1=0, loss_v2=0, nll_loss=0.49, ntokens=1057.5, nsentences=144, sample_size=1057.5, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=428.3, ups=0.41, wpb=1057.5, bsz=144, num_updates=4430, lr=1.32107e-05, gnorm=1.461, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10961
2023-08-02 15:03:52 - progress_bar.py[line:272] - INFO: epoch 001:   4448 / 5589 loss=1.787, loss_v1=0, loss_v2=0, nll_loss=0.478, ntokens=1054.4, nsentences=144, sample_size=1054.4, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=427, ups=0.4, wpb=1054.4, bsz=144, num_updates=4440, lr=1.32406e-05, gnorm=1.399, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=10986
2023-08-02 15:04:16 - progress_bar.py[line:272] - INFO: epoch 001:   4458 / 5589 loss=1.795, loss_v1=0, loss_v2=0, nll_loss=0.485, ntokens=1055.6, nsentences=144, sample_size=1055.6, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=427.6, ups=0.41, wpb=1055.6, bsz=144, num_updates=4450, lr=1.32704e-05, gnorm=1.415, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11011
2023-08-02 15:04:41 - progress_bar.py[line:272] - INFO: epoch 001:   4468 / 5589 loss=1.787, loss_v1=0, loss_v2=0, nll_loss=0.477, ntokens=1058.6, nsentences=144, sample_size=1058.6, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=428.6, ups=0.4, wpb=1058.6, bsz=144, num_updates=4460, lr=1.33002e-05, gnorm=1.438, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11035
2023-08-02 15:05:06 - progress_bar.py[line:272] - INFO: epoch 001:   4478 / 5589 loss=1.799, loss_v1=0, loss_v2=0, nll_loss=0.491, ntokens=1055.2, nsentences=144, sample_size=1055.2, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=427.6, ups=0.41, wpb=1055.2, bsz=144, num_updates=4470, lr=1.333e-05, gnorm=1.386, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11060
2023-08-02 15:05:30 - progress_bar.py[line:272] - INFO: epoch 001:   4488 / 5589 loss=1.81, loss_v1=0, loss_v2=0, nll_loss=0.504, ntokens=1052.7, nsentences=144, sample_size=1052.7, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=426.9, ups=0.41, wpb=1052.7, bsz=144, num_updates=4480, lr=1.33598e-05, gnorm=1.592, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11085
2023-08-02 15:05:55 - progress_bar.py[line:272] - INFO: epoch 001:   4498 / 5589 loss=1.804, loss_v1=0, loss_v2=0, nll_loss=0.495, ntokens=1054.8, nsentences=144, sample_size=1054.8, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=427.8, ups=0.41, wpb=1054.8, bsz=144, num_updates=4490, lr=1.33897e-05, gnorm=1.456, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11109
2023-08-02 15:06:20 - progress_bar.py[line:272] - INFO: epoch 001:   4508 / 5589 loss=1.805, loss_v1=0, loss_v2=0, nll_loss=0.499, ntokens=1057.3, nsentences=144, sample_size=1057.3, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=428.6, ups=0.41, wpb=1057.3, bsz=144, num_updates=4500, lr=1.34195e-05, gnorm=1.476, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11134
2023-08-02 15:06:44 - progress_bar.py[line:272] - INFO: epoch 001:   4518 / 5589 loss=1.805, loss_v1=0, loss_v2=0, nll_loss=0.497, ntokens=1055, nsentences=144, sample_size=1055, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=427.9, ups=0.41, wpb=1055, bsz=144, num_updates=4510, lr=1.34493e-05, gnorm=1.467, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11159
2023-08-02 15:07:09 - progress_bar.py[line:272] - INFO: epoch 001:   4528 / 5589 loss=1.805, loss_v1=0, loss_v2=0, nll_loss=0.499, ntokens=1057.5, nsentences=144, sample_size=1057.5, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=428.9, ups=0.41, wpb=1057.5, bsz=144, num_updates=4520, lr=1.34791e-05, gnorm=1.436, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11183
2023-08-02 15:07:34 - progress_bar.py[line:272] - INFO: epoch 001:   4538 / 5589 loss=1.795, loss_v1=0, loss_v2=0, nll_loss=0.484, ntokens=1055, nsentences=144, sample_size=1055, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=427.9, ups=0.41, wpb=1055, bsz=144, num_updates=4530, lr=1.35089e-05, gnorm=1.364, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11208
2023-08-02 15:07:58 - progress_bar.py[line:272] - INFO: epoch 001:   4548 / 5589 loss=1.799, loss_v1=0, loss_v2=0, nll_loss=0.492, ntokens=1054.7, nsentences=144, sample_size=1054.7, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=427.7, ups=0.41, wpb=1054.7, bsz=144, num_updates=4540, lr=1.35388e-05, gnorm=1.375, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11233
2023-08-02 15:08:23 - progress_bar.py[line:272] - INFO: epoch 001:   4558 / 5589 loss=1.797, loss_v1=0, loss_v2=0, nll_loss=0.487, ntokens=1051.2, nsentences=144, sample_size=1051.2, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=426.3, ups=0.41, wpb=1051.2, bsz=144, num_updates=4550, lr=1.35686e-05, gnorm=1.488, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11257
2023-08-02 15:08:48 - progress_bar.py[line:272] - INFO: epoch 001:   4568 / 5589 loss=1.797, loss_v1=0, loss_v2=0, nll_loss=0.49, ntokens=1056.3, nsentences=144, sample_size=1056.3, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=428.5, ups=0.41, wpb=1056.3, bsz=144, num_updates=4560, lr=1.35984e-05, gnorm=1.503, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11282
2023-08-02 15:09:12 - progress_bar.py[line:272] - INFO: epoch 001:   4578 / 5589 loss=1.798, loss_v1=0, loss_v2=0, nll_loss=0.49, ntokens=1056.2, nsentences=144, sample_size=1056.2, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=428.2, ups=0.41, wpb=1056.2, bsz=144, num_updates=4570, lr=1.36282e-05, gnorm=1.487, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11307
2023-08-02 15:09:37 - progress_bar.py[line:272] - INFO: epoch 001:   4588 / 5589 loss=1.794, loss_v1=0, loss_v2=0, nll_loss=0.487, ntokens=1053.1, nsentences=144, sample_size=1053.1, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=426.5, ups=0.4, wpb=1053.1, bsz=144, num_updates=4580, lr=1.36581e-05, gnorm=1.439, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11331
2023-08-02 15:10:02 - progress_bar.py[line:272] - INFO: epoch 001:   4598 / 5589 loss=1.784, loss_v1=0, loss_v2=0, nll_loss=0.473, ntokens=1057.7, nsentences=144, sample_size=1057.7, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=428.3, ups=0.4, wpb=1057.7, bsz=144, num_updates=4590, lr=1.36879e-05, gnorm=1.443, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11356
2023-08-02 15:10:27 - progress_bar.py[line:272] - INFO: epoch 001:   4608 / 5589 loss=1.797, loss_v1=0, loss_v2=0, nll_loss=0.488, ntokens=1055.9, nsentences=144, sample_size=1055.9, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=427.6, ups=0.4, wpb=1055.9, bsz=144, num_updates=4600, lr=1.37177e-05, gnorm=1.526, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11381
2023-08-02 15:10:51 - progress_bar.py[line:272] - INFO: epoch 001:   4618 / 5589 loss=1.797, loss_v1=0, loss_v2=0, nll_loss=0.49, ntokens=1053.3, nsentences=144, sample_size=1053.3, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=426.4, ups=0.4, wpb=1053.3, bsz=144, num_updates=4610, lr=1.37475e-05, gnorm=1.442, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11405
2023-08-02 15:11:16 - progress_bar.py[line:272] - INFO: epoch 001:   4628 / 5589 loss=1.793, loss_v1=0, loss_v2=0, nll_loss=0.486, ntokens=1055.4, nsentences=144, sample_size=1055.4, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=427.4, ups=0.4, wpb=1055.4, bsz=144, num_updates=4620, lr=1.37773e-05, gnorm=1.429, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11430
2023-08-02 15:11:41 - progress_bar.py[line:272] - INFO: epoch 001:   4638 / 5589 loss=1.795, loss_v1=0, loss_v2=0, nll_loss=0.485, ntokens=1055, nsentences=144, sample_size=1055, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=427.3, ups=0.4, wpb=1055, bsz=144, num_updates=4630, lr=1.38072e-05, gnorm=1.536, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11455
2023-08-02 15:12:05 - progress_bar.py[line:272] - INFO: epoch 001:   4648 / 5589 loss=1.787, loss_v1=0, loss_v2=0, nll_loss=0.479, ntokens=1057.6, nsentences=144, sample_size=1057.6, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=428.3, ups=0.4, wpb=1057.6, bsz=144, num_updates=4640, lr=1.3837e-05, gnorm=1.397, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11480
2023-08-02 15:12:30 - progress_bar.py[line:272] - INFO: epoch 001:   4658 / 5589 loss=1.793, loss_v1=0, loss_v2=0, nll_loss=0.484, ntokens=1057.9, nsentences=144, sample_size=1057.9, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=428, ups=0.4, wpb=1057.9, bsz=144, num_updates=4650, lr=1.38668e-05, gnorm=1.415, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11504
2023-08-02 15:12:55 - progress_bar.py[line:272] - INFO: epoch 001:   4668 / 5589 loss=1.789, loss_v1=0, loss_v2=0, nll_loss=0.481, ntokens=1054.4, nsentences=144, sample_size=1054.4, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=426.8, ups=0.4, wpb=1054.4, bsz=144, num_updates=4660, lr=1.38966e-05, gnorm=1.44, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11529
2023-08-02 15:13:19 - progress_bar.py[line:272] - INFO: epoch 001:   4678 / 5589 loss=1.791, loss_v1=0, loss_v2=0, nll_loss=0.482, ntokens=1053.7, nsentences=144, sample_size=1053.7, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=426.9, ups=0.41, wpb=1053.7, bsz=144, num_updates=4670, lr=1.39264e-05, gnorm=1.487, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11554
2023-08-02 15:13:44 - progress_bar.py[line:272] - INFO: epoch 001:   4688 / 5589 loss=1.788, loss_v1=0, loss_v2=0, nll_loss=0.48, ntokens=1056.3, nsentences=144, sample_size=1056.3, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=427.7, ups=0.4, wpb=1056.3, bsz=144, num_updates=4680, lr=1.39563e-05, gnorm=1.366, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11578
2023-08-02 15:14:09 - progress_bar.py[line:272] - INFO: epoch 001:   4698 / 5589 loss=1.78, loss_v1=0, loss_v2=0, nll_loss=0.47, ntokens=1053.2, nsentences=144, sample_size=1053.2, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=426.8, ups=0.41, wpb=1053.2, bsz=144, num_updates=4690, lr=1.39861e-05, gnorm=1.444, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11603
2023-08-02 15:14:33 - progress_bar.py[line:272] - INFO: epoch 001:   4708 / 5589 loss=1.798, loss_v1=0, loss_v2=0, nll_loss=0.492, ntokens=1054.8, nsentences=144, sample_size=1054.8, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=427.5, ups=0.41, wpb=1054.8, bsz=144, num_updates=4700, lr=1.40159e-05, gnorm=1.476, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11628
2023-08-02 15:14:58 - progress_bar.py[line:272] - INFO: epoch 001:   4718 / 5589 loss=1.787, loss_v1=0, loss_v2=0, nll_loss=0.48, ntokens=1053.5, nsentences=144, sample_size=1053.5, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=426.9, ups=0.41, wpb=1053.5, bsz=144, num_updates=4710, lr=1.40457e-05, gnorm=1.461, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11652
2023-08-02 15:15:23 - progress_bar.py[line:272] - INFO: epoch 001:   4728 / 5589 loss=1.784, loss_v1=0, loss_v2=0, nll_loss=0.477, ntokens=1055.4, nsentences=144, sample_size=1055.4, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=427.5, ups=0.41, wpb=1055.4, bsz=144, num_updates=4720, lr=1.40755e-05, gnorm=1.338, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11677
2023-08-02 15:15:48 - progress_bar.py[line:272] - INFO: epoch 001:   4738 / 5589 loss=1.792, loss_v1=0, loss_v2=0, nll_loss=0.484, ntokens=1057.1, nsentences=144, sample_size=1057.1, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=427.9, ups=0.4, wpb=1057.1, bsz=144, num_updates=4730, lr=1.41054e-05, gnorm=1.451, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11702
2023-08-02 15:16:12 - progress_bar.py[line:272] - INFO: epoch 001:   4748 / 5589 loss=1.788, loss_v1=0, loss_v2=0, nll_loss=0.481, ntokens=1059.1, nsentences=144, sample_size=1059.1, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=429.3, ups=0.41, wpb=1059.1, bsz=144, num_updates=4740, lr=1.41352e-05, gnorm=1.367, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11727
2023-08-02 15:16:37 - progress_bar.py[line:272] - INFO: epoch 001:   4758 / 5589 loss=1.79, loss_v1=0, loss_v2=0, nll_loss=0.484, ntokens=1054.2, nsentences=144, sample_size=1054.2, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=427.7, ups=0.41, wpb=1054.2, bsz=144, num_updates=4750, lr=1.4165e-05, gnorm=1.466, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=11751
2023-08-02 15:17:02 - progress_bar.py[line:272] - INFO: epoch 001:   4768 / 5589 loss=1.798, loss_v1=0, loss_v2=0, nll_loss=0.49, ntokens=1059.9, nsentences=144, sample_size=1059.9, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=430, ups=0.41, wpb=1059.9, bsz=144, num_updates=4760, lr=1.41948e-05, gnorm=1.412, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=11776
2023-08-02 15:17:26 - progress_bar.py[line:272] - INFO: epoch 001:   4778 / 5589 loss=1.795, loss_v1=0, loss_v2=0, nll_loss=0.492, ntokens=1055.8, nsentences=144, sample_size=1055.8, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=428.2, ups=0.41, wpb=1055.8, bsz=144, num_updates=4770, lr=1.42247e-05, gnorm=1.419, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=11800
2023-08-02 15:17:51 - progress_bar.py[line:272] - INFO: epoch 001:   4788 / 5589 loss=1.79, loss_v1=0, loss_v2=0, nll_loss=0.482, ntokens=1055.7, nsentences=144, sample_size=1055.7, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=428.3, ups=0.41, wpb=1055.7, bsz=144, num_updates=4780, lr=1.42545e-05, gnorm=1.398, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=11825
2023-08-02 15:18:16 - progress_bar.py[line:272] - INFO: epoch 001:   4798 / 5589 loss=1.784, loss_v1=0, loss_v2=0, nll_loss=0.479, ntokens=1056.9, nsentences=144, sample_size=1056.9, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=428.7, ups=0.41, wpb=1056.9, bsz=144, num_updates=4790, lr=1.42843e-05, gnorm=1.522, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=11850
2023-08-02 15:18:40 - progress_bar.py[line:272] - INFO: epoch 001:   4808 / 5589 loss=1.784, loss_v1=0, loss_v2=0, nll_loss=0.474, ntokens=1050.7, nsentences=144, sample_size=1050.7, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=426.3, ups=0.41, wpb=1050.7, bsz=144, num_updates=4800, lr=1.43141e-05, gnorm=1.347, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=11874
2023-08-02 15:19:05 - progress_bar.py[line:272] - INFO: epoch 001:   4818 / 5589 loss=1.794, loss_v1=0, loss_v2=0, nll_loss=0.487, ntokens=1053.9, nsentences=144, sample_size=1053.9, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=427.4, ups=0.41, wpb=1053.9, bsz=144, num_updates=4810, lr=1.43439e-05, gnorm=1.547, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=11899
2023-08-02 15:19:29 - progress_bar.py[line:272] - INFO: epoch 001:   4828 / 5589 loss=1.776, loss_v1=0, loss_v2=0, nll_loss=0.468, ntokens=1057.1, nsentences=144, sample_size=1057.1, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=428.7, ups=0.41, wpb=1057.1, bsz=144, num_updates=4820, lr=1.43738e-05, gnorm=1.423, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=11924
2023-08-02 15:19:54 - progress_bar.py[line:272] - INFO: epoch 001:   4838 / 5589 loss=1.778, loss_v1=0, loss_v2=0, nll_loss=0.469, ntokens=1056.1, nsentences=144, sample_size=1056.1, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=428.1, ups=0.41, wpb=1056.1, bsz=144, num_updates=4830, lr=1.44036e-05, gnorm=1.322, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=11948
2023-08-02 15:20:19 - progress_bar.py[line:272] - INFO: epoch 001:   4848 / 5589 loss=1.784, loss_v1=0, loss_v2=0, nll_loss=0.476, ntokens=1054.6, nsentences=144, sample_size=1054.6, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=427.9, ups=0.41, wpb=1054.6, bsz=144, num_updates=4840, lr=1.44334e-05, gnorm=1.342, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=11973
2023-08-02 15:20:43 - progress_bar.py[line:272] - INFO: epoch 001:   4858 / 5589 loss=1.785, loss_v1=0, loss_v2=0, nll_loss=0.477, ntokens=1059, nsentences=144, sample_size=1059, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=429.6, ups=0.41, wpb=1059, bsz=144, num_updates=4850, lr=1.44632e-05, gnorm=1.393, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=11998
2023-08-02 15:20:58 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-08-02 15:21:11 - progress_bar.py[line:272] - INFO: epoch 001:   4869 / 5589 loss=1.785, loss_v1=0, loss_v2=0, nll_loss=0.477, ntokens=1055.4, nsentences=144, sample_size=1055.4, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=389.3, ups=0.37, wpb=1055.4, bsz=144, num_updates=4860, lr=1.4493e-05, gnorm=1.476, clip=100, loss_scale=128, train_wall=27, gb_free=6.6, wall=12025
2023-08-02 15:21:35 - progress_bar.py[line:272] - INFO: epoch 001:   4879 / 5589 loss=1.776, loss_v1=0, loss_v2=0, nll_loss=0.467, ntokens=1053.4, nsentences=144, sample_size=1053.4, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=427.2, ups=0.41, wpb=1053.4, bsz=144, num_updates=4870, lr=1.45229e-05, gnorm=1.394, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12050
2023-08-02 15:22:00 - progress_bar.py[line:272] - INFO: epoch 001:   4889 / 5589 loss=1.772, loss_v1=0, loss_v2=0, nll_loss=0.463, ntokens=1053.7, nsentences=144, sample_size=1053.7, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=427.5, ups=0.41, wpb=1053.7, bsz=144, num_updates=4880, lr=1.45527e-05, gnorm=1.406, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12074
2023-08-02 15:22:25 - progress_bar.py[line:272] - INFO: epoch 001:   4899 / 5589 loss=1.785, loss_v1=0, loss_v2=0, nll_loss=0.476, ntokens=1057.8, nsentences=144, sample_size=1057.8, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=428.9, ups=0.41, wpb=1057.8, bsz=144, num_updates=4890, lr=1.45825e-05, gnorm=1.447, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12099
2023-08-02 15:22:49 - progress_bar.py[line:272] - INFO: epoch 001:   4909 / 5589 loss=1.782, loss_v1=0, loss_v2=0, nll_loss=0.476, ntokens=1056.4, nsentences=144, sample_size=1056.4, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=428.5, ups=0.41, wpb=1056.4, bsz=144, num_updates=4900, lr=1.46123e-05, gnorm=1.442, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12123
2023-08-02 15:23:14 - progress_bar.py[line:272] - INFO: epoch 001:   4919 / 5589 loss=1.786, loss_v1=0, loss_v2=0, nll_loss=0.479, ntokens=1055.9, nsentences=144, sample_size=1055.9, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=428.2, ups=0.41, wpb=1055.9, bsz=144, num_updates=4910, lr=1.46421e-05, gnorm=1.381, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12148
2023-08-02 15:23:39 - progress_bar.py[line:272] - INFO: epoch 001:   4929 / 5589 loss=1.78, loss_v1=0, loss_v2=0, nll_loss=0.472, ntokens=1053.1, nsentences=144, sample_size=1053.1, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=426.4, ups=0.4, wpb=1053.1, bsz=144, num_updates=4920, lr=1.4672e-05, gnorm=1.363, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12173
2023-08-02 15:24:03 - progress_bar.py[line:272] - INFO: epoch 001:   4939 / 5589 loss=1.789, loss_v1=0, loss_v2=0, nll_loss=0.483, ntokens=1055.5, nsentences=144, sample_size=1055.5, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=427.7, ups=0.41, wpb=1055.5, bsz=144, num_updates=4930, lr=1.47018e-05, gnorm=1.457, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12198
2023-08-02 15:24:28 - progress_bar.py[line:272] - INFO: epoch 001:   4949 / 5589 loss=1.785, loss_v1=0, loss_v2=0, nll_loss=0.478, ntokens=1055, nsentences=144, sample_size=1055, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=427.4, ups=0.41, wpb=1055, bsz=144, num_updates=4940, lr=1.47316e-05, gnorm=1.455, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12222
2023-08-02 15:24:53 - progress_bar.py[line:272] - INFO: epoch 001:   4959 / 5589 loss=1.78, loss_v1=0, loss_v2=0, nll_loss=0.471, ntokens=1057, nsentences=144, sample_size=1057, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=428.4, ups=0.41, wpb=1057, bsz=144, num_updates=4950, lr=1.47614e-05, gnorm=1.628, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12247
2023-08-02 15:25:17 - progress_bar.py[line:272] - INFO: epoch 001:   4969 / 5589 loss=1.787, loss_v1=0, loss_v2=0, nll_loss=0.482, ntokens=1055.2, nsentences=144, sample_size=1055.2, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=428, ups=0.41, wpb=1055.2, bsz=144, num_updates=4960, lr=1.47913e-05, gnorm=1.442, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12272
2023-08-02 15:25:42 - progress_bar.py[line:272] - INFO: epoch 001:   4979 / 5589 loss=1.776, loss_v1=0, loss_v2=0, nll_loss=0.468, ntokens=1054.1, nsentences=144, sample_size=1054.1, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=427.6, ups=0.41, wpb=1054.1, bsz=144, num_updates=4970, lr=1.48211e-05, gnorm=1.469, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12296
2023-08-02 15:26:07 - progress_bar.py[line:272] - INFO: epoch 001:   4989 / 5589 loss=1.793, loss_v1=0, loss_v2=0, nll_loss=0.488, ntokens=1056.8, nsentences=144, sample_size=1056.8, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=428.4, ups=0.41, wpb=1056.8, bsz=144, num_updates=4980, lr=1.48509e-05, gnorm=1.483, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12321
2023-08-02 15:26:31 - progress_bar.py[line:272] - INFO: epoch 001:   4999 / 5589 loss=1.773, loss_v1=0, loss_v2=0, nll_loss=0.464, ntokens=1053.3, nsentences=144, sample_size=1053.3, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=427.3, ups=0.41, wpb=1053.3, bsz=144, num_updates=4990, lr=1.48807e-05, gnorm=1.368, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12346
2023-08-02 15:26:56 - progress_bar.py[line:272] - INFO: epoch 001:   5009 / 5589 loss=1.781, loss_v1=0, loss_v2=0, nll_loss=0.475, ntokens=1054.6, nsentences=144, sample_size=1054.6, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=427.7, ups=0.41, wpb=1054.6, bsz=144, num_updates=5000, lr=1.49105e-05, gnorm=1.494, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12370
2023-08-02 15:27:21 - progress_bar.py[line:272] - INFO: epoch 001:   5019 / 5589 loss=1.784, loss_v1=0, loss_v2=0, nll_loss=0.478, ntokens=1055.5, nsentences=144, sample_size=1055.5, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=427.9, ups=0.41, wpb=1055.5, bsz=144, num_updates=5010, lr=1.49404e-05, gnorm=1.433, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12395
2023-08-02 15:27:45 - progress_bar.py[line:272] - INFO: epoch 001:   5029 / 5589 loss=1.782, loss_v1=0, loss_v2=0, nll_loss=0.474, ntokens=1056.7, nsentences=144, sample_size=1056.7, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=428.4, ups=0.41, wpb=1056.7, bsz=144, num_updates=5020, lr=1.49702e-05, gnorm=1.492, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12420
2023-08-02 15:28:10 - progress_bar.py[line:272] - INFO: epoch 001:   5039 / 5589 loss=1.78, loss_v1=0, loss_v2=0, nll_loss=0.476, ntokens=1055.3, nsentences=144, sample_size=1055.3, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=427.8, ups=0.41, wpb=1055.3, bsz=144, num_updates=5030, lr=1.5e-05, gnorm=1.42, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12444
2023-08-02 15:28:35 - progress_bar.py[line:272] - INFO: epoch 001:   5049 / 5589 loss=1.769, loss_v1=0, loss_v2=0, nll_loss=0.461, ntokens=1057.5, nsentences=144, sample_size=1057.5, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=428.9, ups=0.41, wpb=1057.5, bsz=144, num_updates=5040, lr=1.50298e-05, gnorm=1.415, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12469
2023-08-02 15:28:59 - progress_bar.py[line:272] - INFO: epoch 001:   5059 / 5589 loss=1.786, loss_v1=0, loss_v2=0, nll_loss=0.48, ntokens=1053, nsentences=144, sample_size=1053, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=427, ups=0.41, wpb=1053, bsz=144, num_updates=5050, lr=1.50596e-05, gnorm=1.446, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12494
2023-08-02 15:29:24 - progress_bar.py[line:272] - INFO: epoch 001:   5069 / 5589 loss=1.77, loss_v1=0, loss_v2=0, nll_loss=0.463, ntokens=1054.6, nsentences=144, sample_size=1054.6, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=427.8, ups=0.41, wpb=1054.6, bsz=144, num_updates=5060, lr=1.50895e-05, gnorm=1.361, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12518
2023-08-02 15:29:49 - progress_bar.py[line:272] - INFO: epoch 001:   5079 / 5589 loss=1.78, loss_v1=0, loss_v2=0, nll_loss=0.473, ntokens=1051.6, nsentences=144, sample_size=1051.6, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=425.4, ups=0.4, wpb=1051.6, bsz=144, num_updates=5070, lr=1.51193e-05, gnorm=1.435, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12543
2023-08-02 15:30:13 - progress_bar.py[line:272] - INFO: epoch 001:   5089 / 5589 loss=1.768, loss_v1=0, loss_v2=0, nll_loss=0.462, ntokens=1058.3, nsentences=144, sample_size=1058.3, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=428, ups=0.4, wpb=1058.3, bsz=144, num_updates=5080, lr=1.51491e-05, gnorm=1.348, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12568
2023-08-02 15:30:38 - progress_bar.py[line:272] - INFO: epoch 001:   5099 / 5589 loss=1.789, loss_v1=0, loss_v2=0, nll_loss=0.48, ntokens=1058, nsentences=144, sample_size=1058, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=428.6, ups=0.41, wpb=1058, bsz=144, num_updates=5090, lr=1.51789e-05, gnorm=1.506, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12592
2023-08-02 15:31:03 - progress_bar.py[line:272] - INFO: epoch 001:   5109 / 5589 loss=1.785, loss_v1=0, loss_v2=0, nll_loss=0.48, ntokens=1056.4, nsentences=144, sample_size=1056.4, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=428.1, ups=0.41, wpb=1056.4, bsz=144, num_updates=5100, lr=1.52087e-05, gnorm=1.478, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12617
2023-08-02 15:31:27 - progress_bar.py[line:272] - INFO: epoch 001:   5119 / 5589 loss=1.778, loss_v1=0, loss_v2=0, nll_loss=0.472, ntokens=1052, nsentences=144, sample_size=1052, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=426.8, ups=0.41, wpb=1052, bsz=144, num_updates=5110, lr=1.52386e-05, gnorm=1.381, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12642
2023-08-02 15:31:52 - progress_bar.py[line:272] - INFO: epoch 001:   5129 / 5589 loss=1.765, loss_v1=0, loss_v2=0, nll_loss=0.456, ntokens=1053.6, nsentences=144, sample_size=1053.6, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=427.6, ups=0.41, wpb=1053.6, bsz=144, num_updates=5120, lr=1.52684e-05, gnorm=1.379, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12666
2023-08-02 15:32:17 - progress_bar.py[line:272] - INFO: epoch 001:   5139 / 5589 loss=1.78, loss_v1=0, loss_v2=0, nll_loss=0.473, ntokens=1057.5, nsentences=144, sample_size=1057.5, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=428.7, ups=0.41, wpb=1057.5, bsz=144, num_updates=5130, lr=1.52982e-05, gnorm=1.495, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12691
2023-08-02 15:32:41 - progress_bar.py[line:272] - INFO: epoch 001:   5149 / 5589 loss=1.768, loss_v1=0, loss_v2=0, nll_loss=0.46, ntokens=1053.6, nsentences=144, sample_size=1053.6, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=427.4, ups=0.41, wpb=1053.6, bsz=144, num_updates=5140, lr=1.5328e-05, gnorm=1.439, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12716
2023-08-02 15:33:06 - progress_bar.py[line:272] - INFO: epoch 001:   5159 / 5589 loss=1.764, loss_v1=0, loss_v2=0, nll_loss=0.453, ntokens=1054.2, nsentences=144, sample_size=1054.2, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=427.3, ups=0.41, wpb=1054.2, bsz=144, num_updates=5150, lr=1.53579e-05, gnorm=1.452, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12740
2023-08-02 15:33:31 - progress_bar.py[line:272] - INFO: epoch 001:   5169 / 5589 loss=1.777, loss_v1=0, loss_v2=0, nll_loss=0.471, ntokens=1054.5, nsentences=144, sample_size=1054.5, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=427.8, ups=0.41, wpb=1054.5, bsz=144, num_updates=5160, lr=1.53877e-05, gnorm=1.555, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12765
2023-08-02 15:33:55 - progress_bar.py[line:272] - INFO: epoch 001:   5179 / 5589 loss=1.762, loss_v1=0, loss_v2=0, nll_loss=0.453, ntokens=1054.8, nsentences=144, sample_size=1054.8, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=427.9, ups=0.41, wpb=1054.8, bsz=144, num_updates=5170, lr=1.54175e-05, gnorm=1.457, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12790
2023-08-02 15:34:20 - progress_bar.py[line:272] - INFO: epoch 001:   5189 / 5589 loss=1.77, loss_v1=0, loss_v2=0, nll_loss=0.463, ntokens=1060.4, nsentences=144, sample_size=1060.4, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=430.3, ups=0.41, wpb=1060.4, bsz=144, num_updates=5180, lr=1.54473e-05, gnorm=1.349, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12814
2023-08-02 15:34:45 - progress_bar.py[line:272] - INFO: epoch 001:   5199 / 5589 loss=1.768, loss_v1=0, loss_v2=0, nll_loss=0.458, ntokens=1052.7, nsentences=144, sample_size=1052.7, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=427, ups=0.41, wpb=1052.7, bsz=144, num_updates=5190, lr=1.54771e-05, gnorm=1.399, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12839
2023-08-02 15:35:09 - progress_bar.py[line:272] - INFO: epoch 001:   5209 / 5589 loss=1.769, loss_v1=0, loss_v2=0, nll_loss=0.462, ntokens=1057, nsentences=144, sample_size=1057, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=428.8, ups=0.41, wpb=1057, bsz=144, num_updates=5200, lr=1.5507e-05, gnorm=1.389, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12864
2023-08-02 15:35:34 - progress_bar.py[line:272] - INFO: epoch 001:   5219 / 5589 loss=1.789, loss_v1=0, loss_v2=0, nll_loss=0.485, ntokens=1054.6, nsentences=144, sample_size=1054.6, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=427.7, ups=0.41, wpb=1054.6, bsz=144, num_updates=5210, lr=1.55368e-05, gnorm=1.489, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12888
2023-08-02 15:35:59 - progress_bar.py[line:272] - INFO: epoch 001:   5229 / 5589 loss=1.768, loss_v1=0, loss_v2=0, nll_loss=0.46, ntokens=1052.1, nsentences=144, sample_size=1052.1, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=426.8, ups=0.41, wpb=1052.1, bsz=144, num_updates=5220, lr=1.55666e-05, gnorm=1.355, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12913
2023-08-02 15:36:23 - progress_bar.py[line:272] - INFO: epoch 001:   5239 / 5589 loss=1.768, loss_v1=0, loss_v2=0, nll_loss=0.46, ntokens=1057, nsentences=144, sample_size=1057, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=428.7, ups=0.41, wpb=1057, bsz=144, num_updates=5230, lr=1.55964e-05, gnorm=1.454, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12938
2023-08-02 15:36:48 - progress_bar.py[line:272] - INFO: epoch 001:   5249 / 5589 loss=1.782, loss_v1=0, loss_v2=0, nll_loss=0.477, ntokens=1057, nsentences=144, sample_size=1057, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=429, ups=0.41, wpb=1057, bsz=144, num_updates=5240, lr=1.56262e-05, gnorm=1.474, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12962
2023-08-02 15:37:13 - progress_bar.py[line:272] - INFO: epoch 001:   5259 / 5589 loss=1.76, loss_v1=0, loss_v2=0, nll_loss=0.45, ntokens=1052.2, nsentences=144, sample_size=1052.2, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=426.9, ups=0.41, wpb=1052.2, bsz=144, num_updates=5250, lr=1.56561e-05, gnorm=1.426, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=12987
2023-08-02 15:37:37 - progress_bar.py[line:272] - INFO: epoch 001:   5269 / 5589 loss=1.77, loss_v1=0, loss_v2=0, nll_loss=0.462, ntokens=1054.2, nsentences=144, sample_size=1054.2, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=428, ups=0.41, wpb=1054.2, bsz=144, num_updates=5260, lr=1.56859e-05, gnorm=1.479, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13011
2023-08-02 15:38:02 - progress_bar.py[line:272] - INFO: epoch 001:   5279 / 5589 loss=1.766, loss_v1=0, loss_v2=0, nll_loss=0.459, ntokens=1055.9, nsentences=144, sample_size=1055.9, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=428.5, ups=0.41, wpb=1055.9, bsz=144, num_updates=5270, lr=1.57157e-05, gnorm=1.478, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13036
2023-08-02 15:38:26 - progress_bar.py[line:272] - INFO: epoch 001:   5289 / 5589 loss=1.773, loss_v1=0, loss_v2=0, nll_loss=0.466, ntokens=1051.9, nsentences=144, sample_size=1051.9, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=426.8, ups=0.41, wpb=1051.9, bsz=144, num_updates=5280, lr=1.57455e-05, gnorm=1.431, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13061
2023-08-02 15:38:51 - progress_bar.py[line:272] - INFO: epoch 001:   5299 / 5589 loss=1.774, loss_v1=0, loss_v2=0, nll_loss=0.469, ntokens=1056, nsentences=144, sample_size=1056, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=428.5, ups=0.41, wpb=1056, bsz=144, num_updates=5290, lr=1.57753e-05, gnorm=1.449, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13085
2023-08-02 15:39:16 - progress_bar.py[line:272] - INFO: epoch 001:   5309 / 5589 loss=1.776, loss_v1=0, loss_v2=0, nll_loss=0.469, ntokens=1060.2, nsentences=144, sample_size=1060.2, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=430.2, ups=0.41, wpb=1060.2, bsz=144, num_updates=5300, lr=1.58052e-05, gnorm=1.386, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13110
2023-08-02 15:39:40 - progress_bar.py[line:272] - INFO: epoch 001:   5319 / 5589 loss=1.769, loss_v1=0, loss_v2=0, nll_loss=0.462, ntokens=1058.5, nsentences=144, sample_size=1058.5, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=429.5, ups=0.41, wpb=1058.5, bsz=144, num_updates=5310, lr=1.5835e-05, gnorm=1.346, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13135
2023-08-02 15:40:05 - progress_bar.py[line:272] - INFO: epoch 001:   5329 / 5589 loss=1.775, loss_v1=0, loss_v2=0, nll_loss=0.47, ntokens=1056.2, nsentences=144, sample_size=1056.2, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=428.4, ups=0.41, wpb=1056.2, bsz=144, num_updates=5320, lr=1.58648e-05, gnorm=1.376, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13159
2023-08-02 15:40:30 - progress_bar.py[line:272] - INFO: epoch 001:   5339 / 5589 loss=1.756, loss_v1=0, loss_v2=0, nll_loss=0.448, ntokens=1053.1, nsentences=144, sample_size=1053.1, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=427.3, ups=0.41, wpb=1053.1, bsz=144, num_updates=5330, lr=1.58946e-05, gnorm=1.431, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13184
2023-08-02 15:40:54 - progress_bar.py[line:272] - INFO: epoch 001:   5349 / 5589 loss=1.773, loss_v1=0, loss_v2=0, nll_loss=0.465, ntokens=1055.3, nsentences=144, sample_size=1055.3, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=427.5, ups=0.41, wpb=1055.3, bsz=144, num_updates=5340, lr=1.59245e-05, gnorm=1.41, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13209
2023-08-02 15:41:19 - progress_bar.py[line:272] - INFO: epoch 001:   5359 / 5589 loss=1.76, loss_v1=0, loss_v2=0, nll_loss=0.452, ntokens=1057.4, nsentences=144, sample_size=1057.4, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=428.6, ups=0.41, wpb=1057.4, bsz=144, num_updates=5350, lr=1.59543e-05, gnorm=1.382, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13233
2023-08-02 15:41:44 - progress_bar.py[line:272] - INFO: epoch 001:   5369 / 5589 loss=1.755, loss_v1=0, loss_v2=0, nll_loss=0.447, ntokens=1055.8, nsentences=144, sample_size=1055.8, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=427.8, ups=0.41, wpb=1055.8, bsz=144, num_updates=5360, lr=1.59841e-05, gnorm=1.37, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13258
2023-08-02 15:42:08 - progress_bar.py[line:272] - INFO: epoch 001:   5379 / 5589 loss=1.755, loss_v1=0, loss_v2=0, nll_loss=0.447, ntokens=1055.6, nsentences=144, sample_size=1055.6, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=427.8, ups=0.41, wpb=1055.6, bsz=144, num_updates=5370, lr=1.60139e-05, gnorm=1.274, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=13283
2023-08-02 15:42:33 - progress_bar.py[line:272] - INFO: epoch 001:   5389 / 5589 loss=1.77, loss_v1=0, loss_v2=0, nll_loss=0.464, ntokens=1055.9, nsentences=144, sample_size=1055.9, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=427.8, ups=0.41, wpb=1055.9, bsz=144, num_updates=5380, lr=1.60437e-05, gnorm=1.338, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=13307
2023-08-02 15:42:58 - progress_bar.py[line:272] - INFO: epoch 001:   5399 / 5589 loss=1.761, loss_v1=0, loss_v2=0, nll_loss=0.453, ntokens=1057.6, nsentences=144, sample_size=1057.6, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=428.4, ups=0.41, wpb=1057.6, bsz=144, num_updates=5390, lr=1.60736e-05, gnorm=1.386, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=13332
2023-08-02 15:43:10 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-08-02 15:43:25 - progress_bar.py[line:272] - INFO: epoch 001:   5410 / 5589 loss=1.761, loss_v1=0, loss_v2=0, nll_loss=0.454, ntokens=1058.4, nsentences=144, sample_size=1058.4, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=390, ups=0.37, wpb=1058.4, bsz=144, num_updates=5400, lr=1.61034e-05, gnorm=1.41, clip=100, loss_scale=128, train_wall=27, gb_free=6.6, wall=13359
2023-08-02 15:43:50 - progress_bar.py[line:272] - INFO: epoch 001:   5420 / 5589 loss=1.755, loss_v1=0, loss_v2=0, nll_loss=0.447, ntokens=1059.5, nsentences=144, sample_size=1059.5, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=429.2, ups=0.41, wpb=1059.5, bsz=144, num_updates=5410, lr=1.61332e-05, gnorm=1.404, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13384
2023-08-02 15:44:14 - progress_bar.py[line:272] - INFO: epoch 001:   5430 / 5589 loss=1.757, loss_v1=0, loss_v2=0, nll_loss=0.451, ntokens=1059.4, nsentences=144, sample_size=1059.4, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=429, ups=0.4, wpb=1059.4, bsz=144, num_updates=5420, lr=1.6163e-05, gnorm=1.363, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13409
2023-08-02 15:44:39 - progress_bar.py[line:272] - INFO: epoch 001:   5440 / 5589 loss=1.758, loss_v1=0, loss_v2=0, nll_loss=0.45, ntokens=1054.2, nsentences=144, sample_size=1054.2, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=426.6, ups=0.4, wpb=1054.2, bsz=144, num_updates=5430, lr=1.61928e-05, gnorm=1.483, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13433
2023-08-02 15:45:04 - progress_bar.py[line:272] - INFO: epoch 001:   5450 / 5589 loss=1.758, loss_v1=0, loss_v2=0, nll_loss=0.447, ntokens=1056.2, nsentences=144, sample_size=1056.2, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=427.8, ups=0.41, wpb=1056.2, bsz=144, num_updates=5440, lr=1.62227e-05, gnorm=1.449, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13458
2023-08-02 15:45:28 - progress_bar.py[line:272] - INFO: epoch 001:   5460 / 5589 loss=1.761, loss_v1=0, loss_v2=0, nll_loss=0.455, ntokens=1056.3, nsentences=144, sample_size=1056.3, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=427.6, ups=0.4, wpb=1056.3, bsz=144, num_updates=5450, lr=1.62525e-05, gnorm=1.445, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13483
2023-08-02 15:45:53 - progress_bar.py[line:272] - INFO: epoch 001:   5470 / 5589 loss=1.765, loss_v1=0, loss_v2=0, nll_loss=0.457, ntokens=1055.5, nsentences=144, sample_size=1055.5, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=427.8, ups=0.41, wpb=1055.5, bsz=144, num_updates=5460, lr=1.62823e-05, gnorm=1.445, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13507
2023-08-02 15:46:18 - progress_bar.py[line:272] - INFO: epoch 001:   5480 / 5589 loss=1.762, loss_v1=0, loss_v2=0, nll_loss=0.455, ntokens=1057.4, nsentences=144, sample_size=1057.4, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=428.4, ups=0.41, wpb=1057.4, bsz=144, num_updates=5470, lr=1.63121e-05, gnorm=1.355, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13532
2023-08-02 15:46:43 - progress_bar.py[line:272] - INFO: epoch 001:   5490 / 5589 loss=1.765, loss_v1=0, loss_v2=0, nll_loss=0.46, ntokens=1053.7, nsentences=144, sample_size=1053.7, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=427, ups=0.41, wpb=1053.7, bsz=144, num_updates=5480, lr=1.63419e-05, gnorm=1.389, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13557
2023-08-02 15:47:07 - progress_bar.py[line:272] - INFO: epoch 001:   5500 / 5589 loss=1.762, loss_v1=0, loss_v2=0, nll_loss=0.454, ntokens=1053.7, nsentences=144, sample_size=1053.7, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=427.1, ups=0.41, wpb=1053.7, bsz=144, num_updates=5490, lr=1.63718e-05, gnorm=1.381, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13581
2023-08-02 15:47:32 - progress_bar.py[line:272] - INFO: epoch 001:   5510 / 5589 loss=1.76, loss_v1=0, loss_v2=0, nll_loss=0.456, ntokens=1053, nsentences=144, sample_size=1053, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=426.8, ups=0.41, wpb=1053, bsz=144, num_updates=5500, lr=1.64016e-05, gnorm=1.416, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13606
2023-08-02 15:47:57 - progress_bar.py[line:272] - INFO: epoch 001:   5520 / 5589 loss=1.746, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=1056.5, nsentences=144, sample_size=1056.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=428, ups=0.41, wpb=1056.5, bsz=144, num_updates=5510, lr=1.64314e-05, gnorm=1.432, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13631
2023-08-02 15:48:21 - progress_bar.py[line:272] - INFO: epoch 001:   5530 / 5589 loss=1.756, loss_v1=0, loss_v2=0, nll_loss=0.449, ntokens=1052.2, nsentences=144, sample_size=1052.2, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=425.6, ups=0.4, wpb=1052.2, bsz=144, num_updates=5520, lr=1.64612e-05, gnorm=1.327, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13656
2023-08-02 15:48:46 - progress_bar.py[line:272] - INFO: epoch 001:   5540 / 5589 loss=1.759, loss_v1=0, loss_v2=0, nll_loss=0.451, ntokens=1058.6, nsentences=144, sample_size=1058.6, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=429.2, ups=0.41, wpb=1058.6, bsz=144, num_updates=5530, lr=1.64911e-05, gnorm=1.362, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13680
2023-08-02 15:49:11 - progress_bar.py[line:272] - INFO: epoch 001:   5550 / 5589 loss=1.765, loss_v1=0, loss_v2=0, nll_loss=0.459, ntokens=1054.7, nsentences=144, sample_size=1054.7, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=427.3, ups=0.41, wpb=1054.7, bsz=144, num_updates=5540, lr=1.65209e-05, gnorm=1.468, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13705
2023-08-02 15:49:35 - progress_bar.py[line:272] - INFO: epoch 001:   5560 / 5589 loss=1.761, loss_v1=0, loss_v2=0, nll_loss=0.455, ntokens=1056.7, nsentences=144, sample_size=1056.7, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=427.8, ups=0.4, wpb=1056.7, bsz=144, num_updates=5550, lr=1.65507e-05, gnorm=1.378, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13730
2023-08-02 15:50:00 - progress_bar.py[line:272] - INFO: epoch 001:   5570 / 5589 loss=1.734, loss_v1=0, loss_v2=0, nll_loss=0.424, ntokens=1060.1, nsentences=144, sample_size=1060.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=427.8, ups=0.4, wpb=1060.1, bsz=144, num_updates=5560, lr=1.65805e-05, gnorm=1.38, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13754
2023-08-02 15:50:25 - progress_bar.py[line:272] - INFO: epoch 001:   5580 / 5589 loss=1.76, loss_v1=0, loss_v2=0, nll_loss=0.452, ntokens=1056.4, nsentences=144, sample_size=1056.4, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=427.7, ups=0.4, wpb=1056.4, bsz=144, num_updates=5570, lr=1.66103e-05, gnorm=1.455, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13779
2023-08-02 15:50:46 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 5579 updates
2023-08-02 15:50:46 - trainer.py[line:431] - INFO: Saving checkpoint to ../../checkpoints/OFA/vrd2_checkpoints/_30_3e-5_512_balanced/checkpoint1.pt
local datafile ../../dataset/OFA_data/vrd2_balanced/vg_train_full.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/vrd2_balanced/vg_train_full.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/vrd2_balanced/vg_train_full.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/vrd2_balanced/vg_train_full.tsv slice_id 1 row count 268254 total row count 804762
local datafile ../../dataset/OFA_data/vrd2_balanced/vg_train_full.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/vrd2_balanced/vg_train_full.tsv slice_id 2 row count 268254 total row count 804762
slice_id 2 seek offset 536508
slice_id 1 seek offset 268254
2023-08-02 15:50:50 - trainer.py[line:441] - INFO: Finished saving checkpoint to ../../checkpoints/OFA/vrd2_checkpoints/_30_3e-5_512_balanced/checkpoint1.pt
2023-08-02 15:50:51 - checkpoint_utils.py[line:133] - INFO: Saved checkpoint ../../checkpoints/OFA/vrd2_checkpoints/_30_3e-5_512_balanced/checkpoint1.pt (epoch 1 @ 5579 updates, score None) (writing took 4.795876230025897 seconds)
2023-08-02 15:50:51 - train.py[line:332] - INFO: end of epoch 1 (average epoch stats below)
2023-08-02 15:50:51 - progress_bar.py[line:282] - INFO: epoch 001 | loss 2.646 | loss_v1 0 | loss_v2 0 | nll_loss 1.371 | ntokens 1055.64 | nsentences 143.99 | sample_size 1055.64 | sample_size_v1 0 | sample_size_v2 0 | ppl 2.59 | wps 427.3 | ups 0.4 | wpb 1055.6 | bsz 144 | num_updates 5579 | lr 1.66372e-05 | gnorm 3.692 | clip 100 | loss_scale 128 | train_wall 13773 | gb_free 6.6 | wall 13806
2023-08-02 15:50:51 - trainer.py[line:639] - INFO: loading train data for epoch 2
local datafile ../../dataset/OFA_data/vrd2_balanced/vg_train_full.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/vrd2_balanced/vg_train_full.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/vrd2_balanced/vg_train_full.tsv slice_id 0 row count 268254 total row count 804762
/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
slice_id 0 seek offset 0
2023-08-02 15:50:52 - trainer.py[line:703] - INFO: begin training epoch 2
2023-08-02 15:50:52 - train.py[line:305] - INFO: Start iterating over samples
2023-08-02 15:50:55 - progress_bar.py[line:272] - INFO: epoch 002:      1 / 5589 loss=1.767, loss_v1=0, loss_v2=0, nll_loss=0.461, ntokens=1031.3, nsentences=140.4, sample_size=1031.3, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=347.4, ups=0.34, wpb=1031.3, bsz=140.4, num_updates=5580, lr=1.66402e-05, gnorm=1.431, clip=100, loss_scale=128, train_wall=24, gb_free=6.6, wall=13809
2023-08-02 15:51:19 - progress_bar.py[line:272] - INFO: epoch 002:     11 / 5589 loss=1.762, loss_v1=0, loss_v2=0, nll_loss=0.457, ntokens=1057.1, nsentences=144, sample_size=1057.1, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=428.7, ups=0.41, wpb=1057.1, bsz=144, num_updates=5590, lr=1.667e-05, gnorm=1.375, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13833
2023-08-02 15:51:44 - progress_bar.py[line:272] - INFO: epoch 002:     21 / 5589 loss=1.757, loss_v1=0, loss_v2=0, nll_loss=0.449, ntokens=1058.8, nsentences=144, sample_size=1058.8, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=429.4, ups=0.41, wpb=1058.8, bsz=144, num_updates=5600, lr=1.66998e-05, gnorm=1.414, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13858
2023-08-02 15:52:08 - progress_bar.py[line:272] - INFO: epoch 002:     31 / 5589 loss=1.757, loss_v1=0, loss_v2=0, nll_loss=0.451, ntokens=1051.5, nsentences=144, sample_size=1051.5, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=426.5, ups=0.41, wpb=1051.5, bsz=144, num_updates=5610, lr=1.67296e-05, gnorm=1.379, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13883
2023-08-02 15:52:33 - progress_bar.py[line:272] - INFO: epoch 002:     41 / 5589 loss=1.772, loss_v1=0, loss_v2=0, nll_loss=0.468, ntokens=1058, nsentences=144, sample_size=1058, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=429.2, ups=0.41, wpb=1058, bsz=144, num_updates=5620, lr=1.67594e-05, gnorm=1.302, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13907
2023-08-02 15:52:58 - progress_bar.py[line:272] - INFO: epoch 002:     51 / 5589 loss=1.758, loss_v1=0, loss_v2=0, nll_loss=0.453, ntokens=1056.9, nsentences=144, sample_size=1056.9, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=429, ups=0.41, wpb=1056.9, bsz=144, num_updates=5630, lr=1.67893e-05, gnorm=1.31, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13932
2023-08-02 15:53:22 - progress_bar.py[line:272] - INFO: epoch 002:     61 / 5589 loss=1.76, loss_v1=0, loss_v2=0, nll_loss=0.451, ntokens=1053.3, nsentences=144, sample_size=1053.3, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=427.4, ups=0.41, wpb=1053.3, bsz=144, num_updates=5640, lr=1.68191e-05, gnorm=1.39, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13957
2023-08-02 15:53:47 - progress_bar.py[line:272] - INFO: epoch 002:     71 / 5589 loss=1.757, loss_v1=0, loss_v2=0, nll_loss=0.453, ntokens=1054.3, nsentences=144, sample_size=1054.3, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=427.6, ups=0.41, wpb=1054.3, bsz=144, num_updates=5650, lr=1.68489e-05, gnorm=1.333, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=13981
2023-08-02 15:54:12 - progress_bar.py[line:272] - INFO: epoch 002:     81 / 5589 loss=1.758, loss_v1=0, loss_v2=0, nll_loss=0.45, ntokens=1055.4, nsentences=144, sample_size=1055.4, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=427.9, ups=0.41, wpb=1055.4, bsz=144, num_updates=5660, lr=1.68787e-05, gnorm=1.389, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14006
2023-08-02 15:54:36 - progress_bar.py[line:272] - INFO: epoch 002:     91 / 5589 loss=1.76, loss_v1=0, loss_v2=0, nll_loss=0.455, ntokens=1056.1, nsentences=144, sample_size=1056.1, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=428.3, ups=0.41, wpb=1056.1, bsz=144, num_updates=5670, lr=1.69085e-05, gnorm=1.409, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14031
2023-08-02 15:55:01 - progress_bar.py[line:272] - INFO: epoch 002:    101 / 5589 loss=1.757, loss_v1=0, loss_v2=0, nll_loss=0.451, ntokens=1056.6, nsentences=144, sample_size=1056.6, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=428.7, ups=0.41, wpb=1056.6, bsz=144, num_updates=5680, lr=1.69384e-05, gnorm=1.427, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14055
2023-08-02 15:55:26 - progress_bar.py[line:272] - INFO: epoch 002:    111 / 5589 loss=1.745, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=1053.5, nsentences=144, sample_size=1053.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=427.3, ups=0.41, wpb=1053.5, bsz=144, num_updates=5690, lr=1.69682e-05, gnorm=1.278, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14080
2023-08-02 15:55:50 - progress_bar.py[line:272] - INFO: epoch 002:    121 / 5589 loss=1.755, loss_v1=0, loss_v2=0, nll_loss=0.45, ntokens=1057.4, nsentences=144, sample_size=1057.4, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=428.9, ups=0.41, wpb=1057.4, bsz=144, num_updates=5700, lr=1.6998e-05, gnorm=1.477, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14105
2023-08-02 15:56:15 - progress_bar.py[line:272] - INFO: epoch 002:    131 / 5589 loss=1.761, loss_v1=0, loss_v2=0, nll_loss=0.455, ntokens=1054.9, nsentences=144, sample_size=1054.9, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=428.2, ups=0.41, wpb=1054.9, bsz=144, num_updates=5710, lr=1.70278e-05, gnorm=1.355, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14129
2023-08-02 15:56:40 - progress_bar.py[line:272] - INFO: epoch 002:    141 / 5589 loss=1.759, loss_v1=0, loss_v2=0, nll_loss=0.453, ntokens=1057.2, nsentences=144, sample_size=1057.2, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=429, ups=0.41, wpb=1057.2, bsz=144, num_updates=5720, lr=1.70577e-05, gnorm=1.403, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14154
2023-08-02 15:57:04 - progress_bar.py[line:272] - INFO: epoch 002:    151 / 5589 loss=1.745, loss_v1=0, loss_v2=0, nll_loss=0.438, ntokens=1059, nsentences=144, sample_size=1059, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=429.7, ups=0.41, wpb=1059, bsz=144, num_updates=5730, lr=1.70875e-05, gnorm=1.276, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14179
2023-08-02 15:57:29 - progress_bar.py[line:272] - INFO: epoch 002:    161 / 5589 loss=1.761, loss_v1=0, loss_v2=0, nll_loss=0.457, ntokens=1054.6, nsentences=144, sample_size=1054.6, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=428, ups=0.41, wpb=1054.6, bsz=144, num_updates=5740, lr=1.71173e-05, gnorm=1.431, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14203
2023-08-02 15:57:54 - progress_bar.py[line:272] - INFO: epoch 002:    171 / 5589 loss=1.746, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=1055.8, nsentences=144, sample_size=1055.8, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=428, ups=0.41, wpb=1055.8, bsz=144, num_updates=5750, lr=1.71471e-05, gnorm=1.348, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14228
2023-08-02 15:58:18 - progress_bar.py[line:272] - INFO: epoch 002:    181 / 5589 loss=1.752, loss_v1=0, loss_v2=0, nll_loss=0.447, ntokens=1057.5, nsentences=144, sample_size=1057.5, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=428.9, ups=0.41, wpb=1057.5, bsz=144, num_updates=5760, lr=1.71769e-05, gnorm=1.406, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14253
2023-08-02 15:58:43 - progress_bar.py[line:272] - INFO: epoch 002:    191 / 5589 loss=1.754, loss_v1=0, loss_v2=0, nll_loss=0.446, ntokens=1055.2, nsentences=144, sample_size=1055.2, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=428, ups=0.41, wpb=1055.2, bsz=144, num_updates=5770, lr=1.72068e-05, gnorm=1.39, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14277
2023-08-02 15:59:08 - progress_bar.py[line:272] - INFO: epoch 002:    201 / 5589 loss=1.745, loss_v1=0, loss_v2=0, nll_loss=0.439, ntokens=1057.4, nsentences=144, sample_size=1057.4, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=428.9, ups=0.41, wpb=1057.4, bsz=144, num_updates=5780, lr=1.72366e-05, gnorm=1.312, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14302
2023-08-02 15:59:32 - progress_bar.py[line:272] - INFO: epoch 002:    211 / 5589 loss=1.765, loss_v1=0, loss_v2=0, nll_loss=0.461, ntokens=1054.2, nsentences=144, sample_size=1054.2, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=427.6, ups=0.41, wpb=1054.2, bsz=144, num_updates=5790, lr=1.72664e-05, gnorm=1.443, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14327
2023-08-02 15:59:57 - progress_bar.py[line:272] - INFO: epoch 002:    221 / 5589 loss=1.757, loss_v1=0, loss_v2=0, nll_loss=0.451, ntokens=1057, nsentences=144, sample_size=1057, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=428.6, ups=0.41, wpb=1057, bsz=144, num_updates=5800, lr=1.72962e-05, gnorm=1.336, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14351
2023-08-02 16:00:22 - progress_bar.py[line:272] - INFO: epoch 002:    231 / 5589 loss=1.747, loss_v1=0, loss_v2=0, nll_loss=0.441, ntokens=1056.7, nsentences=144, sample_size=1056.7, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=428.5, ups=0.41, wpb=1056.7, bsz=144, num_updates=5810, lr=1.7326e-05, gnorm=1.371, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14376
2023-08-02 16:00:46 - progress_bar.py[line:272] - INFO: epoch 002:    241 / 5589 loss=1.766, loss_v1=0, loss_v2=0, nll_loss=0.459, ntokens=1053.2, nsentences=144, sample_size=1053.2, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=426.9, ups=0.41, wpb=1053.2, bsz=144, num_updates=5820, lr=1.73559e-05, gnorm=1.333, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14401
2023-08-02 16:01:11 - progress_bar.py[line:272] - INFO: epoch 002:    251 / 5589 loss=1.766, loss_v1=0, loss_v2=0, nll_loss=0.464, ntokens=1056.7, nsentences=144, sample_size=1056.7, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=428.2, ups=0.41, wpb=1056.7, bsz=144, num_updates=5830, lr=1.73857e-05, gnorm=1.442, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14425
2023-08-02 16:01:36 - progress_bar.py[line:272] - INFO: epoch 002:    261 / 5589 loss=1.751, loss_v1=0, loss_v2=0, nll_loss=0.443, ntokens=1055.7, nsentences=144, sample_size=1055.7, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=427.8, ups=0.41, wpb=1055.7, bsz=144, num_updates=5840, lr=1.74155e-05, gnorm=1.472, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14450
2023-08-02 16:02:00 - progress_bar.py[line:272] - INFO: epoch 002:    271 / 5589 loss=1.756, loss_v1=0, loss_v2=0, nll_loss=0.451, ntokens=1052.5, nsentences=144, sample_size=1052.5, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=426.5, ups=0.41, wpb=1052.5, bsz=144, num_updates=5850, lr=1.74453e-05, gnorm=1.314, clip=90, loss_scale=128, train_wall=25, gb_free=6.6, wall=14475
2023-08-02 16:02:25 - progress_bar.py[line:272] - INFO: epoch 002:    281 / 5589 loss=1.758, loss_v1=0, loss_v2=0, nll_loss=0.453, ntokens=1055.8, nsentences=144, sample_size=1055.8, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=428.2, ups=0.41, wpb=1055.8, bsz=144, num_updates=5860, lr=1.74751e-05, gnorm=1.375, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14499
2023-08-02 16:02:50 - progress_bar.py[line:272] - INFO: epoch 002:    291 / 5589 loss=1.752, loss_v1=0, loss_v2=0, nll_loss=0.446, ntokens=1051.5, nsentences=144, sample_size=1051.5, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=426.6, ups=0.41, wpb=1051.5, bsz=144, num_updates=5870, lr=1.7505e-05, gnorm=1.404, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14524
2023-08-02 16:03:14 - progress_bar.py[line:272] - INFO: epoch 002:    301 / 5589 loss=1.762, loss_v1=0, loss_v2=0, nll_loss=0.457, ntokens=1056.1, nsentences=144, sample_size=1056.1, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=428.8, ups=0.41, wpb=1056.1, bsz=144, num_updates=5880, lr=1.75348e-05, gnorm=1.381, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14549
2023-08-02 16:03:39 - progress_bar.py[line:272] - INFO: epoch 002:    311 / 5589 loss=1.75, loss_v1=0, loss_v2=0, nll_loss=0.443, ntokens=1056.5, nsentences=144, sample_size=1056.5, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=428.9, ups=0.41, wpb=1056.5, bsz=144, num_updates=5890, lr=1.75646e-05, gnorm=1.418, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14573
2023-08-02 16:04:04 - progress_bar.py[line:272] - INFO: epoch 002:    321 / 5589 loss=1.751, loss_v1=0, loss_v2=0, nll_loss=0.445, ntokens=1054.2, nsentences=144, sample_size=1054.2, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=427.6, ups=0.41, wpb=1054.2, bsz=144, num_updates=5900, lr=1.75944e-05, gnorm=1.311, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14598
2023-08-02 16:04:28 - progress_bar.py[line:272] - INFO: epoch 002:    331 / 5589 loss=1.733, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=1060.9, nsentences=144, sample_size=1060.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=430.1, ups=0.41, wpb=1060.9, bsz=144, num_updates=5910, lr=1.76243e-05, gnorm=1.344, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=14622
2023-08-02 16:04:53 - progress_bar.py[line:272] - INFO: epoch 002:    341 / 5589 loss=1.75, loss_v1=0, loss_v2=0, nll_loss=0.443, ntokens=1055.8, nsentences=144, sample_size=1055.8, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=428.1, ups=0.41, wpb=1055.8, bsz=144, num_updates=5920, lr=1.76541e-05, gnorm=1.463, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=14647
2023-08-02 16:05:18 - progress_bar.py[line:272] - INFO: epoch 002:    351 / 5589 loss=1.765, loss_v1=0, loss_v2=0, nll_loss=0.463, ntokens=1055.9, nsentences=144, sample_size=1055.9, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=428.3, ups=0.41, wpb=1055.9, bsz=144, num_updates=5930, lr=1.76839e-05, gnorm=1.404, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=14672
2023-08-02 16:05:25 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-08-02 16:05:45 - progress_bar.py[line:272] - INFO: epoch 002:    362 / 5589 loss=1.744, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=1055.4, nsentences=144, sample_size=1055.4, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=389.4, ups=0.37, wpb=1055.4, bsz=144, num_updates=5940, lr=1.77137e-05, gnorm=1.338, clip=100, loss_scale=128, train_wall=27, gb_free=6.6, wall=14699
2023-08-02 16:06:09 - progress_bar.py[line:272] - INFO: epoch 002:    372 / 5589 loss=1.735, loss_v1=0, loss_v2=0, nll_loss=0.428, ntokens=1052.8, nsentences=144, sample_size=1052.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=427.1, ups=0.41, wpb=1052.8, bsz=144, num_updates=5950, lr=1.77435e-05, gnorm=1.276, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14724
2023-08-02 16:06:34 - progress_bar.py[line:272] - INFO: epoch 002:    382 / 5589 loss=1.762, loss_v1=0, loss_v2=0, nll_loss=0.457, ntokens=1055.1, nsentences=144, sample_size=1055.1, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=428.1, ups=0.41, wpb=1055.1, bsz=144, num_updates=5960, lr=1.77734e-05, gnorm=1.443, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14748
2023-08-02 16:06:59 - progress_bar.py[line:272] - INFO: epoch 002:    392 / 5589 loss=1.753, loss_v1=0, loss_v2=0, nll_loss=0.447, ntokens=1056.8, nsentences=144, sample_size=1056.8, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=428.9, ups=0.41, wpb=1056.8, bsz=144, num_updates=5970, lr=1.78032e-05, gnorm=1.45, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14773
2023-08-02 16:07:23 - progress_bar.py[line:272] - INFO: epoch 002:    402 / 5589 loss=1.755, loss_v1=0, loss_v2=0, nll_loss=0.452, ntokens=1057.3, nsentences=144, sample_size=1057.3, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=428.8, ups=0.41, wpb=1057.3, bsz=144, num_updates=5980, lr=1.7833e-05, gnorm=1.301, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14798
2023-08-02 16:07:48 - progress_bar.py[line:272] - INFO: epoch 002:    412 / 5589 loss=1.734, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=1058.6, nsentences=144, sample_size=1058.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=429.2, ups=0.41, wpb=1058.6, bsz=144, num_updates=5990, lr=1.78628e-05, gnorm=1.352, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14822
2023-08-02 16:08:13 - progress_bar.py[line:272] - INFO: epoch 002:    422 / 5589 loss=1.75, loss_v1=0, loss_v2=0, nll_loss=0.444, ntokens=1055, nsentences=144, sample_size=1055, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=427.7, ups=0.41, wpb=1055, bsz=144, num_updates=6000, lr=1.78926e-05, gnorm=1.402, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14847
2023-08-02 16:08:37 - progress_bar.py[line:272] - INFO: epoch 002:    432 / 5589 loss=1.739, loss_v1=0, loss_v2=0, nll_loss=0.432, ntokens=1053.7, nsentences=144, sample_size=1053.7, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=427.5, ups=0.41, wpb=1053.7, bsz=144, num_updates=6010, lr=1.79225e-05, gnorm=1.46, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14872
2023-08-02 16:09:02 - progress_bar.py[line:272] - INFO: epoch 002:    442 / 5589 loss=1.728, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=1057.7, nsentences=144, sample_size=1057.7, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=429.2, ups=0.41, wpb=1057.7, bsz=144, num_updates=6020, lr=1.79523e-05, gnorm=1.512, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14896
2023-08-02 16:09:27 - progress_bar.py[line:272] - INFO: epoch 002:    452 / 5589 loss=1.747, loss_v1=0, loss_v2=0, nll_loss=0.441, ntokens=1048.8, nsentences=144, sample_size=1048.8, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=425.7, ups=0.41, wpb=1048.8, bsz=144, num_updates=6030, lr=1.79821e-05, gnorm=1.326, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14921
2023-08-02 16:09:51 - progress_bar.py[line:272] - INFO: epoch 002:    462 / 5589 loss=1.743, loss_v1=0, loss_v2=0, nll_loss=0.436, ntokens=1055.2, nsentences=144, sample_size=1055.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=427.9, ups=0.41, wpb=1055.2, bsz=144, num_updates=6040, lr=1.80119e-05, gnorm=1.325, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14945
2023-08-02 16:10:16 - progress_bar.py[line:272] - INFO: epoch 002:    472 / 5589 loss=1.75, loss_v1=0, loss_v2=0, nll_loss=0.444, ntokens=1059.9, nsentences=144, sample_size=1059.9, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=430.1, ups=0.41, wpb=1059.9, bsz=144, num_updates=6050, lr=1.80417e-05, gnorm=1.372, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14970
2023-08-02 16:10:40 - progress_bar.py[line:272] - INFO: epoch 002:    482 / 5589 loss=1.749, loss_v1=0, loss_v2=0, nll_loss=0.446, ntokens=1054.7, nsentences=144, sample_size=1054.7, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=428.1, ups=0.41, wpb=1054.7, bsz=144, num_updates=6060, lr=1.80716e-05, gnorm=1.403, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=14995
2023-08-02 16:11:05 - progress_bar.py[line:272] - INFO: epoch 002:    492 / 5589 loss=1.74, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=1053.8, nsentences=144, sample_size=1053.8, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=427.6, ups=0.41, wpb=1053.8, bsz=144, num_updates=6070, lr=1.81014e-05, gnorm=1.318, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15019
2023-08-02 16:11:30 - progress_bar.py[line:272] - INFO: epoch 002:    502 / 5589 loss=1.746, loss_v1=0, loss_v2=0, nll_loss=0.44, ntokens=1057.8, nsentences=144, sample_size=1057.8, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=429.2, ups=0.41, wpb=1057.8, bsz=144, num_updates=6080, lr=1.81312e-05, gnorm=1.37, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15044
2023-08-02 16:11:54 - progress_bar.py[line:272] - INFO: epoch 002:    512 / 5589 loss=1.743, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=1054.1, nsentences=144, sample_size=1054.1, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=427.4, ups=0.41, wpb=1054.1, bsz=144, num_updates=6090, lr=1.8161e-05, gnorm=1.418, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15069
2023-08-02 16:12:19 - progress_bar.py[line:272] - INFO: epoch 002:    522 / 5589 loss=1.739, loss_v1=0, loss_v2=0, nll_loss=0.432, ntokens=1055.3, nsentences=144, sample_size=1055.3, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=427.4, ups=0.41, wpb=1055.3, bsz=144, num_updates=6100, lr=1.81909e-05, gnorm=1.36, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15093
2023-08-02 16:12:44 - progress_bar.py[line:272] - INFO: epoch 002:    532 / 5589 loss=1.745, loss_v1=0, loss_v2=0, nll_loss=0.439, ntokens=1058.5, nsentences=144, sample_size=1058.5, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=428.7, ups=0.4, wpb=1058.5, bsz=144, num_updates=6110, lr=1.82207e-05, gnorm=1.45, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15118
2023-08-02 16:13:09 - progress_bar.py[line:272] - INFO: epoch 002:    542 / 5589 loss=1.743, loss_v1=0, loss_v2=0, nll_loss=0.439, ntokens=1051.8, nsentences=144, sample_size=1051.8, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=425.1, ups=0.4, wpb=1051.8, bsz=144, num_updates=6120, lr=1.82505e-05, gnorm=1.408, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15143
2023-08-02 16:13:33 - progress_bar.py[line:272] - INFO: epoch 002:    552 / 5589 loss=1.761, loss_v1=0, loss_v2=0, nll_loss=0.456, ntokens=1059.6, nsentences=144, sample_size=1059.6, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=429.1, ups=0.4, wpb=1059.6, bsz=144, num_updates=6130, lr=1.82803e-05, gnorm=1.383, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15168
2023-08-02 16:13:58 - progress_bar.py[line:272] - INFO: epoch 002:    562 / 5589 loss=1.739, loss_v1=0, loss_v2=0, nll_loss=0.433, ntokens=1059.7, nsentences=144, sample_size=1059.7, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=428.9, ups=0.4, wpb=1059.7, bsz=144, num_updates=6140, lr=1.83101e-05, gnorm=1.42, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15192
2023-08-02 16:14:23 - progress_bar.py[line:272] - INFO: epoch 002:    572 / 5589 loss=1.741, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=1054.8, nsentences=144, sample_size=1054.8, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=427.5, ups=0.41, wpb=1054.8, bsz=144, num_updates=6150, lr=1.834e-05, gnorm=1.446, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15217
2023-08-02 16:14:47 - progress_bar.py[line:272] - INFO: epoch 002:    582 / 5589 loss=1.742, loss_v1=0, loss_v2=0, nll_loss=0.435, ntokens=1054.2, nsentences=144, sample_size=1054.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=427.6, ups=0.41, wpb=1054.2, bsz=144, num_updates=6160, lr=1.83698e-05, gnorm=1.367, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15242
2023-08-02 16:15:12 - progress_bar.py[line:272] - INFO: epoch 002:    592 / 5589 loss=1.747, loss_v1=0, loss_v2=0, nll_loss=0.441, ntokens=1058.8, nsentences=144, sample_size=1058.8, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=429.4, ups=0.41, wpb=1058.8, bsz=144, num_updates=6170, lr=1.83996e-05, gnorm=1.508, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15266
2023-08-02 16:15:37 - progress_bar.py[line:272] - INFO: epoch 002:    602 / 5589 loss=1.745, loss_v1=0, loss_v2=0, nll_loss=0.44, ntokens=1052, nsentences=144, sample_size=1052, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=426.6, ups=0.41, wpb=1052, bsz=144, num_updates=6180, lr=1.84294e-05, gnorm=1.384, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15291
2023-08-02 16:16:01 - progress_bar.py[line:272] - INFO: epoch 002:    612 / 5589 loss=1.744, loss_v1=0, loss_v2=0, nll_loss=0.439, ntokens=1059.4, nsentences=144, sample_size=1059.4, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=429.7, ups=0.41, wpb=1059.4, bsz=144, num_updates=6190, lr=1.84592e-05, gnorm=1.321, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15316
2023-08-02 16:16:26 - progress_bar.py[line:272] - INFO: epoch 002:    622 / 5589 loss=1.747, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=1057.3, nsentences=144, sample_size=1057.3, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=428.9, ups=0.41, wpb=1057.3, bsz=144, num_updates=6200, lr=1.84891e-05, gnorm=1.426, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15340
2023-08-02 16:16:51 - progress_bar.py[line:272] - INFO: epoch 002:    632 / 5589 loss=1.748, loss_v1=0, loss_v2=0, nll_loss=0.444, ntokens=1056, nsentences=144, sample_size=1056, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=428.2, ups=0.41, wpb=1056, bsz=144, num_updates=6210, lr=1.85189e-05, gnorm=1.415, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15365
2023-08-02 16:17:15 - progress_bar.py[line:272] - INFO: epoch 002:    642 / 5589 loss=1.747, loss_v1=0, loss_v2=0, nll_loss=0.44, ntokens=1054.3, nsentences=144, sample_size=1054.3, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=427.6, ups=0.41, wpb=1054.3, bsz=144, num_updates=6220, lr=1.85487e-05, gnorm=1.462, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15390
2023-08-02 16:17:40 - progress_bar.py[line:272] - INFO: epoch 002:    652 / 5589 loss=1.735, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=1054.2, nsentences=144, sample_size=1054.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=427.4, ups=0.41, wpb=1054.2, bsz=144, num_updates=6230, lr=1.85785e-05, gnorm=1.353, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15414
2023-08-02 16:18:05 - progress_bar.py[line:272] - INFO: epoch 002:    662 / 5589 loss=1.743, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=1059, nsentences=144, sample_size=1059, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=429.5, ups=0.41, wpb=1059, bsz=144, num_updates=6240, lr=1.86083e-05, gnorm=1.443, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15439
2023-08-02 16:18:29 - progress_bar.py[line:272] - INFO: epoch 002:    672 / 5589 loss=1.737, loss_v1=0, loss_v2=0, nll_loss=0.432, ntokens=1056.7, nsentences=144, sample_size=1056.7, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=428.5, ups=0.41, wpb=1056.7, bsz=144, num_updates=6250, lr=1.86382e-05, gnorm=1.353, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15464
2023-08-02 16:18:54 - progress_bar.py[line:272] - INFO: epoch 002:    682 / 5589 loss=1.741, loss_v1=0, loss_v2=0, nll_loss=0.435, ntokens=1060, nsentences=144, sample_size=1060, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=429.8, ups=0.41, wpb=1060, bsz=144, num_updates=6260, lr=1.8668e-05, gnorm=1.395, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15488
2023-08-02 16:19:19 - progress_bar.py[line:272] - INFO: epoch 002:    692 / 5589 loss=1.747, loss_v1=0, loss_v2=0, nll_loss=0.443, ntokens=1058, nsentences=144, sample_size=1058, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=429.4, ups=0.41, wpb=1058, bsz=144, num_updates=6270, lr=1.86978e-05, gnorm=1.351, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15513
2023-08-02 16:19:43 - progress_bar.py[line:272] - INFO: epoch 002:    702 / 5589 loss=1.731, loss_v1=0, loss_v2=0, nll_loss=0.424, ntokens=1053.1, nsentences=144, sample_size=1053.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=427.2, ups=0.41, wpb=1053.1, bsz=144, num_updates=6280, lr=1.87276e-05, gnorm=1.283, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15538
2023-08-02 16:20:08 - progress_bar.py[line:272] - INFO: epoch 002:    712 / 5589 loss=1.744, loss_v1=0, loss_v2=0, nll_loss=0.44, ntokens=1052, nsentences=144, sample_size=1052, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=426.7, ups=0.41, wpb=1052, bsz=144, num_updates=6290, lr=1.87575e-05, gnorm=1.488, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15562
2023-08-02 16:20:33 - progress_bar.py[line:272] - INFO: epoch 002:    722 / 5589 loss=1.75, loss_v1=0, loss_v2=0, nll_loss=0.444, ntokens=1058.3, nsentences=144, sample_size=1058.3, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=429.1, ups=0.41, wpb=1058.3, bsz=144, num_updates=6300, lr=1.87873e-05, gnorm=1.405, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15587
2023-08-02 16:20:57 - progress_bar.py[line:272] - INFO: epoch 002:    732 / 5589 loss=1.739, loss_v1=0, loss_v2=0, nll_loss=0.435, ntokens=1054.2, nsentences=144, sample_size=1054.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=427.5, ups=0.41, wpb=1054.2, bsz=144, num_updates=6310, lr=1.88171e-05, gnorm=1.33, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15611
2023-08-02 16:21:22 - progress_bar.py[line:272] - INFO: epoch 002:    742 / 5589 loss=1.738, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=1054.5, nsentences=144, sample_size=1054.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=427.9, ups=0.41, wpb=1054.5, bsz=144, num_updates=6320, lr=1.88469e-05, gnorm=1.362, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15636
2023-08-02 16:21:47 - progress_bar.py[line:272] - INFO: epoch 002:    752 / 5589 loss=1.745, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=1054.5, nsentences=144, sample_size=1054.5, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=427.5, ups=0.41, wpb=1054.5, bsz=144, num_updates=6330, lr=1.88767e-05, gnorm=1.533, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15661
2023-08-02 16:22:11 - progress_bar.py[line:272] - INFO: epoch 002:    762 / 5589 loss=1.738, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=1059.1, nsentences=144, sample_size=1059.1, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=428.6, ups=0.4, wpb=1059.1, bsz=144, num_updates=6340, lr=1.89066e-05, gnorm=1.449, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15686
2023-08-02 16:22:36 - progress_bar.py[line:272] - INFO: epoch 002:    772 / 5589 loss=1.739, loss_v1=0, loss_v2=0, nll_loss=0.436, ntokens=1054, nsentences=144, sample_size=1054, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=426.8, ups=0.4, wpb=1054, bsz=144, num_updates=6350, lr=1.89364e-05, gnorm=1.394, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15710
2023-08-02 16:23:01 - progress_bar.py[line:272] - INFO: epoch 002:    782 / 5589 loss=1.734, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=1055.5, nsentences=144, sample_size=1055.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=427.3, ups=0.4, wpb=1055.5, bsz=144, num_updates=6360, lr=1.89662e-05, gnorm=1.383, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15735
2023-08-02 16:23:25 - progress_bar.py[line:272] - INFO: epoch 002:    792 / 5589 loss=1.734, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=1053.3, nsentences=144, sample_size=1053.3, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=426.4, ups=0.4, wpb=1053.3, bsz=144, num_updates=6370, lr=1.8996e-05, gnorm=1.31, clip=90, loss_scale=128, train_wall=25, gb_free=6.6, wall=15760
2023-08-02 16:23:50 - progress_bar.py[line:272] - INFO: epoch 002:    802 / 5589 loss=1.737, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=1056.8, nsentences=144, sample_size=1056.8, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=427.8, ups=0.4, wpb=1056.8, bsz=144, num_updates=6380, lr=1.90258e-05, gnorm=1.408, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15784
2023-08-02 16:24:15 - progress_bar.py[line:272] - INFO: epoch 002:    812 / 5589 loss=1.734, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=1058.2, nsentences=144, sample_size=1058.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=428.5, ups=0.4, wpb=1058.2, bsz=144, num_updates=6390, lr=1.90557e-05, gnorm=1.36, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15809
2023-08-02 16:24:39 - progress_bar.py[line:272] - INFO: epoch 002:    822 / 5589 loss=1.727, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=1058.3, nsentences=144, sample_size=1058.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=428.6, ups=0.4, wpb=1058.3, bsz=144, num_updates=6400, lr=1.90855e-05, gnorm=1.405, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15834
2023-08-02 16:25:04 - progress_bar.py[line:272] - INFO: epoch 002:    832 / 5589 loss=1.73, loss_v1=0, loss_v2=0, nll_loss=0.424, ntokens=1056.7, nsentences=144, sample_size=1056.7, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=427.8, ups=0.4, wpb=1056.7, bsz=144, num_updates=6410, lr=1.91153e-05, gnorm=1.367, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15858
2023-08-02 16:25:29 - progress_bar.py[line:272] - INFO: epoch 002:    842 / 5589 loss=1.726, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=1058, nsentences=144, sample_size=1058, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=428.4, ups=0.4, wpb=1058, bsz=144, num_updates=6420, lr=1.91451e-05, gnorm=1.378, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15883
2023-08-02 16:25:54 - progress_bar.py[line:272] - INFO: epoch 002:    852 / 5589 loss=1.756, loss_v1=0, loss_v2=0, nll_loss=0.454, ntokens=1052, nsentences=144, sample_size=1052, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=425.7, ups=0.4, wpb=1052, bsz=144, num_updates=6430, lr=1.9175e-05, gnorm=1.533, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15908
2023-08-02 16:26:18 - progress_bar.py[line:272] - INFO: epoch 002:    862 / 5589 loss=1.735, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=1053.6, nsentences=144, sample_size=1053.6, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=426.5, ups=0.4, wpb=1053.6, bsz=144, num_updates=6440, lr=1.92048e-05, gnorm=1.333, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=15933
2023-08-02 16:26:43 - progress_bar.py[line:272] - INFO: epoch 002:    872 / 5589 loss=1.733, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=1058, nsentences=144, sample_size=1058, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=428.3, ups=0.4, wpb=1058, bsz=144, num_updates=6450, lr=1.92346e-05, gnorm=1.316, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=15957
2023-08-02 16:27:08 - progress_bar.py[line:272] - INFO: epoch 002:    882 / 5589 loss=1.742, loss_v1=0, loss_v2=0, nll_loss=0.439, ntokens=1056, nsentences=144, sample_size=1056, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=427.3, ups=0.4, wpb=1056, bsz=144, num_updates=6460, lr=1.92644e-05, gnorm=1.311, clip=90, loss_scale=256, train_wall=25, gb_free=6.6, wall=15982
2023-08-02 16:27:32 - progress_bar.py[line:272] - INFO: epoch 002:    892 / 5589 loss=1.737, loss_v1=0, loss_v2=0, nll_loss=0.432, ntokens=1051.3, nsentences=144, sample_size=1051.3, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=425.8, ups=0.41, wpb=1051.3, bsz=144, num_updates=6470, lr=1.92942e-05, gnorm=1.332, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=16007
2023-08-02 16:27:57 - progress_bar.py[line:272] - INFO: epoch 002:    902 / 5589 loss=1.724, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=1055.1, nsentences=144, sample_size=1055.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=427.4, ups=0.41, wpb=1055.1, bsz=144, num_updates=6480, lr=1.93241e-05, gnorm=1.414, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=16031
2023-08-02 16:28:22 - progress_bar.py[line:272] - INFO: epoch 002:    912 / 5589 loss=1.737, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=1053.8, nsentences=144, sample_size=1053.8, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=426.5, ups=0.4, wpb=1053.8, bsz=144, num_updates=6490, lr=1.93539e-05, gnorm=1.344, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=16056
2023-08-02 16:28:47 - progress_bar.py[line:272] - INFO: epoch 002:    922 / 5589 loss=1.736, loss_v1=0, loss_v2=0, nll_loss=0.432, ntokens=1056.3, nsentences=144, sample_size=1056.3, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=427.7, ups=0.4, wpb=1056.3, bsz=144, num_updates=6500, lr=1.93837e-05, gnorm=1.412, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=16081
2023-08-02 16:29:11 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-08-02 16:29:14 - progress_bar.py[line:272] - INFO: epoch 002:    933 / 5589 loss=1.732, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=1055.7, nsentences=144, sample_size=1055.7, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=388.5, ups=0.37, wpb=1055.7, bsz=144, num_updates=6510, lr=1.94135e-05, gnorm=1.523, clip=100, loss_scale=128, train_wall=27, gb_free=6.6, wall=16108
2023-08-02 16:29:38 - progress_bar.py[line:272] - INFO: epoch 002:    943 / 5589 loss=1.737, loss_v1=0, loss_v2=0, nll_loss=0.433, ntokens=1055, nsentences=144, sample_size=1055, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=427.1, ups=0.4, wpb=1055, bsz=144, num_updates=6520, lr=1.94433e-05, gnorm=1.43, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16133
2023-08-02 16:30:03 - progress_bar.py[line:272] - INFO: epoch 002:    953 / 5589 loss=1.735, loss_v1=0, loss_v2=0, nll_loss=0.428, ntokens=1056.4, nsentences=144, sample_size=1056.4, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=427.7, ups=0.4, wpb=1056.4, bsz=144, num_updates=6530, lr=1.94732e-05, gnorm=1.445, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16157
2023-08-02 16:30:28 - progress_bar.py[line:272] - INFO: epoch 002:    963 / 5589 loss=1.735, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=1052.5, nsentences=144, sample_size=1052.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=426.2, ups=0.4, wpb=1052.5, bsz=144, num_updates=6540, lr=1.9503e-05, gnorm=1.35, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16182
2023-08-02 16:30:52 - progress_bar.py[line:272] - INFO: epoch 002:    973 / 5589 loss=1.738, loss_v1=0, loss_v2=0, nll_loss=0.433, ntokens=1052.2, nsentences=144, sample_size=1052.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=426.1, ups=0.4, wpb=1052.2, bsz=144, num_updates=6550, lr=1.95328e-05, gnorm=1.452, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16207
2023-08-02 16:31:17 - progress_bar.py[line:272] - INFO: epoch 002:    983 / 5589 loss=1.728, loss_v1=0, loss_v2=0, nll_loss=0.422, ntokens=1053.2, nsentences=144, sample_size=1053.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=425.7, ups=0.4, wpb=1053.2, bsz=144, num_updates=6560, lr=1.95626e-05, gnorm=1.366, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16232
2023-08-02 16:31:42 - progress_bar.py[line:272] - INFO: epoch 002:    993 / 5589 loss=1.722, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=1054.2, nsentences=144, sample_size=1054.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=426.7, ups=0.4, wpb=1054.2, bsz=144, num_updates=6570, lr=1.95924e-05, gnorm=1.35, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16256
2023-08-02 16:32:07 - progress_bar.py[line:272] - INFO: epoch 002:   1003 / 5589 loss=1.723, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=1059.9, nsentences=144, sample_size=1059.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=429.3, ups=0.41, wpb=1059.9, bsz=144, num_updates=6580, lr=1.96223e-05, gnorm=1.369, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16281
2023-08-02 16:32:31 - progress_bar.py[line:272] - INFO: epoch 002:   1013 / 5589 loss=1.739, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=1055.3, nsentences=144, sample_size=1055.3, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=427.2, ups=0.4, wpb=1055.3, bsz=144, num_updates=6590, lr=1.96521e-05, gnorm=1.476, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16306
2023-08-02 16:32:56 - progress_bar.py[line:272] - INFO: epoch 002:   1023 / 5589 loss=1.742, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=1056.9, nsentences=144, sample_size=1056.9, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=428.3, ups=0.41, wpb=1056.9, bsz=144, num_updates=6600, lr=1.96819e-05, gnorm=1.478, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16330
2023-08-02 16:33:21 - progress_bar.py[line:272] - INFO: epoch 002:   1033 / 5589 loss=1.718, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=1054.6, nsentences=144, sample_size=1054.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=427.2, ups=0.41, wpb=1054.6, bsz=144, num_updates=6610, lr=1.97117e-05, gnorm=1.431, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16355
2023-08-02 16:33:45 - progress_bar.py[line:272] - INFO: epoch 002:   1043 / 5589 loss=1.741, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=1053.4, nsentences=144, sample_size=1053.4, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=426.2, ups=0.4, wpb=1053.4, bsz=144, num_updates=6620, lr=1.97416e-05, gnorm=1.419, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16380
2023-08-02 16:34:10 - progress_bar.py[line:272] - INFO: epoch 002:   1053 / 5589 loss=1.73, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=1057, nsentences=144, sample_size=1057, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=427.6, ups=0.4, wpb=1057, bsz=144, num_updates=6630, lr=1.97714e-05, gnorm=1.338, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16404
2023-08-02 16:34:35 - progress_bar.py[line:272] - INFO: epoch 002:   1063 / 5589 loss=1.739, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=1056.1, nsentences=144, sample_size=1056.1, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=427.9, ups=0.41, wpb=1056.1, bsz=144, num_updates=6640, lr=1.98012e-05, gnorm=1.439, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16429
2023-08-02 16:35:00 - progress_bar.py[line:272] - INFO: epoch 002:   1073 / 5589 loss=1.73, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=1054, nsentences=144, sample_size=1054, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=426.9, ups=0.41, wpb=1054, bsz=144, num_updates=6650, lr=1.9831e-05, gnorm=1.419, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16454
2023-08-02 16:35:24 - progress_bar.py[line:272] - INFO: epoch 002:   1083 / 5589 loss=1.723, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=1059.7, nsentences=144, sample_size=1059.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=429.4, ups=0.41, wpb=1059.7, bsz=144, num_updates=6660, lr=1.98608e-05, gnorm=1.442, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16478
2023-08-02 16:35:49 - progress_bar.py[line:272] - INFO: epoch 002:   1093 / 5589 loss=1.728, loss_v1=0, loss_v2=0, nll_loss=0.423, ntokens=1057.1, nsentences=144, sample_size=1057.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=428.1, ups=0.4, wpb=1057.1, bsz=144, num_updates=6670, lr=1.98907e-05, gnorm=1.217, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16503
2023-08-02 16:36:14 - progress_bar.py[line:272] - INFO: epoch 002:   1103 / 5589 loss=1.718, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=1057.8, nsentences=144, sample_size=1057.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=428.5, ups=0.41, wpb=1057.8, bsz=144, num_updates=6680, lr=1.99205e-05, gnorm=1.275, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16528
2023-08-02 16:36:38 - progress_bar.py[line:272] - INFO: epoch 002:   1113 / 5589 loss=1.731, loss_v1=0, loss_v2=0, nll_loss=0.427, ntokens=1056.1, nsentences=144, sample_size=1056.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=427.8, ups=0.41, wpb=1056.1, bsz=144, num_updates=6690, lr=1.99503e-05, gnorm=1.252, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16553
2023-08-02 16:37:03 - progress_bar.py[line:272] - INFO: epoch 002:   1123 / 5589 loss=1.73, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=1056.4, nsentences=144, sample_size=1056.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=426.9, ups=0.4, wpb=1056.4, bsz=144, num_updates=6700, lr=1.99801e-05, gnorm=1.314, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16577
2023-08-02 16:37:28 - progress_bar.py[line:272] - INFO: epoch 002:   1133 / 5589 loss=1.722, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=1054.3, nsentences=144, sample_size=1054.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=426.8, ups=0.4, wpb=1054.3, bsz=144, num_updates=6710, lr=2.00099e-05, gnorm=1.388, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16602
2023-08-02 16:37:52 - progress_bar.py[line:272] - INFO: epoch 002:   1143 / 5589 loss=1.743, loss_v1=0, loss_v2=0, nll_loss=0.439, ntokens=1058.2, nsentences=144, sample_size=1058.2, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=428.2, ups=0.4, wpb=1058.2, bsz=144, num_updates=6720, lr=2.00398e-05, gnorm=1.476, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16627
2023-08-02 16:38:17 - progress_bar.py[line:272] - INFO: epoch 002:   1153 / 5589 loss=1.73, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=1056.4, nsentences=144, sample_size=1056.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=428.5, ups=0.41, wpb=1056.4, bsz=144, num_updates=6730, lr=2.00696e-05, gnorm=1.298, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16651
2023-08-02 16:38:42 - progress_bar.py[line:272] - INFO: epoch 002:   1163 / 5589 loss=1.728, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=1058.5, nsentences=144, sample_size=1058.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=429.2, ups=0.41, wpb=1058.5, bsz=144, num_updates=6740, lr=2.00994e-05, gnorm=1.417, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16676
2023-08-02 16:39:06 - progress_bar.py[line:272] - INFO: epoch 002:   1173 / 5589 loss=1.722, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=1056.2, nsentences=144, sample_size=1056.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=428.1, ups=0.41, wpb=1056.2, bsz=144, num_updates=6750, lr=2.01292e-05, gnorm=1.469, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16701
2023-08-02 16:39:31 - progress_bar.py[line:272] - INFO: epoch 002:   1183 / 5589 loss=1.734, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=1059, nsentences=144, sample_size=1059, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=428.8, ups=0.4, wpb=1059, bsz=144, num_updates=6760, lr=2.0159e-05, gnorm=1.452, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16725
2023-08-02 16:39:56 - progress_bar.py[line:272] - INFO: epoch 002:   1193 / 5589 loss=1.723, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=1055.6, nsentences=144, sample_size=1055.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=427.7, ups=0.41, wpb=1055.6, bsz=144, num_updates=6770, lr=2.01889e-05, gnorm=1.379, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16750
2023-08-02 16:40:21 - progress_bar.py[line:272] - INFO: epoch 002:   1203 / 5589 loss=1.731, loss_v1=0, loss_v2=0, nll_loss=0.427, ntokens=1058.7, nsentences=144, sample_size=1058.7, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=429.1, ups=0.41, wpb=1058.7, bsz=144, num_updates=6780, lr=2.02187e-05, gnorm=1.399, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16775
2023-08-02 16:40:45 - progress_bar.py[line:272] - INFO: epoch 002:   1213 / 5589 loss=1.731, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=1053.8, nsentences=144, sample_size=1053.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=426.8, ups=0.41, wpb=1053.8, bsz=144, num_updates=6790, lr=2.02485e-05, gnorm=1.481, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16799
2023-08-02 16:41:10 - progress_bar.py[line:272] - INFO: epoch 002:   1223 / 5589 loss=1.73, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=1054.9, nsentences=144, sample_size=1054.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=427.6, ups=0.41, wpb=1054.9, bsz=144, num_updates=6800, lr=2.02783e-05, gnorm=1.415, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16824
2023-08-02 16:41:35 - progress_bar.py[line:272] - INFO: epoch 002:   1233 / 5589 loss=1.724, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=1055.1, nsentences=144, sample_size=1055.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=428, ups=0.41, wpb=1055.1, bsz=144, num_updates=6810, lr=2.03082e-05, gnorm=1.324, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16849
2023-08-02 16:41:59 - progress_bar.py[line:272] - INFO: epoch 002:   1243 / 5589 loss=1.726, loss_v1=0, loss_v2=0, nll_loss=0.421, ntokens=1056.4, nsentences=144, sample_size=1056.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=428.7, ups=0.41, wpb=1056.4, bsz=144, num_updates=6820, lr=2.0338e-05, gnorm=1.306, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16873
2023-08-02 16:42:24 - progress_bar.py[line:272] - INFO: epoch 002:   1253 / 5589 loss=1.721, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=1052, nsentences=144, sample_size=1052, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=426.7, ups=0.41, wpb=1052, bsz=144, num_updates=6830, lr=2.03678e-05, gnorm=1.384, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16898
2023-08-02 16:42:49 - progress_bar.py[line:272] - INFO: epoch 002:   1263 / 5589 loss=1.71, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=1058.1, nsentences=144, sample_size=1058.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=429.2, ups=0.41, wpb=1058.1, bsz=144, num_updates=6840, lr=2.03976e-05, gnorm=1.382, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16923
2023-08-02 16:43:13 - progress_bar.py[line:272] - INFO: epoch 002:   1273 / 5589 loss=1.722, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=1057.1, nsentences=144, sample_size=1057.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=429, ups=0.41, wpb=1057.1, bsz=144, num_updates=6850, lr=2.04274e-05, gnorm=1.447, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16947
2023-08-02 16:43:38 - progress_bar.py[line:272] - INFO: epoch 002:   1283 / 5589 loss=1.714, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=1054.9, nsentences=144, sample_size=1054.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=428.1, ups=0.41, wpb=1054.9, bsz=144, num_updates=6860, lr=2.04573e-05, gnorm=1.401, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16972
2023-08-02 16:44:02 - progress_bar.py[line:272] - INFO: epoch 002:   1293 / 5589 loss=1.73, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=1057.7, nsentences=144, sample_size=1057.7, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=429.2, ups=0.41, wpb=1057.7, bsz=144, num_updates=6870, lr=2.04871e-05, gnorm=1.407, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=16997
2023-08-02 16:44:27 - progress_bar.py[line:272] - INFO: epoch 002:   1303 / 5589 loss=1.718, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=1056.7, nsentences=144, sample_size=1056.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=428.7, ups=0.41, wpb=1056.7, bsz=144, num_updates=6880, lr=2.05169e-05, gnorm=1.417, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17021
2023-08-02 16:44:52 - progress_bar.py[line:272] - INFO: epoch 002:   1313 / 5589 loss=1.714, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=1059, nsentences=144, sample_size=1059, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=429.7, ups=0.41, wpb=1059, bsz=144, num_updates=6890, lr=2.05467e-05, gnorm=1.362, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17046
2023-08-02 16:45:16 - progress_bar.py[line:272] - INFO: epoch 002:   1323 / 5589 loss=1.719, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=1058.2, nsentences=144, sample_size=1058.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=429.1, ups=0.41, wpb=1058.2, bsz=144, num_updates=6900, lr=2.05765e-05, gnorm=1.303, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17071
2023-08-02 16:45:41 - progress_bar.py[line:272] - INFO: epoch 002:   1333 / 5589 loss=1.723, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=1057.8, nsentences=144, sample_size=1057.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=426, ups=0.4, wpb=1057.8, bsz=144, num_updates=6910, lr=2.06064e-05, gnorm=1.476, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17096
2023-08-02 16:46:06 - progress_bar.py[line:272] - INFO: epoch 002:   1343 / 5589 loss=1.723, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=1052, nsentences=144, sample_size=1052, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=426.9, ups=0.41, wpb=1052, bsz=144, num_updates=6920, lr=2.06362e-05, gnorm=1.523, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17120
2023-08-02 16:46:31 - progress_bar.py[line:272] - INFO: epoch 002:   1353 / 5589 loss=1.721, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=1053, nsentences=144, sample_size=1053, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=427.3, ups=0.41, wpb=1053, bsz=144, num_updates=6930, lr=2.0666e-05, gnorm=1.39, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17145
2023-08-02 16:46:55 - progress_bar.py[line:272] - INFO: epoch 002:   1363 / 5589 loss=1.735, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=1059.3, nsentences=144, sample_size=1059.3, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=429.7, ups=0.41, wpb=1059.3, bsz=144, num_updates=6940, lr=2.06958e-05, gnorm=1.464, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17169
2023-08-02 16:47:20 - progress_bar.py[line:272] - INFO: epoch 002:   1373 / 5589 loss=1.715, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=1052.8, nsentences=144, sample_size=1052.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=427.3, ups=0.41, wpb=1052.8, bsz=144, num_updates=6950, lr=2.07256e-05, gnorm=1.319, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17194
2023-08-02 16:47:44 - progress_bar.py[line:272] - INFO: epoch 002:   1383 / 5589 loss=1.722, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=1055.8, nsentences=144, sample_size=1055.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=428.4, ups=0.41, wpb=1055.8, bsz=144, num_updates=6960, lr=2.07555e-05, gnorm=1.463, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17219
2023-08-02 16:48:09 - progress_bar.py[line:272] - INFO: epoch 002:   1393 / 5589 loss=1.736, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=1055.2, nsentences=144, sample_size=1055.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=427.7, ups=0.41, wpb=1055.2, bsz=144, num_updates=6970, lr=2.07853e-05, gnorm=1.437, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17243
2023-08-02 16:48:34 - progress_bar.py[line:272] - INFO: epoch 002:   1403 / 5589 loss=1.717, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=1054.6, nsentences=144, sample_size=1054.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=427.3, ups=0.41, wpb=1054.6, bsz=144, num_updates=6980, lr=2.08151e-05, gnorm=1.368, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17268
2023-08-02 16:48:59 - progress_bar.py[line:272] - INFO: epoch 002:   1413 / 5589 loss=1.707, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=1057.3, nsentences=144, sample_size=1057.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=428.4, ups=0.41, wpb=1057.3, bsz=144, num_updates=6990, lr=2.08449e-05, gnorm=1.342, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17293
2023-08-02 16:49:23 - progress_bar.py[line:272] - INFO: epoch 002:   1423 / 5589 loss=1.73, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=1052.4, nsentences=144, sample_size=1052.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=426.2, ups=0.4, wpb=1052.4, bsz=144, num_updates=7000, lr=2.08748e-05, gnorm=1.483, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17317
2023-08-02 16:49:48 - progress_bar.py[line:272] - INFO: epoch 002:   1433 / 5589 loss=1.722, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=1053.4, nsentences=144, sample_size=1053.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=426.4, ups=0.4, wpb=1053.4, bsz=144, num_updates=7010, lr=2.09046e-05, gnorm=1.454, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17342
2023-08-02 16:50:13 - progress_bar.py[line:272] - INFO: epoch 002:   1443 / 5589 loss=1.734, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=1054.1, nsentences=144, sample_size=1054.1, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=426.5, ups=0.4, wpb=1054.1, bsz=144, num_updates=7020, lr=2.09344e-05, gnorm=1.397, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17367
2023-08-02 16:50:37 - progress_bar.py[line:272] - INFO: epoch 002:   1453 / 5589 loss=1.718, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=1060.5, nsentences=144, sample_size=1060.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=429.3, ups=0.4, wpb=1060.5, bsz=144, num_updates=7030, lr=2.09642e-05, gnorm=1.3, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=17392
2023-08-02 16:51:02 - progress_bar.py[line:272] - INFO: epoch 002:   1463 / 5589 loss=1.736, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=1054.7, nsentences=144, sample_size=1054.7, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=427.2, ups=0.41, wpb=1054.7, bsz=144, num_updates=7040, lr=2.0994e-05, gnorm=1.475, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=17416
2023-08-02 16:51:07 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-08-02 16:51:29 - progress_bar.py[line:272] - INFO: epoch 002:   1474 / 5589 loss=1.718, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=1057.4, nsentences=144, sample_size=1057.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=389.5, ups=0.37, wpb=1057.4, bsz=144, num_updates=7050, lr=2.10239e-05, gnorm=1.303, clip=100, loss_scale=128, train_wall=27, gb_free=6.6, wall=17443
2023-08-02 16:51:54 - progress_bar.py[line:272] - INFO: epoch 002:   1484 / 5589 loss=1.729, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=1054.3, nsentences=144, sample_size=1054.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=427, ups=0.41, wpb=1054.3, bsz=144, num_updates=7060, lr=2.10537e-05, gnorm=1.367, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17468
2023-08-02 16:52:19 - progress_bar.py[line:272] - INFO: epoch 002:   1494 / 5589 loss=1.727, loss_v1=0, loss_v2=0, nll_loss=0.422, ntokens=1056.6, nsentences=144, sample_size=1056.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=427.9, ups=0.4, wpb=1056.6, bsz=144, num_updates=7070, lr=2.10835e-05, gnorm=1.574, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17493
2023-08-02 16:52:43 - progress_bar.py[line:272] - INFO: epoch 002:   1504 / 5589 loss=1.714, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=1055.1, nsentences=144, sample_size=1055.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=427.1, ups=0.4, wpb=1055.1, bsz=144, num_updates=7080, lr=2.11133e-05, gnorm=1.286, clip=90, loss_scale=128, train_wall=25, gb_free=6.6, wall=17518
2023-08-02 16:53:08 - progress_bar.py[line:272] - INFO: epoch 002:   1514 / 5589 loss=1.718, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=1053.8, nsentences=144, sample_size=1053.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=426.7, ups=0.4, wpb=1053.8, bsz=144, num_updates=7090, lr=2.11431e-05, gnorm=1.262, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17542
2023-08-02 16:53:33 - progress_bar.py[line:272] - INFO: epoch 002:   1524 / 5589 loss=1.725, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=1052.6, nsentences=144, sample_size=1052.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=426.3, ups=0.4, wpb=1052.6, bsz=144, num_updates=7100, lr=2.1173e-05, gnorm=1.452, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17567
2023-08-02 16:53:57 - progress_bar.py[line:272] - INFO: epoch 002:   1534 / 5589 loss=1.727, loss_v1=0, loss_v2=0, nll_loss=0.422, ntokens=1057, nsentences=144, sample_size=1057, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=428.1, ups=0.4, wpb=1057, bsz=144, num_updates=7110, lr=2.12028e-05, gnorm=1.394, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17592
2023-08-02 16:54:22 - progress_bar.py[line:272] - INFO: epoch 002:   1544 / 5589 loss=1.722, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=1055, nsentences=144, sample_size=1055, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=427.1, ups=0.4, wpb=1055, bsz=144, num_updates=7120, lr=2.12326e-05, gnorm=1.402, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17616
2023-08-02 16:54:47 - progress_bar.py[line:272] - INFO: epoch 002:   1554 / 5589 loss=1.738, loss_v1=0, loss_v2=0, nll_loss=0.435, ntokens=1057.9, nsentences=144, sample_size=1057.9, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=428.3, ups=0.4, wpb=1057.9, bsz=144, num_updates=7130, lr=2.12624e-05, gnorm=1.521, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17641
2023-08-02 16:55:11 - progress_bar.py[line:272] - INFO: epoch 002:   1564 / 5589 loss=1.719, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=1054.2, nsentences=144, sample_size=1054.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=426.9, ups=0.4, wpb=1054.2, bsz=144, num_updates=7140, lr=2.12922e-05, gnorm=1.368, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17666
2023-08-02 16:55:36 - progress_bar.py[line:272] - INFO: epoch 002:   1574 / 5589 loss=1.717, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=1057.5, nsentences=144, sample_size=1057.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=428.4, ups=0.41, wpb=1057.5, bsz=144, num_updates=7150, lr=2.13221e-05, gnorm=1.338, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17690
2023-08-02 16:56:01 - progress_bar.py[line:272] - INFO: epoch 002:   1584 / 5589 loss=1.728, loss_v1=0, loss_v2=0, nll_loss=0.423, ntokens=1057.3, nsentences=144, sample_size=1057.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=424.8, ups=0.4, wpb=1057.3, bsz=144, num_updates=7160, lr=2.13519e-05, gnorm=1.438, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17715
2023-08-02 16:56:26 - progress_bar.py[line:272] - INFO: epoch 002:   1594 / 5589 loss=1.711, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=1058.8, nsentences=144, sample_size=1058.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=428.6, ups=0.4, wpb=1058.8, bsz=144, num_updates=7170, lr=2.13817e-05, gnorm=1.327, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17740
2023-08-02 16:56:50 - progress_bar.py[line:272] - INFO: epoch 002:   1604 / 5589 loss=1.718, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=1057.3, nsentences=144, sample_size=1057.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=428, ups=0.4, wpb=1057.3, bsz=144, num_updates=7180, lr=2.14115e-05, gnorm=1.442, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17765
2023-08-02 16:57:15 - progress_bar.py[line:272] - INFO: epoch 002:   1614 / 5589 loss=1.716, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=1054.4, nsentences=144, sample_size=1054.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=426.9, ups=0.4, wpb=1054.4, bsz=144, num_updates=7190, lr=2.14414e-05, gnorm=1.47, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17789
2023-08-02 16:57:40 - progress_bar.py[line:272] - INFO: epoch 002:   1624 / 5589 loss=1.716, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=1056.6, nsentences=144, sample_size=1056.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=427.9, ups=0.4, wpb=1056.6, bsz=144, num_updates=7200, lr=2.14712e-05, gnorm=1.337, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17814
2023-08-02 16:58:05 - progress_bar.py[line:272] - INFO: epoch 002:   1634 / 5589 loss=1.712, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=1057, nsentences=144, sample_size=1057, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=428.2, ups=0.41, wpb=1057, bsz=144, num_updates=7210, lr=2.1501e-05, gnorm=1.51, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17839
2023-08-02 16:58:29 - progress_bar.py[line:272] - INFO: epoch 002:   1644 / 5589 loss=1.718, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=1054.5, nsentences=144, sample_size=1054.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=427, ups=0.4, wpb=1054.5, bsz=144, num_updates=7220, lr=2.15308e-05, gnorm=1.38, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17864
2023-08-02 16:58:54 - progress_bar.py[line:272] - INFO: epoch 002:   1654 / 5589 loss=1.721, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=1053.8, nsentences=144, sample_size=1053.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=426.2, ups=0.4, wpb=1053.8, bsz=144, num_updates=7230, lr=2.15606e-05, gnorm=1.401, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17888
2023-08-02 16:59:19 - progress_bar.py[line:272] - INFO: epoch 002:   1664 / 5589 loss=1.715, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=1055.4, nsentences=144, sample_size=1055.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=427.6, ups=0.41, wpb=1055.4, bsz=144, num_updates=7240, lr=2.15905e-05, gnorm=1.416, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17913
2023-08-02 16:59:43 - progress_bar.py[line:272] - INFO: epoch 002:   1674 / 5589 loss=1.716, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=1056.7, nsentences=144, sample_size=1056.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=427.7, ups=0.4, wpb=1056.7, bsz=144, num_updates=7250, lr=2.16203e-05, gnorm=1.414, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17938
2023-08-02 17:00:08 - progress_bar.py[line:272] - INFO: epoch 002:   1684 / 5589 loss=1.723, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=1054.3, nsentences=144, sample_size=1054.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=426.5, ups=0.4, wpb=1054.3, bsz=144, num_updates=7260, lr=2.16501e-05, gnorm=1.353, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17962
2023-08-02 17:00:33 - progress_bar.py[line:272] - INFO: epoch 002:   1694 / 5589 loss=1.716, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=1057.8, nsentences=144, sample_size=1057.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=428.2, ups=0.4, wpb=1057.8, bsz=144, num_updates=7270, lr=2.16799e-05, gnorm=1.378, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=17987
2023-08-02 17:00:58 - progress_bar.py[line:272] - INFO: epoch 002:   1704 / 5589 loss=1.717, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=1052.2, nsentences=144, sample_size=1052.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=425.7, ups=0.4, wpb=1052.2, bsz=144, num_updates=7280, lr=2.17097e-05, gnorm=1.578, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18012
2023-08-02 17:01:22 - progress_bar.py[line:272] - INFO: epoch 002:   1714 / 5589 loss=1.723, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=1053, nsentences=144, sample_size=1053, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=426.5, ups=0.41, wpb=1053, bsz=144, num_updates=7290, lr=2.17396e-05, gnorm=1.338, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18036
2023-08-02 17:01:47 - progress_bar.py[line:272] - INFO: epoch 002:   1724 / 5589 loss=1.717, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=1056.7, nsentences=144, sample_size=1056.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=428, ups=0.4, wpb=1056.7, bsz=144, num_updates=7300, lr=2.17694e-05, gnorm=1.289, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18061
2023-08-02 17:02:12 - progress_bar.py[line:272] - INFO: epoch 002:   1734 / 5589 loss=1.71, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=1051.9, nsentences=144, sample_size=1051.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=426.1, ups=0.41, wpb=1051.9, bsz=144, num_updates=7310, lr=2.17992e-05, gnorm=1.383, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18086
2023-08-02 17:02:36 - progress_bar.py[line:272] - INFO: epoch 002:   1744 / 5589 loss=1.717, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=1055.5, nsentences=144, sample_size=1055.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=427.5, ups=0.41, wpb=1055.5, bsz=144, num_updates=7320, lr=2.1829e-05, gnorm=1.503, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18111
2023-08-02 17:03:01 - progress_bar.py[line:272] - INFO: epoch 002:   1754 / 5589 loss=1.716, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=1056.2, nsentences=144, sample_size=1056.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=427.8, ups=0.41, wpb=1056.2, bsz=144, num_updates=7330, lr=2.18588e-05, gnorm=1.351, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18135
2023-08-02 17:03:26 - progress_bar.py[line:272] - INFO: epoch 002:   1764 / 5589 loss=1.707, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=1054.7, nsentences=144, sample_size=1054.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=427, ups=0.4, wpb=1054.7, bsz=144, num_updates=7340, lr=2.18887e-05, gnorm=1.312, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18160
2023-08-02 17:03:50 - progress_bar.py[line:272] - INFO: epoch 002:   1774 / 5589 loss=1.716, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=1057.3, nsentences=144, sample_size=1057.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=428.6, ups=0.41, wpb=1057.3, bsz=144, num_updates=7350, lr=2.19185e-05, gnorm=1.391, clip=90, loss_scale=128, train_wall=25, gb_free=6.6, wall=18185
2023-08-02 17:04:15 - progress_bar.py[line:272] - INFO: epoch 002:   1784 / 5589 loss=1.715, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=1053.1, nsentences=144, sample_size=1053.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=426.3, ups=0.4, wpb=1053.1, bsz=144, num_updates=7360, lr=2.19483e-05, gnorm=1.302, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18209
2023-08-02 17:04:40 - progress_bar.py[line:272] - INFO: epoch 002:   1794 / 5589 loss=1.713, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=1058.8, nsentences=144, sample_size=1058.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=428.8, ups=0.4, wpb=1058.8, bsz=144, num_updates=7370, lr=2.19781e-05, gnorm=1.348, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18234
2023-08-02 17:05:04 - progress_bar.py[line:272] - INFO: epoch 002:   1804 / 5589 loss=1.707, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=1054.6, nsentences=144, sample_size=1054.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=427.2, ups=0.41, wpb=1054.6, bsz=144, num_updates=7380, lr=2.2008e-05, gnorm=1.412, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18259
2023-08-02 17:05:29 - progress_bar.py[line:272] - INFO: epoch 002:   1814 / 5589 loss=1.709, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=1056, nsentences=144, sample_size=1056, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=427.9, ups=0.41, wpb=1056, bsz=144, num_updates=7390, lr=2.20378e-05, gnorm=1.281, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18283
2023-08-02 17:05:54 - progress_bar.py[line:272] - INFO: epoch 002:   1824 / 5589 loss=1.718, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=1053.9, nsentences=144, sample_size=1053.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=426.4, ups=0.4, wpb=1053.9, bsz=144, num_updates=7400, lr=2.20676e-05, gnorm=1.418, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18308
2023-08-02 17:06:19 - progress_bar.py[line:272] - INFO: epoch 002:   1834 / 5589 loss=1.71, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=1056.6, nsentences=144, sample_size=1056.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=428.2, ups=0.41, wpb=1056.6, bsz=144, num_updates=7410, lr=2.20974e-05, gnorm=1.373, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18333
2023-08-02 17:06:43 - progress_bar.py[line:272] - INFO: epoch 002:   1844 / 5589 loss=1.698, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=1053.2, nsentences=144, sample_size=1053.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=427.3, ups=0.41, wpb=1053.2, bsz=144, num_updates=7420, lr=2.21272e-05, gnorm=1.337, clip=90, loss_scale=128, train_wall=25, gb_free=6.6, wall=18357
2023-08-02 17:07:08 - progress_bar.py[line:272] - INFO: epoch 002:   1854 / 5589 loss=1.717, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=1054.1, nsentences=144, sample_size=1054.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=427.6, ups=0.41, wpb=1054.1, bsz=144, num_updates=7430, lr=2.21571e-05, gnorm=1.46, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18382
2023-08-02 17:07:32 - progress_bar.py[line:272] - INFO: epoch 002:   1864 / 5589 loss=1.721, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=1052.1, nsentences=144, sample_size=1052.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=426.8, ups=0.41, wpb=1052.1, bsz=144, num_updates=7440, lr=2.21869e-05, gnorm=1.408, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18407
2023-08-02 17:07:57 - progress_bar.py[line:272] - INFO: epoch 002:   1874 / 5589 loss=1.707, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=1055.3, nsentences=144, sample_size=1055.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=427.8, ups=0.41, wpb=1055.3, bsz=144, num_updates=7450, lr=2.22167e-05, gnorm=1.341, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18431
2023-08-02 17:08:22 - progress_bar.py[line:272] - INFO: epoch 002:   1884 / 5589 loss=1.704, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=1058.8, nsentences=144, sample_size=1058.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=429.5, ups=0.41, wpb=1058.8, bsz=144, num_updates=7460, lr=2.22465e-05, gnorm=1.465, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18456
2023-08-02 17:08:46 - progress_bar.py[line:272] - INFO: epoch 002:   1894 / 5589 loss=1.707, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=1056.3, nsentences=144, sample_size=1056.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=428.5, ups=0.41, wpb=1056.3, bsz=144, num_updates=7470, lr=2.22763e-05, gnorm=1.44, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18481
2023-08-02 17:09:11 - progress_bar.py[line:272] - INFO: epoch 002:   1904 / 5589 loss=1.714, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=1053.9, nsentences=144, sample_size=1053.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=427.4, ups=0.41, wpb=1053.9, bsz=144, num_updates=7480, lr=2.23062e-05, gnorm=1.482, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18505
2023-08-02 17:09:36 - progress_bar.py[line:272] - INFO: epoch 002:   1914 / 5589 loss=1.703, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=1057.1, nsentences=144, sample_size=1057.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=429.1, ups=0.41, wpb=1057.1, bsz=144, num_updates=7490, lr=2.2336e-05, gnorm=1.303, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18530
2023-08-02 17:10:00 - progress_bar.py[line:272] - INFO: epoch 002:   1924 / 5589 loss=1.718, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=1058.1, nsentences=144, sample_size=1058.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=429, ups=0.41, wpb=1058.1, bsz=144, num_updates=7500, lr=2.23658e-05, gnorm=1.404, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18555
2023-08-02 17:10:25 - progress_bar.py[line:272] - INFO: epoch 002:   1934 / 5589 loss=1.711, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=1056.3, nsentences=144, sample_size=1056.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=428.4, ups=0.41, wpb=1056.3, bsz=144, num_updates=7510, lr=2.23956e-05, gnorm=1.318, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18579
2023-08-02 17:10:50 - progress_bar.py[line:272] - INFO: epoch 002:   1944 / 5589 loss=1.71, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=1061.2, nsentences=144, sample_size=1061.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=429.9, ups=0.41, wpb=1061.2, bsz=144, num_updates=7520, lr=2.24254e-05, gnorm=1.384, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18604
2023-08-02 17:11:15 - progress_bar.py[line:272] - INFO: epoch 002:   1954 / 5589 loss=1.711, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=1056.3, nsentences=144, sample_size=1056.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=427.4, ups=0.4, wpb=1056.3, bsz=144, num_updates=7530, lr=2.24553e-05, gnorm=1.392, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18629
2023-08-02 17:11:39 - progress_bar.py[line:272] - INFO: epoch 002:   1964 / 5589 loss=1.712, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=1055.6, nsentences=144, sample_size=1055.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=427, ups=0.4, wpb=1055.6, bsz=144, num_updates=7540, lr=2.24851e-05, gnorm=1.418, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18654
2023-08-02 17:12:04 - progress_bar.py[line:272] - INFO: epoch 002:   1974 / 5589 loss=1.708, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=1056.5, nsentences=144, sample_size=1056.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=428.2, ups=0.41, wpb=1056.5, bsz=144, num_updates=7550, lr=2.25149e-05, gnorm=1.403, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18678
2023-08-02 17:12:29 - progress_bar.py[line:272] - INFO: epoch 002:   1984 / 5589 loss=1.714, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=1055.6, nsentences=144, sample_size=1055.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=428.1, ups=0.41, wpb=1055.6, bsz=144, num_updates=7560, lr=2.25447e-05, gnorm=1.396, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=18703
2023-08-02 17:12:53 - progress_bar.py[line:272] - INFO: epoch 002:   1994 / 5589 loss=1.704, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=1054.7, nsentences=144, sample_size=1054.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=427.6, ups=0.41, wpb=1054.7, bsz=144, num_updates=7570, lr=2.25746e-05, gnorm=1.348, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=18728
2023-08-02 17:13:18 - progress_bar.py[line:272] - INFO: epoch 002:   2004 / 5589 loss=1.71, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=1053.5, nsentences=144, sample_size=1053.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=427.4, ups=0.41, wpb=1053.5, bsz=144, num_updates=7580, lr=2.26044e-05, gnorm=1.429, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=18752
2023-08-02 17:13:43 - progress_bar.py[line:272] - INFO: epoch 002:   2014 / 5589 loss=1.703, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=1059.8, nsentences=144, sample_size=1059.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=430, ups=0.41, wpb=1059.8, bsz=144, num_updates=7590, lr=2.26342e-05, gnorm=1.384, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=18777
2023-08-02 17:13:50 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-08-02 17:14:10 - progress_bar.py[line:272] - INFO: epoch 002:   2025 / 5589 loss=1.695, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=1055.1, nsentences=144, sample_size=1055.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=389.4, ups=0.37, wpb=1055.1, bsz=144, num_updates=7600, lr=2.2664e-05, gnorm=1.285, clip=100, loss_scale=128, train_wall=27, gb_free=6.6, wall=18804
2023-08-02 17:14:34 - progress_bar.py[line:272] - INFO: epoch 002:   2035 / 5589 loss=1.702, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=1055.1, nsentences=144, sample_size=1055.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=428.1, ups=0.41, wpb=1055.1, bsz=144, num_updates=7610, lr=2.26938e-05, gnorm=1.409, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18829
2023-08-02 17:14:59 - progress_bar.py[line:272] - INFO: epoch 002:   2045 / 5589 loss=1.706, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=1051.3, nsentences=144, sample_size=1051.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=426.7, ups=0.41, wpb=1051.3, bsz=144, num_updates=7620, lr=2.27237e-05, gnorm=1.424, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18853
2023-08-02 17:15:24 - progress_bar.py[line:272] - INFO: epoch 002:   2055 / 5589 loss=1.71, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=1053.9, nsentences=144, sample_size=1053.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=427.2, ups=0.41, wpb=1053.9, bsz=144, num_updates=7630, lr=2.27535e-05, gnorm=1.385, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18878
2023-08-02 17:15:48 - progress_bar.py[line:272] - INFO: epoch 002:   2065 / 5589 loss=1.703, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=1056.5, nsentences=144, sample_size=1056.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=429.2, ups=0.41, wpb=1056.5, bsz=144, num_updates=7640, lr=2.27833e-05, gnorm=1.411, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18903
2023-08-02 17:16:13 - progress_bar.py[line:272] - INFO: epoch 002:   2075 / 5589 loss=1.71, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=1054.3, nsentences=144, sample_size=1054.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=428.1, ups=0.41, wpb=1054.3, bsz=144, num_updates=7650, lr=2.28131e-05, gnorm=1.378, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18927
2023-08-02 17:16:38 - progress_bar.py[line:272] - INFO: epoch 002:   2085 / 5589 loss=1.718, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=1053.1, nsentences=144, sample_size=1053.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=426.9, ups=0.41, wpb=1053.1, bsz=144, num_updates=7660, lr=2.28429e-05, gnorm=1.418, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18952
2023-08-02 17:17:02 - progress_bar.py[line:272] - INFO: epoch 002:   2095 / 5589 loss=1.715, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=1055.8, nsentences=144, sample_size=1055.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=428.1, ups=0.41, wpb=1055.8, bsz=144, num_updates=7670, lr=2.28728e-05, gnorm=1.347, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=18976
2023-08-02 17:17:27 - progress_bar.py[line:272] - INFO: epoch 002:   2105 / 5589 loss=1.704, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=1055.1, nsentences=144, sample_size=1055.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=427.9, ups=0.41, wpb=1055.1, bsz=144, num_updates=7680, lr=2.29026e-05, gnorm=1.318, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19001
2023-08-02 17:17:52 - progress_bar.py[line:272] - INFO: epoch 002:   2115 / 5589 loss=1.701, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=1054.9, nsentences=144, sample_size=1054.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=427.8, ups=0.41, wpb=1054.9, bsz=144, num_updates=7690, lr=2.29324e-05, gnorm=1.383, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19026
2023-08-02 17:18:16 - progress_bar.py[line:272] - INFO: epoch 002:   2125 / 5589 loss=1.713, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=1061.6, nsentences=144, sample_size=1061.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=431, ups=0.41, wpb=1061.6, bsz=144, num_updates=7700, lr=2.29622e-05, gnorm=1.401, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19050
2023-08-02 17:18:41 - progress_bar.py[line:272] - INFO: epoch 002:   2135 / 5589 loss=1.699, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=1056.5, nsentences=144, sample_size=1056.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=428.9, ups=0.41, wpb=1056.5, bsz=144, num_updates=7710, lr=2.2992e-05, gnorm=1.428, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19075
2023-08-02 17:19:05 - progress_bar.py[line:272] - INFO: epoch 002:   2145 / 5589 loss=1.7, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=1053.6, nsentences=144, sample_size=1053.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=427.3, ups=0.41, wpb=1053.6, bsz=144, num_updates=7720, lr=2.30219e-05, gnorm=1.443, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19100
2023-08-02 17:19:30 - progress_bar.py[line:272] - INFO: epoch 002:   2155 / 5589 loss=1.703, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=1060, nsentences=144, sample_size=1060, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=427.7, ups=0.4, wpb=1060, bsz=144, num_updates=7730, lr=2.30517e-05, gnorm=1.307, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19125
2023-08-02 17:19:55 - progress_bar.py[line:272] - INFO: epoch 002:   2165 / 5589 loss=1.709, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=1051.7, nsentences=144, sample_size=1051.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=425.1, ups=0.4, wpb=1051.7, bsz=144, num_updates=7740, lr=2.30815e-05, gnorm=1.402, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19149
2023-08-02 17:20:20 - progress_bar.py[line:272] - INFO: epoch 002:   2175 / 5589 loss=1.716, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=1054.7, nsentences=144, sample_size=1054.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=427, ups=0.4, wpb=1054.7, bsz=144, num_updates=7750, lr=2.31113e-05, gnorm=1.473, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19174
2023-08-02 17:20:44 - progress_bar.py[line:272] - INFO: epoch 002:   2185 / 5589 loss=1.699, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=1056.4, nsentences=144, sample_size=1056.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=427.7, ups=0.4, wpb=1056.4, bsz=144, num_updates=7760, lr=2.31412e-05, gnorm=1.341, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19199
2023-08-02 17:21:09 - progress_bar.py[line:272] - INFO: epoch 002:   2195 / 5589 loss=1.699, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=1055.2, nsentences=144, sample_size=1055.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=426.9, ups=0.4, wpb=1055.2, bsz=144, num_updates=7770, lr=2.3171e-05, gnorm=1.344, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19223
2023-08-02 17:21:34 - progress_bar.py[line:272] - INFO: epoch 002:   2205 / 5589 loss=1.708, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=1056.5, nsentences=144, sample_size=1056.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=428.3, ups=0.41, wpb=1056.5, bsz=144, num_updates=7780, lr=2.32008e-05, gnorm=1.372, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19248
2023-08-02 17:21:58 - progress_bar.py[line:272] - INFO: epoch 002:   2215 / 5589 loss=1.7, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=1055.4, nsentences=144, sample_size=1055.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=427.9, ups=0.41, wpb=1055.4, bsz=144, num_updates=7790, lr=2.32306e-05, gnorm=1.463, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19273
2023-08-02 17:22:23 - progress_bar.py[line:272] - INFO: epoch 002:   2225 / 5589 loss=1.7, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=1057, nsentences=144, sample_size=1057, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=428.6, ups=0.41, wpb=1057, bsz=144, num_updates=7800, lr=2.32604e-05, gnorm=1.442, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19297
2023-08-02 17:22:48 - progress_bar.py[line:272] - INFO: epoch 002:   2235 / 5589 loss=1.708, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=1057.8, nsentences=144, sample_size=1057.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=428.9, ups=0.41, wpb=1057.8, bsz=144, num_updates=7810, lr=2.32903e-05, gnorm=1.396, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19322
2023-08-02 17:23:12 - progress_bar.py[line:272] - INFO: epoch 002:   2245 / 5589 loss=1.688, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=1050.2, nsentences=144, sample_size=1050.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=425.7, ups=0.41, wpb=1050.2, bsz=144, num_updates=7820, lr=2.33201e-05, gnorm=1.364, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19347
2023-08-02 17:23:37 - progress_bar.py[line:272] - INFO: epoch 002:   2255 / 5589 loss=1.697, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=1058.8, nsentences=144, sample_size=1058.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=428.7, ups=0.4, wpb=1058.8, bsz=144, num_updates=7830, lr=2.33499e-05, gnorm=1.316, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19371
2023-08-02 17:24:02 - progress_bar.py[line:272] - INFO: epoch 002:   2265 / 5589 loss=1.703, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=1055.7, nsentences=144, sample_size=1055.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=427.1, ups=0.4, wpb=1055.7, bsz=144, num_updates=7840, lr=2.33797e-05, gnorm=1.414, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19396
2023-08-02 17:24:27 - progress_bar.py[line:272] - INFO: epoch 002:   2275 / 5589 loss=1.704, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=1055.6, nsentences=144, sample_size=1055.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=427.3, ups=0.4, wpb=1055.6, bsz=144, num_updates=7850, lr=2.34095e-05, gnorm=1.429, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19421
2023-08-02 17:24:51 - progress_bar.py[line:272] - INFO: epoch 002:   2285 / 5589 loss=1.698, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=1057.8, nsentences=144, sample_size=1057.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=427.9, ups=0.4, wpb=1057.8, bsz=144, num_updates=7860, lr=2.34394e-05, gnorm=1.378, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19446
2023-08-02 17:25:16 - progress_bar.py[line:272] - INFO: epoch 002:   2295 / 5589 loss=1.704, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=1056.5, nsentences=144, sample_size=1056.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=427.7, ups=0.4, wpb=1056.5, bsz=144, num_updates=7870, lr=2.34692e-05, gnorm=1.408, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19470
2023-08-02 17:25:41 - progress_bar.py[line:272] - INFO: epoch 002:   2305 / 5589 loss=1.695, loss_v1=0, loss_v2=0, nll_loss=0.39, ntokens=1056.9, nsentences=144, sample_size=1056.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=427.9, ups=0.4, wpb=1056.9, bsz=144, num_updates=7880, lr=2.3499e-05, gnorm=1.289, clip=90, loss_scale=128, train_wall=25, gb_free=6.6, wall=19495
2023-08-02 17:26:05 - progress_bar.py[line:272] - INFO: epoch 002:   2315 / 5589 loss=1.697, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=1053.5, nsentences=144, sample_size=1053.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=426.1, ups=0.4, wpb=1053.5, bsz=144, num_updates=7890, lr=2.35288e-05, gnorm=1.499, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19520
2023-08-02 17:26:30 - progress_bar.py[line:272] - INFO: epoch 002:   2325 / 5589 loss=1.703, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=1053.9, nsentences=144, sample_size=1053.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=426.6, ups=0.4, wpb=1053.9, bsz=144, num_updates=7900, lr=2.35586e-05, gnorm=1.494, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19544
2023-08-02 17:26:55 - progress_bar.py[line:272] - INFO: epoch 002:   2335 / 5589 loss=1.702, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=1051.9, nsentences=144, sample_size=1051.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=425.9, ups=0.4, wpb=1051.9, bsz=144, num_updates=7910, lr=2.35885e-05, gnorm=1.36, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19569
2023-08-02 17:27:20 - progress_bar.py[line:272] - INFO: epoch 002:   2345 / 5589 loss=1.7, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=1054.2, nsentences=144, sample_size=1054.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=426.6, ups=0.4, wpb=1054.2, bsz=144, num_updates=7920, lr=2.36183e-05, gnorm=1.458, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19594
2023-08-02 17:27:44 - progress_bar.py[line:272] - INFO: epoch 002:   2355 / 5589 loss=1.699, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=1054.2, nsentences=144, sample_size=1054.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=426.8, ups=0.4, wpb=1054.2, bsz=144, num_updates=7930, lr=2.36481e-05, gnorm=1.312, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19619
2023-08-02 17:28:09 - progress_bar.py[line:272] - INFO: epoch 002:   2365 / 5589 loss=1.7, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=1058.8, nsentences=144, sample_size=1058.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=427.9, ups=0.4, wpb=1058.8, bsz=144, num_updates=7940, lr=2.36779e-05, gnorm=1.367, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19643
2023-08-02 17:28:34 - progress_bar.py[line:272] - INFO: epoch 002:   2375 / 5589 loss=1.702, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=1053.9, nsentences=144, sample_size=1053.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=426.7, ups=0.4, wpb=1053.9, bsz=144, num_updates=7950, lr=2.37078e-05, gnorm=1.436, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19668
2023-08-02 17:28:58 - progress_bar.py[line:272] - INFO: epoch 002:   2385 / 5589 loss=1.684, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=1058.6, nsentences=144, sample_size=1058.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=428.6, ups=0.4, wpb=1058.6, bsz=144, num_updates=7960, lr=2.37376e-05, gnorm=1.341, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19693
2023-08-02 17:29:23 - progress_bar.py[line:272] - INFO: epoch 002:   2395 / 5589 loss=1.714, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=1058.9, nsentences=144, sample_size=1058.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=428.8, ups=0.4, wpb=1058.9, bsz=144, num_updates=7970, lr=2.37674e-05, gnorm=1.472, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19717
2023-08-02 17:29:48 - progress_bar.py[line:272] - INFO: epoch 002:   2405 / 5589 loss=1.705, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=1056, nsentences=144, sample_size=1056, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=427.3, ups=0.4, wpb=1056, bsz=144, num_updates=7980, lr=2.37972e-05, gnorm=1.365, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19742
2023-08-02 17:30:13 - progress_bar.py[line:272] - INFO: epoch 002:   2415 / 5589 loss=1.696, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=1056.7, nsentences=144, sample_size=1056.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=427.7, ups=0.4, wpb=1056.7, bsz=144, num_updates=7990, lr=2.3827e-05, gnorm=1.362, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19767
2023-08-02 17:30:37 - progress_bar.py[line:272] - INFO: epoch 002:   2425 / 5589 loss=1.701, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=1056.3, nsentences=144, sample_size=1056.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=428.1, ups=0.41, wpb=1056.3, bsz=144, num_updates=8000, lr=2.38569e-05, gnorm=1.398, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19791
2023-08-02 17:31:02 - progress_bar.py[line:272] - INFO: epoch 002:   2435 / 5589 loss=1.691, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=1054.2, nsentences=144, sample_size=1054.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=427.6, ups=0.41, wpb=1054.2, bsz=144, num_updates=8010, lr=2.38867e-05, gnorm=1.357, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19816
2023-08-02 17:31:27 - progress_bar.py[line:272] - INFO: epoch 002:   2445 / 5589 loss=1.697, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=1052.9, nsentences=144, sample_size=1052.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=426.3, ups=0.4, wpb=1052.9, bsz=144, num_updates=8020, lr=2.39165e-05, gnorm=1.383, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19841
2023-08-02 17:31:51 - progress_bar.py[line:272] - INFO: epoch 002:   2455 / 5589 loss=1.701, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=1058.1, nsentences=144, sample_size=1058.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=428.6, ups=0.41, wpb=1058.1, bsz=144, num_updates=8030, lr=2.39463e-05, gnorm=1.465, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19866
2023-08-02 17:32:16 - progress_bar.py[line:272] - INFO: epoch 002:   2465 / 5589 loss=1.691, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=1059.1, nsentences=144, sample_size=1059.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=429.1, ups=0.41, wpb=1059.1, bsz=144, num_updates=8040, lr=2.39761e-05, gnorm=1.403, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19890
2023-08-02 17:32:41 - progress_bar.py[line:272] - INFO: epoch 002:   2475 / 5589 loss=1.7, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=1052.3, nsentences=144, sample_size=1052.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=426.1, ups=0.4, wpb=1052.3, bsz=144, num_updates=8050, lr=2.4006e-05, gnorm=1.36, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19915
2023-08-02 17:33:05 - progress_bar.py[line:272] - INFO: epoch 002:   2485 / 5589 loss=1.7, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=1051.8, nsentences=144, sample_size=1051.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=425.9, ups=0.4, wpb=1051.8, bsz=144, num_updates=8060, lr=2.40358e-05, gnorm=1.406, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19940
2023-08-02 17:33:30 - progress_bar.py[line:272] - INFO: epoch 002:   2495 / 5589 loss=1.7, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=1054.7, nsentences=144, sample_size=1054.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=426.9, ups=0.4, wpb=1054.7, bsz=144, num_updates=8070, lr=2.40656e-05, gnorm=1.385, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19964
2023-08-02 17:33:55 - progress_bar.py[line:272] - INFO: epoch 002:   2505 / 5589 loss=1.702, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=1057.9, nsentences=144, sample_size=1057.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=429, ups=0.41, wpb=1057.9, bsz=144, num_updates=8080, lr=2.40954e-05, gnorm=1.357, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=19989
2023-08-02 17:34:19 - progress_bar.py[line:272] - INFO: epoch 002:   2515 / 5589 loss=1.688, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=1058.3, nsentences=144, sample_size=1058.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=429.2, ups=0.41, wpb=1058.3, bsz=144, num_updates=8090, lr=2.41252e-05, gnorm=1.322, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20014
2023-08-02 17:34:44 - progress_bar.py[line:272] - INFO: epoch 002:   2525 / 5589 loss=1.702, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=1056.2, nsentences=144, sample_size=1056.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=428.6, ups=0.41, wpb=1056.2, bsz=144, num_updates=8100, lr=2.41551e-05, gnorm=1.49, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20038
2023-08-02 17:35:09 - progress_bar.py[line:272] - INFO: epoch 002:   2535 / 5589 loss=1.705, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=1055.6, nsentences=144, sample_size=1055.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=428.3, ups=0.41, wpb=1055.6, bsz=144, num_updates=8110, lr=2.41849e-05, gnorm=1.441, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=20063
2023-08-02 17:35:33 - progress_bar.py[line:272] - INFO: epoch 002:   2545 / 5589 loss=1.696, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=1057.4, nsentences=144, sample_size=1057.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=429, ups=0.41, wpb=1057.4, bsz=144, num_updates=8120, lr=2.42147e-05, gnorm=1.421, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=20088
2023-08-02 17:35:58 - progress_bar.py[line:272] - INFO: epoch 002:   2555 / 5589 loss=1.69, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=1053.7, nsentences=144, sample_size=1053.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=427.4, ups=0.41, wpb=1053.7, bsz=144, num_updates=8130, lr=2.42445e-05, gnorm=1.269, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=20112
2023-08-02 17:36:23 - progress_bar.py[line:272] - INFO: epoch 002:   2565 / 5589 loss=1.688, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=1053.6, nsentences=144, sample_size=1053.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=427.2, ups=0.41, wpb=1053.6, bsz=144, num_updates=8140, lr=2.42744e-05, gnorm=1.294, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=20137
2023-08-02 17:36:47 - progress_bar.py[line:272] - INFO: epoch 002:   2575 / 5589 loss=1.688, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=1059.8, nsentences=144, sample_size=1059.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=429.9, ups=0.41, wpb=1059.8, bsz=144, num_updates=8150, lr=2.43042e-05, gnorm=1.325, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=20162
2023-08-02 17:37:12 - progress_bar.py[line:272] - INFO: epoch 002:   2585 / 5589 loss=1.697, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=1054.1, nsentences=144, sample_size=1054.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=427.7, ups=0.41, wpb=1054.1, bsz=144, num_updates=8160, lr=2.4334e-05, gnorm=1.527, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=20186
2023-08-02 17:37:19 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-08-02 17:37:39 - progress_bar.py[line:272] - INFO: epoch 002:   2596 / 5589 loss=1.696, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=1063.5, nsentences=144, sample_size=1063.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=392.6, ups=0.37, wpb=1063.5, bsz=144, num_updates=8170, lr=2.43638e-05, gnorm=1.322, clip=100, loss_scale=128, train_wall=27, gb_free=6.6, wall=20213
2023-08-02 17:38:04 - progress_bar.py[line:272] - INFO: epoch 002:   2606 / 5589 loss=1.699, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=1056.2, nsentences=144, sample_size=1056.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=428.4, ups=0.41, wpb=1056.2, bsz=144, num_updates=8180, lr=2.43936e-05, gnorm=1.378, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20238
2023-08-02 17:38:28 - progress_bar.py[line:272] - INFO: epoch 002:   2616 / 5589 loss=1.687, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=1053.7, nsentences=144, sample_size=1053.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=427.4, ups=0.41, wpb=1053.7, bsz=144, num_updates=8190, lr=2.44235e-05, gnorm=1.254, clip=90, loss_scale=128, train_wall=25, gb_free=6.6, wall=20263
2023-08-02 17:38:53 - progress_bar.py[line:272] - INFO: epoch 002:   2626 / 5589 loss=1.703, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=1058.6, nsentences=144, sample_size=1058.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=429.1, ups=0.41, wpb=1058.6, bsz=144, num_updates=8200, lr=2.44533e-05, gnorm=1.351, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20287
2023-08-02 17:39:18 - progress_bar.py[line:272] - INFO: epoch 002:   2636 / 5589 loss=1.698, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=1058.3, nsentences=144, sample_size=1058.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=429.4, ups=0.41, wpb=1058.3, bsz=144, num_updates=8210, lr=2.44831e-05, gnorm=1.284, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20312
2023-08-02 17:39:42 - progress_bar.py[line:272] - INFO: epoch 002:   2646 / 5589 loss=1.707, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=1058.6, nsentences=144, sample_size=1058.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=429.3, ups=0.41, wpb=1058.6, bsz=144, num_updates=8220, lr=2.45129e-05, gnorm=1.339, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20337
2023-08-02 17:40:07 - progress_bar.py[line:272] - INFO: epoch 002:   2656 / 5589 loss=1.7, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=1056.8, nsentences=144, sample_size=1056.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=428.7, ups=0.41, wpb=1056.8, bsz=144, num_updates=8230, lr=2.45427e-05, gnorm=1.463, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20361
2023-08-02 17:40:32 - progress_bar.py[line:272] - INFO: epoch 002:   2666 / 5589 loss=1.702, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=1056.4, nsentences=144, sample_size=1056.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=428.6, ups=0.41, wpb=1056.4, bsz=144, num_updates=8240, lr=2.45726e-05, gnorm=1.406, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20386
2023-08-02 17:40:56 - progress_bar.py[line:272] - INFO: epoch 002:   2676 / 5589 loss=1.696, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=1057, nsentences=144, sample_size=1057, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=428.6, ups=0.41, wpb=1057, bsz=144, num_updates=8250, lr=2.46024e-05, gnorm=1.376, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20411
2023-08-02 17:41:21 - progress_bar.py[line:272] - INFO: epoch 002:   2686 / 5589 loss=1.692, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=1058.3, nsentences=144, sample_size=1058.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=429.6, ups=0.41, wpb=1058.3, bsz=144, num_updates=8260, lr=2.46322e-05, gnorm=1.349, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20435
2023-08-02 17:41:46 - progress_bar.py[line:272] - INFO: epoch 002:   2696 / 5589 loss=1.687, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=1057.2, nsentences=144, sample_size=1057.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=429.2, ups=0.41, wpb=1057.2, bsz=144, num_updates=8270, lr=2.4662e-05, gnorm=1.334, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20460
2023-08-02 17:42:10 - progress_bar.py[line:272] - INFO: epoch 002:   2706 / 5589 loss=1.693, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=1057, nsentences=144, sample_size=1057, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=429, ups=0.41, wpb=1057, bsz=144, num_updates=8280, lr=2.46918e-05, gnorm=1.462, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20484
2023-08-02 17:42:35 - progress_bar.py[line:272] - INFO: epoch 002:   2716 / 5589 loss=1.695, loss_v1=0, loss_v2=0, nll_loss=0.39, ntokens=1053.8, nsentences=144, sample_size=1053.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=427.2, ups=0.41, wpb=1053.8, bsz=144, num_updates=8290, lr=2.47217e-05, gnorm=1.377, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20509
2023-08-02 17:43:00 - progress_bar.py[line:272] - INFO: epoch 002:   2726 / 5589 loss=1.694, loss_v1=0, loss_v2=0, nll_loss=0.39, ntokens=1053.4, nsentences=144, sample_size=1053.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=426.6, ups=0.4, wpb=1053.4, bsz=144, num_updates=8300, lr=2.47515e-05, gnorm=1.305, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20534
2023-08-02 17:43:24 - progress_bar.py[line:272] - INFO: epoch 002:   2736 / 5589 loss=1.686, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=1057.2, nsentences=144, sample_size=1057.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=428.8, ups=0.41, wpb=1057.2, bsz=144, num_updates=8310, lr=2.47813e-05, gnorm=1.307, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20559
2023-08-02 17:43:49 - progress_bar.py[line:272] - INFO: epoch 002:   2746 / 5589 loss=1.684, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=1056.4, nsentences=144, sample_size=1056.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=428.6, ups=0.41, wpb=1056.4, bsz=144, num_updates=8320, lr=2.48111e-05, gnorm=1.368, clip=90, loss_scale=128, train_wall=25, gb_free=6.6, wall=20583
2023-08-02 17:44:14 - progress_bar.py[line:272] - INFO: epoch 002:   2756 / 5589 loss=1.689, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=1057.5, nsentences=144, sample_size=1057.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=428.7, ups=0.41, wpb=1057.5, bsz=144, num_updates=8330, lr=2.4841e-05, gnorm=1.511, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20608
2023-08-02 17:44:38 - progress_bar.py[line:272] - INFO: epoch 002:   2766 / 5589 loss=1.71, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=1056.6, nsentences=144, sample_size=1056.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=428.5, ups=0.41, wpb=1056.6, bsz=144, num_updates=8340, lr=2.48708e-05, gnorm=1.507, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20633
2023-08-02 17:45:03 - progress_bar.py[line:272] - INFO: epoch 002:   2776 / 5589 loss=1.687, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=1054.8, nsentences=144, sample_size=1054.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=428, ups=0.41, wpb=1054.8, bsz=144, num_updates=8350, lr=2.49006e-05, gnorm=1.286, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20657
2023-08-02 17:45:28 - progress_bar.py[line:272] - INFO: epoch 002:   2786 / 5589 loss=1.695, loss_v1=0, loss_v2=0, nll_loss=0.39, ntokens=1057.1, nsentences=144, sample_size=1057.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=428.9, ups=0.41, wpb=1057.1, bsz=144, num_updates=8360, lr=2.49304e-05, gnorm=1.355, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20682
2023-08-02 17:45:52 - progress_bar.py[line:272] - INFO: epoch 002:   2796 / 5589 loss=1.691, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=1061.1, nsentences=144, sample_size=1061.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=430.6, ups=0.41, wpb=1061.1, bsz=144, num_updates=8370, lr=2.49602e-05, gnorm=1.308, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20706
2023-08-02 17:46:17 - progress_bar.py[line:272] - INFO: epoch 002:   2806 / 5589 loss=1.696, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=1054.7, nsentences=144, sample_size=1054.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=428, ups=0.41, wpb=1054.7, bsz=144, num_updates=8380, lr=2.49901e-05, gnorm=1.418, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20731
2023-08-02 17:46:41 - progress_bar.py[line:272] - INFO: epoch 002:   2816 / 5589 loss=1.698, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=1054.3, nsentences=144, sample_size=1054.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=427.7, ups=0.41, wpb=1054.3, bsz=144, num_updates=8390, lr=2.50199e-05, gnorm=1.378, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20756
2023-08-02 17:47:06 - progress_bar.py[line:272] - INFO: epoch 002:   2826 / 5589 loss=1.687, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=1057.9, nsentences=144, sample_size=1057.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=429.2, ups=0.41, wpb=1057.9, bsz=144, num_updates=8400, lr=2.50497e-05, gnorm=1.35, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20780
2023-08-02 17:47:31 - progress_bar.py[line:272] - INFO: epoch 002:   2836 / 5589 loss=1.697, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=1056.1, nsentences=144, sample_size=1056.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=428.7, ups=0.41, wpb=1056.1, bsz=144, num_updates=8410, lr=2.50795e-05, gnorm=1.44, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20805
2023-08-02 17:47:55 - progress_bar.py[line:272] - INFO: epoch 002:   2846 / 5589 loss=1.687, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=1058.9, nsentences=144, sample_size=1058.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=429.6, ups=0.41, wpb=1058.9, bsz=144, num_updates=8420, lr=2.51093e-05, gnorm=1.302, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20830
2023-08-02 17:48:20 - progress_bar.py[line:272] - INFO: epoch 002:   2856 / 5589 loss=1.698, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=1056.4, nsentences=144, sample_size=1056.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=427.9, ups=0.41, wpb=1056.4, bsz=144, num_updates=8430, lr=2.51392e-05, gnorm=1.402, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20854
2023-08-02 17:48:45 - progress_bar.py[line:272] - INFO: epoch 002:   2866 / 5589 loss=1.688, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=1055, nsentences=144, sample_size=1055, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=425.7, ups=0.4, wpb=1055, bsz=144, num_updates=8440, lr=2.5169e-05, gnorm=1.398, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20879
2023-08-02 17:49:10 - progress_bar.py[line:272] - INFO: epoch 002:   2876 / 5589 loss=1.685, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=1054.4, nsentences=144, sample_size=1054.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=426.9, ups=0.4, wpb=1054.4, bsz=144, num_updates=8450, lr=2.51988e-05, gnorm=1.365, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20904
2023-08-02 17:49:34 - progress_bar.py[line:272] - INFO: epoch 002:   2886 / 5589 loss=1.687, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=1053.5, nsentences=144, sample_size=1053.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=426.2, ups=0.4, wpb=1053.5, bsz=144, num_updates=8460, lr=2.52286e-05, gnorm=1.484, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20929
2023-08-02 17:49:59 - progress_bar.py[line:272] - INFO: epoch 002:   2896 / 5589 loss=1.704, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=1054.6, nsentences=144, sample_size=1054.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=427, ups=0.4, wpb=1054.6, bsz=144, num_updates=8470, lr=2.52584e-05, gnorm=1.5, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20953
2023-08-02 17:50:24 - progress_bar.py[line:272] - INFO: epoch 002:   2906 / 5589 loss=1.681, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=1057.6, nsentences=144, sample_size=1057.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=428.2, ups=0.4, wpb=1057.6, bsz=144, num_updates=8480, lr=2.52883e-05, gnorm=1.421, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=20978
2023-08-02 17:50:48 - progress_bar.py[line:272] - INFO: epoch 002:   2916 / 5589 loss=1.687, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=1058.5, nsentences=144, sample_size=1058.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=428.4, ups=0.4, wpb=1058.5, bsz=144, num_updates=8490, lr=2.53181e-05, gnorm=1.417, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21003
2023-08-02 17:51:13 - progress_bar.py[line:272] - INFO: epoch 002:   2926 / 5589 loss=1.698, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=1053.5, nsentences=144, sample_size=1053.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=426.6, ups=0.4, wpb=1053.5, bsz=144, num_updates=8500, lr=2.53479e-05, gnorm=1.412, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21027
2023-08-02 17:51:38 - progress_bar.py[line:272] - INFO: epoch 002:   2936 / 5589 loss=1.679, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=1055.8, nsentences=144, sample_size=1055.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=427.6, ups=0.4, wpb=1055.8, bsz=144, num_updates=8510, lr=2.53777e-05, gnorm=1.348, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21052
2023-08-02 17:52:03 - progress_bar.py[line:272] - INFO: epoch 002:   2946 / 5589 loss=1.685, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=1058.9, nsentences=144, sample_size=1058.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=428.8, ups=0.4, wpb=1058.9, bsz=144, num_updates=8520, lr=2.54076e-05, gnorm=1.339, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21077
2023-08-02 17:52:27 - progress_bar.py[line:272] - INFO: epoch 002:   2956 / 5589 loss=1.7, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=1055.9, nsentences=144, sample_size=1055.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=427.7, ups=0.41, wpb=1055.9, bsz=144, num_updates=8530, lr=2.54374e-05, gnorm=1.352, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21101
2023-08-02 17:52:52 - progress_bar.py[line:272] - INFO: epoch 002:   2966 / 5589 loss=1.693, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=1054.2, nsentences=144, sample_size=1054.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=427, ups=0.41, wpb=1054.2, bsz=144, num_updates=8540, lr=2.54672e-05, gnorm=1.418, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21126
2023-08-02 17:53:17 - progress_bar.py[line:272] - INFO: epoch 002:   2976 / 5589 loss=1.697, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=1057.6, nsentences=144, sample_size=1057.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=428.7, ups=0.41, wpb=1057.6, bsz=144, num_updates=8550, lr=2.5497e-05, gnorm=1.491, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21151
2023-08-02 17:53:41 - progress_bar.py[line:272] - INFO: epoch 002:   2986 / 5589 loss=1.693, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=1055.9, nsentences=144, sample_size=1055.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=428, ups=0.41, wpb=1055.9, bsz=144, num_updates=8560, lr=2.55268e-05, gnorm=1.321, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21176
2023-08-02 17:54:06 - progress_bar.py[line:272] - INFO: epoch 002:   2996 / 5589 loss=1.688, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=1057.4, nsentences=144, sample_size=1057.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=428.3, ups=0.41, wpb=1057.4, bsz=144, num_updates=8570, lr=2.55567e-05, gnorm=1.339, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21200
2023-08-02 17:54:31 - progress_bar.py[line:272] - INFO: epoch 002:   3006 / 5589 loss=1.69, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=1060.1, nsentences=144, sample_size=1060.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=429.4, ups=0.41, wpb=1060.1, bsz=144, num_updates=8580, lr=2.55865e-05, gnorm=1.322, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21225
2023-08-02 17:54:55 - progress_bar.py[line:272] - INFO: epoch 002:   3016 / 5589 loss=1.681, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=1056.7, nsentences=144, sample_size=1056.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=427.9, ups=0.4, wpb=1056.7, bsz=144, num_updates=8590, lr=2.56163e-05, gnorm=1.378, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21250
2023-08-02 17:55:20 - progress_bar.py[line:272] - INFO: epoch 002:   3026 / 5589 loss=1.683, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=1058.4, nsentences=144, sample_size=1058.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=428.4, ups=0.4, wpb=1058.4, bsz=144, num_updates=8600, lr=2.56461e-05, gnorm=1.329, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21274
2023-08-02 17:55:45 - progress_bar.py[line:272] - INFO: epoch 002:   3036 / 5589 loss=1.697, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=1054.9, nsentences=144, sample_size=1054.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=427.2, ups=0.4, wpb=1054.9, bsz=144, num_updates=8610, lr=2.56759e-05, gnorm=1.33, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21299
2023-08-02 17:56:09 - progress_bar.py[line:272] - INFO: epoch 002:   3046 / 5589 loss=1.684, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=1056.1, nsentences=144, sample_size=1056.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=427.8, ups=0.41, wpb=1056.1, bsz=144, num_updates=8620, lr=2.57058e-05, gnorm=1.35, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21324
2023-08-02 17:56:34 - progress_bar.py[line:272] - INFO: epoch 002:   3056 / 5589 loss=1.697, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=1056.1, nsentences=144, sample_size=1056.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=427.7, ups=0.4, wpb=1056.1, bsz=144, num_updates=8630, lr=2.57356e-05, gnorm=1.408, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21348
2023-08-02 17:56:59 - progress_bar.py[line:272] - INFO: epoch 002:   3066 / 5589 loss=1.701, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=1057.9, nsentences=144, sample_size=1057.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=428.4, ups=0.4, wpb=1057.9, bsz=144, num_updates=8640, lr=2.57654e-05, gnorm=1.416, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21373
2023-08-02 17:57:24 - progress_bar.py[line:272] - INFO: epoch 002:   3076 / 5589 loss=1.694, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=1053.1, nsentences=144, sample_size=1053.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=426.4, ups=0.4, wpb=1053.1, bsz=144, num_updates=8650, lr=2.57952e-05, gnorm=1.438, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21398
2023-08-02 17:57:48 - progress_bar.py[line:272] - INFO: epoch 002:   3086 / 5589 loss=1.684, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=1058.5, nsentences=144, sample_size=1058.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=428.4, ups=0.4, wpb=1058.5, bsz=144, num_updates=8660, lr=2.5825e-05, gnorm=1.293, clip=90, loss_scale=128, train_wall=25, gb_free=6.6, wall=21423
2023-08-02 17:58:13 - progress_bar.py[line:272] - INFO: epoch 002:   3096 / 5589 loss=1.683, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=1053.4, nsentences=144, sample_size=1053.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=426.4, ups=0.4, wpb=1053.4, bsz=144, num_updates=8670, lr=2.58549e-05, gnorm=1.411, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21447
2023-08-02 17:58:38 - progress_bar.py[line:272] - INFO: epoch 002:   3106 / 5589 loss=1.697, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=1053.1, nsentences=144, sample_size=1053.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=426.4, ups=0.4, wpb=1053.1, bsz=144, num_updates=8680, lr=2.58847e-05, gnorm=1.341, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=21472
2023-08-02 17:59:02 - progress_bar.py[line:272] - INFO: epoch 002:   3116 / 5589 loss=1.683, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=1057.6, nsentences=144, sample_size=1057.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=427.9, ups=0.4, wpb=1057.6, bsz=144, num_updates=8690, lr=2.59145e-05, gnorm=1.354, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=21497
2023-08-02 17:59:27 - progress_bar.py[line:272] - INFO: epoch 002:   3126 / 5589 loss=1.699, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=1053.4, nsentences=144, sample_size=1053.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=426.2, ups=0.4, wpb=1053.4, bsz=144, num_updates=8700, lr=2.59443e-05, gnorm=1.468, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=21521
2023-08-02 17:59:52 - progress_bar.py[line:272] - INFO: epoch 002:   3136 / 5589 loss=1.678, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=1059.6, nsentences=144, sample_size=1059.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=428.7, ups=0.4, wpb=1059.6, bsz=144, num_updates=8710, lr=2.59742e-05, gnorm=1.391, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=21546
2023-08-02 18:00:17 - progress_bar.py[line:272] - INFO: epoch 002:   3146 / 5589 loss=1.681, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=1050.7, nsentences=144, sample_size=1050.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=425.3, ups=0.4, wpb=1050.7, bsz=144, num_updates=8720, lr=2.6004e-05, gnorm=1.381, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=21571
2023-08-02 18:00:41 - progress_bar.py[line:272] - INFO: epoch 002:   3156 / 5589 loss=1.68, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=1054.6, nsentences=144, sample_size=1054.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=428, ups=0.41, wpb=1054.6, bsz=144, num_updates=8730, lr=2.60338e-05, gnorm=1.373, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=21595
2023-08-02 18:00:53 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-08-02 18:01:08 - progress_bar.py[line:272] - INFO: epoch 002:   3167 / 5589 loss=1.692, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=1054.6, nsentences=144, sample_size=1054.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=389.5, ups=0.37, wpb=1054.6, bsz=144, num_updates=8740, lr=2.60636e-05, gnorm=1.449, clip=100, loss_scale=128, train_wall=27, gb_free=6.6, wall=21622
2023-08-02 18:01:33 - progress_bar.py[line:272] - INFO: epoch 002:   3177 / 5589 loss=1.69, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=1053.4, nsentences=144, sample_size=1053.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=426.7, ups=0.41, wpb=1053.4, bsz=144, num_updates=8750, lr=2.60934e-05, gnorm=1.444, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21647
2023-08-02 18:01:58 - progress_bar.py[line:272] - INFO: epoch 002:   3187 / 5589 loss=1.684, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=1053.3, nsentences=144, sample_size=1053.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=426.5, ups=0.4, wpb=1053.3, bsz=144, num_updates=8760, lr=2.61233e-05, gnorm=1.318, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21672
2023-08-02 18:02:22 - progress_bar.py[line:272] - INFO: epoch 002:   3197 / 5589 loss=1.689, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=1053.9, nsentences=144, sample_size=1053.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=426.9, ups=0.41, wpb=1053.9, bsz=144, num_updates=8770, lr=2.61531e-05, gnorm=1.454, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21697
2023-08-02 18:02:47 - progress_bar.py[line:272] - INFO: epoch 002:   3207 / 5589 loss=1.706, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=1054.5, nsentences=144, sample_size=1054.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=426.9, ups=0.4, wpb=1054.5, bsz=144, num_updates=8780, lr=2.61829e-05, gnorm=1.359, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21721
2023-08-02 18:03:12 - progress_bar.py[line:272] - INFO: epoch 002:   3217 / 5589 loss=1.68, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=1053.6, nsentences=144, sample_size=1053.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=426.7, ups=0.4, wpb=1053.6, bsz=144, num_updates=8790, lr=2.62127e-05, gnorm=1.34, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21746
2023-08-02 18:03:36 - progress_bar.py[line:272] - INFO: epoch 002:   3227 / 5589 loss=1.696, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=1058.1, nsentences=144, sample_size=1058.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=428.7, ups=0.41, wpb=1058.1, bsz=144, num_updates=8800, lr=2.62425e-05, gnorm=1.394, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21771
2023-08-02 18:04:01 - progress_bar.py[line:272] - INFO: epoch 002:   3237 / 5589 loss=1.679, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=1054.1, nsentences=144, sample_size=1054.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=427.1, ups=0.41, wpb=1054.1, bsz=144, num_updates=8810, lr=2.62724e-05, gnorm=1.318, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21795
2023-08-02 18:04:26 - progress_bar.py[line:272] - INFO: epoch 002:   3247 / 5589 loss=1.679, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=1053.6, nsentences=144, sample_size=1053.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=426.8, ups=0.41, wpb=1053.6, bsz=144, num_updates=8820, lr=2.63022e-05, gnorm=1.335, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21820
2023-08-02 18:04:50 - progress_bar.py[line:272] - INFO: epoch 002:   3257 / 5589 loss=1.699, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=1050.8, nsentences=144, sample_size=1050.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=425.5, ups=0.4, wpb=1050.8, bsz=144, num_updates=8830, lr=2.6332e-05, gnorm=1.374, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21845
2023-08-02 18:05:15 - progress_bar.py[line:272] - INFO: epoch 002:   3267 / 5589 loss=1.682, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=1058.2, nsentences=144, sample_size=1058.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=428.4, ups=0.4, wpb=1058.2, bsz=144, num_updates=8840, lr=2.63618e-05, gnorm=1.499, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21869
2023-08-02 18:05:40 - progress_bar.py[line:272] - INFO: epoch 002:   3277 / 5589 loss=1.683, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=1058.9, nsentences=144, sample_size=1058.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=429.2, ups=0.41, wpb=1058.9, bsz=144, num_updates=8850, lr=2.63917e-05, gnorm=1.277, clip=90, loss_scale=128, train_wall=25, gb_free=6.6, wall=21894
2023-08-02 18:06:05 - progress_bar.py[line:272] - INFO: epoch 002:   3287 / 5589 loss=1.688, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=1056.4, nsentences=144, sample_size=1056.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=427.5, ups=0.4, wpb=1056.4, bsz=144, num_updates=8860, lr=2.64215e-05, gnorm=1.479, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21919
2023-08-02 18:06:29 - progress_bar.py[line:272] - INFO: epoch 002:   3297 / 5589 loss=1.677, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=1055.9, nsentences=144, sample_size=1055.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=427.4, ups=0.4, wpb=1055.9, bsz=144, num_updates=8870, lr=2.64513e-05, gnorm=1.336, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21944
2023-08-02 18:06:54 - progress_bar.py[line:272] - INFO: epoch 002:   3307 / 5589 loss=1.688, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=1060.5, nsentences=144, sample_size=1060.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=429.4, ups=0.4, wpb=1060.5, bsz=144, num_updates=8880, lr=2.64811e-05, gnorm=1.49, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21968
2023-08-02 18:07:19 - progress_bar.py[line:272] - INFO: epoch 002:   3317 / 5589 loss=1.686, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=1052.2, nsentences=144, sample_size=1052.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=426.2, ups=0.41, wpb=1052.2, bsz=144, num_updates=8890, lr=2.65109e-05, gnorm=1.561, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=21993
2023-08-02 18:07:43 - progress_bar.py[line:272] - INFO: epoch 002:   3327 / 5589 loss=1.678, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=1055.7, nsentences=144, sample_size=1055.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=427.3, ups=0.4, wpb=1055.7, bsz=144, num_updates=8900, lr=2.65408e-05, gnorm=1.375, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22018
2023-08-02 18:08:08 - progress_bar.py[line:272] - INFO: epoch 002:   3337 / 5589 loss=1.682, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=1053, nsentences=144, sample_size=1053, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=426.7, ups=0.41, wpb=1053, bsz=144, num_updates=8910, lr=2.65706e-05, gnorm=1.411, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22042
2023-08-02 18:08:33 - progress_bar.py[line:272] - INFO: epoch 002:   3347 / 5589 loss=1.686, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=1056.6, nsentences=144, sample_size=1056.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=428.4, ups=0.41, wpb=1056.6, bsz=144, num_updates=8920, lr=2.66004e-05, gnorm=1.601, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22067
2023-08-02 18:08:57 - progress_bar.py[line:272] - INFO: epoch 002:   3357 / 5589 loss=1.682, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=1055, nsentences=144, sample_size=1055, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=427.3, ups=0.4, wpb=1055, bsz=144, num_updates=8930, lr=2.66302e-05, gnorm=1.35, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22092
2023-08-02 18:09:22 - progress_bar.py[line:272] - INFO: epoch 002:   3367 / 5589 loss=1.677, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=1054.9, nsentences=144, sample_size=1054.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=427.7, ups=0.41, wpb=1054.9, bsz=144, num_updates=8940, lr=2.666e-05, gnorm=1.508, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22116
2023-08-02 18:09:47 - progress_bar.py[line:272] - INFO: epoch 002:   3377 / 5589 loss=1.673, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=1057.1, nsentences=144, sample_size=1057.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=428.4, ups=0.41, wpb=1057.1, bsz=144, num_updates=8950, lr=2.66899e-05, gnorm=1.399, clip=90, loss_scale=128, train_wall=25, gb_free=6.6, wall=22141
2023-08-02 18:10:11 - progress_bar.py[line:272] - INFO: epoch 002:   3387 / 5589 loss=1.682, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=1057.6, nsentences=144, sample_size=1057.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=428.4, ups=0.41, wpb=1057.6, bsz=144, num_updates=8960, lr=2.67197e-05, gnorm=1.298, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22166
2023-08-02 18:10:36 - progress_bar.py[line:272] - INFO: epoch 002:   3397 / 5589 loss=1.688, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=1056.8, nsentences=144, sample_size=1056.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=428.2, ups=0.41, wpb=1056.8, bsz=144, num_updates=8970, lr=2.67495e-05, gnorm=1.318, clip=90, loss_scale=128, train_wall=25, gb_free=6.6, wall=22190
2023-08-02 18:11:01 - progress_bar.py[line:272] - INFO: epoch 002:   3407 / 5589 loss=1.678, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=1052.4, nsentences=144, sample_size=1052.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=426.7, ups=0.41, wpb=1052.4, bsz=144, num_updates=8980, lr=2.67793e-05, gnorm=1.297, clip=90, loss_scale=128, train_wall=25, gb_free=6.6, wall=22215
2023-08-02 18:11:25 - progress_bar.py[line:272] - INFO: epoch 002:   3417 / 5589 loss=1.686, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=1052.9, nsentences=144, sample_size=1052.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=427.2, ups=0.41, wpb=1052.9, bsz=144, num_updates=8990, lr=2.68091e-05, gnorm=1.461, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22240
2023-08-02 18:11:50 - progress_bar.py[line:272] - INFO: epoch 002:   3427 / 5589 loss=1.682, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=1053.8, nsentences=144, sample_size=1053.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=427.7, ups=0.41, wpb=1053.8, bsz=144, num_updates=9000, lr=2.6839e-05, gnorm=1.462, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22264
2023-08-02 18:12:15 - progress_bar.py[line:272] - INFO: epoch 002:   3437 / 5589 loss=1.683, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=1057, nsentences=144, sample_size=1057, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=428.8, ups=0.41, wpb=1057, bsz=144, num_updates=9010, lr=2.68688e-05, gnorm=1.333, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22289
2023-08-02 18:12:39 - progress_bar.py[line:272] - INFO: epoch 002:   3447 / 5589 loss=1.677, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=1056.1, nsentences=144, sample_size=1056.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=428.5, ups=0.41, wpb=1056.1, bsz=144, num_updates=9020, lr=2.68986e-05, gnorm=1.388, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22314
2023-08-02 18:13:04 - progress_bar.py[line:272] - INFO: epoch 002:   3457 / 5589 loss=1.681, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=1052.9, nsentences=144, sample_size=1052.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=427.3, ups=0.41, wpb=1052.9, bsz=144, num_updates=9030, lr=2.69284e-05, gnorm=1.417, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22338
2023-08-02 18:13:29 - progress_bar.py[line:272] - INFO: epoch 002:   3467 / 5589 loss=1.682, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=1051.4, nsentences=144, sample_size=1051.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=426.6, ups=0.41, wpb=1051.4, bsz=144, num_updates=9040, lr=2.69583e-05, gnorm=1.411, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22363
2023-08-02 18:13:53 - progress_bar.py[line:272] - INFO: epoch 002:   3477 / 5589 loss=1.678, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=1054.7, nsentences=144, sample_size=1054.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=427.9, ups=0.41, wpb=1054.7, bsz=144, num_updates=9050, lr=2.69881e-05, gnorm=1.272, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22388
2023-08-02 18:14:18 - progress_bar.py[line:272] - INFO: epoch 002:   3487 / 5589 loss=1.688, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=1056.6, nsentences=144, sample_size=1056.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=428.5, ups=0.41, wpb=1056.6, bsz=144, num_updates=9060, lr=2.70179e-05, gnorm=1.488, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22412
2023-08-02 18:14:43 - progress_bar.py[line:272] - INFO: epoch 002:   3497 / 5589 loss=1.691, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=1055, nsentences=144, sample_size=1055, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=428.1, ups=0.41, wpb=1055, bsz=144, num_updates=9070, lr=2.70477e-05, gnorm=1.287, clip=90, loss_scale=128, train_wall=25, gb_free=6.6, wall=22437
2023-08-02 18:15:07 - progress_bar.py[line:272] - INFO: epoch 002:   3507 / 5589 loss=1.677, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=1056.1, nsentences=144, sample_size=1056.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=428.7, ups=0.41, wpb=1056.1, bsz=144, num_updates=9080, lr=2.70775e-05, gnorm=1.334, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22462
2023-08-02 18:15:32 - progress_bar.py[line:272] - INFO: epoch 002:   3517 / 5589 loss=1.687, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=1058.8, nsentences=144, sample_size=1058.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=429.9, ups=0.41, wpb=1058.8, bsz=144, num_updates=9090, lr=2.71074e-05, gnorm=1.386, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22486
2023-08-02 18:15:57 - progress_bar.py[line:272] - INFO: epoch 002:   3527 / 5589 loss=1.686, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=1054.9, nsentences=144, sample_size=1054.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=428.1, ups=0.41, wpb=1054.9, bsz=144, num_updates=9100, lr=2.71372e-05, gnorm=1.504, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22511
2023-08-02 18:16:21 - progress_bar.py[line:272] - INFO: epoch 002:   3537 / 5589 loss=1.686, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=1056.5, nsentences=144, sample_size=1056.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=428.7, ups=0.41, wpb=1056.5, bsz=144, num_updates=9110, lr=2.7167e-05, gnorm=1.506, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22535
2023-08-02 18:16:46 - progress_bar.py[line:272] - INFO: epoch 002:   3547 / 5589 loss=1.688, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=1059.4, nsentences=144, sample_size=1059.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=429.5, ups=0.41, wpb=1059.4, bsz=144, num_updates=9120, lr=2.71968e-05, gnorm=1.342, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22560
2023-08-02 18:17:11 - progress_bar.py[line:272] - INFO: epoch 002:   3557 / 5589 loss=1.682, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=1059.3, nsentences=144, sample_size=1059.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=429.6, ups=0.41, wpb=1059.3, bsz=144, num_updates=9130, lr=2.72266e-05, gnorm=1.381, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22585
2023-08-02 18:17:35 - progress_bar.py[line:272] - INFO: epoch 002:   3567 / 5589 loss=1.682, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=1061.8, nsentences=144, sample_size=1061.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=429.9, ups=0.4, wpb=1061.8, bsz=144, num_updates=9140, lr=2.72565e-05, gnorm=1.351, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22610
2023-08-02 18:18:00 - progress_bar.py[line:272] - INFO: epoch 002:   3577 / 5589 loss=1.69, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=1052.3, nsentences=144, sample_size=1052.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=426.3, ups=0.41, wpb=1052.3, bsz=144, num_updates=9150, lr=2.72863e-05, gnorm=1.402, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22634
2023-08-02 18:18:25 - progress_bar.py[line:272] - INFO: epoch 002:   3587 / 5589 loss=1.684, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=1057.8, nsentences=144, sample_size=1057.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=428.9, ups=0.41, wpb=1057.8, bsz=144, num_updates=9160, lr=2.73161e-05, gnorm=1.403, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22659
2023-08-02 18:18:49 - progress_bar.py[line:272] - INFO: epoch 002:   3597 / 5589 loss=1.692, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=1056.5, nsentences=144, sample_size=1056.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=428.5, ups=0.41, wpb=1056.5, bsz=144, num_updates=9170, lr=2.73459e-05, gnorm=1.318, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22684
2023-08-02 18:19:14 - progress_bar.py[line:272] - INFO: epoch 002:   3607 / 5589 loss=1.684, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=1052.7, nsentences=144, sample_size=1052.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=427.2, ups=0.41, wpb=1052.7, bsz=144, num_updates=9180, lr=2.73757e-05, gnorm=1.282, clip=90, loss_scale=128, train_wall=25, gb_free=6.6, wall=22708
2023-08-02 18:19:39 - progress_bar.py[line:272] - INFO: epoch 002:   3617 / 5589 loss=1.684, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=1059.3, nsentences=144, sample_size=1059.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=429.8, ups=0.41, wpb=1059.3, bsz=144, num_updates=9190, lr=2.74056e-05, gnorm=1.33, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22733
2023-08-02 18:20:03 - progress_bar.py[line:272] - INFO: epoch 002:   3627 / 5589 loss=1.68, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=1058.6, nsentences=144, sample_size=1058.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=429.5, ups=0.41, wpb=1058.6, bsz=144, num_updates=9200, lr=2.74354e-05, gnorm=1.321, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22757
2023-08-02 18:20:28 - progress_bar.py[line:272] - INFO: epoch 002:   3637 / 5589 loss=1.676, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=1055.5, nsentences=144, sample_size=1055.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=428.1, ups=0.41, wpb=1055.5, bsz=144, num_updates=9210, lr=2.74652e-05, gnorm=1.379, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22782
2023-08-02 18:20:53 - progress_bar.py[line:272] - INFO: epoch 002:   3647 / 5589 loss=1.677, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=1057.4, nsentences=144, sample_size=1057.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=428.2, ups=0.4, wpb=1057.4, bsz=144, num_updates=9220, lr=2.7495e-05, gnorm=1.394, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22807
2023-08-02 18:21:17 - progress_bar.py[line:272] - INFO: epoch 002:   3657 / 5589 loss=1.685, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=1054.8, nsentences=144, sample_size=1054.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=427, ups=0.4, wpb=1054.8, bsz=144, num_updates=9230, lr=2.75249e-05, gnorm=1.476, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22832
2023-08-02 18:21:42 - progress_bar.py[line:272] - INFO: epoch 002:   3667 / 5589 loss=1.682, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=1056.5, nsentences=144, sample_size=1056.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=428.2, ups=0.41, wpb=1056.5, bsz=144, num_updates=9240, lr=2.75547e-05, gnorm=1.486, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22856
2023-08-02 18:22:07 - progress_bar.py[line:272] - INFO: epoch 002:   3677 / 5589 loss=1.68, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=1055.4, nsentences=144, sample_size=1055.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=428, ups=0.41, wpb=1055.4, bsz=144, num_updates=9250, lr=2.75845e-05, gnorm=1.357, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=22881
2023-08-02 18:22:31 - progress_bar.py[line:272] - INFO: epoch 002:   3687 / 5589 loss=1.677, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=1052.3, nsentences=144, sample_size=1052.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=427.1, ups=0.41, wpb=1052.3, bsz=144, num_updates=9260, lr=2.76143e-05, gnorm=1.328, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=22906
2023-08-02 18:22:53 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-08-02 18:22:58 - progress_bar.py[line:272] - INFO: epoch 002:   3698 / 5589 loss=1.672, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=1050.1, nsentences=144, sample_size=1050.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=388.1, ups=0.37, wpb=1050.1, bsz=144, num_updates=9270, lr=2.76441e-05, gnorm=1.539, clip=100, loss_scale=128, train_wall=27, gb_free=6.6, wall=22933
2023-08-02 18:23:23 - progress_bar.py[line:272] - INFO: epoch 002:   3708 / 5589 loss=1.685, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=1054.6, nsentences=144, sample_size=1054.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=428.2, ups=0.41, wpb=1054.6, bsz=144, num_updates=9280, lr=2.7674e-05, gnorm=1.364, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22957
2023-08-02 18:23:48 - progress_bar.py[line:272] - INFO: epoch 002:   3718 / 5589 loss=1.681, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=1057.3, nsentences=144, sample_size=1057.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=429.3, ups=0.41, wpb=1057.3, bsz=144, num_updates=9290, lr=2.77038e-05, gnorm=1.355, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=22982
2023-08-02 18:24:12 - progress_bar.py[line:272] - INFO: epoch 002:   3728 / 5589 loss=1.673, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=1054.5, nsentences=144, sample_size=1054.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=427.7, ups=0.41, wpb=1054.5, bsz=144, num_updates=9300, lr=2.77336e-05, gnorm=1.437, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23007
2023-08-02 18:24:37 - progress_bar.py[line:272] - INFO: epoch 002:   3738 / 5589 loss=1.682, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=1053.2, nsentences=144, sample_size=1053.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=427.3, ups=0.41, wpb=1053.2, bsz=144, num_updates=9310, lr=2.77634e-05, gnorm=1.35, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23031
2023-08-02 18:25:02 - progress_bar.py[line:272] - INFO: epoch 002:   3748 / 5589 loss=1.67, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=1057.2, nsentences=144, sample_size=1057.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=428.5, ups=0.41, wpb=1057.2, bsz=144, num_updates=9320, lr=2.77932e-05, gnorm=1.421, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23056
2023-08-02 18:25:26 - progress_bar.py[line:272] - INFO: epoch 002:   3758 / 5589 loss=1.684, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=1055.7, nsentences=144, sample_size=1055.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=428.5, ups=0.41, wpb=1055.7, bsz=144, num_updates=9330, lr=2.78231e-05, gnorm=1.478, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23080
2023-08-02 18:25:51 - progress_bar.py[line:272] - INFO: epoch 002:   3768 / 5589 loss=1.673, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=1055.1, nsentences=144, sample_size=1055.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=428.5, ups=0.41, wpb=1055.1, bsz=144, num_updates=9340, lr=2.78529e-05, gnorm=1.444, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23105
2023-08-02 18:26:15 - progress_bar.py[line:272] - INFO: epoch 002:   3778 / 5589 loss=1.665, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=1054.8, nsentences=144, sample_size=1054.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=428.1, ups=0.41, wpb=1054.8, bsz=144, num_updates=9350, lr=2.78827e-05, gnorm=1.35, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23130
2023-08-02 18:26:40 - progress_bar.py[line:272] - INFO: epoch 002:   3788 / 5589 loss=1.672, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=1057.2, nsentences=144, sample_size=1057.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=429, ups=0.41, wpb=1057.2, bsz=144, num_updates=9360, lr=2.79125e-05, gnorm=1.394, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23154
2023-08-02 18:27:05 - progress_bar.py[line:272] - INFO: epoch 002:   3798 / 5589 loss=1.671, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=1051.8, nsentences=144, sample_size=1051.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=426.8, ups=0.41, wpb=1051.8, bsz=144, num_updates=9370, lr=2.79423e-05, gnorm=1.41, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23179
2023-08-02 18:27:29 - progress_bar.py[line:272] - INFO: epoch 002:   3808 / 5589 loss=1.68, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=1053.5, nsentences=144, sample_size=1053.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=427.6, ups=0.41, wpb=1053.5, bsz=144, num_updates=9380, lr=2.79722e-05, gnorm=1.388, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23204
2023-08-02 18:27:54 - progress_bar.py[line:272] - INFO: epoch 002:   3818 / 5589 loss=1.668, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=1052.9, nsentences=144, sample_size=1052.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=427.2, ups=0.41, wpb=1052.9, bsz=144, num_updates=9390, lr=2.8002e-05, gnorm=1.392, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23228
2023-08-02 18:28:19 - progress_bar.py[line:272] - INFO: epoch 002:   3828 / 5589 loss=1.676, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=1052.4, nsentences=144, sample_size=1052.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=426.8, ups=0.41, wpb=1052.4, bsz=144, num_updates=9400, lr=2.80318e-05, gnorm=1.313, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23253
2023-08-02 18:28:43 - progress_bar.py[line:272] - INFO: epoch 002:   3838 / 5589 loss=1.661, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=1055.9, nsentences=144, sample_size=1055.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=428.5, ups=0.41, wpb=1055.9, bsz=144, num_updates=9410, lr=2.80616e-05, gnorm=1.366, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23278
2023-08-02 18:29:08 - progress_bar.py[line:272] - INFO: epoch 002:   3848 / 5589 loss=1.668, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=1056.7, nsentences=144, sample_size=1056.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=428.6, ups=0.41, wpb=1056.7, bsz=144, num_updates=9420, lr=2.80915e-05, gnorm=1.449, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23302
2023-08-02 18:29:33 - progress_bar.py[line:272] - INFO: epoch 002:   3858 / 5589 loss=1.675, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=1054.3, nsentences=144, sample_size=1054.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=427.8, ups=0.41, wpb=1054.3, bsz=144, num_updates=9430, lr=2.81213e-05, gnorm=1.376, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23327
2023-08-02 18:29:57 - progress_bar.py[line:272] - INFO: epoch 002:   3868 / 5589 loss=1.669, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=1056.3, nsentences=144, sample_size=1056.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=428.8, ups=0.41, wpb=1056.3, bsz=144, num_updates=9440, lr=2.81511e-05, gnorm=1.339, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23352
2023-08-02 18:30:22 - progress_bar.py[line:272] - INFO: epoch 002:   3878 / 5589 loss=1.66, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=1057.5, nsentences=144, sample_size=1057.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=429.4, ups=0.41, wpb=1057.5, bsz=144, num_updates=9450, lr=2.81809e-05, gnorm=1.292, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23376
2023-08-02 18:30:47 - progress_bar.py[line:272] - INFO: epoch 002:   3888 / 5589 loss=1.677, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=1055.8, nsentences=144, sample_size=1055.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=428.6, ups=0.41, wpb=1055.8, bsz=144, num_updates=9460, lr=2.82107e-05, gnorm=1.403, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23401
2023-08-02 18:31:11 - progress_bar.py[line:272] - INFO: epoch 002:   3898 / 5589 loss=1.677, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=1059.7, nsentences=144, sample_size=1059.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=430, ups=0.41, wpb=1059.7, bsz=144, num_updates=9470, lr=2.82406e-05, gnorm=1.414, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23425
2023-08-02 18:31:36 - progress_bar.py[line:272] - INFO: epoch 002:   3908 / 5589 loss=1.677, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=1058.8, nsentences=144, sample_size=1058.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=429.6, ups=0.41, wpb=1058.8, bsz=144, num_updates=9480, lr=2.82704e-05, gnorm=1.374, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23450
2023-08-02 18:32:01 - progress_bar.py[line:272] - INFO: epoch 002:   3918 / 5589 loss=1.68, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=1054.9, nsentences=144, sample_size=1054.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=428, ups=0.41, wpb=1054.9, bsz=144, num_updates=9490, lr=2.83002e-05, gnorm=1.399, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23475
2023-08-02 18:32:25 - progress_bar.py[line:272] - INFO: epoch 002:   3928 / 5589 loss=1.675, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=1056.5, nsentences=144, sample_size=1056.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=428.7, ups=0.41, wpb=1056.5, bsz=144, num_updates=9500, lr=2.833e-05, gnorm=1.478, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23499
2023-08-02 18:32:50 - progress_bar.py[line:272] - INFO: epoch 002:   3938 / 5589 loss=1.672, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=1054.7, nsentences=144, sample_size=1054.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=428, ups=0.41, wpb=1054.7, bsz=144, num_updates=9510, lr=2.83598e-05, gnorm=1.428, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23524
2023-08-02 18:33:14 - progress_bar.py[line:272] - INFO: epoch 002:   3948 / 5589 loss=1.675, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=1056.1, nsentences=144, sample_size=1056.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=428.5, ups=0.41, wpb=1056.1, bsz=144, num_updates=9520, lr=2.83897e-05, gnorm=1.368, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23549
2023-08-02 18:33:39 - progress_bar.py[line:272] - INFO: epoch 002:   3958 / 5589 loss=1.668, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=1055.9, nsentences=144, sample_size=1055.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=428.3, ups=0.41, wpb=1055.9, bsz=144, num_updates=9530, lr=2.84195e-05, gnorm=1.32, clip=90, loss_scale=128, train_wall=25, gb_free=6.6, wall=23573
2023-08-02 18:34:04 - progress_bar.py[line:272] - INFO: epoch 002:   3968 / 5589 loss=1.674, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=1059.1, nsentences=144, sample_size=1059.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=429.3, ups=0.41, wpb=1059.1, bsz=144, num_updates=9540, lr=2.84493e-05, gnorm=1.307, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23598
2023-08-02 18:34:28 - progress_bar.py[line:272] - INFO: epoch 002:   3978 / 5589 loss=1.66, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=1056.9, nsentences=144, sample_size=1056.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=428.6, ups=0.41, wpb=1056.9, bsz=144, num_updates=9550, lr=2.84791e-05, gnorm=1.323, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23623
2023-08-02 18:34:53 - progress_bar.py[line:272] - INFO: epoch 002:   3988 / 5589 loss=1.676, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=1051.7, nsentences=144, sample_size=1051.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=426.6, ups=0.41, wpb=1051.7, bsz=144, num_updates=9560, lr=2.85089e-05, gnorm=1.352, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23647
2023-08-02 18:35:18 - progress_bar.py[line:272] - INFO: epoch 002:   3998 / 5589 loss=1.672, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=1054.6, nsentences=144, sample_size=1054.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=427.9, ups=0.41, wpb=1054.6, bsz=144, num_updates=9570, lr=2.85388e-05, gnorm=1.333, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23672
2023-08-02 18:35:42 - progress_bar.py[line:272] - INFO: epoch 002:   4008 / 5589 loss=1.671, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=1049.8, nsentences=144, sample_size=1049.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=426.3, ups=0.41, wpb=1049.8, bsz=144, num_updates=9580, lr=2.85686e-05, gnorm=1.472, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23697
2023-08-02 18:36:07 - progress_bar.py[line:272] - INFO: epoch 002:   4018 / 5589 loss=1.662, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=1055.1, nsentences=144, sample_size=1055.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=428.4, ups=0.41, wpb=1055.1, bsz=144, num_updates=9590, lr=2.85984e-05, gnorm=1.382, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23721
2023-08-02 18:36:32 - progress_bar.py[line:272] - INFO: epoch 002:   4028 / 5589 loss=1.676, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=1058.7, nsentences=144, sample_size=1058.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=429.8, ups=0.41, wpb=1058.7, bsz=144, num_updates=9600, lr=2.86282e-05, gnorm=1.382, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23746
2023-08-02 18:36:56 - progress_bar.py[line:272] - INFO: epoch 002:   4038 / 5589 loss=1.681, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=1054.3, nsentences=144, sample_size=1054.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=428.1, ups=0.41, wpb=1054.3, bsz=144, num_updates=9610, lr=2.86581e-05, gnorm=1.355, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23771
2023-08-02 18:37:21 - progress_bar.py[line:272] - INFO: epoch 002:   4048 / 5589 loss=1.678, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=1056.9, nsentences=144, sample_size=1056.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=429.1, ups=0.41, wpb=1056.9, bsz=144, num_updates=9620, lr=2.86879e-05, gnorm=1.603, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23795
2023-08-02 18:37:46 - progress_bar.py[line:272] - INFO: epoch 002:   4058 / 5589 loss=1.686, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=1051, nsentences=144, sample_size=1051, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=426.1, ups=0.41, wpb=1051, bsz=144, num_updates=9630, lr=2.87177e-05, gnorm=1.387, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23820
2023-08-02 18:38:10 - progress_bar.py[line:272] - INFO: epoch 002:   4068 / 5589 loss=1.678, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=1053.6, nsentences=144, sample_size=1053.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=426.9, ups=0.41, wpb=1053.6, bsz=144, num_updates=9640, lr=2.87475e-05, gnorm=1.39, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23845
2023-08-02 18:38:35 - progress_bar.py[line:272] - INFO: epoch 002:   4078 / 5589 loss=1.663, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=1056.1, nsentences=144, sample_size=1056.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=427.6, ups=0.4, wpb=1056.1, bsz=144, num_updates=9650, lr=2.87773e-05, gnorm=1.359, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23869
2023-08-02 18:39:00 - progress_bar.py[line:272] - INFO: epoch 002:   4088 / 5589 loss=1.668, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=1054.5, nsentences=144, sample_size=1054.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=426.9, ups=0.4, wpb=1054.5, bsz=144, num_updates=9660, lr=2.88072e-05, gnorm=1.366, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23894
2023-08-02 18:39:24 - progress_bar.py[line:272] - INFO: epoch 002:   4098 / 5589 loss=1.671, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=1056.3, nsentences=144, sample_size=1056.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=428, ups=0.41, wpb=1056.3, bsz=144, num_updates=9670, lr=2.8837e-05, gnorm=1.38, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23919
2023-08-02 18:39:49 - progress_bar.py[line:272] - INFO: epoch 002:   4108 / 5589 loss=1.672, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=1053.9, nsentences=144, sample_size=1053.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=427.2, ups=0.41, wpb=1053.9, bsz=144, num_updates=9680, lr=2.88668e-05, gnorm=1.446, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23943
2023-08-02 18:40:14 - progress_bar.py[line:272] - INFO: epoch 002:   4118 / 5589 loss=1.668, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=1056.8, nsentences=144, sample_size=1056.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=428.6, ups=0.41, wpb=1056.8, bsz=144, num_updates=9690, lr=2.88966e-05, gnorm=1.384, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23968
2023-08-02 18:40:38 - progress_bar.py[line:272] - INFO: epoch 002:   4128 / 5589 loss=1.667, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=1060.5, nsentences=144, sample_size=1060.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=430, ups=0.41, wpb=1060.5, bsz=144, num_updates=9700, lr=2.89264e-05, gnorm=1.4, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=23993
2023-08-02 18:41:03 - progress_bar.py[line:272] - INFO: epoch 002:   4138 / 5589 loss=1.661, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=1052.5, nsentences=144, sample_size=1052.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=427.5, ups=0.41, wpb=1052.5, bsz=144, num_updates=9710, lr=2.89563e-05, gnorm=1.455, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24017
2023-08-02 18:41:28 - progress_bar.py[line:272] - INFO: epoch 002:   4148 / 5589 loss=1.673, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=1052.2, nsentences=144, sample_size=1052.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=427.2, ups=0.41, wpb=1052.2, bsz=144, num_updates=9720, lr=2.89861e-05, gnorm=1.34, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24042
2023-08-02 18:41:52 - progress_bar.py[line:272] - INFO: epoch 002:   4158 / 5589 loss=1.668, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=1057.3, nsentences=144, sample_size=1057.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=429.4, ups=0.41, wpb=1057.3, bsz=144, num_updates=9730, lr=2.90159e-05, gnorm=1.345, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24067
2023-08-02 18:42:17 - progress_bar.py[line:272] - INFO: epoch 002:   4168 / 5589 loss=1.662, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=1058, nsentences=144, sample_size=1058, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=429.7, ups=0.41, wpb=1058, bsz=144, num_updates=9740, lr=2.90457e-05, gnorm=1.342, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24091
2023-08-02 18:42:42 - progress_bar.py[line:272] - INFO: epoch 002:   4178 / 5589 loss=1.668, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=1058.9, nsentences=144, sample_size=1058.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=430.1, ups=0.41, wpb=1058.9, bsz=144, num_updates=9750, lr=2.90755e-05, gnorm=1.427, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24116
2023-08-02 18:43:06 - progress_bar.py[line:272] - INFO: epoch 002:   4188 / 5589 loss=1.671, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=1055.4, nsentences=144, sample_size=1055.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=428.5, ups=0.41, wpb=1055.4, bsz=144, num_updates=9760, lr=2.91054e-05, gnorm=1.297, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24140
2023-08-02 18:43:31 - progress_bar.py[line:272] - INFO: epoch 002:   4198 / 5589 loss=1.671, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=1055.3, nsentences=144, sample_size=1055.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=428.4, ups=0.41, wpb=1055.3, bsz=144, num_updates=9770, lr=2.91352e-05, gnorm=1.474, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24165
2023-08-02 18:43:55 - progress_bar.py[line:272] - INFO: epoch 002:   4208 / 5589 loss=1.668, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=1056.3, nsentences=144, sample_size=1056.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=428.7, ups=0.41, wpb=1056.3, bsz=144, num_updates=9780, lr=2.9165e-05, gnorm=1.325, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=24190
2023-08-02 18:44:20 - progress_bar.py[line:272] - INFO: epoch 002:   4218 / 5589 loss=1.681, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=1056.9, nsentences=144, sample_size=1056.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=428.5, ups=0.41, wpb=1056.9, bsz=144, num_updates=9790, lr=2.91948e-05, gnorm=1.446, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=24214
2023-08-02 18:44:45 - progress_bar.py[line:272] - INFO: epoch 002:   4228 / 5589 loss=1.679, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=1054.2, nsentences=144, sample_size=1054.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=427.5, ups=0.41, wpb=1054.2, bsz=144, num_updates=9800, lr=2.92247e-05, gnorm=1.481, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=24239
2023-08-02 18:44:52 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-08-02 18:45:12 - progress_bar.py[line:272] - INFO: epoch 002:   4239 / 5589 loss=1.67, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=1055.4, nsentences=144, sample_size=1055.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=389.5, ups=0.37, wpb=1055.4, bsz=144, num_updates=9810, lr=2.92545e-05, gnorm=1.428, clip=100, loss_scale=128, train_wall=27, gb_free=6.6, wall=24266
2023-08-02 18:45:36 - progress_bar.py[line:272] - INFO: epoch 002:   4249 / 5589 loss=1.661, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=1055.1, nsentences=144, sample_size=1055.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=428.1, ups=0.41, wpb=1055.1, bsz=144, num_updates=9820, lr=2.92843e-05, gnorm=1.349, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24291
2023-08-02 18:46:01 - progress_bar.py[line:272] - INFO: epoch 002:   4259 / 5589 loss=1.669, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=1053.8, nsentences=144, sample_size=1053.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=427.6, ups=0.41, wpb=1053.8, bsz=144, num_updates=9830, lr=2.93141e-05, gnorm=1.303, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24315
2023-08-02 18:46:26 - progress_bar.py[line:272] - INFO: epoch 002:   4269 / 5589 loss=1.677, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=1059.6, nsentences=144, sample_size=1059.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=429.7, ups=0.41, wpb=1059.6, bsz=144, num_updates=9840, lr=2.93439e-05, gnorm=1.402, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24340
2023-08-02 18:46:50 - progress_bar.py[line:272] - INFO: epoch 002:   4279 / 5589 loss=1.67, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=1055.1, nsentences=144, sample_size=1055.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=428, ups=0.41, wpb=1055.1, bsz=144, num_updates=9850, lr=2.93738e-05, gnorm=1.48, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24365
2023-08-02 18:47:15 - progress_bar.py[line:272] - INFO: epoch 002:   4289 / 5589 loss=1.669, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=1055, nsentences=144, sample_size=1055, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=427.8, ups=0.41, wpb=1055, bsz=144, num_updates=9860, lr=2.94036e-05, gnorm=1.34, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24389
2023-08-02 18:47:40 - progress_bar.py[line:272] - INFO: epoch 002:   4299 / 5589 loss=1.663, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=1055.6, nsentences=144, sample_size=1055.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=428.3, ups=0.41, wpb=1055.6, bsz=144, num_updates=9870, lr=2.94334e-05, gnorm=1.343, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24414
2023-08-02 18:48:04 - progress_bar.py[line:272] - INFO: epoch 002:   4309 / 5589 loss=1.655, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=1056.1, nsentences=144, sample_size=1056.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=428.6, ups=0.41, wpb=1056.1, bsz=144, num_updates=9880, lr=2.94632e-05, gnorm=1.281, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24439
2023-08-02 18:48:29 - progress_bar.py[line:272] - INFO: epoch 002:   4319 / 5589 loss=1.668, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=1054.8, nsentences=144, sample_size=1054.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=428.2, ups=0.41, wpb=1054.8, bsz=144, num_updates=9890, lr=2.9493e-05, gnorm=1.488, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24463
2023-08-02 18:48:54 - progress_bar.py[line:272] - INFO: epoch 002:   4329 / 5589 loss=1.67, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=1052.5, nsentences=144, sample_size=1052.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=427.2, ups=0.41, wpb=1052.5, bsz=144, num_updates=9900, lr=2.95229e-05, gnorm=1.35, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24488
2023-08-02 18:49:18 - progress_bar.py[line:272] - INFO: epoch 002:   4339 / 5589 loss=1.671, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=1053.5, nsentences=144, sample_size=1053.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=427.2, ups=0.41, wpb=1053.5, bsz=144, num_updates=9910, lr=2.95527e-05, gnorm=1.407, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24513
2023-08-02 18:49:43 - progress_bar.py[line:272] - INFO: epoch 002:   4349 / 5589 loss=1.659, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=1051.8, nsentences=144, sample_size=1051.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=426.5, ups=0.41, wpb=1051.8, bsz=144, num_updates=9920, lr=2.95825e-05, gnorm=1.323, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24537
2023-08-02 18:50:08 - progress_bar.py[line:272] - INFO: epoch 002:   4359 / 5589 loss=1.667, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=1053.7, nsentences=144, sample_size=1053.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=427.2, ups=0.41, wpb=1053.7, bsz=144, num_updates=9930, lr=2.96123e-05, gnorm=1.406, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24562
2023-08-02 18:50:32 - progress_bar.py[line:272] - INFO: epoch 002:   4369 / 5589 loss=1.666, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=1053.8, nsentences=144, sample_size=1053.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=427.6, ups=0.41, wpb=1053.8, bsz=144, num_updates=9940, lr=2.96421e-05, gnorm=1.369, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24587
2023-08-02 18:50:57 - progress_bar.py[line:272] - INFO: epoch 002:   4379 / 5589 loss=1.677, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=1054.4, nsentences=144, sample_size=1054.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=427.8, ups=0.41, wpb=1054.4, bsz=144, num_updates=9950, lr=2.9672e-05, gnorm=1.471, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24611
2023-08-02 18:51:22 - progress_bar.py[line:272] - INFO: epoch 002:   4389 / 5589 loss=1.663, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=1058.2, nsentences=144, sample_size=1058.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=429, ups=0.41, wpb=1058.2, bsz=144, num_updates=9960, lr=2.97018e-05, gnorm=1.429, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24636
2023-08-02 18:51:46 - progress_bar.py[line:272] - INFO: epoch 002:   4399 / 5589 loss=1.661, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=1056.4, nsentences=144, sample_size=1056.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=428.5, ups=0.41, wpb=1056.4, bsz=144, num_updates=9970, lr=2.97316e-05, gnorm=1.366, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24661
2023-08-02 18:52:11 - progress_bar.py[line:272] - INFO: epoch 002:   4409 / 5589 loss=1.652, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=1057.9, nsentences=144, sample_size=1057.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=428.9, ups=0.41, wpb=1057.9, bsz=144, num_updates=9980, lr=2.97614e-05, gnorm=1.36, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24685
2023-08-02 18:52:36 - progress_bar.py[line:272] - INFO: epoch 002:   4419 / 5589 loss=1.67, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=1053.5, nsentences=144, sample_size=1053.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=426.4, ups=0.4, wpb=1053.5, bsz=144, num_updates=9990, lr=2.97913e-05, gnorm=1.37, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24710
2023-08-02 18:53:00 - progress_bar.py[line:272] - INFO: epoch 002:   4429 / 5589 loss=1.659, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=1052.4, nsentences=144, sample_size=1052.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=425.9, ups=0.4, wpb=1052.4, bsz=144, num_updates=10000, lr=2.98211e-05, gnorm=1.549, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24735
2023-08-02 18:53:25 - progress_bar.py[line:272] - INFO: epoch 002:   4439 / 5589 loss=1.666, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=1058.7, nsentences=144, sample_size=1058.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=428.3, ups=0.4, wpb=1058.7, bsz=144, num_updates=10010, lr=2.98509e-05, gnorm=1.512, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24759
2023-08-02 18:53:50 - progress_bar.py[line:272] - INFO: epoch 002:   4449 / 5589 loss=1.658, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=1054.5, nsentences=144, sample_size=1054.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=427, ups=0.4, wpb=1054.5, bsz=144, num_updates=10020, lr=2.98807e-05, gnorm=1.389, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24784
2023-08-02 18:54:15 - progress_bar.py[line:272] - INFO: epoch 002:   4459 / 5589 loss=1.66, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=1057, nsentences=144, sample_size=1057, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=427.8, ups=0.4, wpb=1057, bsz=144, num_updates=10030, lr=2.99105e-05, gnorm=1.231, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24809
2023-08-02 18:54:39 - progress_bar.py[line:272] - INFO: epoch 002:   4469 / 5589 loss=1.662, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=1056.7, nsentences=144, sample_size=1056.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=427.9, ups=0.4, wpb=1056.7, bsz=144, num_updates=10040, lr=2.99404e-05, gnorm=1.379, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24834
2023-08-02 18:55:04 - progress_bar.py[line:272] - INFO: epoch 002:   4479 / 5589 loss=1.667, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=1055.7, nsentences=144, sample_size=1055.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=428, ups=0.41, wpb=1055.7, bsz=144, num_updates=10050, lr=2.99702e-05, gnorm=1.399, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24858
2023-08-02 18:55:29 - progress_bar.py[line:272] - INFO: epoch 002:   4489 / 5589 loss=1.672, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=1052.6, nsentences=144, sample_size=1052.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=426, ups=0.4, wpb=1052.6, bsz=144, num_updates=10060, lr=3e-05, gnorm=1.384, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24883
2023-08-02 18:55:53 - progress_bar.py[line:272] - INFO: epoch 002:   4499 / 5589 loss=1.664, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=1054.7, nsentences=144, sample_size=1054.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=426.8, ups=0.4, wpb=1054.7, bsz=144, num_updates=10070, lr=2.99981e-05, gnorm=1.368, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24908
2023-08-02 18:56:18 - progress_bar.py[line:272] - INFO: epoch 002:   4509 / 5589 loss=1.666, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=1059.3, nsentences=144, sample_size=1059.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=429.2, ups=0.41, wpb=1059.3, bsz=144, num_updates=10080, lr=2.99962e-05, gnorm=1.358, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24932
2023-08-02 18:56:43 - progress_bar.py[line:272] - INFO: epoch 002:   4519 / 5589 loss=1.672, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=1052.4, nsentences=144, sample_size=1052.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=426.9, ups=0.41, wpb=1052.4, bsz=144, num_updates=10090, lr=2.99943e-05, gnorm=1.51, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24957
2023-08-02 18:57:07 - progress_bar.py[line:272] - INFO: epoch 002:   4529 / 5589 loss=1.673, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=1056.8, nsentences=144, sample_size=1056.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=428.5, ups=0.41, wpb=1056.8, bsz=144, num_updates=10100, lr=2.99924e-05, gnorm=1.502, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=24982
2023-08-02 18:57:32 - progress_bar.py[line:272] - INFO: epoch 002:   4539 / 5589 loss=1.653, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=1056.7, nsentences=144, sample_size=1056.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=428.1, ups=0.41, wpb=1056.7, bsz=144, num_updates=10110, lr=2.99905e-05, gnorm=1.253, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25006
2023-08-02 18:57:57 - progress_bar.py[line:272] - INFO: epoch 002:   4549 / 5589 loss=1.658, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=1053.3, nsentences=144, sample_size=1053.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=427.7, ups=0.41, wpb=1053.3, bsz=144, num_updates=10120, lr=2.99886e-05, gnorm=1.338, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25031
2023-08-02 18:58:21 - progress_bar.py[line:272] - INFO: epoch 002:   4559 / 5589 loss=1.664, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=1052.9, nsentences=144, sample_size=1052.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=424.3, ups=0.4, wpb=1052.9, bsz=144, num_updates=10130, lr=2.99867e-05, gnorm=1.46, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25056
2023-08-02 18:58:46 - progress_bar.py[line:272] - INFO: epoch 002:   4569 / 5589 loss=1.678, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=1056.6, nsentences=144, sample_size=1056.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=429.1, ups=0.41, wpb=1056.6, bsz=144, num_updates=10140, lr=2.99848e-05, gnorm=1.516, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25080
2023-08-02 18:59:11 - progress_bar.py[line:272] - INFO: epoch 002:   4579 / 5589 loss=1.672, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=1056, nsentences=144, sample_size=1056, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=428.5, ups=0.41, wpb=1056, bsz=144, num_updates=10150, lr=2.99829e-05, gnorm=1.361, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25105
2023-08-02 18:59:35 - progress_bar.py[line:272] - INFO: epoch 002:   4589 / 5589 loss=1.664, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=1052.6, nsentences=144, sample_size=1052.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=427.4, ups=0.41, wpb=1052.6, bsz=144, num_updates=10160, lr=2.9981e-05, gnorm=1.298, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25130
2023-08-02 19:00:00 - progress_bar.py[line:272] - INFO: epoch 002:   4599 / 5589 loss=1.648, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=1058, nsentences=144, sample_size=1058, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=429.3, ups=0.41, wpb=1058, bsz=144, num_updates=10170, lr=2.99791e-05, gnorm=1.396, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25154
2023-08-02 19:00:25 - progress_bar.py[line:272] - INFO: epoch 002:   4609 / 5589 loss=1.662, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=1054.2, nsentences=144, sample_size=1054.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=426.8, ups=0.4, wpb=1054.2, bsz=144, num_updates=10180, lr=2.99772e-05, gnorm=1.352, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25179
2023-08-02 19:00:49 - progress_bar.py[line:272] - INFO: epoch 002:   4619 / 5589 loss=1.667, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=1054.2, nsentences=144, sample_size=1054.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=426.4, ups=0.4, wpb=1054.2, bsz=144, num_updates=10190, lr=2.99753e-05, gnorm=1.409, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25204
2023-08-02 19:01:14 - progress_bar.py[line:272] - INFO: epoch 002:   4629 / 5589 loss=1.671, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=1056.6, nsentences=144, sample_size=1056.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=427.5, ups=0.4, wpb=1056.6, bsz=144, num_updates=10200, lr=2.99734e-05, gnorm=1.439, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25228
2023-08-02 19:01:39 - progress_bar.py[line:272] - INFO: epoch 002:   4639 / 5589 loss=1.662, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=1054.6, nsentences=144, sample_size=1054.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=427.2, ups=0.41, wpb=1054.6, bsz=144, num_updates=10210, lr=2.99714e-05, gnorm=1.367, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25253
2023-08-02 19:02:04 - progress_bar.py[line:272] - INFO: epoch 002:   4649 / 5589 loss=1.66, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=1057.7, nsentences=144, sample_size=1057.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=427.6, ups=0.4, wpb=1057.7, bsz=144, num_updates=10220, lr=2.99695e-05, gnorm=1.465, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25278
2023-08-02 19:02:28 - progress_bar.py[line:272] - INFO: epoch 002:   4659 / 5589 loss=1.661, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=1058.1, nsentences=144, sample_size=1058.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=428.1, ups=0.4, wpb=1058.1, bsz=144, num_updates=10230, lr=2.99676e-05, gnorm=1.362, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25303
2023-08-02 19:02:53 - progress_bar.py[line:272] - INFO: epoch 002:   4669 / 5589 loss=1.664, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=1054.6, nsentences=144, sample_size=1054.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=427.1, ups=0.4, wpb=1054.6, bsz=144, num_updates=10240, lr=2.99657e-05, gnorm=1.39, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25327
2023-08-02 19:03:18 - progress_bar.py[line:272] - INFO: epoch 002:   4679 / 5589 loss=1.666, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=1052.3, nsentences=144, sample_size=1052.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=426.2, ups=0.41, wpb=1052.3, bsz=144, num_updates=10250, lr=2.99638e-05, gnorm=1.457, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25352
2023-08-02 19:03:42 - progress_bar.py[line:272] - INFO: epoch 002:   4689 / 5589 loss=1.652, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=1056.8, nsentences=144, sample_size=1056.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=427.9, ups=0.4, wpb=1056.8, bsz=144, num_updates=10260, lr=2.99619e-05, gnorm=1.328, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25377
2023-08-02 19:04:07 - progress_bar.py[line:272] - INFO: epoch 002:   4699 / 5589 loss=1.662, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=1053.1, nsentences=144, sample_size=1053.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=427.1, ups=0.41, wpb=1053.1, bsz=144, num_updates=10270, lr=2.996e-05, gnorm=1.453, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25401
2023-08-02 19:04:32 - progress_bar.py[line:272] - INFO: epoch 002:   4709 / 5589 loss=1.666, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=1054.6, nsentences=144, sample_size=1054.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=427.9, ups=0.41, wpb=1054.6, bsz=144, num_updates=10280, lr=2.99581e-05, gnorm=1.512, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25426
2023-08-02 19:04:56 - progress_bar.py[line:272] - INFO: epoch 002:   4719 / 5589 loss=1.665, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=1054.2, nsentences=144, sample_size=1054.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=427.7, ups=0.41, wpb=1054.2, bsz=144, num_updates=10290, lr=2.99562e-05, gnorm=1.489, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25451
2023-08-02 19:05:21 - progress_bar.py[line:272] - INFO: epoch 002:   4729 / 5589 loss=1.66, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=1055.2, nsentences=144, sample_size=1055.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=428.1, ups=0.41, wpb=1055.2, bsz=144, num_updates=10300, lr=2.99543e-05, gnorm=1.313, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25475
2023-08-02 19:05:46 - progress_bar.py[line:272] - INFO: epoch 002:   4739 / 5589 loss=1.66, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=1057.4, nsentences=144, sample_size=1057.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=428.9, ups=0.41, wpb=1057.4, bsz=144, num_updates=10310, lr=2.99524e-05, gnorm=1.294, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25500
2023-08-02 19:06:10 - progress_bar.py[line:272] - INFO: epoch 002:   4749 / 5589 loss=1.666, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=1058.4, nsentences=144, sample_size=1058.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=429.4, ups=0.41, wpb=1058.4, bsz=144, num_updates=10320, lr=2.99505e-05, gnorm=1.356, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=25525
2023-08-02 19:06:35 - progress_bar.py[line:272] - INFO: epoch 002:   4759 / 5589 loss=1.656, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=1054.2, nsentences=144, sample_size=1054.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=427.9, ups=0.41, wpb=1054.2, bsz=144, num_updates=10330, lr=2.99486e-05, gnorm=1.327, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=25549
2023-08-02 19:07:00 - progress_bar.py[line:272] - INFO: epoch 002:   4769 / 5589 loss=1.671, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=1059.4, nsentences=144, sample_size=1059.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=430.2, ups=0.41, wpb=1059.4, bsz=144, num_updates=10340, lr=2.99467e-05, gnorm=1.44, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=25574
2023-08-02 19:07:24 - progress_bar.py[line:272] - INFO: epoch 002:   4779 / 5589 loss=1.669, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=1056.2, nsentences=144, sample_size=1056.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=428.5, ups=0.41, wpb=1056.2, bsz=144, num_updates=10350, lr=2.99448e-05, gnorm=1.381, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=25599
2023-08-02 19:07:49 - progress_bar.py[line:272] - INFO: epoch 002:   4789 / 5589 loss=1.659, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=1056.8, nsentences=144, sample_size=1056.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=428.7, ups=0.41, wpb=1056.8, bsz=144, num_updates=10360, lr=2.99429e-05, gnorm=1.354, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=25623
2023-08-02 19:08:14 - progress_bar.py[line:272] - INFO: epoch 002:   4799 / 5589 loss=1.662, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=1056.1, nsentences=144, sample_size=1056.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=428.5, ups=0.41, wpb=1056.1, bsz=144, num_updates=10370, lr=2.9941e-05, gnorm=1.401, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=25648
2023-08-02 19:08:21 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-08-02 19:08:41 - progress_bar.py[line:272] - INFO: epoch 002:   4810 / 5589 loss=1.667, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=1050.2, nsentences=144, sample_size=1050.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=387.5, ups=0.37, wpb=1050.2, bsz=144, num_updates=10380, lr=2.99391e-05, gnorm=1.368, clip=100, loss_scale=128, train_wall=27, gb_free=6.6, wall=25675
2023-08-02 19:09:05 - progress_bar.py[line:272] - INFO: epoch 002:   4820 / 5589 loss=1.659, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=1053.8, nsentences=144, sample_size=1053.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=427.5, ups=0.41, wpb=1053.8, bsz=144, num_updates=10390, lr=2.99372e-05, gnorm=1.333, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25700
2023-08-02 19:09:30 - progress_bar.py[line:272] - INFO: epoch 002:   4830 / 5589 loss=1.661, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=1059, nsentences=144, sample_size=1059, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=429.6, ups=0.41, wpb=1059, bsz=144, num_updates=10400, lr=2.99353e-05, gnorm=1.368, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25724
2023-08-02 19:09:55 - progress_bar.py[line:272] - INFO: epoch 002:   4840 / 5589 loss=1.66, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=1054.9, nsentences=144, sample_size=1054.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=428, ups=0.41, wpb=1054.9, bsz=144, num_updates=10410, lr=2.99334e-05, gnorm=1.369, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25749
2023-08-02 19:10:19 - progress_bar.py[line:272] - INFO: epoch 002:   4850 / 5589 loss=1.659, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=1055.9, nsentences=144, sample_size=1055.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=425.2, ups=0.4, wpb=1055.9, bsz=144, num_updates=10420, lr=2.99315e-05, gnorm=1.334, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25774
2023-08-02 19:10:44 - progress_bar.py[line:272] - INFO: epoch 002:   4860 / 5589 loss=1.665, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=1056.7, nsentences=144, sample_size=1056.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=425.4, ups=0.4, wpb=1056.7, bsz=144, num_updates=10430, lr=2.99296e-05, gnorm=1.364, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25799
2023-08-02 19:11:09 - progress_bar.py[line:272] - INFO: epoch 002:   4870 / 5589 loss=1.664, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=1056.4, nsentences=144, sample_size=1056.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=428.6, ups=0.41, wpb=1056.4, bsz=144, num_updates=10440, lr=2.99277e-05, gnorm=1.377, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25823
2023-08-02 19:11:34 - progress_bar.py[line:272] - INFO: epoch 002:   4880 / 5589 loss=1.657, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=1051.5, nsentences=144, sample_size=1051.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=426.8, ups=0.41, wpb=1051.5, bsz=144, num_updates=10450, lr=2.99258e-05, gnorm=1.4, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25848
2023-08-02 19:11:58 - progress_bar.py[line:272] - INFO: epoch 002:   4890 / 5589 loss=1.649, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=1056.3, nsentences=144, sample_size=1056.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=428.3, ups=0.41, wpb=1056.3, bsz=144, num_updates=10460, lr=2.99239e-05, gnorm=1.425, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25873
2023-08-02 19:12:23 - progress_bar.py[line:272] - INFO: epoch 002:   4900 / 5589 loss=1.664, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=1057.1, nsentences=144, sample_size=1057.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=428.8, ups=0.41, wpb=1057.1, bsz=144, num_updates=10470, lr=2.9922e-05, gnorm=1.435, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25897
2023-08-02 19:12:48 - progress_bar.py[line:272] - INFO: epoch 002:   4910 / 5589 loss=1.648, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=1055.9, nsentences=144, sample_size=1055.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=428.7, ups=0.41, wpb=1055.9, bsz=144, num_updates=10480, lr=2.99201e-05, gnorm=1.298, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25922
2023-08-02 19:13:12 - progress_bar.py[line:272] - INFO: epoch 002:   4920 / 5589 loss=1.667, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=1055.1, nsentences=144, sample_size=1055.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=428, ups=0.41, wpb=1055.1, bsz=144, num_updates=10490, lr=2.99182e-05, gnorm=1.421, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25946
2023-08-02 19:13:37 - progress_bar.py[line:272] - INFO: epoch 002:   4930 / 5589 loss=1.657, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=1053.4, nsentences=144, sample_size=1053.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=427.4, ups=0.41, wpb=1053.4, bsz=144, num_updates=10500, lr=2.99162e-05, gnorm=1.528, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25971
2023-08-02 19:14:02 - progress_bar.py[line:272] - INFO: epoch 002:   4940 / 5589 loss=1.662, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=1057, nsentences=144, sample_size=1057, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=428.9, ups=0.41, wpb=1057, bsz=144, num_updates=10510, lr=2.99143e-05, gnorm=1.406, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=25996
2023-08-02 19:14:26 - progress_bar.py[line:272] - INFO: epoch 002:   4950 / 5589 loss=1.663, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=1054.8, nsentences=144, sample_size=1054.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=427.8, ups=0.41, wpb=1054.8, bsz=144, num_updates=10520, lr=2.99124e-05, gnorm=1.395, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26020
2023-08-02 19:14:51 - progress_bar.py[line:272] - INFO: epoch 002:   4960 / 5589 loss=1.663, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=1056.1, nsentences=144, sample_size=1056.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=428.6, ups=0.41, wpb=1056.1, bsz=144, num_updates=10530, lr=2.99105e-05, gnorm=1.478, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26045
2023-08-02 19:15:15 - progress_bar.py[line:272] - INFO: epoch 002:   4970 / 5589 loss=1.655, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=1053.9, nsentences=144, sample_size=1053.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=427.7, ups=0.41, wpb=1053.9, bsz=144, num_updates=10540, lr=2.99086e-05, gnorm=1.395, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26070
2023-08-02 19:15:40 - progress_bar.py[line:272] - INFO: epoch 002:   4980 / 5589 loss=1.657, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=1055.6, nsentences=144, sample_size=1055.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=428.3, ups=0.41, wpb=1055.6, bsz=144, num_updates=10550, lr=2.99067e-05, gnorm=1.401, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26094
2023-08-02 19:16:05 - progress_bar.py[line:272] - INFO: epoch 002:   4990 / 5589 loss=1.661, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=1055.4, nsentences=144, sample_size=1055.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=428.1, ups=0.41, wpb=1055.4, bsz=144, num_updates=10560, lr=2.99048e-05, gnorm=1.38, clip=90, loss_scale=128, train_wall=25, gb_free=6.6, wall=26119
2023-08-02 19:16:29 - progress_bar.py[line:272] - INFO: epoch 002:   5000 / 5589 loss=1.655, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=1053.1, nsentences=144, sample_size=1053.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=427.3, ups=0.41, wpb=1053.1, bsz=144, num_updates=10570, lr=2.99029e-05, gnorm=1.345, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26144
2023-08-02 19:16:54 - progress_bar.py[line:272] - INFO: epoch 002:   5010 / 5589 loss=1.658, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=1056.7, nsentences=144, sample_size=1056.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=428.7, ups=0.41, wpb=1056.7, bsz=144, num_updates=10580, lr=2.9901e-05, gnorm=1.346, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26168
2023-08-02 19:17:19 - progress_bar.py[line:272] - INFO: epoch 002:   5020 / 5589 loss=1.657, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=1055.4, nsentences=144, sample_size=1055.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=428.1, ups=0.41, wpb=1055.4, bsz=144, num_updates=10590, lr=2.98991e-05, gnorm=1.391, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26193
2023-08-02 19:17:43 - progress_bar.py[line:272] - INFO: epoch 002:   5030 / 5589 loss=1.668, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=1056.6, nsentences=144, sample_size=1056.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=427.8, ups=0.4, wpb=1056.6, bsz=144, num_updates=10600, lr=2.98972e-05, gnorm=1.557, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26218
2023-08-02 19:18:08 - progress_bar.py[line:272] - INFO: epoch 002:   5040 / 5589 loss=1.649, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=1056.3, nsentences=144, sample_size=1056.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=427.4, ups=0.4, wpb=1056.3, bsz=144, num_updates=10610, lr=2.98953e-05, gnorm=1.33, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26242
2023-08-02 19:18:33 - progress_bar.py[line:272] - INFO: epoch 002:   5050 / 5589 loss=1.666, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=1055.3, nsentences=144, sample_size=1055.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=427.1, ups=0.4, wpb=1055.3, bsz=144, num_updates=10620, lr=2.98934e-05, gnorm=1.505, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26267
2023-08-02 19:18:58 - progress_bar.py[line:272] - INFO: epoch 002:   5060 / 5589 loss=1.66, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=1052.5, nsentences=144, sample_size=1052.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=426, ups=0.4, wpb=1052.5, bsz=144, num_updates=10630, lr=2.98915e-05, gnorm=1.364, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26292
2023-08-02 19:19:22 - progress_bar.py[line:272] - INFO: epoch 002:   5070 / 5589 loss=1.65, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=1055.6, nsentences=144, sample_size=1055.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=427.3, ups=0.4, wpb=1055.6, bsz=144, num_updates=10640, lr=2.98896e-05, gnorm=1.313, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26317
2023-08-02 19:19:47 - progress_bar.py[line:272] - INFO: epoch 002:   5080 / 5589 loss=1.66, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=1052.1, nsentences=144, sample_size=1052.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=425.8, ups=0.4, wpb=1052.1, bsz=144, num_updates=10650, lr=2.98877e-05, gnorm=1.273, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26341
2023-08-02 19:20:12 - progress_bar.py[line:272] - INFO: epoch 002:   5090 / 5589 loss=1.646, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=1058.3, nsentences=144, sample_size=1058.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=428.3, ups=0.4, wpb=1058.3, bsz=144, num_updates=10660, lr=2.98858e-05, gnorm=1.305, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26366
2023-08-02 19:20:36 - progress_bar.py[line:272] - INFO: epoch 002:   5100 / 5589 loss=1.666, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=1058, nsentences=144, sample_size=1058, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=428.7, ups=0.41, wpb=1058, bsz=144, num_updates=10670, lr=2.98839e-05, gnorm=1.394, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26391
2023-08-02 19:21:01 - progress_bar.py[line:272] - INFO: epoch 002:   5110 / 5589 loss=1.659, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=1055.8, nsentences=144, sample_size=1055.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=427.4, ups=0.4, wpb=1055.8, bsz=144, num_updates=10680, lr=2.9882e-05, gnorm=1.402, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26415
2023-08-02 19:21:26 - progress_bar.py[line:272] - INFO: epoch 002:   5120 / 5589 loss=1.662, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=1052.2, nsentences=144, sample_size=1052.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=426.5, ups=0.41, wpb=1052.2, bsz=144, num_updates=10690, lr=2.98801e-05, gnorm=1.42, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26440
2023-08-02 19:21:50 - progress_bar.py[line:272] - INFO: epoch 002:   5130 / 5589 loss=1.653, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=1053.9, nsentences=144, sample_size=1053.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=427.4, ups=0.41, wpb=1053.9, bsz=144, num_updates=10700, lr=2.98782e-05, gnorm=1.353, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26465
2023-08-02 19:22:15 - progress_bar.py[line:272] - INFO: epoch 002:   5140 / 5589 loss=1.656, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=1057.7, nsentences=144, sample_size=1057.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=428.9, ups=0.41, wpb=1057.7, bsz=144, num_updates=10710, lr=2.98763e-05, gnorm=1.377, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26489
2023-08-02 19:22:40 - progress_bar.py[line:272] - INFO: epoch 002:   5150 / 5589 loss=1.66, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=1052.4, nsentences=144, sample_size=1052.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=426.8, ups=0.41, wpb=1052.4, bsz=144, num_updates=10720, lr=2.98744e-05, gnorm=1.35, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26514
2023-08-02 19:23:04 - progress_bar.py[line:272] - INFO: epoch 002:   5160 / 5589 loss=1.645, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=1055.2, nsentences=144, sample_size=1055.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=427.8, ups=0.41, wpb=1055.2, bsz=144, num_updates=10730, lr=2.98725e-05, gnorm=1.365, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26539
2023-08-02 19:23:29 - progress_bar.py[line:272] - INFO: epoch 002:   5170 / 5589 loss=1.658, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=1055.4, nsentences=144, sample_size=1055.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=427.9, ups=0.41, wpb=1055.4, bsz=144, num_updates=10740, lr=2.98706e-05, gnorm=1.458, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26563
2023-08-02 19:23:53 - progress_bar.py[line:272] - INFO: epoch 002:   5180 / 5589 loss=1.644, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=1040.4, nsentences=142.2, sample_size=1040.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=426.7, ups=0.41, wpb=1040.4, bsz=142.2, num_updates=10750, lr=2.98687e-05, gnorm=1.361, clip=100, loss_scale=128, train_wall=24, gb_free=6.6, wall=26588
2023-08-02 19:24:18 - progress_bar.py[line:272] - INFO: epoch 002:   5190 / 5589 loss=1.655, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=1060.6, nsentences=144, sample_size=1060.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=430.1, ups=0.41, wpb=1060.6, bsz=144, num_updates=10760, lr=2.98668e-05, gnorm=1.437, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26612
2023-08-02 19:24:43 - progress_bar.py[line:272] - INFO: epoch 002:   5200 / 5589 loss=1.641, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=1052.5, nsentences=144, sample_size=1052.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=427, ups=0.41, wpb=1052.5, bsz=144, num_updates=10770, lr=2.98649e-05, gnorm=1.382, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26637
2023-08-02 19:25:07 - progress_bar.py[line:272] - INFO: epoch 002:   5210 / 5589 loss=1.65, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=1057.7, nsentences=144, sample_size=1057.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=429.1, ups=0.41, wpb=1057.7, bsz=144, num_updates=10780, lr=2.9863e-05, gnorm=1.393, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26662
2023-08-02 19:25:32 - progress_bar.py[line:272] - INFO: epoch 002:   5220 / 5589 loss=1.666, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=1053.5, nsentences=144, sample_size=1053.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=427.2, ups=0.41, wpb=1053.5, bsz=144, num_updates=10790, lr=2.9861e-05, gnorm=1.4, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26686
2023-08-02 19:25:57 - progress_bar.py[line:272] - INFO: epoch 002:   5230 / 5589 loss=1.646, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=1054.2, nsentences=144, sample_size=1054.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=427.9, ups=0.41, wpb=1054.2, bsz=144, num_updates=10800, lr=2.98591e-05, gnorm=1.323, clip=90, loss_scale=128, train_wall=25, gb_free=6.6, wall=26711
2023-08-02 19:26:21 - progress_bar.py[line:272] - INFO: epoch 002:   5240 / 5589 loss=1.648, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=1055.4, nsentences=144, sample_size=1055.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=428.2, ups=0.41, wpb=1055.4, bsz=144, num_updates=10810, lr=2.98572e-05, gnorm=1.377, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26736
2023-08-02 19:26:46 - progress_bar.py[line:272] - INFO: epoch 002:   5250 / 5589 loss=1.658, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=1055.6, nsentences=144, sample_size=1055.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=428.2, ups=0.41, wpb=1055.6, bsz=144, num_updates=10820, lr=2.98553e-05, gnorm=1.408, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26760
2023-08-02 19:27:11 - progress_bar.py[line:272] - INFO: epoch 002:   5260 / 5589 loss=1.643, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=1054.3, nsentences=144, sample_size=1054.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=427.9, ups=0.41, wpb=1054.3, bsz=144, num_updates=10830, lr=2.98534e-05, gnorm=1.454, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26785
2023-08-02 19:27:35 - progress_bar.py[line:272] - INFO: epoch 002:   5270 / 5589 loss=1.657, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=1054.9, nsentences=144, sample_size=1054.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=427.7, ups=0.41, wpb=1054.9, bsz=144, num_updates=10840, lr=2.98515e-05, gnorm=1.558, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26810
2023-08-02 19:28:00 - progress_bar.py[line:272] - INFO: epoch 002:   5280 / 5589 loss=1.653, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=1054.6, nsentences=144, sample_size=1054.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=427.7, ups=0.41, wpb=1054.6, bsz=144, num_updates=10850, lr=2.98496e-05, gnorm=1.444, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26834
2023-08-02 19:28:25 - progress_bar.py[line:272] - INFO: epoch 002:   5290 / 5589 loss=1.658, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=1052.4, nsentences=144, sample_size=1052.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=426.7, ups=0.41, wpb=1052.4, bsz=144, num_updates=10860, lr=2.98477e-05, gnorm=1.412, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26859
2023-08-02 19:28:49 - progress_bar.py[line:272] - INFO: epoch 002:   5300 / 5589 loss=1.649, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=1057.1, nsentences=144, sample_size=1057.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=427.9, ups=0.4, wpb=1057.1, bsz=144, num_updates=10870, lr=2.98458e-05, gnorm=1.303, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26884
2023-08-02 19:29:14 - progress_bar.py[line:272] - INFO: epoch 002:   5310 / 5589 loss=1.656, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=1058, nsentences=144, sample_size=1058, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=426.8, ups=0.4, wpb=1058, bsz=144, num_updates=10880, lr=2.98439e-05, gnorm=1.347, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=26908
2023-08-02 19:29:39 - progress_bar.py[line:272] - INFO: epoch 002:   5320 / 5589 loss=1.651, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=1060.2, nsentences=144, sample_size=1060.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=429, ups=0.4, wpb=1060.2, bsz=144, num_updates=10890, lr=2.9842e-05, gnorm=1.421, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=26933
2023-08-02 19:30:04 - progress_bar.py[line:272] - INFO: epoch 002:   5330 / 5589 loss=1.662, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=1054.6, nsentences=144, sample_size=1054.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=428.1, ups=0.41, wpb=1054.6, bsz=144, num_updates=10900, lr=2.98401e-05, gnorm=1.427, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=26958
2023-08-02 19:30:28 - progress_bar.py[line:272] - INFO: epoch 002:   5340 / 5589 loss=1.648, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=1053.6, nsentences=144, sample_size=1053.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=427.5, ups=0.41, wpb=1053.6, bsz=144, num_updates=10910, lr=2.98382e-05, gnorm=1.287, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=26982
2023-08-02 19:30:53 - progress_bar.py[line:272] - INFO: epoch 002:   5350 / 5589 loss=1.658, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=1055.6, nsentences=144, sample_size=1055.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=428.6, ups=0.41, wpb=1055.6, bsz=144, num_updates=10920, lr=2.98363e-05, gnorm=1.397, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=27007
2023-08-02 19:31:17 - progress_bar.py[line:272] - INFO: epoch 002:   5360 / 5589 loss=1.645, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=1057.5, nsentences=144, sample_size=1057.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=429, ups=0.41, wpb=1057.5, bsz=144, num_updates=10930, lr=2.98344e-05, gnorm=1.319, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=27032
2023-08-02 19:31:42 - progress_bar.py[line:272] - INFO: epoch 002:   5370 / 5589 loss=1.646, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=1055.1, nsentences=144, sample_size=1055.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=428.4, ups=0.41, wpb=1055.1, bsz=144, num_updates=10940, lr=2.98325e-05, gnorm=1.342, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=27056
2023-08-02 19:32:07 - progress_bar.py[line:272] - INFO: epoch 002:   5380 / 5589 loss=1.642, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=1055.4, nsentences=144, sample_size=1055.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=427.5, ups=0.41, wpb=1055.4, bsz=144, num_updates=10950, lr=2.98306e-05, gnorm=1.379, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=27081
2023-08-02 19:32:32 - progress_bar.py[line:272] - INFO: epoch 002:   5390 / 5589 loss=1.65, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=1056.5, nsentences=144, sample_size=1056.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=427.6, ups=0.4, wpb=1056.5, bsz=144, num_updates=10960, lr=2.98287e-05, gnorm=1.314, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=27106
2023-08-02 19:32:56 - progress_bar.py[line:272] - INFO: epoch 002:   5400 / 5589 loss=1.651, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=1057.2, nsentences=144, sample_size=1057.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=428.3, ups=0.41, wpb=1057.2, bsz=144, num_updates=10970, lr=2.98268e-05, gnorm=1.431, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=27130
2023-08-02 19:33:21 - progress_bar.py[line:272] - INFO: epoch 002:   5410 / 5589 loss=1.648, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=1059.4, nsentences=144, sample_size=1059.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=429.1, ups=0.41, wpb=1059.4, bsz=144, num_updates=10980, lr=2.98249e-05, gnorm=1.426, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=27155
2023-08-02 19:33:46 - progress_bar.py[line:272] - INFO: epoch 002:   5420 / 5589 loss=1.64, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=1059.5, nsentences=144, sample_size=1059.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=429.3, ups=0.41, wpb=1059.5, bsz=144, num_updates=10990, lr=2.9823e-05, gnorm=1.3, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=27180
2023-08-02 19:34:10 - progress_bar.py[line:272] - INFO: epoch 002:   5430 / 5589 loss=1.652, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=1059.4, nsentences=144, sample_size=1059.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=428.8, ups=0.4, wpb=1059.4, bsz=144, num_updates=11000, lr=2.98211e-05, gnorm=1.338, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=27205
2023-08-02 19:34:35 - progress_bar.py[line:272] - INFO: epoch 002:   5440 / 5589 loss=1.646, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=1054.2, nsentences=144, sample_size=1054.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=426.6, ups=0.4, wpb=1054.2, bsz=144, num_updates=11010, lr=2.98192e-05, gnorm=1.291, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=27229
2023-08-02 19:35:00 - progress_bar.py[line:272] - INFO: epoch 002:   5450 / 5589 loss=1.638, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=1056.2, nsentences=144, sample_size=1056.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=427.9, ups=0.41, wpb=1056.2, bsz=144, num_updates=11020, lr=2.98173e-05, gnorm=1.449, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=27254
2023-08-02 19:35:10 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-08-02 19:35:27 - progress_bar.py[line:272] - INFO: epoch 002:   5461 / 5589 loss=1.655, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=1055.1, nsentences=144, sample_size=1055.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=388.9, ups=0.37, wpb=1055.1, bsz=144, num_updates=11030, lr=2.98154e-05, gnorm=1.464, clip=100, loss_scale=128, train_wall=27, gb_free=6.6, wall=27281
2023-08-02 19:35:52 - progress_bar.py[line:272] - INFO: epoch 002:   5471 / 5589 loss=1.639, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=1055, nsentences=144, sample_size=1055, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=427.4, ups=0.41, wpb=1055, bsz=144, num_updates=11040, lr=2.98135e-05, gnorm=1.357, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=27306
2023-08-02 19:36:16 - progress_bar.py[line:272] - INFO: epoch 002:   5481 / 5589 loss=1.642, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=1058.5, nsentences=144, sample_size=1058.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=428.6, ups=0.4, wpb=1058.5, bsz=144, num_updates=11050, lr=2.98116e-05, gnorm=1.299, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=27330
2023-08-02 19:36:41 - progress_bar.py[line:272] - INFO: epoch 002:   5491 / 5589 loss=1.655, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=1053.5, nsentences=144, sample_size=1053.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=427.4, ups=0.41, wpb=1053.5, bsz=144, num_updates=11060, lr=2.98097e-05, gnorm=1.391, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=27355
2023-08-02 19:37:06 - progress_bar.py[line:272] - INFO: epoch 002:   5501 / 5589 loss=1.64, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=1052.4, nsentences=144, sample_size=1052.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=427, ups=0.41, wpb=1052.4, bsz=144, num_updates=11070, lr=2.98078e-05, gnorm=1.492, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=27380
2023-08-02 19:37:30 - progress_bar.py[line:272] - INFO: epoch 002:   5511 / 5589 loss=1.647, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=1053.2, nsentences=144, sample_size=1053.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=427.1, ups=0.41, wpb=1053.2, bsz=144, num_updates=11080, lr=2.98058e-05, gnorm=1.377, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=27404
2023-08-02 19:37:55 - progress_bar.py[line:272] - INFO: epoch 002:   5521 / 5589 loss=1.634, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=1057.4, nsentences=144, sample_size=1057.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=429.3, ups=0.41, wpb=1057.4, bsz=144, num_updates=11090, lr=2.98039e-05, gnorm=1.313, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=27429
2023-08-02 19:38:19 - progress_bar.py[line:272] - INFO: epoch 002:   5531 / 5589 loss=1.646, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=1052.2, nsentences=144, sample_size=1052.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=427.2, ups=0.41, wpb=1052.2, bsz=144, num_updates=11100, lr=2.9802e-05, gnorm=1.367, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=27454
2023-08-02 19:38:44 - progress_bar.py[line:272] - INFO: epoch 002:   5541 / 5589 loss=1.65, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=1058.4, nsentences=144, sample_size=1058.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=429.9, ups=0.41, wpb=1058.4, bsz=144, num_updates=11110, lr=2.98001e-05, gnorm=1.302, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=27478
2023-08-02 19:39:09 - progress_bar.py[line:272] - INFO: epoch 002:   5551 / 5589 loss=1.648, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=1053.8, nsentences=144, sample_size=1053.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=427.9, ups=0.41, wpb=1053.8, bsz=144, num_updates=11120, lr=2.97982e-05, gnorm=1.354, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=27503
2023-08-02 19:39:33 - progress_bar.py[line:272] - INFO: epoch 002:   5561 / 5589 loss=1.65, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=1059.5, nsentences=144, sample_size=1059.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=430.1, ups=0.41, wpb=1059.5, bsz=144, num_updates=11130, lr=2.97963e-05, gnorm=1.354, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=27528
2023-08-02 19:39:58 - progress_bar.py[line:272] - INFO: epoch 002:   5571 / 5589 loss=1.633, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=1057.3, nsentences=144, sample_size=1057.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=429, ups=0.41, wpb=1057.3, bsz=144, num_updates=11140, lr=2.97944e-05, gnorm=1.263, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=27552
2023-08-02 19:40:23 - progress_bar.py[line:272] - INFO: epoch 002:   5581 / 5589 loss=1.645, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=1057.5, nsentences=144, sample_size=1057.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=428.9, ups=0.41, wpb=1057.5, bsz=144, num_updates=11150, lr=2.97925e-05, gnorm=1.392, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=27577
2023-08-02 19:40:42 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 2 @ 11158 updates
local datafile ../../dataset/OFA_data/vrd2_balanced/vg_train_full.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../dataset/OFA_data/vrd2_balanced/vg_train_full.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
2023-08-02 19:40:42 - trainer.py[line:431] - INFO: Saving checkpoint to ../../checkpoints/OFA/vrd2_checkpoints/_30_3e-5_512_balanced/checkpoint2.pt

local datafile ../../dataset/OFA_data/vrd2_balanced/vg_train_full.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/vrd2_balanced/vg_train_full.tsv slice_id 1 row count 268254 total row count 804762
local datafile ../../dataset/OFA_data/vrd2_balanced/vg_train_full.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/vrd2_balanced/vg_train_full.tsv slice_id 2 row count 268254 total row count 804762
slice_id 2 seek offset 536508
slice_id 1 seek offset 268254
2023-08-02 19:40:45 - trainer.py[line:441] - INFO: Finished saving checkpoint to ../../checkpoints/OFA/vrd2_checkpoints/_30_3e-5_512_balanced/checkpoint2.pt
2023-08-02 19:40:47 - checkpoint_utils.py[line:133] - INFO: Saved checkpoint ../../checkpoints/OFA/vrd2_checkpoints/_30_3e-5_512_balanced/checkpoint2.pt (epoch 2 @ 11158 updates, score None) (writing took 5.661803069990128 seconds)
2023-08-02 19:40:47 - train.py[line:332] - INFO: end of epoch 2 (average epoch stats below)
2023-08-02 19:40:47 - progress_bar.py[line:282] - INFO: epoch 002 | loss 1.697 | loss_v1 0 | loss_v2 0 | nll_loss 0.391 | ntokens 1055.64 | nsentences 143.99 | sample_size 1055.64 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.31 | wps 426.9 | ups 0.4 | wpb 1055.6 | bsz 144 | num_updates 11158 | lr 2.9791e-05 | gnorm 1.388 | clip 99.7 | loss_scale 128 | train_wall 13772 | gb_free 6.6 | wall 27602
2023-08-02 19:40:47 - trainer.py[line:639] - INFO: loading train data for epoch 3
local datafile ../../dataset/OFA_data/vrd2_balanced/vg_train_full.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/vrd2_balanced/vg_train_full.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/vrd2_balanced/vg_train_full.tsv slice_id 0 row count 268254 total row count 804762
slice_id 0 seek offset 0
2023-08-02 19:40:48 - trainer.py[line:703] - INFO: begin training epoch 3
2023-08-02 19:40:48 - train.py[line:305] - INFO: Start iterating over samples
2023-08-02 19:40:53 - progress_bar.py[line:272] - INFO: epoch 003:      2 / 5589 loss=1.654, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=1031.1, nsentences=140.4, sample_size=1031.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=338.3, ups=0.33, wpb=1031.1, bsz=140.4, num_updates=11160, lr=2.97906e-05, gnorm=1.283, clip=100, loss_scale=128, train_wall=24, gb_free=6.6, wall=27607
2023-08-02 19:41:18 - progress_bar.py[line:272] - INFO: epoch 003:     12 / 5589 loss=1.641, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=1058, nsentences=144, sample_size=1058, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=427.6, ups=0.4, wpb=1058, bsz=144, num_updates=11170, lr=2.97887e-05, gnorm=1.354, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=27632
2023-08-02 19:41:43 - progress_bar.py[line:272] - INFO: epoch 003:     22 / 5589 loss=1.645, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=1057, nsentences=144, sample_size=1057, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=427.8, ups=0.4, wpb=1057, bsz=144, num_updates=11180, lr=2.97868e-05, gnorm=1.397, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=27657
2023-08-02 19:42:07 - progress_bar.py[line:272] - INFO: epoch 003:     32 / 5589 loss=1.652, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=1051.6, nsentences=144, sample_size=1051.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=425.6, ups=0.4, wpb=1051.6, bsz=144, num_updates=11190, lr=2.97849e-05, gnorm=1.496, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=27682
2023-08-02 19:42:32 - progress_bar.py[line:272] - INFO: epoch 003:     42 / 5589 loss=1.652, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=1059.3, nsentences=144, sample_size=1059.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=429.3, ups=0.41, wpb=1059.3, bsz=144, num_updates=11200, lr=2.9783e-05, gnorm=1.448, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=27706
2023-08-02 19:42:57 - progress_bar.py[line:272] - INFO: epoch 003:     52 / 5589 loss=1.645, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=1056.5, nsentences=144, sample_size=1056.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=428.8, ups=0.41, wpb=1056.5, bsz=144, num_updates=11210, lr=2.97811e-05, gnorm=1.32, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=27731
2023-08-02 19:43:21 - progress_bar.py[line:272] - INFO: epoch 003:     62 / 5589 loss=1.649, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=1053.1, nsentences=144, sample_size=1053.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=427.4, ups=0.41, wpb=1053.1, bsz=144, num_updates=11220, lr=2.97792e-05, gnorm=1.363, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=27756
2023-08-02 19:43:46 - progress_bar.py[line:272] - INFO: epoch 003:     72 / 5589 loss=1.64, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=1054.6, nsentences=144, sample_size=1054.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=428.3, ups=0.41, wpb=1054.6, bsz=144, num_updates=11230, lr=2.97773e-05, gnorm=1.299, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=27780
2023-08-02 19:44:11 - progress_bar.py[line:272] - INFO: epoch 003:     82 / 5589 loss=1.653, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=1055.5, nsentences=144, sample_size=1055.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=428.3, ups=0.41, wpb=1055.5, bsz=144, num_updates=11240, lr=2.97754e-05, gnorm=1.248, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=27805
2023-08-02 19:44:35 - progress_bar.py[line:272] - INFO: epoch 003:     92 / 5589 loss=1.651, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=1056.6, nsentences=144, sample_size=1056.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=429.1, ups=0.41, wpb=1056.6, bsz=144, num_updates=11250, lr=2.97735e-05, gnorm=1.38, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=27829
2023-08-02 19:45:00 - progress_bar.py[line:272] - INFO: epoch 003:    102 / 5589 loss=1.65, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=1057.8, nsentences=144, sample_size=1057.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=429.5, ups=0.41, wpb=1057.8, bsz=144, num_updates=11260, lr=2.97716e-05, gnorm=1.364, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=27854
2023-08-02 19:45:24 - progress_bar.py[line:272] - INFO: epoch 003:    112 / 5589 loss=1.639, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=1051.5, nsentences=144, sample_size=1051.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=427.1, ups=0.41, wpb=1051.5, bsz=144, num_updates=11270, lr=2.97697e-05, gnorm=1.255, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=27879
2023-08-02 19:45:49 - progress_bar.py[line:272] - INFO: epoch 003:    122 / 5589 loss=1.647, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=1058.6, nsentences=144, sample_size=1058.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=430.1, ups=0.41, wpb=1058.6, bsz=144, num_updates=11280, lr=2.97678e-05, gnorm=1.44, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=27903
2023-08-02 19:46:14 - progress_bar.py[line:272] - INFO: epoch 003:    132 / 5589 loss=1.65, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=1053.9, nsentences=144, sample_size=1053.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=428.2, ups=0.41, wpb=1053.9, bsz=144, num_updates=11290, lr=2.97659e-05, gnorm=1.325, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=27928
2023-08-02 19:46:38 - progress_bar.py[line:272] - INFO: epoch 003:    142 / 5589 loss=1.642, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=1058.7, nsentences=144, sample_size=1058.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=430, ups=0.41, wpb=1058.7, bsz=144, num_updates=11300, lr=2.9764e-05, gnorm=1.315, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=27953
2023-08-02 19:47:03 - progress_bar.py[line:272] - INFO: epoch 003:    152 / 5589 loss=1.647, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=1057.2, nsentences=144, sample_size=1057.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=429.5, ups=0.41, wpb=1057.2, bsz=144, num_updates=11310, lr=2.97621e-05, gnorm=1.34, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=27977
2023-08-02 19:47:28 - progress_bar.py[line:272] - INFO: epoch 003:    162 / 5589 loss=1.654, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=1055.2, nsentences=144, sample_size=1055.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=428.5, ups=0.41, wpb=1055.2, bsz=144, num_updates=11320, lr=2.97602e-05, gnorm=1.424, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28002
2023-08-02 19:47:52 - progress_bar.py[line:272] - INFO: epoch 003:    172 / 5589 loss=1.642, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=1057.1, nsentences=144, sample_size=1057.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=429.3, ups=0.41, wpb=1057.1, bsz=144, num_updates=11330, lr=2.97583e-05, gnorm=1.31, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28026
2023-08-02 19:48:17 - progress_bar.py[line:272] - INFO: epoch 003:    182 / 5589 loss=1.638, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=1055.7, nsentences=144, sample_size=1055.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=428.6, ups=0.41, wpb=1055.7, bsz=144, num_updates=11340, lr=2.97564e-05, gnorm=1.344, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28051
2023-08-02 19:48:41 - progress_bar.py[line:272] - INFO: epoch 003:    192 / 5589 loss=1.646, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=1055.6, nsentences=144, sample_size=1055.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=428.6, ups=0.41, wpb=1055.6, bsz=144, num_updates=11350, lr=2.97545e-05, gnorm=1.304, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28076
2023-08-02 19:49:06 - progress_bar.py[line:272] - INFO: epoch 003:    202 / 5589 loss=1.641, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=1056.4, nsentences=144, sample_size=1056.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=429, ups=0.41, wpb=1056.4, bsz=144, num_updates=11360, lr=2.97526e-05, gnorm=1.394, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28100
2023-08-02 19:49:31 - progress_bar.py[line:272] - INFO: epoch 003:    212 / 5589 loss=1.647, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=1056.3, nsentences=144, sample_size=1056.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=428.6, ups=0.41, wpb=1056.3, bsz=144, num_updates=11370, lr=2.97507e-05, gnorm=1.365, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28125
2023-08-02 19:49:55 - progress_bar.py[line:272] - INFO: epoch 003:    222 / 5589 loss=1.653, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=1056, nsentences=144, sample_size=1056, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=428.6, ups=0.41, wpb=1056, bsz=144, num_updates=11380, lr=2.97487e-05, gnorm=1.368, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28150
2023-08-02 19:50:20 - progress_bar.py[line:272] - INFO: epoch 003:    232 / 5589 loss=1.637, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=1055.8, nsentences=144, sample_size=1055.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=428.9, ups=0.41, wpb=1055.8, bsz=144, num_updates=11390, lr=2.97468e-05, gnorm=1.253, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28174
2023-08-02 19:50:45 - progress_bar.py[line:272] - INFO: epoch 003:    242 / 5589 loss=1.654, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=1055.1, nsentences=144, sample_size=1055.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=428.6, ups=0.41, wpb=1055.1, bsz=144, num_updates=11400, lr=2.97449e-05, gnorm=1.351, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28199
2023-08-02 19:51:09 - progress_bar.py[line:272] - INFO: epoch 003:    252 / 5589 loss=1.648, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=1054.8, nsentences=144, sample_size=1054.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=428.5, ups=0.41, wpb=1054.8, bsz=144, num_updates=11410, lr=2.9743e-05, gnorm=1.338, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28223
2023-08-02 19:51:34 - progress_bar.py[line:272] - INFO: epoch 003:    262 / 5589 loss=1.652, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=1055.9, nsentences=144, sample_size=1055.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=429.2, ups=0.41, wpb=1055.9, bsz=144, num_updates=11420, lr=2.97411e-05, gnorm=1.377, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28248
2023-08-02 19:51:58 - progress_bar.py[line:272] - INFO: epoch 003:    272 / 5589 loss=1.649, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=1053.4, nsentences=144, sample_size=1053.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=427.9, ups=0.41, wpb=1053.4, bsz=144, num_updates=11430, lr=2.97392e-05, gnorm=1.399, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28273
2023-08-02 19:52:23 - progress_bar.py[line:272] - INFO: epoch 003:    282 / 5589 loss=1.644, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=1054, nsentences=144, sample_size=1054, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=428.3, ups=0.41, wpb=1054, bsz=144, num_updates=11440, lr=2.97373e-05, gnorm=1.31, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28297
2023-08-02 19:52:48 - progress_bar.py[line:272] - INFO: epoch 003:    292 / 5589 loss=1.65, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=1053.4, nsentences=144, sample_size=1053.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=428, ups=0.41, wpb=1053.4, bsz=144, num_updates=11450, lr=2.97354e-05, gnorm=1.393, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28322
2023-08-02 19:53:12 - progress_bar.py[line:272] - INFO: epoch 003:    302 / 5589 loss=1.642, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=1055.7, nsentences=144, sample_size=1055.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=428.9, ups=0.41, wpb=1055.7, bsz=144, num_updates=11460, lr=2.97335e-05, gnorm=1.417, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28347
2023-08-02 19:53:37 - progress_bar.py[line:272] - INFO: epoch 003:    312 / 5589 loss=1.642, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=1056.1, nsentences=144, sample_size=1056.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=429.2, ups=0.41, wpb=1056.1, bsz=144, num_updates=11470, lr=2.97316e-05, gnorm=1.372, clip=90, loss_scale=128, train_wall=25, gb_free=6.6, wall=28371
2023-08-02 19:54:01 - progress_bar.py[line:272] - INFO: epoch 003:    322 / 5589 loss=1.64, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=1053.7, nsentences=144, sample_size=1053.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=428.1, ups=0.41, wpb=1053.7, bsz=144, num_updates=11480, lr=2.97297e-05, gnorm=1.406, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28396
2023-08-02 19:54:26 - progress_bar.py[line:272] - INFO: epoch 003:    332 / 5589 loss=1.628, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=1062.7, nsentences=144, sample_size=1062.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=431.8, ups=0.41, wpb=1062.7, bsz=144, num_updates=11490, lr=2.97278e-05, gnorm=1.333, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28420
2023-08-02 19:54:51 - progress_bar.py[line:272] - INFO: epoch 003:    342 / 5589 loss=1.645, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=1054.1, nsentences=144, sample_size=1054.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=428.1, ups=0.41, wpb=1054.1, bsz=144, num_updates=11500, lr=2.97259e-05, gnorm=1.432, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28445
2023-08-02 19:55:15 - progress_bar.py[line:272] - INFO: epoch 003:    352 / 5589 loss=1.647, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=1056, nsentences=144, sample_size=1056, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=429.1, ups=0.41, wpb=1056, bsz=144, num_updates=11510, lr=2.9724e-05, gnorm=1.45, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28470
2023-08-02 19:55:40 - progress_bar.py[line:272] - INFO: epoch 003:    362 / 5589 loss=1.649, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=1057.4, nsentences=144, sample_size=1057.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=429.6, ups=0.41, wpb=1057.4, bsz=144, num_updates=11520, lr=2.97221e-05, gnorm=1.377, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28494
2023-08-02 19:56:05 - progress_bar.py[line:272] - INFO: epoch 003:    372 / 5589 loss=1.635, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=1052.8, nsentences=144, sample_size=1052.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=427.7, ups=0.41, wpb=1052.8, bsz=144, num_updates=11530, lr=2.97202e-05, gnorm=1.239, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28519
2023-08-02 19:56:29 - progress_bar.py[line:272] - INFO: epoch 003:    382 / 5589 loss=1.653, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=1055.1, nsentences=144, sample_size=1055.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=428.2, ups=0.41, wpb=1055.1, bsz=144, num_updates=11540, lr=2.97183e-05, gnorm=1.39, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=28543
2023-08-02 19:56:54 - progress_bar.py[line:272] - INFO: epoch 003:    392 / 5589 loss=1.644, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=1056.8, nsentences=144, sample_size=1056.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=428.8, ups=0.41, wpb=1056.8, bsz=144, num_updates=11550, lr=2.97164e-05, gnorm=1.453, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=28568
2023-08-02 19:57:19 - progress_bar.py[line:272] - INFO: epoch 003:    402 / 5589 loss=1.643, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=1057.3, nsentences=144, sample_size=1057.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=429, ups=0.41, wpb=1057.3, bsz=144, num_updates=11560, lr=2.97145e-05, gnorm=1.294, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=28593
2023-08-02 19:57:43 - progress_bar.py[line:272] - INFO: epoch 003:    412 / 5589 loss=1.638, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=1058.6, nsentences=144, sample_size=1058.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=429.6, ups=0.41, wpb=1058.6, bsz=144, num_updates=11570, lr=2.97126e-05, gnorm=1.416, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=28617
2023-08-02 19:58:08 - progress_bar.py[line:272] - INFO: epoch 003:    422 / 5589 loss=1.649, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=1055, nsentences=144, sample_size=1055, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=428.1, ups=0.41, wpb=1055, bsz=144, num_updates=11580, lr=2.97107e-05, gnorm=1.402, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=28642
2023-08-02 19:58:32 - progress_bar.py[line:272] - INFO: epoch 003:    432 / 5589 loss=1.636, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=1053.7, nsentences=144, sample_size=1053.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=427.7, ups=0.41, wpb=1053.7, bsz=144, num_updates=11590, lr=2.97088e-05, gnorm=1.31, clip=100, loss_scale=256, train_wall=25, gb_free=6.6, wall=28667
2023-08-02 19:58:57 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-08-02 19:59:00 - progress_bar.py[line:272] - INFO: epoch 003:    443 / 5589 loss=1.629, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=1056.5, nsentences=144, sample_size=1056.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=390.1, ups=0.37, wpb=1056.5, bsz=144, num_updates=11600, lr=2.97069e-05, gnorm=1.385, clip=100, loss_scale=128, train_wall=27, gb_free=6.6, wall=28694
2023-08-02 19:59:24 - progress_bar.py[line:272] - INFO: epoch 003:    453 / 5589 loss=1.644, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=1049.1, nsentences=144, sample_size=1049.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=425.9, ups=0.41, wpb=1049.1, bsz=144, num_updates=11610, lr=2.9705e-05, gnorm=1.459, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28718
2023-08-02 19:59:49 - progress_bar.py[line:272] - INFO: epoch 003:    463 / 5589 loss=1.642, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=1055.7, nsentences=144, sample_size=1055.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=428.5, ups=0.41, wpb=1055.7, bsz=144, num_updates=11620, lr=2.97031e-05, gnorm=1.469, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28743
2023-08-02 20:00:13 - progress_bar.py[line:272] - INFO: epoch 003:    473 / 5589 loss=1.642, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=1060.2, nsentences=144, sample_size=1060.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=430.3, ups=0.41, wpb=1060.2, bsz=144, num_updates=11630, lr=2.97012e-05, gnorm=1.485, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28768
2023-08-02 20:00:38 - progress_bar.py[line:272] - INFO: epoch 003:    483 / 5589 loss=1.645, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=1053.1, nsentences=144, sample_size=1053.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=427.4, ups=0.41, wpb=1053.1, bsz=144, num_updates=11640, lr=2.96993e-05, gnorm=1.459, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28792
2023-08-02 20:01:03 - progress_bar.py[line:272] - INFO: epoch 003:    493 / 5589 loss=1.636, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=1054.9, nsentences=144, sample_size=1054.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=428.2, ups=0.41, wpb=1054.9, bsz=144, num_updates=11650, lr=2.96974e-05, gnorm=1.363, clip=90, loss_scale=128, train_wall=25, gb_free=6.6, wall=28817
2023-08-02 20:01:27 - progress_bar.py[line:272] - INFO: epoch 003:    503 / 5589 loss=1.639, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=1057.9, nsentences=144, sample_size=1057.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=429.4, ups=0.41, wpb=1057.9, bsz=144, num_updates=11660, lr=2.96955e-05, gnorm=1.407, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28842
2023-08-02 20:01:52 - progress_bar.py[line:272] - INFO: epoch 003:    513 / 5589 loss=1.637, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=1053.4, nsentences=144, sample_size=1053.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=427.6, ups=0.41, wpb=1053.4, bsz=144, num_updates=11670, lr=2.96935e-05, gnorm=1.304, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28866
2023-08-02 20:02:17 - progress_bar.py[line:272] - INFO: epoch 003:    523 / 5589 loss=1.635, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=1056.1, nsentences=144, sample_size=1056.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=428.7, ups=0.41, wpb=1056.1, bsz=144, num_updates=11680, lr=2.96916e-05, gnorm=1.315, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28891
2023-08-02 20:02:41 - progress_bar.py[line:272] - INFO: epoch 003:    533 / 5589 loss=1.647, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=1058.9, nsentences=144, sample_size=1058.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=429.8, ups=0.41, wpb=1058.9, bsz=144, num_updates=11690, lr=2.96897e-05, gnorm=1.471, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28916
2023-08-02 20:03:06 - progress_bar.py[line:272] - INFO: epoch 003:    543 / 5589 loss=1.644, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=1051.4, nsentences=144, sample_size=1051.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=426.5, ups=0.41, wpb=1051.4, bsz=144, num_updates=11700, lr=2.96878e-05, gnorm=1.437, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28940
2023-08-02 20:03:31 - progress_bar.py[line:272] - INFO: epoch 003:    553 / 5589 loss=1.651, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=1060.3, nsentences=144, sample_size=1060.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=429.4, ups=0.4, wpb=1060.3, bsz=144, num_updates=11710, lr=2.96859e-05, gnorm=1.479, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28965
2023-08-02 20:03:55 - progress_bar.py[line:272] - INFO: epoch 003:    563 / 5589 loss=1.637, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=1058.3, nsentences=144, sample_size=1058.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=428.8, ups=0.41, wpb=1058.3, bsz=144, num_updates=11720, lr=2.9684e-05, gnorm=1.329, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=28990
2023-08-02 20:04:20 - progress_bar.py[line:272] - INFO: epoch 003:    573 / 5589 loss=1.636, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=1054.6, nsentences=144, sample_size=1054.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=427.2, ups=0.41, wpb=1054.6, bsz=144, num_updates=11730, lr=2.96821e-05, gnorm=1.326, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=29014
2023-08-02 20:04:45 - progress_bar.py[line:272] - INFO: epoch 003:    583 / 5589 loss=1.636, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=1054, nsentences=144, sample_size=1054, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=427.2, ups=0.41, wpb=1054, bsz=144, num_updates=11740, lr=2.96802e-05, gnorm=1.446, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=29039
2023-08-02 20:05:09 - progress_bar.py[line:272] - INFO: epoch 003:    593 / 5589 loss=1.647, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=1058.1, nsentences=144, sample_size=1058.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=428.7, ups=0.41, wpb=1058.1, bsz=144, num_updates=11750, lr=2.96783e-05, gnorm=1.238, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=29064
2023-08-02 20:05:34 - progress_bar.py[line:272] - INFO: epoch 003:    603 / 5589 loss=1.651, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=1053.4, nsentences=144, sample_size=1053.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=426.9, ups=0.41, wpb=1053.4, bsz=144, num_updates=11760, lr=2.96764e-05, gnorm=1.385, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=29088
2023-08-02 20:05:59 - progress_bar.py[line:272] - INFO: epoch 003:    613 / 5589 loss=1.636, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=1060.5, nsentences=144, sample_size=1060.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=429.5, ups=0.4, wpb=1060.5, bsz=144, num_updates=11770, lr=2.96745e-05, gnorm=1.296, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=29113
2023-08-02 20:06:23 - progress_bar.py[line:272] - INFO: epoch 003:    623 / 5589 loss=1.641, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=1056, nsentences=144, sample_size=1056, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=427.9, ups=0.41, wpb=1056, bsz=144, num_updates=11780, lr=2.96726e-05, gnorm=1.404, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=29138
2023-08-02 20:06:48 - progress_bar.py[line:272] - INFO: epoch 003:    633 / 5589 loss=1.646, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=1056.7, nsentences=144, sample_size=1056.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=428.2, ups=0.41, wpb=1056.7, bsz=144, num_updates=11790, lr=2.96707e-05, gnorm=1.389, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=29162
2023-08-02 20:07:13 - progress_bar.py[line:272] - INFO: epoch 003:    643 / 5589 loss=1.65, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=1053.2, nsentences=144, sample_size=1053.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=426.5, ups=0.4, wpb=1053.2, bsz=144, num_updates=11800, lr=2.96688e-05, gnorm=1.409, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=29187
2023-08-02 20:07:37 - progress_bar.py[line:272] - INFO: epoch 003:    653 / 5589 loss=1.63, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=1054.9, nsentences=144, sample_size=1054.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=427.4, ups=0.41, wpb=1054.9, bsz=144, num_updates=11810, lr=2.96669e-05, gnorm=1.276, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=29212
2023-08-02 20:08:02 - progress_bar.py[line:272] - INFO: epoch 003:    663 / 5589 loss=1.64, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=1058.8, nsentences=144, sample_size=1058.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=429.1, ups=0.41, wpb=1058.8, bsz=144, num_updates=11820, lr=2.9665e-05, gnorm=1.402, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=29236
2023-08-02 20:08:27 - progress_bar.py[line:272] - INFO: epoch 003:    673 / 5589 loss=1.636, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=1056.4, nsentences=144, sample_size=1056.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=428.2, ups=0.41, wpb=1056.4, bsz=144, num_updates=11830, lr=2.96631e-05, gnorm=1.376, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=29261
2023-08-02 20:08:52 - progress_bar.py[line:272] - INFO: epoch 003:    683 / 5589 loss=1.644, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=1060.5, nsentences=144, sample_size=1060.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=429.6, ups=0.41, wpb=1060.5, bsz=144, num_updates=11840, lr=2.96612e-05, gnorm=1.393, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=29286
2023-08-02 20:09:16 - progress_bar.py[line:272] - INFO: epoch 003:    693 / 5589 loss=1.639, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=1058.1, nsentences=144, sample_size=1058.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=428.8, ups=0.41, wpb=1058.1, bsz=144, num_updates=11850, lr=2.96593e-05, gnorm=1.355, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=29310
2023-08-02 20:09:41 - progress_bar.py[line:272] - INFO: epoch 003:    703 / 5589 loss=1.641, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=1052.3, nsentences=144, sample_size=1052.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=426.7, ups=0.41, wpb=1052.3, bsz=144, num_updates=11860, lr=2.96574e-05, gnorm=1.395, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=29335
2023-08-02 20:10:06 - progress_bar.py[line:272] - INFO: epoch 003:    713 / 5589 loss=1.635, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=1053, nsentences=144, sample_size=1053, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=427.3, ups=0.41, wpb=1053, bsz=144, num_updates=11870, lr=2.96555e-05, gnorm=1.33, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=29360
2023-08-02 20:10:30 - progress_bar.py[line:272] - INFO: epoch 003:    723 / 5589 loss=1.649, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=1057.7, nsentences=144, sample_size=1057.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=429.3, ups=0.41, wpb=1057.7, bsz=144, num_updates=11880, lr=2.96536e-05, gnorm=1.405, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=29384
2023-08-02 20:10:55 - progress_bar.py[line:272] - INFO: epoch 003:    733 / 5589 loss=1.63, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=1054, nsentences=144, sample_size=1054, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=427.8, ups=0.41, wpb=1054, bsz=144, num_updates=11890, lr=2.96517e-05, gnorm=1.382, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=29409
2023-08-02 20:11:19 - progress_bar.py[line:272] - INFO: epoch 003:    743 / 5589 loss=1.642, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=1053.9, nsentences=144, sample_size=1053.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=427.7, ups=0.41, wpb=1053.9, bsz=144, num_updates=11900, lr=2.96498e-05, gnorm=1.366, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=29434
2023-08-02 20:11:44 - progress_bar.py[line:272] - INFO: epoch 003:    753 / 5589 loss=1.641, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=1057, nsentences=144, sample_size=1057, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=428.8, ups=0.41, wpb=1057, bsz=144, num_updates=11910, lr=2.96479e-05, gnorm=1.473, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=29458
2023-08-02 20:12:09 - progress_bar.py[line:272] - INFO: epoch 003:    763 / 5589 loss=1.648, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=1056.5, nsentences=144, sample_size=1056.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=428.3, ups=0.41, wpb=1056.5, bsz=144, num_updates=11920, lr=2.9646e-05, gnorm=1.33, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=29483
2023-08-02 20:12:33 - progress_bar.py[line:272] - INFO: epoch 003:    773 / 5589 loss=1.636, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=1055.4, nsentences=144, sample_size=1055.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=427.5, ups=0.41, wpb=1055.4, bsz=144, num_updates=11930, lr=2.96441e-05, gnorm=1.391, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=29508
2023-08-02 20:12:58 - progress_bar.py[line:272] - INFO: epoch 003:    783 / 5589 loss=1.637, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=1055.1, nsentences=144, sample_size=1055.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=427.5, ups=0.41, wpb=1055.1, bsz=144, num_updates=11940, lr=2.96422e-05, gnorm=1.304, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=29532
2023-08-02 20:13:23 - progress_bar.py[line:272] - INFO: epoch 003:    793 / 5589 loss=1.629, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=1054.4, nsentences=144, sample_size=1054.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=427.6, ups=0.41, wpb=1054.4, bsz=144, num_updates=11950, lr=2.96403e-05, gnorm=1.351, clip=90, loss_scale=128, train_wall=25, gb_free=6.6, wall=29557
2023-08-02 20:13:47 - progress_bar.py[line:272] - INFO: epoch 003:    803 / 5589 loss=1.638, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=1057.4, nsentences=144, sample_size=1057.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=429, ups=0.41, wpb=1057.4, bsz=144, num_updates=11960, lr=2.96383e-05, gnorm=1.454, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=29582
2023-08-02 20:14:12 - progress_bar.py[line:272] - INFO: epoch 003:    813 / 5589 loss=1.629, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=1057.4, nsentences=144, sample_size=1057.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=429, ups=0.41, wpb=1057.4, bsz=144, num_updates=11970, lr=2.96364e-05, gnorm=1.341, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=29606
2023-08-02 20:14:37 - progress_bar.py[line:272] - INFO: epoch 003:    823 / 5589 loss=1.627, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=1057.6, nsentences=144, sample_size=1057.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=429.4, ups=0.41, wpb=1057.6, bsz=144, num_updates=11980, lr=2.96345e-05, gnorm=1.351, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=29631
2023-08-02 20:15:01 - progress_bar.py[line:272] - INFO: epoch 003:    833 / 5589 loss=1.63, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=1057.4, nsentences=144, sample_size=1057.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=429.1, ups=0.41, wpb=1057.4, bsz=144, num_updates=11990, lr=2.96326e-05, gnorm=1.421, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=29656
2023-08-02 20:15:26 - progress_bar.py[line:272] - INFO: epoch 003:    843 / 5589 loss=1.626, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=1057.6, nsentences=144, sample_size=1057.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=428.8, ups=0.41, wpb=1057.6, bsz=144, num_updates=12000, lr=2.96307e-05, gnorm=1.302, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=29680
2023-08-02 20:15:51 - progress_bar.py[line:272] - INFO: epoch 003:    853 / 5589 loss=1.648, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=1052, nsentences=144, sample_size=1052, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=425.5, ups=0.4, wpb=1052, bsz=144, num_updates=12010, lr=2.96288e-05, gnorm=1.482, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=29705
2023-08-02 20:16:15 - progress_bar.py[line:272] - INFO: epoch 003:    863 / 5589 loss=1.632, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=1053.1, nsentences=144, sample_size=1053.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=426.3, ups=0.4, wpb=1053.1, bsz=144, num_updates=12020, lr=2.96269e-05, gnorm=1.472, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=29730
2023-08-02 20:16:40 - progress_bar.py[line:272] - INFO: epoch 003:    873 / 5589 loss=1.634, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=1059.3, nsentences=144, sample_size=1059.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=429.2, ups=0.41, wpb=1059.3, bsz=144, num_updates=12030, lr=2.9625e-05, gnorm=1.515, clip=100, loss_scale=128, train_wall=25, gb_free=6.6, wall=29754
WARNING:torch.distributed.elastic.agent.server.api:Received 2 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 657480 closing signal SIGINT
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 657481 closing signal SIGINT
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 657482 closing signal SIGINT
Traceback (most recent call last):
  File "../../train.py", line 539, in <module>
Traceback (most recent call last):
  File "../../train.py", line 539, in <module>
    cli_main()
  File "../../train.py", line 532, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/zcai75/Github/OFA/fairseq/fairseq/distributed/utils.py", line 374, in call_main
    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
  File "/home/zcai75/Github/OFA/fairseq/fairseq/distributed/utils.py", line 348, in distributed_main
    cli_main()
  File "../../train.py", line 532, in cli_main
    main(cfg, **kwargs)
  File "../../train.py", line 199, in main
    distributed_utils.call_main(cfg, main)
      File "/home/zcai75/Github/OFA/fairseq/fairseq/distributed/utils.py", line 374, in call_main
valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
      File "../../train.py", line 310, in train
distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
  File "/home/zcai75/Github/OFA/fairseq/fairseq/distributed/utils.py", line 348, in distributed_main
    log_output = trainer.train_step(samples)
      File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/contextlib.py", line 75, in inner
main(cfg, **kwargs)
  File "../../train.py", line 199, in main
    return func(*args, **kwds)
  File "/home/zcai75/Github/OFA_forked/trainer.py", line 773, in train_step
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "../../train.py", line 310, in train
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/home/zcai75/Github/OFA_forked/tasks/ofa_task.py", line 338, in train_step
    log_output = trainer.train_step(samples)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zcai75/Github/OFA_forked/trainer.py", line 773, in train_step
    optimizer.backward(loss)
  File "/home/zcai75/Github/OFA/fairseq/fairseq/optim/fp16_optimizer.py", line 105, in backward
    loss.backward()
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/home/zcai75/Github/OFA_forked/tasks/ofa_task.py", line 338, in train_step
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    optimizer.backward(loss)
  File "/home/zcai75/Github/OFA/fairseq/fairseq/optim/fp16_optimizer.py", line 105, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt    loss.backward()

  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Traceback (most recent call last):
  File "../../train.py", line 539, in <module>
    cli_main()
  File "../../train.py", line 532, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/zcai75/Github/OFA/fairseq/fairseq/distributed/utils.py", line 374, in call_main
    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
  File "/home/zcai75/Github/OFA/fairseq/fairseq/distributed/utils.py", line 348, in distributed_main
    main(cfg, **kwargs)
  File "../../train.py", line 199, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "../../train.py", line 310, in train
    log_output = trainer.train_step(samples)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zcai75/Github/OFA_forked/trainer.py", line 773, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/home/zcai75/Github/OFA_forked/tasks/ofa_task.py", line 338, in train_step
    optimizer.backward(loss)
  File "/home/zcai75/Github/OFA/fairseq/fairseq/optim/fp16_optimizer.py", line 105, in backward
    loss.backward()
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
wandb: Waiting for W&B process to finish... (failed 255). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:                  train/bsz ▁▁
wandb:                 train/clip █▁
wandb:              train/gb_free ▁▁
wandb:                train/gnorm █▁
wandb:                 train/loss █▁
wandb:           train/loss_scale ▁▁
wandb:              train/loss_v1 ▁▁
wandb:              train/loss_v2 ▁▁
wandb:                   train/lr ▁█
wandb:             train/nll_loss █▁
wandb:           train/nsentences ▁▁
wandb:              train/ntokens █▁
wandb:                  train/ppl █▁
wandb:          train/sample_size █▁
wandb:       train/sample_size_v1 ▁▁
wandb:       train/sample_size_v2 ▁▁
wandb:           train/train_wall █▁
wandb:                  train/ups ▁▁
wandb:                 train/wall ▁█
wandb:                  train/wpb ▁▁
wandb:                  train/wps █▁
wandb:            train_inner/bsz ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:           train_inner/clip ███████████████████████████▁████████████
wandb:        train_inner/gb_free ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train_inner/gnorm █▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:           train_inner/loss █▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_inner/loss_scale ▁▁▁▁▂▃▃▄█▄▄▄▄▄▄▄▄▄▄█▄▄▄█▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄
wandb:        train_inner/loss_v1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        train_inner/loss_v2 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:             train_inner/lr ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇█████████
wandb:       train_inner/nll_loss █▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_inner/nsentences ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        train_inner/ntokens ▅▅▄▄▅▇▇▁▄▅▅▄▃▂▄▇▅▃█▆▇▇▇▅▄▅▆▆▇▇█▆▅██▃▅▆▆▇
wandb:            train_inner/ppl █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    train_inner/sample_size ▅▅▄▄▅▇▇▁▄▅▅▄▃▂▄▇▅▃█▆▇▇▇▅▄▅▆▆▇▇█▆▅██▃▅▆▆▇
wandb: train_inner/sample_size_v1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_inner/sample_size_v2 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_inner/train_wall ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            train_inner/ups █▁████▁██████▁█▁█████▁▁█▁███▁███████████
wandb:           train_inner/wall ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:            train_inner/wpb ▅▅▄▄▅▇▇▁▄▅▅▄▃▂▄▇▅▃█▆▇▇▇▅▄▅▆▆▇▇█▆▅██▃▅▆▆▇
wandb:            train_inner/wps ▆▅▅▄▅▇▆▂▅▆▅▅▄▁▅▆▆▄█▆▇▇▆▅▄▆▆▇▆▆█▇▇▇█▄▅▇▇▇
wandb: 
wandb: Run summary:
wandb:                  train/bsz 144.0
wandb:                 train/clip 99.7
wandb:              train/gb_free 6.6
wandb:                train/gnorm 1.388
wandb:                 train/loss 1.697
wandb:           train/loss_scale 128.0
wandb:              train/loss_v1 0.0
wandb:              train/loss_v2 0.0
wandb:                   train/lr 3e-05
wandb:             train/nll_loss 0.391
wandb:           train/nsentences 143.99
wandb:              train/ntokens 1055.637
wandb:                  train/ppl 1.31
wandb:          train/sample_size 1055.637
wandb:       train/sample_size_v1 0.0
wandb:       train/sample_size_v2 0.0
wandb:           train/train_wall 13772.0
wandb:                  train/ups 0.4
wandb:                 train/wall 27602.0
wandb:                  train/wpb 1055.6
wandb:                  train/wps 426.9
wandb:            train_inner/bsz 144.0
wandb:           train_inner/clip 100.0
wandb:        train_inner/gb_free 6.6
wandb:          train_inner/gnorm 1.515
wandb:           train_inner/loss 1.634
wandb:     train_inner/loss_scale 128.0
wandb:        train_inner/loss_v1 0.0
wandb:        train_inner/loss_v2 0.0
wandb:             train_inner/lr 3e-05
wandb:       train_inner/nll_loss 0.327
wandb:     train_inner/nsentences 144.0
wandb:        train_inner/ntokens 1059.3
wandb:            train_inner/ppl 1.25
wandb:    train_inner/sample_size 1059.3
wandb: train_inner/sample_size_v1 0.0
wandb: train_inner/sample_size_v2 0.0
wandb:     train_inner/train_wall 25.0
wandb:            train_inner/ups 0.41
wandb:           train_inner/wall 29754.0
wandb:            train_inner/wpb 1059.3
wandb:            train_inner/wps 429.2
wandb: 
wandb: 🚀 View run _30_3e-5_512_balanced at: https://wandb.ai/jackcai1206/OFA-VG/runs/17tr21jj
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230802_120050-17tr21jj/logs
Traceback (most recent call last):
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 850, in _invoke_run
    time.sleep(monitor_interval)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 60, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 657442 got signal: 2

/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-08-01 16:21:08 - utils.py[line:255] - INFO: distributed init (rank 0): env://
2023-08-01 16:21:08 - utils.py[line:261] - INFO: Start init
2023-08-01 16:21:08 - distributed_c10d.py[line:228] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-08-01 16:21:08 - utils.py[line:255] - INFO: distributed init (rank 2): env://
2023-08-01 16:21:08 - utils.py[line:261] - INFO: Start init
2023-08-01 16:21:08 - distributed_c10d.py[line:228] - INFO: Added key: store_based_barrier_key:1 to store for rank: 2
2023-08-01 16:21:08 - utils.py[line:255] - INFO: distributed init (rank 1): env://
2023-08-01 16:21:08 - utils.py[line:261] - INFO: Start init
2023-08-01 16:21:08 - distributed_c10d.py[line:228] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-08-01 16:21:08 - distributed_c10d.py[line:262] - INFO: Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 3 nodes.
2023-08-01 16:21:08 - utils.py[line:271] - INFO: initialized host AMD4RTX3090GPU14 as rank 1
single-machine distributed training is initialized.
2023-08-01 16:21:08 - distributed_c10d.py[line:262] - INFO: Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 3 nodes.
2023-08-01 16:21:08 - utils.py[line:271] - INFO: initialized host AMD4RTX3090GPU14 as rank 2
single-machine distributed training is initialized.
2023-08-01 16:21:08 - distributed_c10d.py[line:262] - INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 3 nodes.
2023-08-01 16:21:08 - utils.py[line:271] - INFO: initialized host AMD4RTX3090GPU14 as rank 0
single-machine distributed training is initialized.
2023-08-01 16:21:09 - train.py[line:77] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './tensorboard/_4_3e-5_512', 'wandb_project': 'OFA-VG', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 2097152, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 3, 'distributed_num_procs': 3, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 3, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 12, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 12, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 4, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [4], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '../../checkpoints/OFA/vrd2_checkpoints/_4_3e-5_512_mix_wearing', 'restore_file': '../../checkpoints/OFA/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 2, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 3}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_type_embedding=True, all_gather_list_size=2097152, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=12, batch_size_valid=12, best_checkpoint_metric='loss', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='../../dataset/OFA_data/vrd_mix/vg_train_wearing.tsv,../../dataset/OFA_data/vrd_mix/vg_val_wearing.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=3, distributed_port=-1, distributed_rank=0, distributed_world_size=3, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"max_len_a":0,"max_len_b":200}', eval_print_samples=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=False, freeze_encoder_embedding=False, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=4, max_source_positions=1024, max_src_length=100, max_target_positions=1024, max_tgt_length=100, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=3, num_bins=1000, num_shards=1, num_workers=0, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=512, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='../../checkpoints/OFA/ofa_base.pt', sample_patch_num=196, save_dir='../../checkpoints/OFA/vrd2_checkpoints/_4_3e-5_512_mix_wearing', save_interval=2, save_interval_updates=0, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols=None, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, simul_type=None, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, sync_bn=False, task='vrd2', tensorboard_logdir='./tensorboard/_4_3e-5_512', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[4], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=0, vg_json_dir=None, wandb_project='OFA-VG', warmup_ratio=0.06, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vrd2', 'data': '../../dataset/OFA_data/vrd_mix/vg_train_wearing.tsv,../../dataset/OFA_data/vrd_mix/vg_val_wearing.tsv', 'selected_cols': None, 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 100, 'max_tgt_length': 100, 'code_dict_size': 8192, 'patch_image_size': 512, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'eval_args': '{"beam":5,"max_len_a":0,"max_len_b":200}', 'eval_print_samples': False, 'vg_json_dir': None}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.06, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2023-08-01 16:21:09 - vrd2.py[line:90] - INFO: vrd setup: source dictionary: 59461 types
2023-08-01 16:21:09 - vrd2.py[line:91] - INFO: vrd setup: target dictionary: 59461 types
/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
2023-08-01 16:21:12 - train.py[line:110] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59461, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59461, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59461, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-08-01 16:21:12 - train.py[line:111] - INFO: task: VRD2Task
2023-08-01 16:21:12 - train.py[line:112] - INFO: model: OFAModel
2023-08-01 16:21:12 - train.py[line:113] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-08-01 16:21:12 - train.py[line:114] - INFO: num. shared model params: 182,241,608 (num. trained: 182,241,608)
2023-08-01 16:21:12 - train.py[line:121] - INFO: num. expert model params: 0 (num. trained: 0)
local datafile ../../dataset/OFA_data/vrd_mix/vg_val_wearing.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/vrd_mix/vg_val_wearing.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/vrd_mix/vg_val_wearing.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/vrd_mix/vg_val_wearing.tsv slice_id 0 row count 54498 total row count 163494
/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
local datafile ../../dataset/OFA_data/vrd_mix/vg_val_wearing.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/vrd_mix/vg_val_wearing.tsv slice_id 1 row count 54498 total row count 163494
/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
2023-08-01 16:21:12 - distributed_c10d.py[line:228] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
local datafile ../../dataset/OFA_data/vrd_mix/vg_val_wearing.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/vrd_mix/vg_val_wearing.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/vrd_mix/vg_val_wearing.tsv slice_id 2 row count 54498 total row count 163494
/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
2023-08-01 16:21:12 - distributed_c10d.py[line:262] - INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 3 nodes.
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-08-01 16:21:12 - trainer.py[line:123] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-08-01 16:21:12 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 3 workers***********************
2023-08-01 16:21:12 - utils.py[line:761] - INFO: rank   0: capabilities =  8.6  ; total memory = 23.688 GB ; name = NVIDIA GeForce RTX 3090 Ti              
2023-08-01 16:21:12 - utils.py[line:761] - INFO: rank   1: capabilities =  8.6  ; total memory = 23.688 GB ; name = NVIDIA GeForce RTX 3090 Ti              
2023-08-01 16:21:12 - utils.py[line:761] - INFO: rank   2: capabilities =  8.6  ; total memory = 23.688 GB ; name = NVIDIA GeForce RTX 3090 Ti              
2023-08-01 16:21:12 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 3 workers***********************
2023-08-01 16:21:12 - train.py[line:152] - INFO: training on 3 devices (GPUs/TPUs)
2023-08-01 16:21:12 - train.py[line:157] - INFO: max tokens per device = None and max sentences per device = 12
2023-08-01 16:21:12 - trainer.py[line:458] - INFO: Preparing to load checkpoint ../../checkpoints/OFA/ofa_base.pt
local datafile ../../dataset/OFA_data/vrd_mix/vg_train_wearing.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
2023-08-01 16:21:15 - trainer.py[line:617] - INFO: Loaded checkpoint ../../checkpoints/OFA/ofa_base.pt (epoch 48 @ 0 updates)
2023-08-01 16:21:15 - trainer.py[line:639] - INFO: loading train data for epoch 1
local datafile ../../dataset/OFA_data/vrd_mix/vg_train_wearing.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/vrd_mix/vg_train_wearing.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/vrd_mix/vg_train_wearing.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../dataset/OFA_data/vrd_mix/vg_train_wearing.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../dataset/OFA_data/vrd_mix/vg_train_wearing.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping


file ../../dataset/OFA_data/vrd_mix/vg_train_wearing.tsv slice_id 0 row count 131725 total row count 395175file ../../dataset/OFA_data/vrd_mix/vg_train_wearing.tsv slice_id 2 row count 131725 total row count 395175

file ../../dataset/OFA_data/vrd_mix/vg_train_wearing.tsv slice_id 1 row count 131725 total row count 395175
slice_id 1 seek offset 131725
Total steps 10980, warmup steps 658, warmup_factor 0.001519756838905775
slice_id 2 seek offset 263450
Total steps 10980, warmup steps 658, warmup_factor 0.001519756838905775
slice_id 0 seek offset 0
Total steps 10980, warmup steps 658, warmup_factor 0.001519756838905775
wandb: Currently logged in as: jackcai1206. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /home/zcai75/Github/OFA_forked/run_scripts/vrd_2/wandb/run-20230801_162116-t4mrlmdu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run _4_3e-5_512_mix_wearing
wandb: ⭐️ View project at https://wandb.ai/jackcai1206/OFA-VG
wandb: 🚀 View run at https://wandb.ai/jackcai1206/OFA-VG/runs/t4mrlmdu
2023-08-01 16:21:22 - trainer.py[line:703] - INFO: begin training epoch 1
2023-08-01 16:21:22 - train.py[line:305] - INFO: Start iterating over samples
2023-08-01 16:21:27 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-01 16:21:30 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-01 16:21:32 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-01 16:21:35 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-01 16:21:59 - progress_bar.py[line:272] - INFO: epoch 001:     14 / 2745 loss=12.54, loss_v1=0, loss_v2=0, nll_loss=11.868, ntokens=1015.4, nsentences=144, sample_size=1015.4, sample_size_v1=0, sample_size_v2=0, ppl=3737.11, wps=413.2, ups=0.41, wpb=1015.4, bsz=144, num_updates=10, lr=4.55927e-07, gnorm=42.414, clip=100, loss_scale=8, train_wall=37, gb_free=6.5, wall=47
2023-08-01 16:22:24 - progress_bar.py[line:272] - INFO: epoch 001:     24 / 2745 loss=12.452, loss_v1=0, loss_v2=0, nll_loss=11.775, ntokens=1014.4, nsentences=144, sample_size=1014.4, sample_size_v1=0, sample_size_v2=0, ppl=3505.57, wps=414.5, ups=0.41, wpb=1014.4, bsz=144, num_updates=20, lr=9.11854e-07, gnorm=41.458, clip=100, loss_scale=8, train_wall=24, gb_free=6.5, wall=72
2023-08-01 16:22:48 - progress_bar.py[line:272] - INFO: epoch 001:     34 / 2745 loss=12.07, loss_v1=0, loss_v2=0, nll_loss=11.394, ntokens=1015.3, nsentences=144, sample_size=1015.3, sample_size_v1=0, sample_size_v2=0, ppl=2690.69, wps=414.1, ups=0.41, wpb=1015.3, bsz=144, num_updates=30, lr=1.36778e-06, gnorm=37.357, clip=100, loss_scale=8, train_wall=24, gb_free=6.5, wall=96
2023-08-01 16:23:13 - progress_bar.py[line:272] - INFO: epoch 001:     44 / 2745 loss=11.36, loss_v1=0, loss_v2=0, nll_loss=10.676, ntokens=1014.1, nsentences=144, sample_size=1014.1, sample_size_v1=0, sample_size_v2=0, ppl=1636.07, wps=412.3, ups=0.41, wpb=1014.1, bsz=144, num_updates=40, lr=1.82371e-06, gnorm=30.97, clip=100, loss_scale=8, train_wall=25, gb_free=6.5, wall=121
2023-08-01 16:23:38 - progress_bar.py[line:272] - INFO: epoch 001:     54 / 2745 loss=10.409, loss_v1=0, loss_v2=0, nll_loss=9.735, ntokens=1016.4, nsentences=144, sample_size=1016.4, sample_size_v1=0, sample_size_v2=0, ppl=852.3, wps=415.5, ups=0.41, wpb=1016.4, bsz=144, num_updates=50, lr=2.27964e-06, gnorm=24.016, clip=100, loss_scale=8, train_wall=24, gb_free=6.5, wall=145
2023-08-01 16:24:02 - progress_bar.py[line:272] - INFO: epoch 001:     64 / 2745 loss=9.633, loss_v1=0, loss_v2=0, nll_loss=8.973, ntokens=1005.1, nsentences=144, sample_size=1005.1, sample_size_v1=0, sample_size_v2=0, ppl=502.39, wps=410.1, ups=0.41, wpb=1005.1, bsz=144, num_updates=60, lr=2.73556e-06, gnorm=19.326, clip=100, loss_scale=8, train_wall=24, gb_free=6.5, wall=170
2023-08-01 16:24:27 - progress_bar.py[line:272] - INFO: epoch 001:     74 / 2745 loss=8.697, loss_v1=0, loss_v2=0, nll_loss=8.02, ntokens=1018, nsentences=144, sample_size=1018, sample_size_v1=0, sample_size_v2=0, ppl=259.59, wps=415.5, ups=0.41, wpb=1018, bsz=144, num_updates=70, lr=3.19149e-06, gnorm=17.13, clip=100, loss_scale=8, train_wall=24, gb_free=6.5, wall=194
2023-08-01 16:24:51 - progress_bar.py[line:272] - INFO: epoch 001:     84 / 2745 loss=7.796, loss_v1=0, loss_v2=0, nll_loss=7.066, ntokens=1008.4, nsentences=144, sample_size=1008.4, sample_size_v1=0, sample_size_v2=0, ppl=134, wps=410.8, ups=0.41, wpb=1008.4, bsz=144, num_updates=80, lr=3.64742e-06, gnorm=15.258, clip=100, loss_scale=8, train_wall=25, gb_free=6.5, wall=219
2023-08-01 16:25:16 - progress_bar.py[line:272] - INFO: epoch 001:     94 / 2745 loss=6.87, loss_v1=0, loss_v2=0, nll_loss=6.053, ntokens=1013.2, nsentences=144, sample_size=1013.2, sample_size_v1=0, sample_size_v2=0, ppl=66.41, wps=412.8, ups=0.41, wpb=1013.2, bsz=144, num_updates=90, lr=4.10334e-06, gnorm=10.848, clip=100, loss_scale=8, train_wall=25, gb_free=6.5, wall=243
2023-08-01 16:25:40 - progress_bar.py[line:272] - INFO: epoch 001:    104 / 2745 loss=6.022, loss_v1=0, loss_v2=0, nll_loss=5.139, ntokens=1015.5, nsentences=144, sample_size=1015.5, sample_size_v1=0, sample_size_v2=0, ppl=35.24, wps=414.8, ups=0.41, wpb=1015.5, bsz=144, num_updates=100, lr=4.55927e-06, gnorm=9.1, clip=100, loss_scale=8, train_wall=24, gb_free=6.5, wall=268
2023-08-01 16:26:05 - progress_bar.py[line:272] - INFO: epoch 001:    114 / 2745 loss=5.389, loss_v1=0, loss_v2=0, nll_loss=4.349, ntokens=1014.2, nsentences=144, sample_size=1014.2, sample_size_v1=0, sample_size_v2=0, ppl=20.38, wps=414.2, ups=0.41, wpb=1014.2, bsz=144, num_updates=110, lr=5.0152e-06, gnorm=6.934, clip=100, loss_scale=8, train_wall=24, gb_free=6.5, wall=292
2023-08-01 16:26:29 - progress_bar.py[line:272] - INFO: epoch 001:    124 / 2745 loss=4.97, loss_v1=0, loss_v2=0, nll_loss=3.844, ntokens=1010.3, nsentences=144, sample_size=1010.3, sample_size_v1=0, sample_size_v2=0, ppl=14.36, wps=412.8, ups=0.41, wpb=1010.3, bsz=144, num_updates=120, lr=5.47112e-06, gnorm=6.169, clip=100, loss_scale=8, train_wall=24, gb_free=6.5, wall=317
2023-08-01 16:26:54 - progress_bar.py[line:272] - INFO: epoch 001:    134 / 2745 loss=4.731, loss_v1=0, loss_v2=0, nll_loss=3.555, ntokens=1017.3, nsentences=144, sample_size=1017.3, sample_size_v1=0, sample_size_v2=0, ppl=11.75, wps=416.5, ups=0.41, wpb=1017.3, bsz=144, num_updates=130, lr=5.92705e-06, gnorm=5.925, clip=100, loss_scale=8, train_wall=24, gb_free=6.5, wall=341
2023-08-01 16:27:18 - progress_bar.py[line:272] - INFO: epoch 001:    144 / 2745 loss=4.49, loss_v1=0, loss_v2=0, nll_loss=3.275, ntokens=1015.8, nsentences=144, sample_size=1015.8, sample_size_v1=0, sample_size_v2=0, ppl=9.68, wps=415.7, ups=0.41, wpb=1015.8, bsz=144, num_updates=140, lr=6.38298e-06, gnorm=5.935, clip=100, loss_scale=8, train_wall=24, gb_free=6.5, wall=366
2023-08-01 16:27:42 - progress_bar.py[line:272] - INFO: epoch 001:    154 / 2745 loss=4.312, loss_v1=0, loss_v2=0, nll_loss=3.058, ntokens=1010, nsentences=144, sample_size=1010, sample_size_v1=0, sample_size_v2=0, ppl=8.33, wps=412.7, ups=0.41, wpb=1010, bsz=144, num_updates=150, lr=6.83891e-06, gnorm=6.569, clip=100, loss_scale=8, train_wall=24, gb_free=6.5, wall=390
2023-08-01 16:28:07 - progress_bar.py[line:272] - INFO: epoch 001:    164 / 2745 loss=4.138, loss_v1=0, loss_v2=0, nll_loss=2.852, ntokens=1010.8, nsentences=144, sample_size=1010.8, sample_size_v1=0, sample_size_v2=0, ppl=7.22, wps=411.9, ups=0.41, wpb=1010.8, bsz=144, num_updates=160, lr=7.29483e-06, gnorm=7.079, clip=100, loss_scale=8, train_wall=25, gb_free=6.5, wall=415
2023-08-01 16:28:32 - progress_bar.py[line:272] - INFO: epoch 001:    174 / 2745 loss=3.947, loss_v1=0, loss_v2=0, nll_loss=2.638, ntokens=1014.3, nsentences=144, sample_size=1014.3, sample_size_v1=0, sample_size_v2=0, ppl=6.23, wps=413.3, ups=0.41, wpb=1014.3, bsz=144, num_updates=170, lr=7.75076e-06, gnorm=7.476, clip=100, loss_scale=8, train_wall=25, gb_free=6.5, wall=439
2023-08-01 16:28:56 - progress_bar.py[line:272] - INFO: epoch 001:    184 / 2745 loss=3.774, loss_v1=0, loss_v2=0, nll_loss=2.457, ntokens=1014, nsentences=144, sample_size=1014, sample_size_v1=0, sample_size_v2=0, ppl=5.49, wps=414.3, ups=0.41, wpb=1014, bsz=144, num_updates=180, lr=8.20669e-06, gnorm=7.571, clip=100, loss_scale=8, train_wall=24, gb_free=6.5, wall=464
2023-08-01 16:29:21 - progress_bar.py[line:272] - INFO: epoch 001:    194 / 2745 loss=3.655, loss_v1=0, loss_v2=0, nll_loss=2.343, ntokens=1017.7, nsentences=144, sample_size=1017.7, sample_size_v1=0, sample_size_v2=0, ppl=5.07, wps=415.4, ups=0.41, wpb=1017.7, bsz=144, num_updates=190, lr=8.66261e-06, gnorm=7.504, clip=100, loss_scale=8, train_wall=24, gb_free=6.5, wall=488
2023-08-01 16:29:45 - progress_bar.py[line:272] - INFO: epoch 001:    204 / 2745 loss=3.515, loss_v1=0, loss_v2=0, nll_loss=2.189, ntokens=1016.7, nsentences=144, sample_size=1016.7, sample_size_v1=0, sample_size_v2=0, ppl=4.56, wps=415.4, ups=0.41, wpb=1016.7, bsz=144, num_updates=200, lr=9.11854e-06, gnorm=7.475, clip=100, loss_scale=8, train_wall=24, gb_free=6.5, wall=513
2023-08-01 16:30:10 - progress_bar.py[line:272] - INFO: epoch 001:    214 / 2745 loss=3.391, loss_v1=0, loss_v2=0, nll_loss=2.064, ntokens=1010.2, nsentences=144, sample_size=1010.2, sample_size_v1=0, sample_size_v2=0, ppl=4.18, wps=411.2, ups=0.41, wpb=1010.2, bsz=144, num_updates=210, lr=9.57447e-06, gnorm=7.38, clip=100, loss_scale=8, train_wall=25, gb_free=6.5, wall=537
2023-08-01 16:30:34 - progress_bar.py[line:272] - INFO: epoch 001:    224 / 2745 loss=3.257, loss_v1=0, loss_v2=0, nll_loss=1.926, ntokens=1015.2, nsentences=144, sample_size=1015.2, sample_size_v1=0, sample_size_v2=0, ppl=3.8, wps=414.5, ups=0.41, wpb=1015.2, bsz=144, num_updates=220, lr=1.00304e-05, gnorm=7.223, clip=100, loss_scale=8, train_wall=24, gb_free=6.5, wall=562
2023-08-01 16:30:59 - progress_bar.py[line:272] - INFO: epoch 001:    234 / 2745 loss=3.133, loss_v1=0, loss_v2=0, nll_loss=1.801, ntokens=1016.8, nsentences=144, sample_size=1016.8, sample_size_v1=0, sample_size_v2=0, ppl=3.49, wps=416, ups=0.41, wpb=1016.8, bsz=144, num_updates=230, lr=1.04863e-05, gnorm=7.027, clip=100, loss_scale=8, train_wall=24, gb_free=6.5, wall=586
2023-08-01 16:31:23 - progress_bar.py[line:272] - INFO: epoch 001:    244 / 2745 loss=3.029, loss_v1=0, loss_v2=0, nll_loss=1.694, ntokens=1011, nsentences=144, sample_size=1011, sample_size_v1=0, sample_size_v2=0, ppl=3.24, wps=413.6, ups=0.41, wpb=1011, bsz=144, num_updates=240, lr=1.09422e-05, gnorm=6.636, clip=100, loss_scale=8, train_wall=24, gb_free=6.5, wall=611
2023-08-01 16:31:47 - progress_bar.py[line:272] - INFO: epoch 001:    254 / 2745 loss=2.91, loss_v1=0, loss_v2=0, nll_loss=1.564, ntokens=1016.4, nsentences=144, sample_size=1016.4, sample_size_v1=0, sample_size_v2=0, ppl=2.96, wps=416.1, ups=0.41, wpb=1016.4, bsz=144, num_updates=250, lr=1.13982e-05, gnorm=6.182, clip=100, loss_scale=8, train_wall=24, gb_free=6.5, wall=635
2023-08-01 16:32:12 - progress_bar.py[line:272] - INFO: epoch 001:    264 / 2745 loss=2.789, loss_v1=0, loss_v2=0, nll_loss=1.443, ntokens=1012.7, nsentences=144, sample_size=1012.7, sample_size_v1=0, sample_size_v2=0, ppl=2.72, wps=413.5, ups=0.41, wpb=1012.7, bsz=144, num_updates=260, lr=1.18541e-05, gnorm=5.623, clip=100, loss_scale=8, train_wall=24, gb_free=6.5, wall=660
2023-08-01 16:32:36 - progress_bar.py[line:272] - INFO: epoch 001:    274 / 2745 loss=2.727, loss_v1=0, loss_v2=0, nll_loss=1.385, ntokens=1016.8, nsentences=144, sample_size=1016.8, sample_size_v1=0, sample_size_v2=0, ppl=2.61, wps=415.2, ups=0.41, wpb=1016.8, bsz=144, num_updates=270, lr=1.231e-05, gnorm=4.939, clip=100, loss_scale=8, train_wall=24, gb_free=6.5, wall=684
2023-08-01 16:33:01 - progress_bar.py[line:272] - INFO: epoch 001:    284 / 2745 loss=2.637, loss_v1=0, loss_v2=0, nll_loss=1.293, ntokens=1010.6, nsentences=144, sample_size=1010.6, sample_size_v1=0, sample_size_v2=0, ppl=2.45, wps=411.7, ups=0.41, wpb=1010.6, bsz=144, num_updates=280, lr=1.2766e-05, gnorm=4.258, clip=100, loss_scale=8, train_wall=25, gb_free=6.5, wall=709
2023-08-01 16:33:25 - progress_bar.py[line:272] - INFO: epoch 001:    294 / 2745 loss=2.569, loss_v1=0, loss_v2=0, nll_loss=1.224, ntokens=1016.3, nsentences=144, sample_size=1016.3, sample_size_v1=0, sample_size_v2=0, ppl=2.34, wps=414, ups=0.41, wpb=1016.3, bsz=144, num_updates=290, lr=1.32219e-05, gnorm=3.672, clip=100, loss_scale=8, train_wall=25, gb_free=6.5, wall=733
2023-08-01 16:33:50 - progress_bar.py[line:272] - INFO: epoch 001:    304 / 2745 loss=2.475, loss_v1=0, loss_v2=0, nll_loss=1.127, ntokens=1011.7, nsentences=144, sample_size=1011.7, sample_size_v1=0, sample_size_v2=0, ppl=2.18, wps=412.2, ups=0.41, wpb=1011.7, bsz=144, num_updates=300, lr=1.36778e-05, gnorm=3.131, clip=100, loss_scale=8, train_wall=25, gb_free=6.5, wall=758
2023-08-01 16:34:15 - progress_bar.py[line:272] - INFO: epoch 001:    314 / 2745 loss=2.454, loss_v1=0, loss_v2=0, nll_loss=1.11, ntokens=1014.8, nsentences=144, sample_size=1014.8, sample_size_v1=0, sample_size_v2=0, ppl=2.16, wps=413, ups=0.41, wpb=1014.8, bsz=144, num_updates=310, lr=1.41337e-05, gnorm=2.768, clip=100, loss_scale=8, train_wall=25, gb_free=6.5, wall=782
2023-08-01 16:34:39 - progress_bar.py[line:272] - INFO: epoch 001:    324 / 2745 loss=2.39, loss_v1=0, loss_v2=0, nll_loss=1.052, ntokens=1014.6, nsentences=144, sample_size=1014.6, sample_size_v1=0, sample_size_v2=0, ppl=2.07, wps=414.5, ups=0.41, wpb=1014.6, bsz=144, num_updates=320, lr=1.45897e-05, gnorm=2.456, clip=100, loss_scale=8, train_wall=24, gb_free=6.5, wall=807
2023-08-01 16:35:04 - progress_bar.py[line:272] - INFO: epoch 001:    334 / 2745 loss=2.321, loss_v1=0, loss_v2=0, nll_loss=0.973, ntokens=1007.5, nsentences=144, sample_size=1007.5, sample_size_v1=0, sample_size_v2=0, ppl=1.96, wps=410, ups=0.41, wpb=1007.5, bsz=144, num_updates=330, lr=1.50456e-05, gnorm=2.157, clip=100, loss_scale=8, train_wall=25, gb_free=6.5, wall=831
2023-08-01 16:35:28 - progress_bar.py[line:272] - INFO: epoch 001:    344 / 2745 loss=2.295, loss_v1=0, loss_v2=0, nll_loss=0.953, ntokens=1015.9, nsentences=144, sample_size=1015.9, sample_size_v1=0, sample_size_v2=0, ppl=1.94, wps=415.2, ups=0.41, wpb=1015.9, bsz=144, num_updates=340, lr=1.55015e-05, gnorm=1.918, clip=100, loss_scale=8, train_wall=24, gb_free=6.5, wall=856
2023-08-01 16:35:53 - progress_bar.py[line:272] - INFO: epoch 001:    354 / 2745 loss=2.266, loss_v1=0, loss_v2=0, nll_loss=0.927, ntokens=1021.1, nsentences=144, sample_size=1021.1, sample_size_v1=0, sample_size_v2=0, ppl=1.9, wps=417, ups=0.41, wpb=1021.1, bsz=144, num_updates=350, lr=1.59574e-05, gnorm=1.774, clip=100, loss_scale=8, train_wall=24, gb_free=6.5, wall=880
2023-08-01 16:36:17 - progress_bar.py[line:272] - INFO: epoch 001:    364 / 2745 loss=2.246, loss_v1=0, loss_v2=0, nll_loss=0.905, ntokens=1014.1, nsentences=144, sample_size=1014.1, sample_size_v1=0, sample_size_v2=0, ppl=1.87, wps=412.7, ups=0.41, wpb=1014.1, bsz=144, num_updates=360, lr=1.64134e-05, gnorm=1.729, clip=100, loss_scale=8, train_wall=25, gb_free=6.5, wall=905
2023-08-01 16:36:42 - progress_bar.py[line:272] - INFO: epoch 001:    374 / 2745 loss=2.225, loss_v1=0, loss_v2=0, nll_loss=0.893, ntokens=1015.3, nsentences=144, sample_size=1015.3, sample_size_v1=0, sample_size_v2=0, ppl=1.86, wps=413.9, ups=0.41, wpb=1015.3, bsz=144, num_updates=370, lr=1.68693e-05, gnorm=1.711, clip=100, loss_scale=8, train_wall=25, gb_free=6.5, wall=929
2023-08-01 16:37:06 - progress_bar.py[line:272] - INFO: epoch 001:    384 / 2745 loss=2.211, loss_v1=0, loss_v2=0, nll_loss=0.876, ntokens=1020.2, nsentences=144, sample_size=1020.2, sample_size_v1=0, sample_size_v2=0, ppl=1.84, wps=417.7, ups=0.41, wpb=1020.2, bsz=144, num_updates=380, lr=1.73252e-05, gnorm=1.701, clip=100, loss_scale=8, train_wall=24, gb_free=6.5, wall=954
2023-08-01 16:37:31 - progress_bar.py[line:272] - INFO: epoch 001:    394 / 2745 loss=2.178, loss_v1=0, loss_v2=0, nll_loss=0.847, ntokens=1015.7, nsentences=144, sample_size=1015.7, sample_size_v1=0, sample_size_v2=0, ppl=1.8, wps=414.7, ups=0.41, wpb=1015.7, bsz=144, num_updates=390, lr=1.77812e-05, gnorm=1.554, clip=100, loss_scale=8, train_wall=24, gb_free=6.5, wall=978
2023-08-01 16:37:55 - progress_bar.py[line:272] - INFO: epoch 001:    404 / 2745 loss=2.133, loss_v1=0, loss_v2=0, nll_loss=0.8, ntokens=1008.3, nsentences=144, sample_size=1008.3, sample_size_v1=0, sample_size_v2=0, ppl=1.74, wps=410.8, ups=0.41, wpb=1008.3, bsz=144, num_updates=400, lr=1.82371e-05, gnorm=1.622, clip=100, loss_scale=8, train_wall=25, gb_free=6.5, wall=1003
2023-08-01 16:38:20 - progress_bar.py[line:272] - INFO: epoch 001:    414 / 2745 loss=2.114, loss_v1=0, loss_v2=0, nll_loss=0.782, ntokens=1012.9, nsentences=144, sample_size=1012.9, sample_size_v1=0, sample_size_v2=0, ppl=1.72, wps=412.3, ups=0.41, wpb=1012.9, bsz=144, num_updates=410, lr=1.8693e-05, gnorm=1.614, clip=100, loss_scale=8, train_wall=25, gb_free=6.5, wall=1027
2023-08-01 16:38:44 - progress_bar.py[line:272] - INFO: epoch 001:    424 / 2745 loss=2.131, loss_v1=0, loss_v2=0, nll_loss=0.803, ntokens=1011.3, nsentences=144, sample_size=1011.3, sample_size_v1=0, sample_size_v2=0, ppl=1.75, wps=411.8, ups=0.41, wpb=1011.3, bsz=144, num_updates=420, lr=1.91489e-05, gnorm=1.685, clip=100, loss_scale=8, train_wall=25, gb_free=6.5, wall=1052
2023-08-01 16:39:09 - progress_bar.py[line:272] - INFO: epoch 001:    434 / 2745 loss=2.112, loss_v1=0, loss_v2=0, nll_loss=0.784, ntokens=1014.6, nsentences=144, sample_size=1014.6, sample_size_v1=0, sample_size_v2=0, ppl=1.72, wps=413.9, ups=0.41, wpb=1014.6, bsz=144, num_updates=430, lr=1.96049e-05, gnorm=1.604, clip=100, loss_scale=8, train_wall=24, gb_free=6.5, wall=1077
2023-08-01 16:39:33 - progress_bar.py[line:272] - INFO: epoch 001:    444 / 2745 loss=2.066, loss_v1=0, loss_v2=0, nll_loss=0.731, ntokens=1011.9, nsentences=144, sample_size=1011.9, sample_size_v1=0, sample_size_v2=0, ppl=1.66, wps=412.2, ups=0.41, wpb=1011.9, bsz=144, num_updates=440, lr=2.00608e-05, gnorm=1.53, clip=100, loss_scale=8, train_wall=25, gb_free=6.5, wall=1101
2023-08-01 16:39:58 - progress_bar.py[line:272] - INFO: epoch 001:    454 / 2745 loss=2.075, loss_v1=0, loss_v2=0, nll_loss=0.749, ntokens=1016, nsentences=144, sample_size=1016, sample_size_v1=0, sample_size_v2=0, ppl=1.68, wps=414.1, ups=0.41, wpb=1016, bsz=144, num_updates=450, lr=2.05167e-05, gnorm=1.574, clip=100, loss_scale=8, train_wall=25, gb_free=6.5, wall=1126
2023-08-01 16:40:23 - progress_bar.py[line:272] - INFO: epoch 001:    464 / 2745 loss=2.048, loss_v1=0, loss_v2=0, nll_loss=0.721, ntokens=1014.5, nsentences=144, sample_size=1014.5, sample_size_v1=0, sample_size_v2=0, ppl=1.65, wps=412.7, ups=0.41, wpb=1014.5, bsz=144, num_updates=460, lr=2.09726e-05, gnorm=1.586, clip=100, loss_scale=8, train_wall=25, gb_free=6.5, wall=1150
2023-08-01 16:40:47 - progress_bar.py[line:272] - INFO: epoch 001:    474 / 2745 loss=2.05, loss_v1=0, loss_v2=0, nll_loss=0.725, ntokens=1016.7, nsentences=144, sample_size=1016.7, sample_size_v1=0, sample_size_v2=0, ppl=1.65, wps=414.5, ups=0.41, wpb=1016.7, bsz=144, num_updates=470, lr=2.14286e-05, gnorm=1.49, clip=100, loss_scale=8, train_wall=24, gb_free=6.5, wall=1175
2023-08-01 16:41:12 - progress_bar.py[line:272] - INFO: epoch 001:    484 / 2745 loss=2.037, loss_v1=0, loss_v2=0, nll_loss=0.712, ntokens=1014.2, nsentences=144, sample_size=1014.2, sample_size_v1=0, sample_size_v2=0, ppl=1.64, wps=413.1, ups=0.41, wpb=1014.2, bsz=144, num_updates=480, lr=2.18845e-05, gnorm=1.561, clip=100, loss_scale=8, train_wall=25, gb_free=6.5, wall=1199
2023-08-01 16:41:36 - progress_bar.py[line:272] - INFO: epoch 001:    494 / 2745 loss=2.027, loss_v1=0, loss_v2=0, nll_loss=0.705, ntokens=1012.2, nsentences=144, sample_size=1012.2, sample_size_v1=0, sample_size_v2=0, ppl=1.63, wps=411.4, ups=0.41, wpb=1012.2, bsz=144, num_updates=490, lr=2.23404e-05, gnorm=1.583, clip=100, loss_scale=8, train_wall=25, gb_free=6.5, wall=1224
2023-08-01 16:42:01 - progress_bar.py[line:272] - INFO: epoch 001:    504 / 2745 loss=2.009, loss_v1=0, loss_v2=0, nll_loss=0.686, ntokens=1020.9, nsentences=144, sample_size=1020.9, sample_size_v1=0, sample_size_v2=0, ppl=1.61, wps=417, ups=0.41, wpb=1020.9, bsz=144, num_updates=500, lr=2.27964e-05, gnorm=1.535, clip=100, loss_scale=8, train_wall=24, gb_free=6.5, wall=1248
2023-08-01 16:42:25 - progress_bar.py[line:272] - INFO: epoch 001:    514 / 2745 loss=2.004, loss_v1=0, loss_v2=0, nll_loss=0.682, ntokens=1017.3, nsentences=144, sample_size=1017.3, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=415.8, ups=0.41, wpb=1017.3, bsz=144, num_updates=510, lr=2.32523e-05, gnorm=1.552, clip=100, loss_scale=8, train_wall=24, gb_free=6.5, wall=1273
2023-08-01 16:42:50 - progress_bar.py[line:272] - INFO: epoch 001:    524 / 2745 loss=1.99, loss_v1=0, loss_v2=0, nll_loss=0.668, ntokens=1013.7, nsentences=144, sample_size=1013.7, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=411.4, ups=0.41, wpb=1013.7, bsz=144, num_updates=520, lr=2.37082e-05, gnorm=1.521, clip=100, loss_scale=16, train_wall=25, gb_free=6.5, wall=1298
2023-08-01 16:43:14 - progress_bar.py[line:272] - INFO: epoch 001:    534 / 2745 loss=1.996, loss_v1=0, loss_v2=0, nll_loss=0.676, ntokens=1016.3, nsentences=144, sample_size=1016.3, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=416, ups=0.41, wpb=1016.3, bsz=144, num_updates=530, lr=2.41641e-05, gnorm=1.538, clip=100, loss_scale=16, train_wall=24, gb_free=6.5, wall=1322
2023-08-01 16:43:39 - progress_bar.py[line:272] - INFO: epoch 001:    544 / 2745 loss=1.978, loss_v1=0, loss_v2=0, nll_loss=0.657, ntokens=1020.1, nsentences=144, sample_size=1020.1, sample_size_v1=0, sample_size_v2=0, ppl=1.58, wps=415.4, ups=0.41, wpb=1020.1, bsz=144, num_updates=540, lr=2.46201e-05, gnorm=1.671, clip=100, loss_scale=16, train_wall=25, gb_free=6.5, wall=1347
2023-08-01 16:44:03 - progress_bar.py[line:272] - INFO: epoch 001:    554 / 2745 loss=1.971, loss_v1=0, loss_v2=0, nll_loss=0.65, ntokens=1011, nsentences=144, sample_size=1011, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=411.4, ups=0.41, wpb=1011, bsz=144, num_updates=550, lr=2.5076e-05, gnorm=1.627, clip=100, loss_scale=16, train_wall=25, gb_free=6.5, wall=1371
2023-08-01 16:44:28 - progress_bar.py[line:272] - INFO: epoch 001:    564 / 2745 loss=1.97, loss_v1=0, loss_v2=0, nll_loss=0.649, ntokens=1016.3, nsentences=144, sample_size=1016.3, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=413.8, ups=0.41, wpb=1016.3, bsz=144, num_updates=560, lr=2.55319e-05, gnorm=1.526, clip=100, loss_scale=16, train_wall=25, gb_free=6.5, wall=1396
2023-08-01 16:44:52 - progress_bar.py[line:272] - INFO: epoch 001:    574 / 2745 loss=1.963, loss_v1=0, loss_v2=0, nll_loss=0.647, ntokens=1015.5, nsentences=144, sample_size=1015.5, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=415.1, ups=0.41, wpb=1015.5, bsz=144, num_updates=570, lr=2.59878e-05, gnorm=1.529, clip=100, loss_scale=16, train_wall=24, gb_free=6.5, wall=1420
2023-08-01 16:45:17 - progress_bar.py[line:272] - INFO: epoch 001:    584 / 2745 loss=1.956, loss_v1=0, loss_v2=0, nll_loss=0.637, ntokens=1013.8, nsentences=144, sample_size=1013.8, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=413.5, ups=0.41, wpb=1013.8, bsz=144, num_updates=580, lr=2.64438e-05, gnorm=1.565, clip=100, loss_scale=16, train_wall=24, gb_free=6.5, wall=1445
2023-08-01 16:45:41 - progress_bar.py[line:272] - INFO: epoch 001:    594 / 2745 loss=1.958, loss_v1=0, loss_v2=0, nll_loss=0.64, ntokens=1010.1, nsentences=144, sample_size=1010.1, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=411.9, ups=0.41, wpb=1010.1, bsz=144, num_updates=590, lr=2.68997e-05, gnorm=1.606, clip=100, loss_scale=16, train_wall=24, gb_free=6.5, wall=1469
2023-08-01 16:46:06 - progress_bar.py[line:272] - INFO: epoch 001:    604 / 2745 loss=1.931, loss_v1=0, loss_v2=0, nll_loss=0.611, ntokens=1018.7, nsentences=144, sample_size=1018.7, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=414.1, ups=0.41, wpb=1018.7, bsz=144, num_updates=600, lr=2.73556e-05, gnorm=1.434, clip=100, loss_scale=16, train_wall=25, gb_free=6.5, wall=1494
2023-08-01 16:46:31 - progress_bar.py[line:272] - INFO: epoch 001:    614 / 2745 loss=1.954, loss_v1=0, loss_v2=0, nll_loss=0.64, ntokens=1015.6, nsentences=144, sample_size=1015.6, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=413.4, ups=0.41, wpb=1015.6, bsz=144, num_updates=610, lr=2.78116e-05, gnorm=1.525, clip=100, loss_scale=16, train_wall=25, gb_free=6.5, wall=1518
2023-08-01 16:46:55 - progress_bar.py[line:272] - INFO: epoch 001:    624 / 2745 loss=1.951, loss_v1=0, loss_v2=0, nll_loss=0.636, ntokens=1022.8, nsentences=144, sample_size=1022.8, sample_size_v1=0, sample_size_v2=0, ppl=1.55, wps=418.2, ups=0.41, wpb=1022.8, bsz=144, num_updates=620, lr=2.82675e-05, gnorm=1.51, clip=100, loss_scale=16, train_wall=24, gb_free=6.5, wall=1543
2023-08-01 16:47:20 - progress_bar.py[line:272] - INFO: epoch 001:    634 / 2745 loss=1.931, loss_v1=0, loss_v2=0, nll_loss=0.613, ntokens=1013.3, nsentences=144, sample_size=1013.3, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=413.2, ups=0.41, wpb=1013.3, bsz=144, num_updates=630, lr=2.87234e-05, gnorm=1.552, clip=100, loss_scale=16, train_wall=24, gb_free=6.5, wall=1567
2023-08-01 16:47:44 - progress_bar.py[line:272] - INFO: epoch 001:    644 / 2745 loss=1.915, loss_v1=0, loss_v2=0, nll_loss=0.599, ntokens=1018.3, nsentences=144, sample_size=1018.3, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=415.3, ups=0.41, wpb=1018.3, bsz=144, num_updates=640, lr=2.91793e-05, gnorm=1.54, clip=100, loss_scale=16, train_wall=24, gb_free=6.5, wall=1592
2023-08-01 16:48:09 - progress_bar.py[line:272] - INFO: epoch 001:    654 / 2745 loss=1.918, loss_v1=0, loss_v2=0, nll_loss=0.602, ntokens=1016.2, nsentences=144, sample_size=1016.2, sample_size_v1=0, sample_size_v2=0, ppl=1.52, wps=415.7, ups=0.41, wpb=1016.2, bsz=144, num_updates=650, lr=2.96353e-05, gnorm=1.512, clip=100, loss_scale=16, train_wall=24, gb_free=6.5, wall=1616
2023-08-01 16:48:33 - progress_bar.py[line:272] - INFO: epoch 001:    664 / 2745 loss=1.912, loss_v1=0, loss_v2=0, nll_loss=0.597, ntokens=1015.7, nsentences=144, sample_size=1015.7, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=414, ups=0.41, wpb=1015.7, bsz=144, num_updates=660, lr=2.99942e-05, gnorm=1.377, clip=100, loss_scale=16, train_wall=25, gb_free=6.5, wall=1641
2023-08-01 16:48:58 - progress_bar.py[line:272] - INFO: epoch 001:    674 / 2745 loss=1.92, loss_v1=0, loss_v2=0, nll_loss=0.607, ntokens=1020.9, nsentences=144, sample_size=1020.9, sample_size_v1=0, sample_size_v2=0, ppl=1.52, wps=416.3, ups=0.41, wpb=1020.9, bsz=144, num_updates=670, lr=2.99651e-05, gnorm=1.405, clip=100, loss_scale=16, train_wall=24, gb_free=6.5, wall=1665
2023-08-01 16:49:22 - progress_bar.py[line:272] - INFO: epoch 001:    684 / 2745 loss=1.923, loss_v1=0, loss_v2=0, nll_loss=0.612, ntokens=1017.3, nsentences=144, sample_size=1017.3, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=414, ups=0.41, wpb=1017.3, bsz=144, num_updates=680, lr=2.99361e-05, gnorm=1.43, clip=100, loss_scale=16, train_wall=25, gb_free=6.5, wall=1690
2023-08-01 16:49:47 - progress_bar.py[line:272] - INFO: epoch 001:    694 / 2745 loss=1.9, loss_v1=0, loss_v2=0, nll_loss=0.584, ntokens=1014.2, nsentences=144, sample_size=1014.2, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=414.2, ups=0.41, wpb=1014.2, bsz=144, num_updates=690, lr=2.9907e-05, gnorm=1.529, clip=100, loss_scale=16, train_wall=24, gb_free=6.5, wall=1714
2023-08-01 16:50:11 - progress_bar.py[line:272] - INFO: epoch 001:    704 / 2745 loss=1.891, loss_v1=0, loss_v2=0, nll_loss=0.575, ntokens=1006.8, nsentences=144, sample_size=1006.8, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=410.2, ups=0.41, wpb=1006.8, bsz=144, num_updates=700, lr=2.98779e-05, gnorm=1.488, clip=100, loss_scale=16, train_wall=25, gb_free=6.5, wall=1739
2023-08-01 16:50:36 - progress_bar.py[line:272] - INFO: epoch 001:    714 / 2745 loss=1.891, loss_v1=0, loss_v2=0, nll_loss=0.576, ntokens=1015.3, nsentences=144, sample_size=1015.3, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=415.1, ups=0.41, wpb=1015.3, bsz=144, num_updates=710, lr=2.98489e-05, gnorm=1.473, clip=100, loss_scale=16, train_wall=24, gb_free=6.5, wall=1763
2023-08-01 16:51:00 - progress_bar.py[line:272] - INFO: epoch 001:    724 / 2745 loss=1.89, loss_v1=0, loss_v2=0, nll_loss=0.576, ntokens=1018.1, nsentences=144, sample_size=1018.1, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=416.4, ups=0.41, wpb=1018.1, bsz=144, num_updates=720, lr=2.98198e-05, gnorm=1.598, clip=100, loss_scale=16, train_wall=24, gb_free=6.5, wall=1788
2023-08-01 16:51:25 - progress_bar.py[line:272] - INFO: epoch 001:    734 / 2745 loss=1.889, loss_v1=0, loss_v2=0, nll_loss=0.575, ntokens=1018.1, nsentences=144, sample_size=1018.1, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=416.3, ups=0.41, wpb=1018.1, bsz=144, num_updates=730, lr=2.97907e-05, gnorm=1.392, clip=100, loss_scale=16, train_wall=24, gb_free=6.5, wall=1812
2023-08-01 16:51:49 - progress_bar.py[line:272] - INFO: epoch 001:    744 / 2745 loss=1.879, loss_v1=0, loss_v2=0, nll_loss=0.566, ntokens=1015.5, nsentences=144, sample_size=1015.5, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=415, ups=0.41, wpb=1015.5, bsz=144, num_updates=740, lr=2.97617e-05, gnorm=1.445, clip=100, loss_scale=16, train_wall=24, gb_free=6.5, wall=1837
2023-08-01 16:52:14 - progress_bar.py[line:272] - INFO: epoch 001:    754 / 2745 loss=1.869, loss_v1=0, loss_v2=0, nll_loss=0.556, ntokens=1016.8, nsentences=144, sample_size=1016.8, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=415.1, ups=0.41, wpb=1016.8, bsz=144, num_updates=750, lr=2.97326e-05, gnorm=1.382, clip=100, loss_scale=16, train_wall=24, gb_free=6.5, wall=1861
2023-08-01 16:52:38 - progress_bar.py[line:272] - INFO: epoch 001:    764 / 2745 loss=1.869, loss_v1=0, loss_v2=0, nll_loss=0.553, ntokens=1019.4, nsentences=144, sample_size=1019.4, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=417.5, ups=0.41, wpb=1019.4, bsz=144, num_updates=760, lr=2.97035e-05, gnorm=1.445, clip=100, loss_scale=16, train_wall=24, gb_free=6.5, wall=1886
2023-08-01 16:53:02 - progress_bar.py[line:272] - INFO: epoch 001:    774 / 2745 loss=1.882, loss_v1=0, loss_v2=0, nll_loss=0.569, ntokens=1017.6, nsentences=144, sample_size=1017.6, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=416.2, ups=0.41, wpb=1017.6, bsz=144, num_updates=770, lr=2.96745e-05, gnorm=1.451, clip=100, loss_scale=16, train_wall=24, gb_free=6.5, wall=1910
2023-08-01 16:53:27 - progress_bar.py[line:272] - INFO: epoch 001:    784 / 2745 loss=1.871, loss_v1=0, loss_v2=0, nll_loss=0.558, ntokens=1012.2, nsentences=144, sample_size=1012.2, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=411.3, ups=0.41, wpb=1012.2, bsz=144, num_updates=780, lr=2.96454e-05, gnorm=1.476, clip=100, loss_scale=16, train_wall=25, gb_free=6.5, wall=1935
2023-08-01 16:53:52 - progress_bar.py[line:272] - INFO: epoch 001:    794 / 2745 loss=1.867, loss_v1=0, loss_v2=0, nll_loss=0.554, ntokens=1014.2, nsentences=144, sample_size=1014.2, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=414.1, ups=0.41, wpb=1014.2, bsz=144, num_updates=790, lr=2.96164e-05, gnorm=1.48, clip=100, loss_scale=16, train_wall=24, gb_free=6.5, wall=1959
2023-08-01 16:54:16 - progress_bar.py[line:272] - INFO: epoch 001:    804 / 2745 loss=1.881, loss_v1=0, loss_v2=0, nll_loss=0.571, ntokens=1017.4, nsentences=144, sample_size=1017.4, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=414.8, ups=0.41, wpb=1017.4, bsz=144, num_updates=800, lr=2.95873e-05, gnorm=1.356, clip=100, loss_scale=16, train_wall=24, gb_free=6.5, wall=1984
2023-08-01 16:54:41 - progress_bar.py[line:272] - INFO: epoch 001:    814 / 2745 loss=1.861, loss_v1=0, loss_v2=0, nll_loss=0.547, ntokens=1012, nsentences=144, sample_size=1012, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=412.2, ups=0.41, wpb=1012, bsz=144, num_updates=810, lr=2.95582e-05, gnorm=1.346, clip=100, loss_scale=16, train_wall=25, gb_free=6.5, wall=2008
2023-08-01 16:55:05 - progress_bar.py[line:272] - INFO: epoch 001:    824 / 2745 loss=1.854, loss_v1=0, loss_v2=0, nll_loss=0.542, ntokens=1020.8, nsentences=144, sample_size=1020.8, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=416.3, ups=0.41, wpb=1020.8, bsz=144, num_updates=820, lr=2.95292e-05, gnorm=1.351, clip=100, loss_scale=16, train_wall=24, gb_free=6.5, wall=2033
2023-08-01 16:55:30 - progress_bar.py[line:272] - INFO: epoch 001:    834 / 2745 loss=1.844, loss_v1=0, loss_v2=0, nll_loss=0.529, ntokens=1005.7, nsentences=144, sample_size=1005.7, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=408.3, ups=0.41, wpb=1005.7, bsz=144, num_updates=830, lr=2.95001e-05, gnorm=1.334, clip=100, loss_scale=16, train_wall=25, gb_free=6.5, wall=2058
2023-08-01 16:55:54 - progress_bar.py[line:272] - INFO: epoch 001:    844 / 2745 loss=1.865, loss_v1=0, loss_v2=0, nll_loss=0.555, ntokens=1005.3, nsentences=144, sample_size=1005.3, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=408.3, ups=0.41, wpb=1005.3, bsz=144, num_updates=840, lr=2.9471e-05, gnorm=1.373, clip=100, loss_scale=16, train_wall=25, gb_free=6.5, wall=2082
2023-08-01 16:56:19 - progress_bar.py[line:272] - INFO: epoch 001:    854 / 2745 loss=1.858, loss_v1=0, loss_v2=0, nll_loss=0.547, ntokens=1014.4, nsentences=144, sample_size=1014.4, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=413.8, ups=0.41, wpb=1014.4, bsz=144, num_updates=850, lr=2.9442e-05, gnorm=1.379, clip=100, loss_scale=16, train_wall=24, gb_free=6.5, wall=2107
2023-08-01 16:56:43 - progress_bar.py[line:272] - INFO: epoch 001:    864 / 2745 loss=1.852, loss_v1=0, loss_v2=0, nll_loss=0.541, ntokens=1017.8, nsentences=144, sample_size=1017.8, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=415.4, ups=0.41, wpb=1017.8, bsz=144, num_updates=860, lr=2.94129e-05, gnorm=1.358, clip=100, loss_scale=16, train_wall=24, gb_free=6.5, wall=2131
2023-08-01 16:57:08 - progress_bar.py[line:272] - INFO: epoch 001:    874 / 2745 loss=1.843, loss_v1=0, loss_v2=0, nll_loss=0.53, ntokens=1018.6, nsentences=144, sample_size=1018.6, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=415.5, ups=0.41, wpb=1018.6, bsz=144, num_updates=870, lr=2.93838e-05, gnorm=1.313, clip=100, loss_scale=16, train_wall=24, gb_free=6.5, wall=2156
2023-08-01 16:57:32 - progress_bar.py[line:272] - INFO: epoch 001:    884 / 2745 loss=1.854, loss_v1=0, loss_v2=0, nll_loss=0.543, ntokens=1019.2, nsentences=144, sample_size=1019.2, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=416.5, ups=0.41, wpb=1019.2, bsz=144, num_updates=880, lr=2.93548e-05, gnorm=1.307, clip=100, loss_scale=16, train_wall=24, gb_free=6.5, wall=2180
2023-08-01 16:57:57 - progress_bar.py[line:272] - INFO: epoch 001:    894 / 2745 loss=1.849, loss_v1=0, loss_v2=0, nll_loss=0.54, ntokens=1011.6, nsentences=144, sample_size=1011.6, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=412.6, ups=0.41, wpb=1011.6, bsz=144, num_updates=890, lr=2.93257e-05, gnorm=1.414, clip=100, loss_scale=16, train_wall=24, gb_free=6.5, wall=2205
2023-08-01 16:58:21 - progress_bar.py[line:272] - INFO: epoch 001:    904 / 2745 loss=1.848, loss_v1=0, loss_v2=0, nll_loss=0.538, ntokens=1014.5, nsentences=144, sample_size=1014.5, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=414.4, ups=0.41, wpb=1014.5, bsz=144, num_updates=900, lr=2.92966e-05, gnorm=1.359, clip=100, loss_scale=16, train_wall=24, gb_free=6.5, wall=2229
2023-08-01 16:58:46 - progress_bar.py[line:272] - INFO: epoch 001:    914 / 2745 loss=1.838, loss_v1=0, loss_v2=0, nll_loss=0.526, ntokens=1013.2, nsentences=144, sample_size=1013.2, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=413.2, ups=0.41, wpb=1013.2, bsz=144, num_updates=910, lr=2.92676e-05, gnorm=1.362, clip=100, loss_scale=16, train_wall=24, gb_free=6.5, wall=2254
2023-08-01 16:59:11 - progress_bar.py[line:272] - INFO: epoch 001:    924 / 2745 loss=1.839, loss_v1=0, loss_v2=0, nll_loss=0.53, ntokens=1013.1, nsentences=144, sample_size=1013.1, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=413, ups=0.41, wpb=1013.1, bsz=144, num_updates=920, lr=2.92385e-05, gnorm=1.233, clip=100, loss_scale=16, train_wall=25, gb_free=6.5, wall=2278
2023-08-01 16:59:35 - progress_bar.py[line:272] - INFO: epoch 001:    934 / 2745 loss=1.833, loss_v1=0, loss_v2=0, nll_loss=0.52, ntokens=1014.5, nsentences=144, sample_size=1014.5, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=413.2, ups=0.41, wpb=1014.5, bsz=144, num_updates=930, lr=2.92095e-05, gnorm=1.256, clip=100, loss_scale=16, train_wall=25, gb_free=6.5, wall=2303
2023-08-01 17:00:00 - progress_bar.py[line:272] - INFO: epoch 001:    944 / 2745 loss=1.839, loss_v1=0, loss_v2=0, nll_loss=0.53, ntokens=1012.1, nsentences=144, sample_size=1012.1, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=411.3, ups=0.41, wpb=1012.1, bsz=144, num_updates=940, lr=2.91804e-05, gnorm=1.314, clip=100, loss_scale=16, train_wall=25, gb_free=6.5, wall=2327
2023-08-01 17:00:24 - progress_bar.py[line:272] - INFO: epoch 001:    954 / 2745 loss=1.835, loss_v1=0, loss_v2=0, nll_loss=0.524, ntokens=1013.1, nsentences=144, sample_size=1013.1, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=412.1, ups=0.41, wpb=1013.1, bsz=144, num_updates=950, lr=2.91513e-05, gnorm=1.289, clip=100, loss_scale=16, train_wall=25, gb_free=6.5, wall=2352
2023-08-01 17:00:49 - progress_bar.py[line:272] - INFO: epoch 001:    964 / 2745 loss=1.832, loss_v1=0, loss_v2=0, nll_loss=0.521, ntokens=1016.8, nsentences=144, sample_size=1016.8, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=413.2, ups=0.41, wpb=1016.8, bsz=144, num_updates=960, lr=2.91223e-05, gnorm=1.398, clip=100, loss_scale=16, train_wall=25, gb_free=6.5, wall=2377
2023-08-01 17:01:13 - progress_bar.py[line:272] - INFO: epoch 001:    974 / 2745 loss=1.829, loss_v1=0, loss_v2=0, nll_loss=0.519, ntokens=1016.7, nsentences=144, sample_size=1016.7, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=415.3, ups=0.41, wpb=1016.7, bsz=144, num_updates=970, lr=2.90932e-05, gnorm=1.377, clip=100, loss_scale=16, train_wall=24, gb_free=6.5, wall=2401
2023-08-01 17:01:38 - progress_bar.py[line:272] - INFO: epoch 001:    984 / 2745 loss=1.832, loss_v1=0, loss_v2=0, nll_loss=0.523, ntokens=1016, nsentences=144, sample_size=1016, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=413.9, ups=0.41, wpb=1016, bsz=144, num_updates=980, lr=2.90641e-05, gnorm=1.298, clip=100, loss_scale=16, train_wall=25, gb_free=6.5, wall=2426
2023-08-01 17:02:03 - progress_bar.py[line:272] - INFO: epoch 001:    994 / 2745 loss=1.838, loss_v1=0, loss_v2=0, nll_loss=0.528, ntokens=1013.4, nsentences=144, sample_size=1013.4, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=412.9, ups=0.41, wpb=1013.4, bsz=144, num_updates=990, lr=2.90351e-05, gnorm=1.272, clip=100, loss_scale=16, train_wall=25, gb_free=6.5, wall=2450
2023-08-01 17:02:27 - progress_bar.py[line:272] - INFO: epoch 001:   1004 / 2745 loss=1.81, loss_v1=0, loss_v2=0, nll_loss=0.497, ntokens=1012.7, nsentences=144, sample_size=1012.7, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=412.4, ups=0.41, wpb=1012.7, bsz=144, num_updates=1000, lr=2.9006e-05, gnorm=1.226, clip=90, loss_scale=16, train_wall=25, gb_free=6.5, wall=2475
2023-08-01 17:02:52 - progress_bar.py[line:272] - INFO: epoch 001:   1014 / 2745 loss=1.811, loss_v1=0, loss_v2=0, nll_loss=0.501, ntokens=1019.7, nsentences=144, sample_size=1019.7, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=412.3, ups=0.4, wpb=1019.7, bsz=144, num_updates=1010, lr=2.89769e-05, gnorm=1.189, clip=100, loss_scale=16, train_wall=25, gb_free=6.5, wall=2499
2023-08-01 17:03:16 - progress_bar.py[line:272] - INFO: epoch 001:   1024 / 2745 loss=1.818, loss_v1=0, loss_v2=0, nll_loss=0.508, ntokens=1017, nsentences=144, sample_size=1017, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=415.7, ups=0.41, wpb=1017, bsz=144, num_updates=1020, lr=2.89479e-05, gnorm=1.272, clip=100, loss_scale=16, train_wall=24, gb_free=6.5, wall=2524
2023-08-01 17:03:41 - progress_bar.py[line:272] - INFO: epoch 001:   1034 / 2745 loss=1.82, loss_v1=0, loss_v2=0, nll_loss=0.509, ntokens=1018, nsentences=144, sample_size=1018, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=415.4, ups=0.41, wpb=1018, bsz=144, num_updates=1030, lr=2.89188e-05, gnorm=1.259, clip=100, loss_scale=32, train_wall=24, gb_free=6.5, wall=2548
2023-08-01 17:04:05 - progress_bar.py[line:272] - INFO: epoch 001:   1044 / 2745 loss=1.829, loss_v1=0, loss_v2=0, nll_loss=0.521, ntokens=1014.7, nsentences=144, sample_size=1014.7, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=414.9, ups=0.41, wpb=1014.7, bsz=144, num_updates=1040, lr=2.88898e-05, gnorm=1.31, clip=100, loss_scale=32, train_wall=24, gb_free=6.5, wall=2573
2023-08-01 17:04:30 - progress_bar.py[line:272] - INFO: epoch 001:   1054 / 2745 loss=1.817, loss_v1=0, loss_v2=0, nll_loss=0.507, ntokens=1018.8, nsentences=144, sample_size=1018.8, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=417.6, ups=0.41, wpb=1018.8, bsz=144, num_updates=1050, lr=2.88607e-05, gnorm=1.293, clip=100, loss_scale=32, train_wall=24, gb_free=6.5, wall=2597
2023-08-01 17:04:54 - progress_bar.py[line:272] - INFO: epoch 001:   1064 / 2745 loss=1.812, loss_v1=0, loss_v2=0, nll_loss=0.502, ntokens=1016.9, nsentences=144, sample_size=1016.9, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=415.9, ups=0.41, wpb=1016.9, bsz=144, num_updates=1060, lr=2.88316e-05, gnorm=1.209, clip=100, loss_scale=32, train_wall=24, gb_free=6.5, wall=2622
2023-08-01 17:05:19 - progress_bar.py[line:272] - INFO: epoch 001:   1074 / 2745 loss=1.81, loss_v1=0, loss_v2=0, nll_loss=0.498, ntokens=1014.3, nsentences=144, sample_size=1014.3, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=413, ups=0.41, wpb=1014.3, bsz=144, num_updates=1070, lr=2.88026e-05, gnorm=1.23, clip=100, loss_scale=32, train_wall=25, gb_free=6.5, wall=2646
2023-08-01 17:05:43 - progress_bar.py[line:272] - INFO: epoch 001:   1084 / 2745 loss=1.814, loss_v1=0, loss_v2=0, nll_loss=0.505, ntokens=1013.7, nsentences=144, sample_size=1013.7, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=412.4, ups=0.41, wpb=1013.7, bsz=144, num_updates=1080, lr=2.87735e-05, gnorm=1.299, clip=100, loss_scale=32, train_wall=25, gb_free=6.5, wall=2671
2023-08-01 17:06:08 - progress_bar.py[line:272] - INFO: epoch 001:   1094 / 2745 loss=1.819, loss_v1=0, loss_v2=0, nll_loss=0.51, ntokens=1015.1, nsentences=144, sample_size=1015.1, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=412.8, ups=0.41, wpb=1015.1, bsz=144, num_updates=1090, lr=2.87444e-05, gnorm=1.323, clip=100, loss_scale=32, train_wall=25, gb_free=6.5, wall=2696
2023-08-01 17:06:32 - progress_bar.py[line:272] - INFO: epoch 001:   1104 / 2745 loss=1.817, loss_v1=0, loss_v2=0, nll_loss=0.509, ntokens=1014.2, nsentences=144, sample_size=1014.2, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=414.5, ups=0.41, wpb=1014.2, bsz=144, num_updates=1100, lr=2.87154e-05, gnorm=1.292, clip=100, loss_scale=32, train_wall=24, gb_free=6.5, wall=2720
2023-08-01 17:06:57 - progress_bar.py[line:272] - INFO: epoch 001:   1114 / 2745 loss=1.806, loss_v1=0, loss_v2=0, nll_loss=0.497, ntokens=1009, nsentences=144, sample_size=1009, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=410.1, ups=0.41, wpb=1009, bsz=144, num_updates=1110, lr=2.86863e-05, gnorm=1.284, clip=100, loss_scale=32, train_wall=25, gb_free=6.5, wall=2745
2023-08-01 17:07:21 - progress_bar.py[line:272] - INFO: epoch 001:   1124 / 2745 loss=1.799, loss_v1=0, loss_v2=0, nll_loss=0.489, ntokens=1014.9, nsentences=144, sample_size=1014.9, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=415, ups=0.41, wpb=1014.9, bsz=144, num_updates=1120, lr=2.86572e-05, gnorm=1.286, clip=100, loss_scale=32, train_wall=24, gb_free=6.5, wall=2769
2023-08-01 17:07:46 - progress_bar.py[line:272] - INFO: epoch 001:   1134 / 2745 loss=1.805, loss_v1=0, loss_v2=0, nll_loss=0.495, ntokens=1004.3, nsentences=144, sample_size=1004.3, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=408.1, ups=0.41, wpb=1004.3, bsz=144, num_updates=1130, lr=2.86282e-05, gnorm=1.262, clip=100, loss_scale=32, train_wall=25, gb_free=6.5, wall=2794
2023-08-01 17:08:10 - progress_bar.py[line:272] - INFO: epoch 001:   1144 / 2745 loss=1.8, loss_v1=0, loss_v2=0, nll_loss=0.491, ntokens=1014.3, nsentences=144, sample_size=1014.3, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=413.7, ups=0.41, wpb=1014.3, bsz=144, num_updates=1140, lr=2.85991e-05, gnorm=1.304, clip=100, loss_scale=32, train_wall=24, gb_free=6.5, wall=2818
2023-08-01 17:08:35 - progress_bar.py[line:272] - INFO: epoch 001:   1154 / 2745 loss=1.792, loss_v1=0, loss_v2=0, nll_loss=0.48, ntokens=1017.9, nsentences=144, sample_size=1017.9, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=411.1, ups=0.4, wpb=1017.9, bsz=144, num_updates=1150, lr=2.857e-05, gnorm=1.216, clip=100, loss_scale=32, train_wall=25, gb_free=6.5, wall=2843
2023-08-01 17:09:00 - progress_bar.py[line:272] - INFO: epoch 001:   1164 / 2745 loss=1.787, loss_v1=0, loss_v2=0, nll_loss=0.475, ntokens=1014.7, nsentences=144, sample_size=1014.7, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=413.6, ups=0.41, wpb=1014.7, bsz=144, num_updates=1160, lr=2.8541e-05, gnorm=1.196, clip=90, loss_scale=32, train_wall=25, gb_free=6.5, wall=2868
2023-08-01 17:09:24 - progress_bar.py[line:272] - INFO: epoch 001:   1174 / 2745 loss=1.815, loss_v1=0, loss_v2=0, nll_loss=0.507, ntokens=1016, nsentences=144, sample_size=1016, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=415.2, ups=0.41, wpb=1016, bsz=144, num_updates=1170, lr=2.85119e-05, gnorm=1.337, clip=100, loss_scale=32, train_wall=24, gb_free=6.5, wall=2892
2023-08-01 17:09:49 - progress_bar.py[line:272] - INFO: epoch 001:   1184 / 2745 loss=1.814, loss_v1=0, loss_v2=0, nll_loss=0.509, ntokens=1020.2, nsentences=144, sample_size=1020.2, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=417.8, ups=0.41, wpb=1020.2, bsz=144, num_updates=1180, lr=2.84829e-05, gnorm=1.221, clip=100, loss_scale=32, train_wall=24, gb_free=6.5, wall=2916
2023-08-01 17:10:13 - progress_bar.py[line:272] - INFO: epoch 001:   1194 / 2745 loss=1.792, loss_v1=0, loss_v2=0, nll_loss=0.482, ntokens=1014.3, nsentences=144, sample_size=1014.3, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=414.8, ups=0.41, wpb=1014.3, bsz=144, num_updates=1190, lr=2.84538e-05, gnorm=1.186, clip=90, loss_scale=32, train_wall=24, gb_free=6.5, wall=2941
2023-08-01 17:10:38 - progress_bar.py[line:272] - INFO: epoch 001:   1204 / 2745 loss=1.8, loss_v1=0, loss_v2=0, nll_loss=0.493, ntokens=1010, nsentences=144, sample_size=1010, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=411.6, ups=0.41, wpb=1010, bsz=144, num_updates=1200, lr=2.84247e-05, gnorm=1.323, clip=100, loss_scale=32, train_wall=25, gb_free=6.5, wall=2965
2023-08-01 17:11:02 - progress_bar.py[line:272] - INFO: epoch 001:   1214 / 2745 loss=1.802, loss_v1=0, loss_v2=0, nll_loss=0.494, ntokens=1019.5, nsentences=144, sample_size=1019.5, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=416.6, ups=0.41, wpb=1019.5, bsz=144, num_updates=1210, lr=2.83957e-05, gnorm=1.222, clip=90, loss_scale=32, train_wall=24, gb_free=6.5, wall=2990
2023-08-01 17:11:27 - progress_bar.py[line:272] - INFO: epoch 001:   1224 / 2745 loss=1.795, loss_v1=0, loss_v2=0, nll_loss=0.489, ntokens=1016.3, nsentences=144, sample_size=1016.3, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=415.1, ups=0.41, wpb=1016.3, bsz=144, num_updates=1220, lr=2.83666e-05, gnorm=1.312, clip=100, loss_scale=32, train_wall=24, gb_free=6.5, wall=3014
2023-08-01 17:11:51 - progress_bar.py[line:272] - INFO: epoch 001:   1234 / 2745 loss=1.792, loss_v1=0, loss_v2=0, nll_loss=0.483, ntokens=1012.7, nsentences=144, sample_size=1012.7, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=413.6, ups=0.41, wpb=1012.7, bsz=144, num_updates=1230, lr=2.83375e-05, gnorm=1.266, clip=100, loss_scale=32, train_wall=24, gb_free=6.5, wall=3039
2023-08-01 17:12:16 - progress_bar.py[line:272] - INFO: epoch 001:   1244 / 2745 loss=1.793, loss_v1=0, loss_v2=0, nll_loss=0.483, ntokens=1017.2, nsentences=144, sample_size=1017.2, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=414.8, ups=0.41, wpb=1017.2, bsz=144, num_updates=1240, lr=2.83085e-05, gnorm=1.277, clip=100, loss_scale=32, train_wall=24, gb_free=6.5, wall=3063
2023-08-01 17:12:40 - progress_bar.py[line:272] - INFO: epoch 001:   1254 / 2745 loss=1.793, loss_v1=0, loss_v2=0, nll_loss=0.484, ntokens=1012.6, nsentences=144, sample_size=1012.6, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=412.5, ups=0.41, wpb=1012.6, bsz=144, num_updates=1250, lr=2.82794e-05, gnorm=1.132, clip=80, loss_scale=32, train_wall=25, gb_free=6.5, wall=3088
2023-08-01 17:13:05 - progress_bar.py[line:272] - INFO: epoch 001:   1264 / 2745 loss=1.816, loss_v1=0, loss_v2=0, nll_loss=0.511, ntokens=1014.6, nsentences=144, sample_size=1014.6, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=414.7, ups=0.41, wpb=1014.6, bsz=144, num_updates=1260, lr=2.82503e-05, gnorm=1.229, clip=100, loss_scale=32, train_wall=24, gb_free=6.5, wall=3112
2023-08-01 17:13:29 - progress_bar.py[line:272] - INFO: epoch 001:   1274 / 2745 loss=1.805, loss_v1=0, loss_v2=0, nll_loss=0.498, ntokens=1014, nsentences=144, sample_size=1014, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=412.3, ups=0.41, wpb=1014, bsz=144, num_updates=1270, lr=2.82213e-05, gnorm=1.276, clip=100, loss_scale=32, train_wall=25, gb_free=6.5, wall=3137
2023-08-01 17:13:54 - progress_bar.py[line:272] - INFO: epoch 001:   1284 / 2745 loss=1.799, loss_v1=0, loss_v2=0, nll_loss=0.492, ntokens=1016.3, nsentences=144, sample_size=1016.3, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=413.7, ups=0.41, wpb=1016.3, bsz=144, num_updates=1280, lr=2.81922e-05, gnorm=1.195, clip=90, loss_scale=32, train_wall=25, gb_free=6.5, wall=3162
2023-08-01 17:14:18 - progress_bar.py[line:272] - INFO: epoch 001:   1294 / 2745 loss=1.796, loss_v1=0, loss_v2=0, nll_loss=0.487, ntokens=1014.9, nsentences=144, sample_size=1014.9, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=412, ups=0.41, wpb=1014.9, bsz=144, num_updates=1290, lr=2.81631e-05, gnorm=1.197, clip=90, loss_scale=32, train_wall=25, gb_free=6.5, wall=3186
2023-08-01 17:14:43 - progress_bar.py[line:272] - INFO: epoch 001:   1304 / 2745 loss=1.801, loss_v1=0, loss_v2=0, nll_loss=0.494, ntokens=1009.8, nsentences=144, sample_size=1009.8, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=412.1, ups=0.41, wpb=1009.8, bsz=144, num_updates=1300, lr=2.81341e-05, gnorm=1.216, clip=100, loss_scale=32, train_wall=24, gb_free=6.5, wall=3211
2023-08-01 17:15:07 - progress_bar.py[line:272] - INFO: epoch 001:   1314 / 2745 loss=1.794, loss_v1=0, loss_v2=0, nll_loss=0.487, ntokens=1015.2, nsentences=144, sample_size=1015.2, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=414.6, ups=0.41, wpb=1015.2, bsz=144, num_updates=1310, lr=2.8105e-05, gnorm=1.185, clip=100, loss_scale=32, train_wall=24, gb_free=6.5, wall=3235
2023-08-01 17:15:32 - progress_bar.py[line:272] - INFO: epoch 001:   1324 / 2745 loss=1.782, loss_v1=0, loss_v2=0, nll_loss=0.473, ntokens=1017.6, nsentences=144, sample_size=1017.6, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=415.5, ups=0.41, wpb=1017.6, bsz=144, num_updates=1320, lr=2.8076e-05, gnorm=1.185, clip=80, loss_scale=32, train_wall=24, gb_free=6.5, wall=3260
2023-08-01 17:15:57 - progress_bar.py[line:272] - INFO: epoch 001:   1334 / 2745 loss=1.783, loss_v1=0, loss_v2=0, nll_loss=0.474, ntokens=1013.1, nsentences=144, sample_size=1013.1, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=413.1, ups=0.41, wpb=1013.1, bsz=144, num_updates=1330, lr=2.80469e-05, gnorm=1.264, clip=100, loss_scale=32, train_wall=24, gb_free=6.5, wall=3284
2023-08-01 17:16:21 - progress_bar.py[line:272] - INFO: epoch 001:   1344 / 2745 loss=1.778, loss_v1=0, loss_v2=0, nll_loss=0.47, ntokens=1007.1, nsentences=144, sample_size=1007.1, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=410.3, ups=0.41, wpb=1007.1, bsz=144, num_updates=1340, lr=2.80178e-05, gnorm=1.235, clip=90, loss_scale=32, train_wall=25, gb_free=6.5, wall=3309
2023-08-01 17:16:45 - progress_bar.py[line:272] - INFO: epoch 001:   1354 / 2745 loss=1.789, loss_v1=0, loss_v2=0, nll_loss=0.481, ntokens=1015.2, nsentences=144, sample_size=1015.2, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=415.8, ups=0.41, wpb=1015.2, bsz=144, num_updates=1350, lr=2.79888e-05, gnorm=1.22, clip=100, loss_scale=32, train_wall=24, gb_free=6.5, wall=3333
2023-08-01 17:17:10 - progress_bar.py[line:272] - INFO: epoch 001:   1364 / 2745 loss=1.778, loss_v1=0, loss_v2=0, nll_loss=0.469, ntokens=1010.7, nsentences=144, sample_size=1010.7, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=411.3, ups=0.41, wpb=1010.7, bsz=144, num_updates=1360, lr=2.79597e-05, gnorm=1.289, clip=100, loss_scale=32, train_wall=25, gb_free=6.5, wall=3358
2023-08-01 17:17:35 - progress_bar.py[line:272] - INFO: epoch 001:   1374 / 2745 loss=1.791, loss_v1=0, loss_v2=0, nll_loss=0.486, ntokens=1016.8, nsentences=144, sample_size=1016.8, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=414.8, ups=0.41, wpb=1016.8, bsz=144, num_updates=1370, lr=2.79306e-05, gnorm=1.328, clip=100, loss_scale=32, train_wall=24, gb_free=6.5, wall=3382
2023-08-01 17:17:59 - progress_bar.py[line:272] - INFO: epoch 001:   1384 / 2745 loss=1.783, loss_v1=0, loss_v2=0, nll_loss=0.474, ntokens=1013.3, nsentences=144, sample_size=1013.3, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=413.8, ups=0.41, wpb=1013.3, bsz=144, num_updates=1380, lr=2.79016e-05, gnorm=1.185, clip=100, loss_scale=32, train_wall=24, gb_free=6.5, wall=3407
2023-08-01 17:18:24 - progress_bar.py[line:272] - INFO: epoch 001:   1394 / 2745 loss=1.788, loss_v1=0, loss_v2=0, nll_loss=0.481, ntokens=1023.3, nsentences=144, sample_size=1023.3, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=418.8, ups=0.41, wpb=1023.3, bsz=144, num_updates=1390, lr=2.78725e-05, gnorm=1.265, clip=100, loss_scale=32, train_wall=24, gb_free=6.5, wall=3431
2023-08-01 17:18:48 - progress_bar.py[line:272] - INFO: epoch 001:   1404 / 2745 loss=1.783, loss_v1=0, loss_v2=0, nll_loss=0.475, ntokens=1011.6, nsentences=144, sample_size=1011.6, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=411.4, ups=0.41, wpb=1011.6, bsz=144, num_updates=1400, lr=2.78434e-05, gnorm=1.182, clip=100, loss_scale=32, train_wall=25, gb_free=6.5, wall=3456
2023-08-01 17:19:13 - progress_bar.py[line:272] - INFO: epoch 001:   1414 / 2745 loss=1.787, loss_v1=0, loss_v2=0, nll_loss=0.481, ntokens=1020, nsentences=144, sample_size=1020, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=417.3, ups=0.41, wpb=1020, bsz=144, num_updates=1410, lr=2.78144e-05, gnorm=1.207, clip=100, loss_scale=32, train_wall=24, gb_free=6.5, wall=3480
2023-08-01 17:19:37 - progress_bar.py[line:272] - INFO: epoch 001:   1424 / 2745 loss=1.786, loss_v1=0, loss_v2=0, nll_loss=0.478, ntokens=1020.8, nsentences=144, sample_size=1020.8, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=416.1, ups=0.41, wpb=1020.8, bsz=144, num_updates=1420, lr=2.77853e-05, gnorm=1.229, clip=100, loss_scale=32, train_wall=25, gb_free=6.5, wall=3505
2023-08-01 17:20:02 - progress_bar.py[line:272] - INFO: epoch 001:   1434 / 2745 loss=1.767, loss_v1=0, loss_v2=0, nll_loss=0.459, ntokens=1008.9, nsentences=144, sample_size=1008.9, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=409.8, ups=0.41, wpb=1008.9, bsz=144, num_updates=1430, lr=2.77562e-05, gnorm=1.159, clip=100, loss_scale=32, train_wall=25, gb_free=6.5, wall=3529
2023-08-01 17:20:26 - progress_bar.py[line:272] - INFO: epoch 001:   1444 / 2745 loss=1.779, loss_v1=0, loss_v2=0, nll_loss=0.473, ntokens=1021.1, nsentences=144, sample_size=1021.1, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=415.8, ups=0.41, wpb=1021.1, bsz=144, num_updates=1440, lr=2.77272e-05, gnorm=1.211, clip=100, loss_scale=32, train_wall=25, gb_free=6.5, wall=3554
2023-08-01 17:20:51 - progress_bar.py[line:272] - INFO: epoch 001:   1454 / 2745 loss=1.77, loss_v1=0, loss_v2=0, nll_loss=0.459, ntokens=1012.4, nsentences=144, sample_size=1012.4, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=411.8, ups=0.41, wpb=1012.4, bsz=144, num_updates=1450, lr=2.76981e-05, gnorm=1.199, clip=100, loss_scale=32, train_wall=25, gb_free=6.5, wall=3579
2023-08-01 17:21:15 - progress_bar.py[line:272] - INFO: epoch 001:   1464 / 2745 loss=1.786, loss_v1=0, loss_v2=0, nll_loss=0.482, ntokens=1014, nsentences=144, sample_size=1014, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=414, ups=0.41, wpb=1014, bsz=144, num_updates=1460, lr=2.76691e-05, gnorm=1.267, clip=100, loss_scale=32, train_wall=24, gb_free=6.5, wall=3603
2023-08-01 17:21:40 - progress_bar.py[line:272] - INFO: epoch 001:   1474 / 2745 loss=1.778, loss_v1=0, loss_v2=0, nll_loss=0.471, ntokens=1018.3, nsentences=144, sample_size=1018.3, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=416.5, ups=0.41, wpb=1018.3, bsz=144, num_updates=1470, lr=2.764e-05, gnorm=1.17, clip=90, loss_scale=32, train_wall=24, gb_free=6.5, wall=3628
2023-08-01 17:22:04 - progress_bar.py[line:272] - INFO: epoch 001:   1484 / 2745 loss=1.778, loss_v1=0, loss_v2=0, nll_loss=0.47, ntokens=1016.9, nsentences=144, sample_size=1016.9, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=415.9, ups=0.41, wpb=1016.9, bsz=144, num_updates=1480, lr=2.76109e-05, gnorm=1.236, clip=100, loss_scale=32, train_wall=24, gb_free=6.5, wall=3652
2023-08-01 17:22:29 - progress_bar.py[line:272] - INFO: epoch 001:   1494 / 2745 loss=1.776, loss_v1=0, loss_v2=0, nll_loss=0.469, ntokens=1014.2, nsentences=144, sample_size=1014.2, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=413.1, ups=0.41, wpb=1014.2, bsz=144, num_updates=1490, lr=2.75819e-05, gnorm=1.132, clip=90, loss_scale=32, train_wall=25, gb_free=6.5, wall=3677
2023-08-01 17:22:53 - progress_bar.py[line:272] - INFO: epoch 001:   1504 / 2745 loss=1.777, loss_v1=0, loss_v2=0, nll_loss=0.469, ntokens=1005.1, nsentences=144, sample_size=1005.1, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=407.7, ups=0.41, wpb=1005.1, bsz=144, num_updates=1500, lr=2.75528e-05, gnorm=1.158, clip=90, loss_scale=32, train_wall=25, gb_free=6.5, wall=3701
2023-08-01 17:23:18 - progress_bar.py[line:272] - INFO: epoch 001:   1514 / 2745 loss=1.776, loss_v1=0, loss_v2=0, nll_loss=0.47, ntokens=1013.6, nsentences=144, sample_size=1013.6, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=413.7, ups=0.41, wpb=1013.6, bsz=144, num_updates=1510, lr=2.75237e-05, gnorm=1.229, clip=100, loss_scale=32, train_wall=24, gb_free=6.5, wall=3726
2023-08-01 17:23:42 - progress_bar.py[line:272] - INFO: epoch 001:   1524 / 2745 loss=1.777, loss_v1=0, loss_v2=0, nll_loss=0.471, ntokens=1019.2, nsentences=144, sample_size=1019.2, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=416.1, ups=0.41, wpb=1019.2, bsz=144, num_updates=1520, lr=2.74947e-05, gnorm=1.204, clip=100, loss_scale=32, train_wall=24, gb_free=6.5, wall=3750
2023-08-01 17:24:07 - progress_bar.py[line:272] - INFO: epoch 001:   1534 / 2745 loss=1.784, loss_v1=0, loss_v2=0, nll_loss=0.477, ntokens=1018.8, nsentences=144, sample_size=1018.8, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=416.4, ups=0.41, wpb=1018.8, bsz=144, num_updates=1530, lr=2.74656e-05, gnorm=1.343, clip=100, loss_scale=32, train_wall=24, gb_free=6.5, wall=3775
2023-08-01 17:24:31 - progress_bar.py[line:272] - INFO: epoch 001:   1544 / 2745 loss=1.764, loss_v1=0, loss_v2=0, nll_loss=0.456, ntokens=1007.2, nsentences=144, sample_size=1007.2, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=410.7, ups=0.41, wpb=1007.2, bsz=144, num_updates=1540, lr=2.74365e-05, gnorm=1.129, clip=70, loss_scale=64, train_wall=24, gb_free=6.5, wall=3799
2023-08-01 17:24:56 - progress_bar.py[line:272] - INFO: epoch 001:   1554 / 2745 loss=1.771, loss_v1=0, loss_v2=0, nll_loss=0.464, ntokens=1009.9, nsentences=144, sample_size=1009.9, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=410.8, ups=0.41, wpb=1009.9, bsz=144, num_updates=1550, lr=2.74075e-05, gnorm=1.123, clip=90, loss_scale=64, train_wall=25, gb_free=6.5, wall=3824
2023-08-01 17:25:20 - progress_bar.py[line:272] - INFO: epoch 001:   1564 / 2745 loss=1.779, loss_v1=0, loss_v2=0, nll_loss=0.475, ntokens=1016.7, nsentences=144, sample_size=1016.7, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=416.2, ups=0.41, wpb=1016.7, bsz=144, num_updates=1560, lr=2.73784e-05, gnorm=1.277, clip=100, loss_scale=64, train_wall=24, gb_free=6.5, wall=3848
2023-08-01 17:25:45 - progress_bar.py[line:272] - INFO: epoch 001:   1574 / 2745 loss=1.767, loss_v1=0, loss_v2=0, nll_loss=0.459, ntokens=1010.9, nsentences=144, sample_size=1010.9, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=412.3, ups=0.41, wpb=1010.9, bsz=144, num_updates=1570, lr=2.73494e-05, gnorm=1.175, clip=100, loss_scale=64, train_wall=24, gb_free=6.5, wall=3873
2023-08-01 17:26:09 - progress_bar.py[line:272] - INFO: epoch 001:   1584 / 2745 loss=1.763, loss_v1=0, loss_v2=0, nll_loss=0.456, ntokens=1014.1, nsentences=144, sample_size=1014.1, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=414.7, ups=0.41, wpb=1014.1, bsz=144, num_updates=1580, lr=2.73203e-05, gnorm=1.273, clip=100, loss_scale=64, train_wall=24, gb_free=6.5, wall=3897
2023-08-01 17:26:34 - progress_bar.py[line:272] - INFO: epoch 001:   1594 / 2745 loss=1.771, loss_v1=0, loss_v2=0, nll_loss=0.466, ntokens=1018, nsentences=144, sample_size=1018, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=415.5, ups=0.41, wpb=1018, bsz=144, num_updates=1590, lr=2.72912e-05, gnorm=1.209, clip=100, loss_scale=64, train_wall=24, gb_free=6.5, wall=3922
2023-08-01 17:26:58 - progress_bar.py[line:272] - INFO: epoch 001:   1604 / 2745 loss=1.776, loss_v1=0, loss_v2=0, nll_loss=0.47, ntokens=1015.9, nsentences=144, sample_size=1015.9, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=414.9, ups=0.41, wpb=1015.9, bsz=144, num_updates=1600, lr=2.72622e-05, gnorm=1.231, clip=100, loss_scale=64, train_wall=24, gb_free=6.5, wall=3946
2023-08-01 17:27:23 - progress_bar.py[line:272] - INFO: epoch 001:   1614 / 2745 loss=1.769, loss_v1=0, loss_v2=0, nll_loss=0.462, ntokens=1016.6, nsentences=144, sample_size=1016.6, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=415.9, ups=0.41, wpb=1016.6, bsz=144, num_updates=1610, lr=2.72331e-05, gnorm=1.222, clip=100, loss_scale=64, train_wall=24, gb_free=6.5, wall=3971
2023-08-01 17:27:47 - progress_bar.py[line:272] - INFO: epoch 001:   1624 / 2745 loss=1.765, loss_v1=0, loss_v2=0, nll_loss=0.458, ntokens=1013.2, nsentences=144, sample_size=1013.2, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=413.4, ups=0.41, wpb=1013.2, bsz=144, num_updates=1620, lr=2.7204e-05, gnorm=1.132, clip=90, loss_scale=64, train_wall=24, gb_free=6.5, wall=3995
2023-08-01 17:28:12 - progress_bar.py[line:272] - INFO: epoch 001:   1634 / 2745 loss=1.76, loss_v1=0, loss_v2=0, nll_loss=0.453, ntokens=1016.3, nsentences=144, sample_size=1016.3, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=415, ups=0.41, wpb=1016.3, bsz=144, num_updates=1630, lr=2.7175e-05, gnorm=1.104, clip=80, loss_scale=64, train_wall=24, gb_free=6.5, wall=4020
2023-08-01 17:28:36 - progress_bar.py[line:272] - INFO: epoch 001:   1644 / 2745 loss=1.779, loss_v1=0, loss_v2=0, nll_loss=0.473, ntokens=1018.2, nsentences=144, sample_size=1018.2, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=415.6, ups=0.41, wpb=1018.2, bsz=144, num_updates=1640, lr=2.71459e-05, gnorm=1.279, clip=100, loss_scale=64, train_wall=24, gb_free=6.5, wall=4044
2023-08-01 17:29:01 - progress_bar.py[line:272] - INFO: epoch 001:   1654 / 2745 loss=1.751, loss_v1=0, loss_v2=0, nll_loss=0.444, ntokens=1017.7, nsentences=144, sample_size=1017.7, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=416, ups=0.41, wpb=1017.7, bsz=144, num_updates=1650, lr=2.71168e-05, gnorm=1.143, clip=90, loss_scale=64, train_wall=24, gb_free=6.5, wall=4069
2023-08-01 17:29:25 - progress_bar.py[line:272] - INFO: epoch 001:   1664 / 2745 loss=1.766, loss_v1=0, loss_v2=0, nll_loss=0.459, ntokens=1021.2, nsentences=144, sample_size=1021.2, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=417.5, ups=0.41, wpb=1021.2, bsz=144, num_updates=1660, lr=2.70878e-05, gnorm=1.165, clip=90, loss_scale=64, train_wall=24, gb_free=6.5, wall=4093
2023-08-01 17:29:50 - progress_bar.py[line:272] - INFO: epoch 001:   1674 / 2745 loss=1.764, loss_v1=0, loss_v2=0, nll_loss=0.457, ntokens=1012.9, nsentences=144, sample_size=1012.9, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=414.1, ups=0.41, wpb=1012.9, bsz=144, num_updates=1670, lr=2.70587e-05, gnorm=1.232, clip=100, loss_scale=64, train_wall=24, gb_free=6.5, wall=4117
2023-08-01 17:30:14 - progress_bar.py[line:272] - INFO: epoch 001:   1684 / 2745 loss=1.762, loss_v1=0, loss_v2=0, nll_loss=0.455, ntokens=989.3, nsentences=140.7, sample_size=989.3, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=410.6, ups=0.42, wpb=989.3, bsz=140.7, num_updates=1680, lr=2.70296e-05, gnorm=1.202, clip=80, loss_scale=64, train_wall=24, gb_free=6.5, wall=4142
2023-08-01 17:30:38 - progress_bar.py[line:272] - INFO: epoch 001:   1694 / 2745 loss=1.766, loss_v1=0, loss_v2=0, nll_loss=0.458, ntokens=1021, nsentences=144, sample_size=1021, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=417.7, ups=0.41, wpb=1021, bsz=144, num_updates=1690, lr=2.70006e-05, gnorm=1.117, clip=100, loss_scale=64, train_wall=24, gb_free=6.5, wall=4166
2023-08-01 17:31:03 - progress_bar.py[line:272] - INFO: epoch 001:   1704 / 2745 loss=1.757, loss_v1=0, loss_v2=0, nll_loss=0.452, ntokens=1009.5, nsentences=144, sample_size=1009.5, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=411.7, ups=0.41, wpb=1009.5, bsz=144, num_updates=1700, lr=2.69715e-05, gnorm=1.168, clip=100, loss_scale=64, train_wall=24, gb_free=6.5, wall=4191
2023-08-01 17:31:27 - progress_bar.py[line:272] - INFO: epoch 001:   1714 / 2745 loss=1.752, loss_v1=0, loss_v2=0, nll_loss=0.443, ntokens=1015.8, nsentences=144, sample_size=1015.8, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=413.8, ups=0.41, wpb=1015.8, bsz=144, num_updates=1710, lr=2.69425e-05, gnorm=1.153, clip=100, loss_scale=64, train_wall=25, gb_free=6.5, wall=4215
2023-08-01 17:31:52 - progress_bar.py[line:272] - INFO: epoch 001:   1724 / 2745 loss=1.77, loss_v1=0, loss_v2=0, nll_loss=0.465, ntokens=1011.6, nsentences=144, sample_size=1011.6, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=412.5, ups=0.41, wpb=1011.6, bsz=144, num_updates=1720, lr=2.69134e-05, gnorm=1.091, clip=90, loss_scale=64, train_wall=24, gb_free=6.5, wall=4240
2023-08-01 17:32:16 - progress_bar.py[line:272] - INFO: epoch 001:   1734 / 2745 loss=1.768, loss_v1=0, loss_v2=0, nll_loss=0.463, ntokens=1015.7, nsentences=144, sample_size=1015.7, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=414.3, ups=0.41, wpb=1015.7, bsz=144, num_updates=1730, lr=2.68843e-05, gnorm=1.145, clip=90, loss_scale=64, train_wall=24, gb_free=6.5, wall=4264
2023-08-01 17:32:41 - progress_bar.py[line:272] - INFO: epoch 001:   1744 / 2745 loss=1.76, loss_v1=0, loss_v2=0, nll_loss=0.452, ntokens=1015.4, nsentences=144, sample_size=1015.4, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=414.7, ups=0.41, wpb=1015.4, bsz=144, num_updates=1740, lr=2.68553e-05, gnorm=1.139, clip=90, loss_scale=64, train_wall=24, gb_free=6.5, wall=4289
2023-08-01 17:33:05 - progress_bar.py[line:272] - INFO: epoch 001:   1754 / 2745 loss=1.748, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=1018.8, nsentences=144, sample_size=1018.8, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=415.4, ups=0.41, wpb=1018.8, bsz=144, num_updates=1750, lr=2.68262e-05, gnorm=1.076, clip=80, loss_scale=64, train_wall=24, gb_free=6.5, wall=4313
2023-08-01 17:33:30 - progress_bar.py[line:272] - INFO: epoch 001:   1764 / 2745 loss=1.77, loss_v1=0, loss_v2=0, nll_loss=0.465, ntokens=1009.8, nsentences=144, sample_size=1009.8, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=410.5, ups=0.41, wpb=1009.8, bsz=144, num_updates=1760, lr=2.67971e-05, gnorm=1.221, clip=100, loss_scale=64, train_wall=25, gb_free=6.5, wall=4338
2023-08-01 17:33:55 - progress_bar.py[line:272] - INFO: epoch 001:   1774 / 2745 loss=1.756, loss_v1=0, loss_v2=0, nll_loss=0.448, ntokens=1014.6, nsentences=144, sample_size=1014.6, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=414.3, ups=0.41, wpb=1014.6, bsz=144, num_updates=1770, lr=2.67681e-05, gnorm=1.219, clip=100, loss_scale=64, train_wall=24, gb_free=6.5, wall=4362
2023-08-01 17:34:19 - progress_bar.py[line:272] - INFO: epoch 001:   1784 / 2745 loss=1.748, loss_v1=0, loss_v2=0, nll_loss=0.441, ntokens=1017.5, nsentences=144, sample_size=1017.5, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=414.9, ups=0.41, wpb=1017.5, bsz=144, num_updates=1780, lr=2.6739e-05, gnorm=1.178, clip=90, loss_scale=64, train_wall=24, gb_free=6.5, wall=4387
2023-08-01 17:34:44 - progress_bar.py[line:272] - INFO: epoch 001:   1794 / 2745 loss=1.758, loss_v1=0, loss_v2=0, nll_loss=0.452, ntokens=1013.6, nsentences=144, sample_size=1013.6, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=413.6, ups=0.41, wpb=1013.6, bsz=144, num_updates=1790, lr=2.67099e-05, gnorm=1.205, clip=100, loss_scale=64, train_wall=24, gb_free=6.5, wall=4411
2023-08-01 17:35:08 - progress_bar.py[line:272] - INFO: epoch 001:   1804 / 2745 loss=1.767, loss_v1=0, loss_v2=0, nll_loss=0.462, ntokens=1015.7, nsentences=144, sample_size=1015.7, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=414, ups=0.41, wpb=1015.7, bsz=144, num_updates=1800, lr=2.66809e-05, gnorm=1.237, clip=100, loss_scale=64, train_wall=25, gb_free=6.5, wall=4436
2023-08-01 17:35:33 - progress_bar.py[line:272] - INFO: epoch 001:   1814 / 2745 loss=1.749, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=1008.9, nsentences=144, sample_size=1008.9, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=409.3, ups=0.41, wpb=1008.9, bsz=144, num_updates=1810, lr=2.66518e-05, gnorm=1.121, clip=90, loss_scale=64, train_wall=25, gb_free=6.5, wall=4460
2023-08-01 17:35:57 - progress_bar.py[line:272] - INFO: epoch 001:   1824 / 2745 loss=1.766, loss_v1=0, loss_v2=0, nll_loss=0.462, ntokens=1020.1, nsentences=144, sample_size=1020.1, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=416.4, ups=0.41, wpb=1020.1, bsz=144, num_updates=1820, lr=2.66227e-05, gnorm=1.149, clip=100, loss_scale=64, train_wall=24, gb_free=6.5, wall=4485
2023-08-01 17:36:22 - progress_bar.py[line:272] - INFO: epoch 001:   1834 / 2745 loss=1.752, loss_v1=0, loss_v2=0, nll_loss=0.446, ntokens=1018.3, nsentences=144, sample_size=1018.3, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=416.4, ups=0.41, wpb=1018.3, bsz=144, num_updates=1830, lr=2.65937e-05, gnorm=1.132, clip=100, loss_scale=64, train_wall=24, gb_free=6.5, wall=4509
2023-08-01 17:36:46 - progress_bar.py[line:272] - INFO: epoch 001:   1844 / 2745 loss=1.759, loss_v1=0, loss_v2=0, nll_loss=0.452, ntokens=1021.7, nsentences=144, sample_size=1021.7, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=417.3, ups=0.41, wpb=1021.7, bsz=144, num_updates=1840, lr=2.65646e-05, gnorm=1.194, clip=100, loss_scale=64, train_wall=24, gb_free=6.5, wall=4534
2023-08-01 17:37:11 - progress_bar.py[line:272] - INFO: epoch 001:   1854 / 2745 loss=1.752, loss_v1=0, loss_v2=0, nll_loss=0.445, ntokens=1016.1, nsentences=144, sample_size=1016.1, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=415.6, ups=0.41, wpb=1016.1, bsz=144, num_updates=1850, lr=2.65356e-05, gnorm=1.27, clip=100, loss_scale=64, train_wall=24, gb_free=6.5, wall=4558
2023-08-01 17:37:35 - progress_bar.py[line:272] - INFO: epoch 001:   1864 / 2745 loss=1.747, loss_v1=0, loss_v2=0, nll_loss=0.44, ntokens=1016.9, nsentences=144, sample_size=1016.9, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=415.9, ups=0.41, wpb=1016.9, bsz=144, num_updates=1860, lr=2.65065e-05, gnorm=1.105, clip=80, loss_scale=64, train_wall=24, gb_free=6.5, wall=4583
2023-08-01 17:38:00 - progress_bar.py[line:272] - INFO: epoch 001:   1874 / 2745 loss=1.755, loss_v1=0, loss_v2=0, nll_loss=0.449, ntokens=1016.7, nsentences=144, sample_size=1016.7, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=414.6, ups=0.41, wpb=1016.7, bsz=144, num_updates=1870, lr=2.64774e-05, gnorm=1.112, clip=100, loss_scale=64, train_wall=24, gb_free=6.5, wall=4607
2023-08-01 17:38:24 - progress_bar.py[line:272] - INFO: epoch 001:   1884 / 2745 loss=1.776, loss_v1=0, loss_v2=0, nll_loss=0.471, ntokens=1014, nsentences=144, sample_size=1014, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=414.2, ups=0.41, wpb=1014, bsz=144, num_updates=1880, lr=2.64484e-05, gnorm=1.186, clip=100, loss_scale=64, train_wall=24, gb_free=6.5, wall=4632
2023-08-01 17:38:49 - progress_bar.py[line:272] - INFO: epoch 001:   1894 / 2745 loss=1.743, loss_v1=0, loss_v2=0, nll_loss=0.435, ntokens=1017.7, nsentences=144, sample_size=1017.7, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=415.9, ups=0.41, wpb=1017.7, bsz=144, num_updates=1890, lr=2.64193e-05, gnorm=1.123, clip=70, loss_scale=64, train_wall=24, gb_free=6.5, wall=4656
2023-08-01 17:39:13 - progress_bar.py[line:272] - INFO: epoch 001:   1904 / 2745 loss=1.766, loss_v1=0, loss_v2=0, nll_loss=0.462, ntokens=1018.2, nsentences=144, sample_size=1018.2, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=416.6, ups=0.41, wpb=1018.2, bsz=144, num_updates=1900, lr=2.63902e-05, gnorm=1.124, clip=80, loss_scale=64, train_wall=24, gb_free=6.5, wall=4681
2023-08-01 17:39:38 - progress_bar.py[line:272] - INFO: epoch 001:   1914 / 2745 loss=1.756, loss_v1=0, loss_v2=0, nll_loss=0.45, ntokens=1015.7, nsentences=144, sample_size=1015.7, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=415, ups=0.41, wpb=1015.7, bsz=144, num_updates=1910, lr=2.63612e-05, gnorm=1.171, clip=90, loss_scale=64, train_wall=24, gb_free=6.5, wall=4705
2023-08-01 17:40:02 - progress_bar.py[line:272] - INFO: epoch 001:   1924 / 2745 loss=1.753, loss_v1=0, loss_v2=0, nll_loss=0.447, ntokens=1023.8, nsentences=144, sample_size=1023.8, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=419.1, ups=0.41, wpb=1023.8, bsz=144, num_updates=1920, lr=2.63321e-05, gnorm=1.115, clip=90, loss_scale=64, train_wall=24, gb_free=6.5, wall=4730
2023-08-01 17:40:26 - progress_bar.py[line:272] - INFO: epoch 001:   1934 / 2745 loss=1.756, loss_v1=0, loss_v2=0, nll_loss=0.451, ntokens=1006.6, nsentences=144, sample_size=1006.6, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=410.8, ups=0.41, wpb=1006.6, bsz=144, num_updates=1930, lr=2.6303e-05, gnorm=1.167, clip=90, loss_scale=64, train_wall=24, gb_free=6.5, wall=4754
2023-08-01 17:40:51 - progress_bar.py[line:272] - INFO: epoch 001:   1944 / 2745 loss=1.75, loss_v1=0, loss_v2=0, nll_loss=0.444, ntokens=1017, nsentences=144, sample_size=1017, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=416, ups=0.41, wpb=1017, bsz=144, num_updates=1940, lr=2.6274e-05, gnorm=1.163, clip=100, loss_scale=64, train_wall=24, gb_free=6.5, wall=4779
2023-08-01 17:41:15 - progress_bar.py[line:272] - INFO: epoch 001:   1954 / 2745 loss=1.748, loss_v1=0, loss_v2=0, nll_loss=0.441, ntokens=1014.2, nsentences=144, sample_size=1014.2, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=414.5, ups=0.41, wpb=1014.2, bsz=144, num_updates=1950, lr=2.62449e-05, gnorm=1.106, clip=90, loss_scale=64, train_wall=24, gb_free=6.5, wall=4803
2023-08-01 17:41:40 - progress_bar.py[line:272] - INFO: epoch 001:   1964 / 2745 loss=1.752, loss_v1=0, loss_v2=0, nll_loss=0.448, ntokens=1013.3, nsentences=144, sample_size=1013.3, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=412.5, ups=0.41, wpb=1013.3, bsz=144, num_updates=1960, lr=2.62158e-05, gnorm=1.17, clip=100, loss_scale=64, train_wall=25, gb_free=6.5, wall=4828
2023-08-01 17:42:04 - progress_bar.py[line:272] - INFO: epoch 001:   1974 / 2745 loss=1.757, loss_v1=0, loss_v2=0, nll_loss=0.452, ntokens=1013, nsentences=144, sample_size=1013, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=413.4, ups=0.41, wpb=1013, bsz=144, num_updates=1970, lr=2.61868e-05, gnorm=1.167, clip=80, loss_scale=64, train_wall=24, gb_free=6.5, wall=4852
2023-08-01 17:42:29 - progress_bar.py[line:272] - INFO: epoch 001:   1984 / 2745 loss=1.756, loss_v1=0, loss_v2=0, nll_loss=0.452, ntokens=1007.6, nsentences=144, sample_size=1007.6, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=411, ups=0.41, wpb=1007.6, bsz=144, num_updates=1980, lr=2.61577e-05, gnorm=1.082, clip=90, loss_scale=64, train_wall=24, gb_free=6.5, wall=4877
2023-08-01 17:42:53 - progress_bar.py[line:272] - INFO: epoch 001:   1994 / 2745 loss=1.765, loss_v1=0, loss_v2=0, nll_loss=0.461, ntokens=1017.8, nsentences=144, sample_size=1017.8, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=415.6, ups=0.41, wpb=1017.8, bsz=144, num_updates=1990, lr=2.61287e-05, gnorm=1.164, clip=100, loss_scale=64, train_wall=24, gb_free=6.5, wall=4901
2023-08-01 17:43:18 - progress_bar.py[line:272] - INFO: epoch 001:   2004 / 2745 loss=1.755, loss_v1=0, loss_v2=0, nll_loss=0.449, ntokens=1017.7, nsentences=144, sample_size=1017.7, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=416.6, ups=0.41, wpb=1017.7, bsz=144, num_updates=2000, lr=2.60996e-05, gnorm=1.155, clip=70, loss_scale=64, train_wall=24, gb_free=6.5, wall=4926
2023-08-01 17:43:42 - progress_bar.py[line:272] - INFO: epoch 001:   2014 / 2745 loss=1.741, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=1015.3, nsentences=144, sample_size=1015.3, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=413.5, ups=0.41, wpb=1015.3, bsz=144, num_updates=2010, lr=2.60705e-05, gnorm=1.091, clip=70, loss_scale=64, train_wall=25, gb_free=6.5, wall=4950
2023-08-01 17:44:07 - progress_bar.py[line:272] - INFO: epoch 001:   2024 / 2745 loss=1.756, loss_v1=0, loss_v2=0, nll_loss=0.451, ntokens=1015.3, nsentences=144, sample_size=1015.3, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=415.1, ups=0.41, wpb=1015.3, bsz=144, num_updates=2020, lr=2.60415e-05, gnorm=1.176, clip=90, loss_scale=64, train_wall=24, gb_free=6.5, wall=4975
2023-08-01 17:44:31 - progress_bar.py[line:272] - INFO: epoch 001:   2034 / 2745 loss=1.743, loss_v1=0, loss_v2=0, nll_loss=0.436, ntokens=1015.8, nsentences=144, sample_size=1015.8, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=414.7, ups=0.41, wpb=1015.8, bsz=144, num_updates=2030, lr=2.60124e-05, gnorm=1.174, clip=80, loss_scale=64, train_wall=24, gb_free=6.5, wall=4999
2023-08-01 17:44:56 - progress_bar.py[line:272] - INFO: epoch 001:   2044 / 2745 loss=1.745, loss_v1=0, loss_v2=0, nll_loss=0.44, ntokens=1010.2, nsentences=144, sample_size=1010.2, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=412.4, ups=0.41, wpb=1010.2, bsz=144, num_updates=2040, lr=2.59833e-05, gnorm=1.155, clip=90, loss_scale=64, train_wall=24, gb_free=6.5, wall=5024
2023-08-01 17:45:20 - progress_bar.py[line:272] - INFO: epoch 001:   2054 / 2745 loss=1.745, loss_v1=0, loss_v2=0, nll_loss=0.439, ntokens=1012.9, nsentences=144, sample_size=1012.9, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=413.8, ups=0.41, wpb=1012.9, bsz=144, num_updates=2050, lr=2.59543e-05, gnorm=1.093, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=5048
2023-08-01 17:45:45 - progress_bar.py[line:272] - INFO: epoch 001:   2064 / 2745 loss=1.743, loss_v1=0, loss_v2=0, nll_loss=0.438, ntokens=1009.4, nsentences=144, sample_size=1009.4, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=411.1, ups=0.41, wpb=1009.4, bsz=144, num_updates=2060, lr=2.59252e-05, gnorm=1.085, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=5073
2023-08-01 17:46:09 - progress_bar.py[line:272] - INFO: epoch 001:   2074 / 2745 loss=1.753, loss_v1=0, loss_v2=0, nll_loss=0.448, ntokens=1019.6, nsentences=144, sample_size=1019.6, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=416.1, ups=0.41, wpb=1019.6, bsz=144, num_updates=2070, lr=2.58961e-05, gnorm=1.189, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=5097
2023-08-01 17:46:34 - progress_bar.py[line:272] - INFO: epoch 001:   2084 / 2745 loss=1.749, loss_v1=0, loss_v2=0, nll_loss=0.443, ntokens=1018.2, nsentences=144, sample_size=1018.2, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=415.3, ups=0.41, wpb=1018.2, bsz=144, num_updates=2080, lr=2.58671e-05, gnorm=1.131, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=5122
2023-08-01 17:46:59 - progress_bar.py[line:272] - INFO: epoch 001:   2094 / 2745 loss=1.742, loss_v1=0, loss_v2=0, nll_loss=0.438, ntokens=1014.2, nsentences=144, sample_size=1014.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=412.8, ups=0.41, wpb=1014.2, bsz=144, num_updates=2090, lr=2.5838e-05, gnorm=1.153, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=5146
2023-08-01 17:47:23 - progress_bar.py[line:272] - INFO: epoch 001:   2104 / 2745 loss=1.742, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=1014.4, nsentences=144, sample_size=1014.4, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=412.8, ups=0.41, wpb=1014.4, bsz=144, num_updates=2100, lr=2.5809e-05, gnorm=1.107, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=5171
2023-08-01 17:47:48 - progress_bar.py[line:272] - INFO: epoch 001:   2114 / 2745 loss=1.749, loss_v1=0, loss_v2=0, nll_loss=0.444, ntokens=1015.1, nsentences=144, sample_size=1015.1, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=414.1, ups=0.41, wpb=1015.1, bsz=144, num_updates=2110, lr=2.57799e-05, gnorm=1.164, clip=100, loss_scale=128, train_wall=24, gb_free=6.5, wall=5195
2023-08-01 17:48:12 - progress_bar.py[line:272] - INFO: epoch 001:   2124 / 2745 loss=1.753, loss_v1=0, loss_v2=0, nll_loss=0.45, ntokens=1017.9, nsentences=144, sample_size=1017.9, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=413.5, ups=0.41, wpb=1017.9, bsz=144, num_updates=2120, lr=2.57508e-05, gnorm=1.142, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=5220
2023-08-01 17:48:37 - progress_bar.py[line:272] - INFO: epoch 001:   2134 / 2745 loss=1.746, loss_v1=0, loss_v2=0, nll_loss=0.438, ntokens=1020, nsentences=144, sample_size=1020, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=417.1, ups=0.41, wpb=1020, bsz=144, num_updates=2130, lr=2.57218e-05, gnorm=1.116, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=5244
2023-08-01 17:49:01 - progress_bar.py[line:272] - INFO: epoch 001:   2144 / 2745 loss=1.736, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=1015.2, nsentences=144, sample_size=1015.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=413.1, ups=0.41, wpb=1015.2, bsz=144, num_updates=2140, lr=2.56927e-05, gnorm=1.151, clip=100, loss_scale=128, train_wall=25, gb_free=6.5, wall=5269
2023-08-01 17:49:26 - progress_bar.py[line:272] - INFO: epoch 001:   2154 / 2745 loss=1.739, loss_v1=0, loss_v2=0, nll_loss=0.433, ntokens=1010.7, nsentences=144, sample_size=1010.7, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=410.4, ups=0.41, wpb=1010.7, bsz=144, num_updates=2150, lr=2.56636e-05, gnorm=1.071, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=5294
2023-08-01 17:49:50 - progress_bar.py[line:272] - INFO: epoch 001:   2164 / 2745 loss=1.747, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=1015.9, nsentences=144, sample_size=1015.9, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=414.1, ups=0.41, wpb=1015.9, bsz=144, num_updates=2160, lr=2.56346e-05, gnorm=1.116, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=5318
2023-08-01 17:50:15 - progress_bar.py[line:272] - INFO: epoch 001:   2174 / 2745 loss=1.749, loss_v1=0, loss_v2=0, nll_loss=0.445, ntokens=1019.2, nsentences=144, sample_size=1019.2, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=415.8, ups=0.41, wpb=1019.2, bsz=144, num_updates=2170, lr=2.56055e-05, gnorm=1.166, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=5343
2023-08-01 17:50:40 - progress_bar.py[line:272] - INFO: epoch 001:   2184 / 2745 loss=1.738, loss_v1=0, loss_v2=0, nll_loss=0.433, ntokens=1013.3, nsentences=144, sample_size=1013.3, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=413.4, ups=0.41, wpb=1013.3, bsz=144, num_updates=2180, lr=2.55764e-05, gnorm=1.139, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=5367
2023-08-01 17:51:04 - progress_bar.py[line:272] - INFO: epoch 001:   2194 / 2745 loss=1.74, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=1017.6, nsentences=144, sample_size=1017.6, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=415, ups=0.41, wpb=1017.6, bsz=144, num_updates=2190, lr=2.55474e-05, gnorm=1.21, clip=100, loss_scale=128, train_wall=24, gb_free=6.5, wall=5392
2023-08-01 17:51:29 - progress_bar.py[line:272] - INFO: epoch 001:   2204 / 2745 loss=1.748, loss_v1=0, loss_v2=0, nll_loss=0.444, ntokens=1015.3, nsentences=144, sample_size=1015.3, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=413.4, ups=0.41, wpb=1015.3, bsz=144, num_updates=2200, lr=2.55183e-05, gnorm=1.074, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=5416
2023-08-01 17:51:53 - progress_bar.py[line:272] - INFO: epoch 001:   2214 / 2745 loss=1.731, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=1012.3, nsentences=144, sample_size=1012.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=412.2, ups=0.41, wpb=1012.3, bsz=144, num_updates=2210, lr=2.54892e-05, gnorm=1.228, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=5441
2023-08-01 17:52:18 - progress_bar.py[line:272] - INFO: epoch 001:   2224 / 2745 loss=1.751, loss_v1=0, loss_v2=0, nll_loss=0.448, ntokens=1016.1, nsentences=144, sample_size=1016.1, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=412.8, ups=0.41, wpb=1016.1, bsz=144, num_updates=2220, lr=2.54602e-05, gnorm=1.186, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=5465
2023-08-01 17:52:42 - progress_bar.py[line:272] - INFO: epoch 001:   2234 / 2745 loss=1.739, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=1019.7, nsentences=144, sample_size=1019.7, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=417.4, ups=0.41, wpb=1019.7, bsz=144, num_updates=2230, lr=2.54311e-05, gnorm=1.121, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=5490
2023-08-01 17:53:07 - progress_bar.py[line:272] - INFO: epoch 001:   2244 / 2745 loss=1.738, loss_v1=0, loss_v2=0, nll_loss=0.433, ntokens=1015, nsentences=144, sample_size=1015, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=413, ups=0.41, wpb=1015, bsz=144, num_updates=2240, lr=2.54021e-05, gnorm=1.128, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=5514
2023-08-01 17:53:31 - progress_bar.py[line:272] - INFO: epoch 001:   2254 / 2745 loss=1.74, loss_v1=0, loss_v2=0, nll_loss=0.435, ntokens=1015.1, nsentences=144, sample_size=1015.1, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=414.7, ups=0.41, wpb=1015.1, bsz=144, num_updates=2250, lr=2.5373e-05, gnorm=1.049, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=5539
2023-08-01 17:53:56 - progress_bar.py[line:272] - INFO: epoch 001:   2264 / 2745 loss=1.745, loss_v1=0, loss_v2=0, nll_loss=0.441, ntokens=1012.9, nsentences=144, sample_size=1012.9, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=414.3, ups=0.41, wpb=1012.9, bsz=144, num_updates=2260, lr=2.53439e-05, gnorm=1.203, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=5563
2023-08-01 17:54:20 - progress_bar.py[line:272] - INFO: epoch 001:   2274 / 2745 loss=1.733, loss_v1=0, loss_v2=0, nll_loss=0.427, ntokens=1014.3, nsentences=144, sample_size=1014.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=412.7, ups=0.41, wpb=1014.3, bsz=144, num_updates=2270, lr=2.53149e-05, gnorm=1.161, clip=100, loss_scale=128, train_wall=25, gb_free=6.5, wall=5588
2023-08-01 17:54:45 - progress_bar.py[line:272] - INFO: epoch 001:   2284 / 2745 loss=1.731, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=1017.6, nsentences=144, sample_size=1017.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=413.6, ups=0.41, wpb=1017.6, bsz=144, num_updates=2280, lr=2.52858e-05, gnorm=1.211, clip=100, loss_scale=128, train_wall=25, gb_free=6.5, wall=5613
2023-08-01 17:55:09 - progress_bar.py[line:272] - INFO: epoch 001:   2294 / 2745 loss=1.749, loss_v1=0, loss_v2=0, nll_loss=0.447, ntokens=1012.8, nsentences=144, sample_size=1012.8, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=412.9, ups=0.41, wpb=1012.8, bsz=144, num_updates=2290, lr=2.52567e-05, gnorm=1.245, clip=100, loss_scale=128, train_wall=24, gb_free=6.5, wall=5637
2023-08-01 17:55:34 - progress_bar.py[line:272] - INFO: epoch 001:   2304 / 2745 loss=1.743, loss_v1=0, loss_v2=0, nll_loss=0.438, ntokens=1013.7, nsentences=144, sample_size=1013.7, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=412, ups=0.41, wpb=1013.7, bsz=144, num_updates=2300, lr=2.52277e-05, gnorm=1.143, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=5662
2023-08-01 17:55:59 - progress_bar.py[line:272] - INFO: epoch 001:   2314 / 2745 loss=1.736, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=1019.6, nsentences=144, sample_size=1019.6, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=416.2, ups=0.41, wpb=1019.6, bsz=144, num_updates=2310, lr=2.51986e-05, gnorm=1.034, clip=60, loss_scale=128, train_wall=24, gb_free=6.5, wall=5686
2023-08-01 17:56:23 - progress_bar.py[line:272] - INFO: epoch 001:   2324 / 2745 loss=1.736, loss_v1=0, loss_v2=0, nll_loss=0.43, ntokens=1015.7, nsentences=144, sample_size=1015.7, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=414.6, ups=0.41, wpb=1015.7, bsz=144, num_updates=2320, lr=2.51695e-05, gnorm=1.19, clip=100, loss_scale=128, train_wall=24, gb_free=6.5, wall=5711
2023-08-01 17:56:48 - progress_bar.py[line:272] - INFO: epoch 001:   2334 / 2745 loss=1.728, loss_v1=0, loss_v2=0, nll_loss=0.422, ntokens=1014.7, nsentences=144, sample_size=1014.7, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=413.6, ups=0.41, wpb=1014.7, bsz=144, num_updates=2330, lr=2.51405e-05, gnorm=1.095, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=5735
2023-08-01 17:57:12 - progress_bar.py[line:272] - INFO: epoch 001:   2344 / 2745 loss=1.745, loss_v1=0, loss_v2=0, nll_loss=0.44, ntokens=1017.9, nsentences=144, sample_size=1017.9, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=415.6, ups=0.41, wpb=1017.9, bsz=144, num_updates=2340, lr=2.51114e-05, gnorm=1.135, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=5760
2023-08-01 17:57:37 - progress_bar.py[line:272] - INFO: epoch 001:   2354 / 2745 loss=1.735, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=1007.1, nsentences=144, sample_size=1007.1, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=410.4, ups=0.41, wpb=1007.1, bsz=144, num_updates=2350, lr=2.50823e-05, gnorm=1.109, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=5784
2023-08-01 17:58:01 - progress_bar.py[line:272] - INFO: epoch 001:   2364 / 2745 loss=1.729, loss_v1=0, loss_v2=0, nll_loss=0.422, ntokens=1014.1, nsentences=144, sample_size=1014.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=414.5, ups=0.41, wpb=1014.1, bsz=144, num_updates=2360, lr=2.50533e-05, gnorm=1.193, clip=100, loss_scale=128, train_wall=24, gb_free=6.5, wall=5809
2023-08-01 17:58:26 - progress_bar.py[line:272] - INFO: epoch 001:   2374 / 2745 loss=1.741, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=1011.1, nsentences=144, sample_size=1011.1, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=412.1, ups=0.41, wpb=1011.1, bsz=144, num_updates=2370, lr=2.50242e-05, gnorm=1.139, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=5833
2023-08-01 17:58:50 - progress_bar.py[line:272] - INFO: epoch 001:   2384 / 2745 loss=1.733, loss_v1=0, loss_v2=0, nll_loss=0.428, ntokens=1017.1, nsentences=144, sample_size=1017.1, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=414.7, ups=0.41, wpb=1017.1, bsz=144, num_updates=2380, lr=2.49952e-05, gnorm=1.134, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=5858
2023-08-01 17:59:15 - progress_bar.py[line:272] - INFO: epoch 001:   2394 / 2745 loss=1.725, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=1020.7, nsentences=144, sample_size=1020.7, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=416.4, ups=0.41, wpb=1020.7, bsz=144, num_updates=2390, lr=2.49661e-05, gnorm=1.087, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=5882
2023-08-01 17:59:39 - progress_bar.py[line:272] - INFO: epoch 001:   2404 / 2745 loss=1.741, loss_v1=0, loss_v2=0, nll_loss=0.438, ntokens=1013.9, nsentences=144, sample_size=1013.9, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=414.5, ups=0.41, wpb=1013.9, bsz=144, num_updates=2400, lr=2.4937e-05, gnorm=1.115, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=5907
2023-08-01 18:00:04 - progress_bar.py[line:272] - INFO: epoch 001:   2414 / 2745 loss=1.728, loss_v1=0, loss_v2=0, nll_loss=0.422, ntokens=1011.3, nsentences=144, sample_size=1011.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=411.1, ups=0.41, wpb=1011.3, bsz=144, num_updates=2410, lr=2.4908e-05, gnorm=1.181, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=5931
2023-08-01 18:00:28 - progress_bar.py[line:272] - INFO: epoch 001:   2424 / 2745 loss=1.745, loss_v1=0, loss_v2=0, nll_loss=0.441, ntokens=1015.9, nsentences=144, sample_size=1015.9, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=415, ups=0.41, wpb=1015.9, bsz=144, num_updates=2420, lr=2.48789e-05, gnorm=1.109, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=5956
2023-08-01 18:00:53 - progress_bar.py[line:272] - INFO: epoch 001:   2434 / 2745 loss=1.734, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=1013.5, nsentences=144, sample_size=1013.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=413.2, ups=0.41, wpb=1013.5, bsz=144, num_updates=2430, lr=2.48498e-05, gnorm=1.138, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=5980
2023-08-01 18:01:17 - progress_bar.py[line:272] - INFO: epoch 001:   2444 / 2745 loss=1.721, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=1010, nsentences=144, sample_size=1010, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=412, ups=0.41, wpb=1010, bsz=144, num_updates=2440, lr=2.48208e-05, gnorm=1.106, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=6005
2023-08-01 18:01:42 - progress_bar.py[line:272] - INFO: epoch 001:   2454 / 2745 loss=1.736, loss_v1=0, loss_v2=0, nll_loss=0.432, ntokens=1010.1, nsentences=144, sample_size=1010.1, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=411.4, ups=0.41, wpb=1010.1, bsz=144, num_updates=2450, lr=2.47917e-05, gnorm=1.246, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=6030
2023-08-01 18:02:06 - progress_bar.py[line:272] - INFO: epoch 001:   2464 / 2745 loss=1.729, loss_v1=0, loss_v2=0, nll_loss=0.424, ntokens=1018, nsentences=144, sample_size=1018, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=416.7, ups=0.41, wpb=1018, bsz=144, num_updates=2460, lr=2.47626e-05, gnorm=1.151, clip=100, loss_scale=128, train_wall=24, gb_free=6.5, wall=6054
2023-08-01 18:02:31 - progress_bar.py[line:272] - INFO: epoch 001:   2474 / 2745 loss=1.755, loss_v1=0, loss_v2=0, nll_loss=0.454, ntokens=1014.6, nsentences=144, sample_size=1014.6, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=414.3, ups=0.41, wpb=1014.6, bsz=144, num_updates=2470, lr=2.47336e-05, gnorm=1.222, clip=100, loss_scale=128, train_wall=24, gb_free=6.5, wall=6078
2023-08-01 18:02:55 - progress_bar.py[line:272] - INFO: epoch 001:   2484 / 2745 loss=1.73, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=1015.4, nsentences=144, sample_size=1015.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=413.8, ups=0.41, wpb=1015.4, bsz=144, num_updates=2480, lr=2.47045e-05, gnorm=1.067, clip=70, loss_scale=128, train_wall=25, gb_free=6.5, wall=6103
2023-08-01 18:03:20 - progress_bar.py[line:272] - INFO: epoch 001:   2494 / 2745 loss=1.722, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=1017, nsentences=144, sample_size=1017, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=414.8, ups=0.41, wpb=1017, bsz=144, num_updates=2490, lr=2.46755e-05, gnorm=1.04, clip=60, loss_scale=128, train_wall=24, gb_free=6.5, wall=6128
2023-08-01 18:03:44 - progress_bar.py[line:272] - INFO: epoch 001:   2504 / 2745 loss=1.72, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=1015.9, nsentences=144, sample_size=1015.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=414.4, ups=0.41, wpb=1015.9, bsz=144, num_updates=2500, lr=2.46464e-05, gnorm=1.07, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=6152
2023-08-01 18:04:09 - progress_bar.py[line:272] - INFO: epoch 001:   2514 / 2745 loss=1.73, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=1012.5, nsentences=144, sample_size=1012.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=412.2, ups=0.41, wpb=1012.5, bsz=144, num_updates=2510, lr=2.46173e-05, gnorm=1.044, clip=50, loss_scale=128, train_wall=25, gb_free=6.5, wall=6177
2023-08-01 18:04:33 - progress_bar.py[line:272] - INFO: epoch 001:   2524 / 2745 loss=1.738, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=1004.2, nsentences=144, sample_size=1004.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=409.3, ups=0.41, wpb=1004.2, bsz=144, num_updates=2520, lr=2.45883e-05, gnorm=1.124, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=6201
2023-08-01 18:04:58 - progress_bar.py[line:272] - INFO: epoch 001:   2534 / 2745 loss=1.738, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=1013.3, nsentences=144, sample_size=1013.3, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=412, ups=0.41, wpb=1013.3, bsz=144, num_updates=2530, lr=2.45592e-05, gnorm=1.179, clip=100, loss_scale=128, train_wall=25, gb_free=6.5, wall=6226
2023-08-01 18:05:23 - progress_bar.py[line:272] - INFO: epoch 001:   2544 / 2745 loss=1.744, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=1019.1, nsentences=144, sample_size=1019.1, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=415.2, ups=0.41, wpb=1019.1, bsz=144, num_updates=2540, lr=2.45301e-05, gnorm=1.178, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=6250
2023-08-01 18:05:47 - progress_bar.py[line:272] - INFO: epoch 001:   2554 / 2745 loss=1.728, loss_v1=0, loss_v2=0, nll_loss=0.422, ntokens=1015.4, nsentences=144, sample_size=1015.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=412.9, ups=0.41, wpb=1015.4, bsz=144, num_updates=2550, lr=2.45011e-05, gnorm=1.138, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=6275
2023-08-01 18:06:12 - progress_bar.py[line:272] - INFO: epoch 001:   2564 / 2745 loss=1.729, loss_v1=0, loss_v2=0, nll_loss=0.423, ntokens=1012, nsentences=144, sample_size=1012, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=411.8, ups=0.41, wpb=1012, bsz=144, num_updates=2560, lr=2.4472e-05, gnorm=1.07, clip=80, loss_scale=256, train_wall=25, gb_free=6.5, wall=6299
2023-08-01 18:06:36 - progress_bar.py[line:272] - INFO: epoch 001:   2574 / 2745 loss=1.729, loss_v1=0, loss_v2=0, nll_loss=0.424, ntokens=1009.6, nsentences=144, sample_size=1009.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=410, ups=0.41, wpb=1009.6, bsz=144, num_updates=2570, lr=2.44429e-05, gnorm=1.101, clip=80, loss_scale=256, train_wall=25, gb_free=6.5, wall=6324
2023-08-01 18:07:01 - progress_bar.py[line:272] - INFO: epoch 001:   2584 / 2745 loss=1.73, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=1019.7, nsentences=144, sample_size=1019.7, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=416, ups=0.41, wpb=1019.7, bsz=144, num_updates=2580, lr=2.44139e-05, gnorm=1.143, clip=60, loss_scale=256, train_wall=24, gb_free=6.5, wall=6349
2023-08-01 18:07:25 - progress_bar.py[line:272] - INFO: epoch 001:   2594 / 2745 loss=1.736, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=1009.6, nsentences=144, sample_size=1009.6, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=411.7, ups=0.41, wpb=1009.6, bsz=144, num_updates=2590, lr=2.43848e-05, gnorm=1.136, clip=100, loss_scale=256, train_wall=24, gb_free=6.5, wall=6373
2023-08-01 18:07:50 - progress_bar.py[line:272] - INFO: epoch 001:   2604 / 2745 loss=1.729, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=1013.6, nsentences=144, sample_size=1013.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=413.3, ups=0.41, wpb=1013.6, bsz=144, num_updates=2600, lr=2.43557e-05, gnorm=1.128, clip=90, loss_scale=256, train_wall=24, gb_free=6.5, wall=6398
2023-08-01 18:08:15 - progress_bar.py[line:272] - INFO: epoch 001:   2614 / 2745 loss=1.735, loss_v1=0, loss_v2=0, nll_loss=0.43, ntokens=1010.7, nsentences=144, sample_size=1010.7, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=411.2, ups=0.41, wpb=1010.7, bsz=144, num_updates=2610, lr=2.43267e-05, gnorm=1.144, clip=100, loss_scale=256, train_wall=25, gb_free=6.5, wall=6422
2023-08-01 18:08:39 - progress_bar.py[line:272] - INFO: epoch 001:   2624 / 2745 loss=1.727, loss_v1=0, loss_v2=0, nll_loss=0.423, ntokens=1019.5, nsentences=144, sample_size=1019.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=415.1, ups=0.41, wpb=1019.5, bsz=144, num_updates=2620, lr=2.42976e-05, gnorm=1.113, clip=80, loss_scale=256, train_wall=25, gb_free=6.5, wall=6447
2023-08-01 18:09:04 - progress_bar.py[line:272] - INFO: epoch 001:   2634 / 2745 loss=1.718, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=1012.6, nsentences=144, sample_size=1012.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=411.9, ups=0.41, wpb=1012.6, bsz=144, num_updates=2630, lr=2.42686e-05, gnorm=1.165, clip=90, loss_scale=256, train_wall=25, gb_free=6.5, wall=6471
2023-08-01 18:09:28 - progress_bar.py[line:272] - INFO: epoch 001:   2644 / 2745 loss=1.736, loss_v1=0, loss_v2=0, nll_loss=0.432, ntokens=1016.9, nsentences=144, sample_size=1016.9, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=414.2, ups=0.41, wpb=1016.9, bsz=144, num_updates=2640, lr=2.42395e-05, gnorm=1.137, clip=80, loss_scale=256, train_wall=25, gb_free=6.5, wall=6496
2023-08-01 18:09:53 - progress_bar.py[line:272] - INFO: epoch 001:   2654 / 2745 loss=1.727, loss_v1=0, loss_v2=0, nll_loss=0.422, ntokens=1014.1, nsentences=144, sample_size=1014.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=411.6, ups=0.41, wpb=1014.1, bsz=144, num_updates=2650, lr=2.42104e-05, gnorm=1.114, clip=70, loss_scale=256, train_wall=25, gb_free=6.5, wall=6521
2023-08-01 18:10:17 - progress_bar.py[line:272] - INFO: epoch 001:   2664 / 2745 loss=1.711, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=1015.1, nsentences=144, sample_size=1015.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=413.5, ups=0.41, wpb=1015.1, bsz=144, num_updates=2660, lr=2.41814e-05, gnorm=1.069, clip=80, loss_scale=256, train_wall=25, gb_free=6.5, wall=6545
2023-08-01 18:10:42 - progress_bar.py[line:272] - INFO: epoch 001:   2674 / 2745 loss=1.726, loss_v1=0, loss_v2=0, nll_loss=0.421, ntokens=1017.9, nsentences=144, sample_size=1017.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=415.3, ups=0.41, wpb=1017.9, bsz=144, num_updates=2670, lr=2.41523e-05, gnorm=1.146, clip=90, loss_scale=256, train_wall=24, gb_free=6.5, wall=6570
2023-08-01 18:10:57 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-08-01 18:11:09 - progress_bar.py[line:272] - INFO: epoch 001:   2685 / 2745 loss=1.724, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=1016.5, nsentences=144, sample_size=1016.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=376.1, ups=0.37, wpb=1016.5, bsz=144, num_updates=2680, lr=2.41232e-05, gnorm=1.054, clip=50, loss_scale=128, train_wall=27, gb_free=6.5, wall=6597
2023-08-01 18:11:34 - progress_bar.py[line:272] - INFO: epoch 001:   2695 / 2745 loss=1.727, loss_v1=0, loss_v2=0, nll_loss=0.422, ntokens=1011.3, nsentences=144, sample_size=1011.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=411.7, ups=0.41, wpb=1011.3, bsz=144, num_updates=2690, lr=2.40942e-05, gnorm=1.133, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=6621
2023-08-01 18:11:58 - progress_bar.py[line:272] - INFO: epoch 001:   2705 / 2745 loss=1.726, loss_v1=0, loss_v2=0, nll_loss=0.422, ntokens=1010.3, nsentences=144, sample_size=1010.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=411, ups=0.41, wpb=1010.3, bsz=144, num_updates=2700, lr=2.40651e-05, gnorm=1.118, clip=70, loss_scale=128, train_wall=25, gb_free=6.5, wall=6646
2023-08-01 18:12:23 - progress_bar.py[line:272] - INFO: epoch 001:   2715 / 2745 loss=1.736, loss_v1=0, loss_v2=0, nll_loss=0.433, ntokens=1013.5, nsentences=144, sample_size=1013.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=413.4, ups=0.41, wpb=1013.5, bsz=144, num_updates=2710, lr=2.4036e-05, gnorm=1.154, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=6670
2023-08-01 18:12:47 - progress_bar.py[line:272] - INFO: epoch 001:   2725 / 2745 loss=1.729, loss_v1=0, loss_v2=0, nll_loss=0.424, ntokens=1014.2, nsentences=144, sample_size=1014.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=413.2, ups=0.41, wpb=1014.2, bsz=144, num_updates=2720, lr=2.4007e-05, gnorm=1.216, clip=100, loss_scale=128, train_wall=25, gb_free=6.5, wall=6695
2023-08-01 18:13:12 - progress_bar.py[line:272] - INFO: epoch 001:   2735 / 2745 loss=1.724, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=1017.2, nsentences=144, sample_size=1017.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=414.1, ups=0.41, wpb=1017.2, bsz=144, num_updates=2730, lr=2.39779e-05, gnorm=1.195, clip=100, loss_scale=128, train_wall=25, gb_free=6.5, wall=6719
local datafile ../../dataset/OFA_data/vrd_mix/vg_train_wearing.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/vrd_mix/vg_train_wearing.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
2023-08-01 18:13:35 - progress_bar.py[line:272] - INFO: epoch 001:   2745 / 2745 loss=1.719, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=965.9, nsentences=136.8, sample_size=965.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=414.3, ups=0.43, wpb=965.9, bsz=136.8, num_updates=2740, lr=2.39488e-05, gnorm=1.203, clip=90, loss_scale=128, train_wall=23, gb_free=6.5, wall=6743
2023-08-01 18:13:35 - train.py[line:332] - INFO: end of epoch 1 (average epoch stats below)
2023-08-01 18:13:35 - progress_bar.py[line:282] - INFO: epoch 001 | loss 2.241 | loss_v1 0 | loss_v2 0 | nll_loss 0.954 | ntokens 1014.55 | nsentences 143.962 | sample_size 1014.55 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.94 | wps 413.7 | ups 0.41 | wpb 1014.5 | bsz 144 | num_updates 2740 | lr 2.39488e-05 | gnorm 2.496 | clip 93.9 | loss_scale 128 | train_wall 6724 | gb_free 6.5 | wall 6743
2023-08-01 18:13:35 - trainer.py[line:639] - INFO: loading train data for epoch 2
local datafile ../../dataset/OFA_data/vrd_mix/vg_train_wearing.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/vrd_mix/vg_train_wearing.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/vrd_mix/vg_train_wearing.tsv slice_id 1 row count 131725 total row count 395175
local datafile ../../dataset/OFA_data/vrd_mix/vg_train_wearing.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/vrd_mix/vg_train_wearing.tsv slice_id 2 row count 131725 total row count 395175
local datafile ../../dataset/OFA_data/vrd_mix/vg_train_wearing.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/vrd_mix/vg_train_wearing.tsv slice_id 0 row count 131725 total row count 395175
/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
slice_id 2 seek offset 263450
slice_id 1 seek offset 131725
slice_id 0 seek offset 0
2023-08-01 18:13:35 - trainer.py[line:703] - INFO: begin training epoch 2
2023-08-01 18:13:35 - train.py[line:305] - INFO: Start iterating over samples
2023-08-01 18:14:00 - progress_bar.py[line:272] - INFO: epoch 002:     10 / 2745 loss=1.727, loss_v1=0, loss_v2=0, nll_loss=0.422, ntokens=1013.9, nsentences=144, sample_size=1013.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=400, ups=0.39, wpb=1013.9, bsz=144, num_updates=2750, lr=2.39198e-05, gnorm=1.151, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=6768
2023-08-01 18:14:25 - progress_bar.py[line:272] - INFO: epoch 002:     20 / 2745 loss=1.734, loss_v1=0, loss_v2=0, nll_loss=0.43, ntokens=1016.4, nsentences=144, sample_size=1016.4, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=414.3, ups=0.41, wpb=1016.4, bsz=144, num_updates=2760, lr=2.38907e-05, gnorm=1.1, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=6793
2023-08-01 18:14:50 - progress_bar.py[line:272] - INFO: epoch 002:     30 / 2745 loss=1.725, loss_v1=0, loss_v2=0, nll_loss=0.423, ntokens=1015.7, nsentences=144, sample_size=1015.7, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=413.2, ups=0.41, wpb=1015.7, bsz=144, num_updates=2770, lr=2.38617e-05, gnorm=1.142, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=6817
2023-08-01 18:15:14 - progress_bar.py[line:272] - INFO: epoch 002:     40 / 2745 loss=1.73, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=1016.3, nsentences=144, sample_size=1016.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=414.1, ups=0.41, wpb=1016.3, bsz=144, num_updates=2780, lr=2.38326e-05, gnorm=1.09, clip=70, loss_scale=128, train_wall=25, gb_free=6.5, wall=6842
2023-08-01 18:15:39 - progress_bar.py[line:272] - INFO: epoch 002:     50 / 2745 loss=1.716, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=1011.5, nsentences=144, sample_size=1011.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=411.3, ups=0.41, wpb=1011.5, bsz=144, num_updates=2790, lr=2.38035e-05, gnorm=1.078, clip=70, loss_scale=128, train_wall=25, gb_free=6.5, wall=6866
2023-08-01 18:16:03 - progress_bar.py[line:272] - INFO: epoch 002:     60 / 2745 loss=1.716, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=1008.8, nsentences=144, sample_size=1008.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=410.6, ups=0.41, wpb=1008.8, bsz=144, num_updates=2800, lr=2.37745e-05, gnorm=1.086, clip=70, loss_scale=128, train_wall=25, gb_free=6.5, wall=6891
2023-08-01 18:16:28 - progress_bar.py[line:272] - INFO: epoch 002:     70 / 2745 loss=1.714, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=1015.6, nsentences=144, sample_size=1015.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=414.6, ups=0.41, wpb=1015.6, bsz=144, num_updates=2810, lr=2.37454e-05, gnorm=1.115, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=6915
2023-08-01 18:16:52 - progress_bar.py[line:272] - INFO: epoch 002:     80 / 2745 loss=1.739, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=1011.9, nsentences=144, sample_size=1011.9, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=412.2, ups=0.41, wpb=1011.9, bsz=144, num_updates=2820, lr=2.37163e-05, gnorm=1.19, clip=100, loss_scale=128, train_wall=25, gb_free=6.5, wall=6940
2023-08-01 18:17:17 - progress_bar.py[line:272] - INFO: epoch 002:     90 / 2745 loss=1.716, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=1012.5, nsentences=144, sample_size=1012.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=411.7, ups=0.41, wpb=1012.5, bsz=144, num_updates=2830, lr=2.36873e-05, gnorm=1.13, clip=70, loss_scale=128, train_wall=25, gb_free=6.5, wall=6965
2023-08-01 18:17:41 - progress_bar.py[line:272] - INFO: epoch 002:    100 / 2745 loss=1.73, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=1017.6, nsentences=144, sample_size=1017.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=414.2, ups=0.41, wpb=1017.6, bsz=144, num_updates=2840, lr=2.36582e-05, gnorm=1.117, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=6989
2023-08-01 18:18:06 - progress_bar.py[line:272] - INFO: epoch 002:    110 / 2745 loss=1.725, loss_v1=0, loss_v2=0, nll_loss=0.421, ntokens=1009.8, nsentences=144, sample_size=1009.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=411.7, ups=0.41, wpb=1009.8, bsz=144, num_updates=2850, lr=2.36291e-05, gnorm=1.115, clip=70, loss_scale=128, train_wall=25, gb_free=6.5, wall=7014
2023-08-01 18:18:31 - progress_bar.py[line:272] - INFO: epoch 002:    120 / 2745 loss=1.705, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=1013.5, nsentences=144, sample_size=1013.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=413.3, ups=0.41, wpb=1013.5, bsz=144, num_updates=2860, lr=2.36001e-05, gnorm=1.124, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=7038
2023-08-01 18:18:55 - progress_bar.py[line:272] - INFO: epoch 002:    130 / 2745 loss=1.72, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=1013.1, nsentences=144, sample_size=1013.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=412.3, ups=0.41, wpb=1013.1, bsz=144, num_updates=2870, lr=2.3571e-05, gnorm=1.145, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=7063
2023-08-01 18:19:20 - progress_bar.py[line:272] - INFO: epoch 002:    140 / 2745 loss=1.72, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=1017.1, nsentences=144, sample_size=1017.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=415.6, ups=0.41, wpb=1017.1, bsz=144, num_updates=2880, lr=2.35419e-05, gnorm=1.099, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=7087
2023-08-01 18:19:44 - progress_bar.py[line:272] - INFO: epoch 002:    150 / 2745 loss=1.713, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=1011.7, nsentences=144, sample_size=1011.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=413.3, ups=0.41, wpb=1011.7, bsz=144, num_updates=2890, lr=2.35129e-05, gnorm=1.113, clip=100, loss_scale=128, train_wall=24, gb_free=6.5, wall=7112
2023-08-01 18:20:09 - progress_bar.py[line:272] - INFO: epoch 002:    160 / 2745 loss=1.739, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=1012.5, nsentences=144, sample_size=1012.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=411.2, ups=0.41, wpb=1012.5, bsz=144, num_updates=2900, lr=2.34838e-05, gnorm=1.114, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=7136
2023-08-01 18:20:33 - progress_bar.py[line:272] - INFO: epoch 002:    170 / 2745 loss=1.723, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=1013.7, nsentences=144, sample_size=1013.7, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=412.6, ups=0.41, wpb=1013.7, bsz=144, num_updates=2910, lr=2.34548e-05, gnorm=1.099, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=7161
2023-08-01 18:20:58 - progress_bar.py[line:272] - INFO: epoch 002:    180 / 2745 loss=1.72, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=1010, nsentences=144, sample_size=1010, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=410.9, ups=0.41, wpb=1010, bsz=144, num_updates=2920, lr=2.34257e-05, gnorm=1.165, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=7186
2023-08-01 18:21:22 - progress_bar.py[line:272] - INFO: epoch 002:    190 / 2745 loss=1.721, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=1013.9, nsentences=144, sample_size=1013.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=412.3, ups=0.41, wpb=1013.9, bsz=144, num_updates=2930, lr=2.33966e-05, gnorm=1.132, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=7210
2023-08-01 18:21:47 - progress_bar.py[line:272] - INFO: epoch 002:    200 / 2745 loss=1.723, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=1021, nsentences=144, sample_size=1021, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=414.8, ups=0.41, wpb=1021, bsz=144, num_updates=2940, lr=2.33676e-05, gnorm=1.11, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=7235
2023-08-01 18:22:12 - progress_bar.py[line:272] - INFO: epoch 002:    210 / 2745 loss=1.713, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=1013.8, nsentences=144, sample_size=1013.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=411.9, ups=0.41, wpb=1013.8, bsz=144, num_updates=2950, lr=2.33385e-05, gnorm=1.097, clip=70, loss_scale=128, train_wall=25, gb_free=6.5, wall=7259
2023-08-01 18:22:36 - progress_bar.py[line:272] - INFO: epoch 002:    220 / 2745 loss=1.713, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=1012.6, nsentences=144, sample_size=1012.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=412, ups=0.41, wpb=1012.6, bsz=144, num_updates=2960, lr=2.33094e-05, gnorm=1.176, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=7284
2023-08-01 18:23:01 - progress_bar.py[line:272] - INFO: epoch 002:    230 / 2745 loss=1.726, loss_v1=0, loss_v2=0, nll_loss=0.424, ntokens=1017.5, nsentences=144, sample_size=1017.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=416.2, ups=0.41, wpb=1017.5, bsz=144, num_updates=2970, lr=2.32804e-05, gnorm=1.123, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=7308
2023-08-01 18:23:25 - progress_bar.py[line:272] - INFO: epoch 002:    240 / 2745 loss=1.721, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=1010.4, nsentences=144, sample_size=1010.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=411.7, ups=0.41, wpb=1010.4, bsz=144, num_updates=2980, lr=2.32513e-05, gnorm=1.109, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=7333
2023-08-01 18:23:50 - progress_bar.py[line:272] - INFO: epoch 002:    250 / 2745 loss=1.723, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=1016.4, nsentences=144, sample_size=1016.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=416.1, ups=0.41, wpb=1016.4, bsz=144, num_updates=2990, lr=2.32222e-05, gnorm=1.076, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=7357
2023-08-01 18:24:14 - progress_bar.py[line:272] - INFO: epoch 002:    260 / 2745 loss=1.707, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=1014.9, nsentences=144, sample_size=1014.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=413.5, ups=0.41, wpb=1014.9, bsz=144, num_updates=3000, lr=2.31932e-05, gnorm=1.039, clip=50, loss_scale=128, train_wall=25, gb_free=6.5, wall=7382
2023-08-01 18:24:39 - progress_bar.py[line:272] - INFO: epoch 002:    270 / 2745 loss=1.733, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=1011, nsentences=144, sample_size=1011, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=412, ups=0.41, wpb=1011, bsz=144, num_updates=3010, lr=2.31641e-05, gnorm=1.116, clip=70, loss_scale=128, train_wall=25, gb_free=6.5, wall=7406
2023-08-01 18:25:03 - progress_bar.py[line:272] - INFO: epoch 002:    280 / 2745 loss=1.721, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=1014.9, nsentences=144, sample_size=1014.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=412.6, ups=0.41, wpb=1014.9, bsz=144, num_updates=3020, lr=2.31351e-05, gnorm=1.114, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=7431
2023-08-01 18:25:28 - progress_bar.py[line:272] - INFO: epoch 002:    290 / 2745 loss=1.725, loss_v1=0, loss_v2=0, nll_loss=0.421, ntokens=1014.5, nsentences=144, sample_size=1014.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=413.4, ups=0.41, wpb=1014.5, bsz=144, num_updates=3030, lr=2.3106e-05, gnorm=1.085, clip=70, loss_scale=128, train_wall=25, gb_free=6.5, wall=7456
2023-08-01 18:25:52 - progress_bar.py[line:272] - INFO: epoch 002:    300 / 2745 loss=1.717, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=1016.4, nsentences=144, sample_size=1016.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=414.8, ups=0.41, wpb=1016.4, bsz=144, num_updates=3040, lr=2.30769e-05, gnorm=1.096, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=7480
2023-08-01 18:26:17 - progress_bar.py[line:272] - INFO: epoch 002:    310 / 2745 loss=1.712, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=1012.7, nsentences=144, sample_size=1012.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=411.5, ups=0.41, wpb=1012.7, bsz=144, num_updates=3050, lr=2.30479e-05, gnorm=1.094, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=7505
2023-08-01 18:26:42 - progress_bar.py[line:272] - INFO: epoch 002:    320 / 2745 loss=1.715, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=1013.3, nsentences=144, sample_size=1013.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=413.6, ups=0.41, wpb=1013.3, bsz=144, num_updates=3060, lr=2.30188e-05, gnorm=1.108, clip=60, loss_scale=128, train_wall=24, gb_free=6.5, wall=7529
2023-08-01 18:27:06 - progress_bar.py[line:272] - INFO: epoch 002:    330 / 2745 loss=1.71, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=1009.2, nsentences=144, sample_size=1009.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=410.2, ups=0.41, wpb=1009.2, bsz=144, num_updates=3070, lr=2.29897e-05, gnorm=1.128, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=7554
2023-08-01 18:27:31 - progress_bar.py[line:272] - INFO: epoch 002:    340 / 2745 loss=1.706, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=1013.8, nsentences=144, sample_size=1013.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=412.9, ups=0.41, wpb=1013.8, bsz=144, num_updates=3080, lr=2.29607e-05, gnorm=1.057, clip=60, loss_scale=128, train_wall=25, gb_free=6.5, wall=7578
2023-08-01 18:27:55 - progress_bar.py[line:272] - INFO: epoch 002:    350 / 2745 loss=1.721, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=1017.1, nsentences=144, sample_size=1017.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=415.1, ups=0.41, wpb=1017.1, bsz=144, num_updates=3090, lr=2.29316e-05, gnorm=1.136, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=7603
2023-08-01 18:28:20 - progress_bar.py[line:272] - INFO: epoch 002:    360 / 2745 loss=1.726, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=1021.2, nsentences=144, sample_size=1021.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=416.8, ups=0.41, wpb=1021.2, bsz=144, num_updates=3100, lr=2.29025e-05, gnorm=1.141, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=7627
2023-08-01 18:28:44 - progress_bar.py[line:272] - INFO: epoch 002:    370 / 2745 loss=1.722, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=1013.3, nsentences=144, sample_size=1013.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=412.7, ups=0.41, wpb=1013.3, bsz=144, num_updates=3110, lr=2.28735e-05, gnorm=1.091, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=7652
2023-08-01 18:29:09 - progress_bar.py[line:272] - INFO: epoch 002:    380 / 2745 loss=1.729, loss_v1=0, loss_v2=0, nll_loss=0.427, ntokens=1017.7, nsentences=144, sample_size=1017.7, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=416, ups=0.41, wpb=1017.7, bsz=144, num_updates=3120, lr=2.28444e-05, gnorm=1.137, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=7676
2023-08-01 18:29:33 - progress_bar.py[line:272] - INFO: epoch 002:    390 / 2745 loss=1.728, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=1016.6, nsentences=144, sample_size=1016.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=414.7, ups=0.41, wpb=1016.6, bsz=144, num_updates=3130, lr=2.28153e-05, gnorm=1.053, clip=60, loss_scale=128, train_wall=24, gb_free=6.5, wall=7701
2023-08-01 18:29:58 - progress_bar.py[line:272] - INFO: epoch 002:    400 / 2745 loss=1.705, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=1009.2, nsentences=144, sample_size=1009.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=410.5, ups=0.41, wpb=1009.2, bsz=144, num_updates=3140, lr=2.27863e-05, gnorm=1.015, clip=60, loss_scale=128, train_wall=25, gb_free=6.5, wall=7726
2023-08-01 18:30:22 - progress_bar.py[line:272] - INFO: epoch 002:    410 / 2745 loss=1.696, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=1011.9, nsentences=144, sample_size=1011.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=411.7, ups=0.41, wpb=1011.9, bsz=144, num_updates=3150, lr=2.27572e-05, gnorm=0.986, clip=30, loss_scale=128, train_wall=25, gb_free=6.5, wall=7750
2023-08-01 18:30:47 - progress_bar.py[line:272] - INFO: epoch 002:    420 / 2745 loss=1.716, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=1015, nsentences=144, sample_size=1015, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=413.7, ups=0.41, wpb=1015, bsz=144, num_updates=3160, lr=2.27282e-05, gnorm=1.021, clip=70, loss_scale=128, train_wall=25, gb_free=6.5, wall=7775
2023-08-01 18:31:11 - progress_bar.py[line:272] - INFO: epoch 002:    430 / 2745 loss=1.724, loss_v1=0, loss_v2=0, nll_loss=0.421, ntokens=1012.9, nsentences=144, sample_size=1012.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=413, ups=0.41, wpb=1012.9, bsz=144, num_updates=3170, lr=2.26991e-05, gnorm=1.077, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=7799
2023-08-01 18:31:36 - progress_bar.py[line:272] - INFO: epoch 002:    440 / 2745 loss=1.704, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=1015.1, nsentences=144, sample_size=1015.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=415.5, ups=0.41, wpb=1015.1, bsz=144, num_updates=3180, lr=2.267e-05, gnorm=1.021, clip=50, loss_scale=128, train_wall=24, gb_free=6.5, wall=7824
2023-08-01 18:32:01 - progress_bar.py[line:272] - INFO: epoch 002:    450 / 2745 loss=1.714, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=1008.5, nsentences=144, sample_size=1008.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=409.7, ups=0.41, wpb=1008.5, bsz=144, num_updates=3190, lr=2.2641e-05, gnorm=1.122, clip=80, loss_scale=256, train_wall=25, gb_free=6.5, wall=7848
2023-08-01 18:32:25 - progress_bar.py[line:272] - INFO: epoch 002:    460 / 2745 loss=1.709, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=1014.8, nsentences=144, sample_size=1014.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=412.6, ups=0.41, wpb=1014.8, bsz=144, num_updates=3200, lr=2.26119e-05, gnorm=1.055, clip=60, loss_scale=256, train_wall=25, gb_free=6.5, wall=7873
2023-08-01 18:32:50 - progress_bar.py[line:272] - INFO: epoch 002:    470 / 2745 loss=1.698, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=1019.3, nsentences=144, sample_size=1019.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=415.6, ups=0.41, wpb=1019.3, bsz=144, num_updates=3210, lr=2.25828e-05, gnorm=1.076, clip=80, loss_scale=256, train_wall=24, gb_free=6.5, wall=7897
2023-08-01 18:33:14 - progress_bar.py[line:272] - INFO: epoch 002:    480 / 2745 loss=1.706, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=1016.1, nsentences=144, sample_size=1016.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=414.3, ups=0.41, wpb=1016.1, bsz=144, num_updates=3220, lr=2.25538e-05, gnorm=1.033, clip=60, loss_scale=256, train_wall=24, gb_free=6.5, wall=7922
2023-08-01 18:33:39 - progress_bar.py[line:272] - INFO: epoch 002:    490 / 2745 loss=1.713, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=1010.8, nsentences=144, sample_size=1010.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=410.7, ups=0.41, wpb=1010.8, bsz=144, num_updates=3230, lr=2.25247e-05, gnorm=1.112, clip=90, loss_scale=256, train_wall=25, gb_free=6.5, wall=7947
2023-08-01 18:34:03 - progress_bar.py[line:272] - INFO: epoch 002:    500 / 2745 loss=1.713, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=1016.3, nsentences=144, sample_size=1016.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=414.5, ups=0.41, wpb=1016.3, bsz=144, num_updates=3240, lr=2.24956e-05, gnorm=1.077, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=7971
2023-08-01 18:34:28 - progress_bar.py[line:272] - INFO: epoch 002:    510 / 2745 loss=1.698, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=1020.4, nsentences=144, sample_size=1020.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=417.2, ups=0.41, wpb=1020.4, bsz=144, num_updates=3250, lr=2.24666e-05, gnorm=1.071, clip=60, loss_scale=256, train_wall=24, gb_free=6.5, wall=7995
2023-08-01 18:34:52 - progress_bar.py[line:272] - INFO: epoch 002:    520 / 2745 loss=1.691, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=1016.8, nsentences=144, sample_size=1016.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=414.6, ups=0.41, wpb=1016.8, bsz=144, num_updates=3260, lr=2.24375e-05, gnorm=1.049, clip=50, loss_scale=256, train_wall=24, gb_free=6.5, wall=8020
2023-08-01 18:35:17 - progress_bar.py[line:272] - INFO: epoch 002:    530 / 2745 loss=1.714, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=1011.9, nsentences=144, sample_size=1011.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=412.5, ups=0.41, wpb=1011.9, bsz=144, num_updates=3270, lr=2.24084e-05, gnorm=1.079, clip=90, loss_scale=256, train_wall=25, gb_free=6.5, wall=8045
2023-08-01 18:35:41 - progress_bar.py[line:272] - INFO: epoch 002:    540 / 2745 loss=1.701, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=1017.9, nsentences=144, sample_size=1017.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=414.7, ups=0.41, wpb=1017.9, bsz=144, num_updates=3280, lr=2.23794e-05, gnorm=1.039, clip=60, loss_scale=256, train_wall=25, gb_free=6.5, wall=8069
2023-08-01 18:36:06 - progress_bar.py[line:272] - INFO: epoch 002:    550 / 2745 loss=1.703, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=1015.7, nsentences=144, sample_size=1015.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=413.6, ups=0.41, wpb=1015.7, bsz=144, num_updates=3290, lr=2.23503e-05, gnorm=1.048, clip=50, loss_scale=256, train_wall=25, gb_free=6.5, wall=8094
2023-08-01 18:36:30 - progress_bar.py[line:272] - INFO: epoch 002:    560 / 2745 loss=1.711, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=1013.3, nsentences=144, sample_size=1013.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=413.3, ups=0.41, wpb=1013.3, bsz=144, num_updates=3300, lr=2.23213e-05, gnorm=1.202, clip=90, loss_scale=256, train_wall=24, gb_free=6.5, wall=8118
2023-08-01 18:36:55 - progress_bar.py[line:272] - INFO: epoch 002:    570 / 2745 loss=1.693, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=1018.1, nsentences=144, sample_size=1018.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=416.5, ups=0.41, wpb=1018.1, bsz=144, num_updates=3310, lr=2.22922e-05, gnorm=1.088, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=8143
2023-08-01 18:37:19 - progress_bar.py[line:272] - INFO: epoch 002:    580 / 2745 loss=1.71, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=1014.3, nsentences=144, sample_size=1014.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=413.3, ups=0.41, wpb=1014.3, bsz=144, num_updates=3320, lr=2.22631e-05, gnorm=1.152, clip=90, loss_scale=256, train_wall=25, gb_free=6.5, wall=8167
2023-08-01 18:37:44 - progress_bar.py[line:272] - INFO: epoch 002:    590 / 2745 loss=1.706, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=1012.7, nsentences=144, sample_size=1012.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=414.2, ups=0.41, wpb=1012.7, bsz=144, num_updates=3330, lr=2.22341e-05, gnorm=1.122, clip=80, loss_scale=256, train_wall=24, gb_free=6.5, wall=8192
2023-08-01 18:38:09 - progress_bar.py[line:272] - INFO: epoch 002:    600 / 2745 loss=1.691, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=1013.8, nsentences=144, sample_size=1013.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=412.8, ups=0.41, wpb=1013.8, bsz=144, num_updates=3340, lr=2.2205e-05, gnorm=1.009, clip=40, loss_scale=256, train_wall=25, gb_free=6.5, wall=8216
2023-08-01 18:38:33 - progress_bar.py[line:272] - INFO: epoch 002:    610 / 2745 loss=1.697, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=1017.2, nsentences=144, sample_size=1017.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=414.2, ups=0.41, wpb=1017.2, bsz=144, num_updates=3350, lr=2.21759e-05, gnorm=1.064, clip=70, loss_scale=256, train_wall=25, gb_free=6.5, wall=8241
2023-08-01 18:38:58 - progress_bar.py[line:272] - INFO: epoch 002:    620 / 2745 loss=1.713, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=1021.9, nsentences=144, sample_size=1021.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=417.8, ups=0.41, wpb=1021.9, bsz=144, num_updates=3360, lr=2.21469e-05, gnorm=1.107, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=8265
2023-08-01 18:39:22 - progress_bar.py[line:272] - INFO: epoch 002:    630 / 2745 loss=1.705, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=1017.9, nsentences=144, sample_size=1017.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=415.7, ups=0.41, wpb=1017.9, bsz=144, num_updates=3370, lr=2.21178e-05, gnorm=1.15, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=8290
2023-08-01 18:39:47 - progress_bar.py[line:272] - INFO: epoch 002:    640 / 2745 loss=1.686, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=1015.6, nsentences=144, sample_size=1015.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=413.8, ups=0.41, wpb=1015.6, bsz=144, num_updates=3380, lr=2.20887e-05, gnorm=1.008, clip=40, loss_scale=256, train_wall=25, gb_free=6.5, wall=8314
2023-08-01 18:40:11 - progress_bar.py[line:272] - INFO: epoch 002:    650 / 2745 loss=1.698, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=1017.2, nsentences=144, sample_size=1017.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=415.5, ups=0.41, wpb=1017.2, bsz=144, num_updates=3390, lr=2.20597e-05, gnorm=1.038, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=8339
2023-08-01 18:40:36 - progress_bar.py[line:272] - INFO: epoch 002:    660 / 2745 loss=1.697, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=1013.3, nsentences=144, sample_size=1013.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=414, ups=0.41, wpb=1013.3, bsz=144, num_updates=3400, lr=2.20306e-05, gnorm=1.115, clip=80, loss_scale=256, train_wall=24, gb_free=6.5, wall=8363
2023-08-01 18:41:00 - progress_bar.py[line:272] - INFO: epoch 002:    670 / 2745 loss=1.699, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=1019, nsentences=144, sample_size=1019, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=414.4, ups=0.41, wpb=1019, bsz=144, num_updates=3410, lr=2.20016e-05, gnorm=1.124, clip=70, loss_scale=256, train_wall=25, gb_free=6.5, wall=8388
2023-08-01 18:41:25 - progress_bar.py[line:272] - INFO: epoch 002:    680 / 2745 loss=1.716, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=1016.6, nsentences=144, sample_size=1016.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=413.3, ups=0.41, wpb=1016.6, bsz=144, num_updates=3420, lr=2.19725e-05, gnorm=1.077, clip=70, loss_scale=256, train_wall=25, gb_free=6.5, wall=8412
2023-08-01 18:41:49 - progress_bar.py[line:272] - INFO: epoch 002:    690 / 2745 loss=1.705, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=1019.8, nsentences=144, sample_size=1019.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=416, ups=0.41, wpb=1019.8, bsz=144, num_updates=3430, lr=2.19434e-05, gnorm=1.154, clip=90, loss_scale=256, train_wall=24, gb_free=6.5, wall=8437
2023-08-01 18:42:14 - progress_bar.py[line:272] - INFO: epoch 002:    700 / 2745 loss=1.698, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=1007.7, nsentences=144, sample_size=1007.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=410.5, ups=0.41, wpb=1007.7, bsz=144, num_updates=3440, lr=2.19144e-05, gnorm=1.069, clip=60, loss_scale=256, train_wall=25, gb_free=6.5, wall=8462
2023-08-01 18:42:38 - progress_bar.py[line:272] - INFO: epoch 002:    710 / 2745 loss=1.692, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=1011.5, nsentences=144, sample_size=1011.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=411.9, ups=0.41, wpb=1011.5, bsz=144, num_updates=3450, lr=2.18853e-05, gnorm=1.098, clip=80, loss_scale=256, train_wall=25, gb_free=6.5, wall=8486
2023-08-01 18:43:03 - progress_bar.py[line:272] - INFO: epoch 002:    720 / 2745 loss=1.703, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=1018.6, nsentences=144, sample_size=1018.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=416.4, ups=0.41, wpb=1018.6, bsz=144, num_updates=3460, lr=2.18562e-05, gnorm=1.156, clip=90, loss_scale=256, train_wall=24, gb_free=6.5, wall=8511
2023-08-01 18:43:27 - progress_bar.py[line:272] - INFO: epoch 002:    730 / 2745 loss=1.7, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=1018.6, nsentences=144, sample_size=1018.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=416.3, ups=0.41, wpb=1018.6, bsz=144, num_updates=3470, lr=2.18272e-05, gnorm=1.142, clip=80, loss_scale=256, train_wall=24, gb_free=6.5, wall=8535
2023-08-01 18:43:52 - progress_bar.py[line:272] - INFO: epoch 002:    740 / 2745 loss=1.696, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=1013.5, nsentences=144, sample_size=1013.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=414.1, ups=0.41, wpb=1013.5, bsz=144, num_updates=3480, lr=2.17981e-05, gnorm=1.098, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=8559
2023-08-01 18:44:16 - progress_bar.py[line:272] - INFO: epoch 002:    750 / 2745 loss=1.699, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=1016.6, nsentences=144, sample_size=1016.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=414.8, ups=0.41, wpb=1016.6, bsz=144, num_updates=3490, lr=2.1769e-05, gnorm=1.156, clip=90, loss_scale=256, train_wall=24, gb_free=6.5, wall=8584
2023-08-01 18:44:41 - progress_bar.py[line:272] - INFO: epoch 002:    760 / 2745 loss=1.688, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=1017.9, nsentences=144, sample_size=1017.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=414.6, ups=0.41, wpb=1017.9, bsz=144, num_updates=3500, lr=2.174e-05, gnorm=1.059, clip=60, loss_scale=256, train_wall=25, gb_free=6.5, wall=8609
2023-08-01 18:45:05 - progress_bar.py[line:272] - INFO: epoch 002:    770 / 2745 loss=1.7, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=1015.9, nsentences=144, sample_size=1015.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=415.3, ups=0.41, wpb=1015.9, bsz=144, num_updates=3510, lr=2.17109e-05, gnorm=1.057, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=8633
2023-08-01 18:45:30 - progress_bar.py[line:272] - INFO: epoch 002:    780 / 2745 loss=1.689, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=1017.2, nsentences=144, sample_size=1017.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=414.7, ups=0.41, wpb=1017.2, bsz=144, num_updates=3520, lr=2.16818e-05, gnorm=1.103, clip=90, loss_scale=256, train_wall=24, gb_free=6.5, wall=8658
2023-08-01 18:45:52 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-08-01 18:45:57 - progress_bar.py[line:272] - INFO: epoch 002:    791 / 2745 loss=1.694, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=1012.3, nsentences=144, sample_size=1012.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=375.7, ups=0.37, wpb=1012.3, bsz=144, num_updates=3530, lr=2.16528e-05, gnorm=1.063, clip=60, loss_scale=128, train_wall=27, gb_free=6.5, wall=8684
2023-08-01 18:46:21 - progress_bar.py[line:272] - INFO: epoch 002:    801 / 2745 loss=1.701, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=1018.2, nsentences=144, sample_size=1018.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=416.4, ups=0.41, wpb=1018.2, bsz=144, num_updates=3540, lr=2.16237e-05, gnorm=1.158, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=8709
2023-08-01 18:46:46 - progress_bar.py[line:272] - INFO: epoch 002:    811 / 2745 loss=1.702, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=1012.1, nsentences=144, sample_size=1012.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=411.4, ups=0.41, wpb=1012.1, bsz=144, num_updates=3550, lr=2.15947e-05, gnorm=1.106, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=8734
2023-08-01 18:47:10 - progress_bar.py[line:272] - INFO: epoch 002:    821 / 2745 loss=1.688, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=1019, nsentences=144, sample_size=1019, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=416.2, ups=0.41, wpb=1019, bsz=144, num_updates=3560, lr=2.15656e-05, gnorm=1.033, clip=50, loss_scale=128, train_wall=24, gb_free=6.5, wall=8758
2023-08-01 18:47:35 - progress_bar.py[line:272] - INFO: epoch 002:    831 / 2745 loss=1.687, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=1007.7, nsentences=144, sample_size=1007.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=409.2, ups=0.41, wpb=1007.7, bsz=144, num_updates=3570, lr=2.15365e-05, gnorm=1.129, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=8783
2023-08-01 18:48:00 - progress_bar.py[line:272] - INFO: epoch 002:    841 / 2745 loss=1.697, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=1007, nsentences=144, sample_size=1007, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=409.1, ups=0.41, wpb=1007, bsz=144, num_updates=3580, lr=2.15075e-05, gnorm=1.157, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=8807
2023-08-01 18:48:24 - progress_bar.py[line:272] - INFO: epoch 002:    851 / 2745 loss=1.703, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=1012.3, nsentences=144, sample_size=1012.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=411.5, ups=0.41, wpb=1012.3, bsz=144, num_updates=3590, lr=2.14784e-05, gnorm=1.075, clip=60, loss_scale=128, train_wall=25, gb_free=6.5, wall=8832
2023-08-01 18:48:49 - progress_bar.py[line:272] - INFO: epoch 002:    861 / 2745 loss=1.7, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=1018.2, nsentences=144, sample_size=1018.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=415.4, ups=0.41, wpb=1018.2, bsz=144, num_updates=3600, lr=2.14493e-05, gnorm=1.138, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=8856
2023-08-01 18:49:13 - progress_bar.py[line:272] - INFO: epoch 002:    871 / 2745 loss=1.686, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=1020.6, nsentences=144, sample_size=1020.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=416.9, ups=0.41, wpb=1020.6, bsz=144, num_updates=3610, lr=2.14203e-05, gnorm=1.101, clip=60, loss_scale=128, train_wall=24, gb_free=6.5, wall=8881
2023-08-01 18:49:38 - progress_bar.py[line:272] - INFO: epoch 002:    881 / 2745 loss=1.7, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=1016.8, nsentences=144, sample_size=1016.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=415.3, ups=0.41, wpb=1016.8, bsz=144, num_updates=3620, lr=2.13912e-05, gnorm=1.177, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=8905
2023-08-01 18:50:02 - progress_bar.py[line:272] - INFO: epoch 002:    891 / 2745 loss=1.691, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=1017.2, nsentences=144, sample_size=1017.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=414.4, ups=0.41, wpb=1017.2, bsz=144, num_updates=3630, lr=2.13621e-05, gnorm=1.14, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=8930
2023-08-01 18:50:27 - progress_bar.py[line:272] - INFO: epoch 002:    901 / 2745 loss=1.694, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=1006.7, nsentences=144, sample_size=1006.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=409.6, ups=0.41, wpb=1006.7, bsz=144, num_updates=3640, lr=2.13331e-05, gnorm=1.086, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=8954
2023-08-01 18:50:51 - progress_bar.py[line:272] - INFO: epoch 002:    911 / 2745 loss=1.69, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=1017.4, nsentences=144, sample_size=1017.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=416, ups=0.41, wpb=1017.4, bsz=144, num_updates=3650, lr=2.1304e-05, gnorm=1.16, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=8979
2023-08-01 18:51:16 - progress_bar.py[line:272] - INFO: epoch 002:    921 / 2745 loss=1.698, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=1014.5, nsentences=144, sample_size=1014.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=413.4, ups=0.41, wpb=1014.5, bsz=144, num_updates=3660, lr=2.12749e-05, gnorm=1.098, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=9004
2023-08-01 18:51:40 - progress_bar.py[line:272] - INFO: epoch 002:    931 / 2745 loss=1.681, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=1012.5, nsentences=144, sample_size=1012.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=413, ups=0.41, wpb=1012.5, bsz=144, num_updates=3670, lr=2.12459e-05, gnorm=1.061, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=9028
2023-08-01 18:52:05 - progress_bar.py[line:272] - INFO: epoch 002:    941 / 2745 loss=1.688, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=1012.5, nsentences=144, sample_size=1012.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=410.8, ups=0.41, wpb=1012.5, bsz=144, num_updates=3680, lr=2.12168e-05, gnorm=1.063, clip=60, loss_scale=128, train_wall=25, gb_free=6.5, wall=9053
2023-08-01 18:52:30 - progress_bar.py[line:272] - INFO: epoch 002:    951 / 2745 loss=1.689, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=1012.7, nsentences=144, sample_size=1012.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=412.6, ups=0.41, wpb=1012.7, bsz=144, num_updates=3690, lr=2.11878e-05, gnorm=1.136, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=9077
2023-08-01 18:52:54 - progress_bar.py[line:272] - INFO: epoch 002:    961 / 2745 loss=1.693, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=1017, nsentences=144, sample_size=1017, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=413.6, ups=0.41, wpb=1017, bsz=144, num_updates=3700, lr=2.11587e-05, gnorm=1.149, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=9102
2023-08-01 18:53:19 - progress_bar.py[line:272] - INFO: epoch 002:    971 / 2745 loss=1.697, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=1017.5, nsentences=144, sample_size=1017.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=414.1, ups=0.41, wpb=1017.5, bsz=144, num_updates=3710, lr=2.11296e-05, gnorm=1.056, clip=70, loss_scale=128, train_wall=25, gb_free=6.5, wall=9126
2023-08-01 18:53:43 - progress_bar.py[line:272] - INFO: epoch 002:    981 / 2745 loss=1.685, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=1016.8, nsentences=144, sample_size=1016.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=414.9, ups=0.41, wpb=1016.8, bsz=144, num_updates=3720, lr=2.11006e-05, gnorm=1.035, clip=60, loss_scale=128, train_wall=24, gb_free=6.5, wall=9151
2023-08-01 18:54:08 - progress_bar.py[line:272] - INFO: epoch 002:    991 / 2745 loss=1.699, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=1011.5, nsentences=144, sample_size=1011.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=411.1, ups=0.41, wpb=1011.5, bsz=144, num_updates=3730, lr=2.10715e-05, gnorm=1.113, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=9176
2023-08-01 18:54:32 - progress_bar.py[line:272] - INFO: epoch 002:   1001 / 2745 loss=1.7, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=1013.2, nsentences=144, sample_size=1013.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=411.5, ups=0.41, wpb=1013.2, bsz=144, num_updates=3740, lr=2.10424e-05, gnorm=1.112, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=9200
2023-08-01 18:54:57 - progress_bar.py[line:272] - INFO: epoch 002:   1011 / 2745 loss=1.689, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=1017.7, nsentences=144, sample_size=1017.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=413.9, ups=0.41, wpb=1017.7, bsz=144, num_updates=3750, lr=2.10134e-05, gnorm=1.091, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=9225
2023-08-01 18:55:22 - progress_bar.py[line:272] - INFO: epoch 002:   1021 / 2745 loss=1.684, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=1019.7, nsentences=144, sample_size=1019.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=416.2, ups=0.41, wpb=1019.7, bsz=144, num_updates=3760, lr=2.09843e-05, gnorm=0.986, clip=30, loss_scale=128, train_wall=24, gb_free=6.5, wall=9249
2023-08-01 18:55:46 - progress_bar.py[line:272] - INFO: epoch 002:   1031 / 2745 loss=1.691, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=1018.3, nsentences=144, sample_size=1018.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=416.2, ups=0.41, wpb=1018.3, bsz=144, num_updates=3770, lr=2.09552e-05, gnorm=1.204, clip=100, loss_scale=128, train_wall=24, gb_free=6.5, wall=9274
2023-08-01 18:56:11 - progress_bar.py[line:272] - INFO: epoch 002:   1041 / 2745 loss=1.694, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=1015.3, nsentences=144, sample_size=1015.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=413.3, ups=0.41, wpb=1015.3, bsz=144, num_updates=3780, lr=2.09262e-05, gnorm=1.098, clip=60, loss_scale=128, train_wall=25, gb_free=6.5, wall=9298
2023-08-01 18:56:35 - progress_bar.py[line:272] - INFO: epoch 002:   1051 / 2745 loss=1.703, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=1017.9, nsentences=144, sample_size=1017.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=416.7, ups=0.41, wpb=1017.9, bsz=144, num_updates=3790, lr=2.08971e-05, gnorm=1.23, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=9323
2023-08-01 18:56:59 - progress_bar.py[line:272] - INFO: epoch 002:   1061 / 2745 loss=1.684, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=1014.4, nsentences=144, sample_size=1014.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=414.6, ups=0.41, wpb=1014.4, bsz=144, num_updates=3800, lr=2.0868e-05, gnorm=1.078, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=9347
2023-08-01 18:57:24 - progress_bar.py[line:272] - INFO: epoch 002:   1071 / 2745 loss=1.692, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=1016.6, nsentences=144, sample_size=1016.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=415.2, ups=0.41, wpb=1016.6, bsz=144, num_updates=3810, lr=2.0839e-05, gnorm=1.035, clip=60, loss_scale=128, train_wall=24, gb_free=6.5, wall=9372
2023-08-01 18:57:48 - progress_bar.py[line:272] - INFO: epoch 002:   1081 / 2745 loss=1.687, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=1013.1, nsentences=144, sample_size=1013.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=413.5, ups=0.41, wpb=1013.1, bsz=144, num_updates=3820, lr=2.08099e-05, gnorm=1.09, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=9396
2023-08-01 18:58:13 - progress_bar.py[line:272] - INFO: epoch 002:   1091 / 2745 loss=1.698, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=1014.3, nsentences=144, sample_size=1014.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=412.5, ups=0.41, wpb=1014.3, bsz=144, num_updates=3830, lr=2.07809e-05, gnorm=1.152, clip=70, loss_scale=128, train_wall=25, gb_free=6.5, wall=9421
2023-08-01 18:58:38 - progress_bar.py[line:272] - INFO: epoch 002:   1101 / 2745 loss=1.69, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=1014.9, nsentences=144, sample_size=1014.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=414.2, ups=0.41, wpb=1014.9, bsz=144, num_updates=3840, lr=2.07518e-05, gnorm=1.095, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=9445
2023-08-01 18:59:02 - progress_bar.py[line:272] - INFO: epoch 002:   1111 / 2745 loss=1.685, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=1014.6, nsentences=144, sample_size=1014.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=414.8, ups=0.41, wpb=1014.6, bsz=144, num_updates=3850, lr=2.07227e-05, gnorm=1.041, clip=60, loss_scale=128, train_wall=24, gb_free=6.5, wall=9470
2023-08-01 18:59:27 - progress_bar.py[line:272] - INFO: epoch 002:   1121 / 2745 loss=1.687, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=1010, nsentences=144, sample_size=1010, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=410.7, ups=0.41, wpb=1010, bsz=144, num_updates=3860, lr=2.06937e-05, gnorm=1.137, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=9494
2023-08-01 18:59:51 - progress_bar.py[line:272] - INFO: epoch 002:   1131 / 2745 loss=1.695, loss_v1=0, loss_v2=0, nll_loss=0.39, ntokens=1005.7, nsentences=144, sample_size=1005.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=409.1, ups=0.41, wpb=1005.7, bsz=144, num_updates=3870, lr=2.06646e-05, gnorm=1.099, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=9519
2023-08-01 19:00:16 - progress_bar.py[line:272] - INFO: epoch 002:   1141 / 2745 loss=1.681, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=1011.8, nsentences=144, sample_size=1011.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=411.6, ups=0.41, wpb=1011.8, bsz=144, num_updates=3880, lr=2.06355e-05, gnorm=1.103, clip=60, loss_scale=128, train_wall=25, gb_free=6.5, wall=9543
2023-08-01 19:00:40 - progress_bar.py[line:272] - INFO: epoch 002:   1151 / 2745 loss=1.683, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=1017.4, nsentences=144, sample_size=1017.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=414.4, ups=0.41, wpb=1017.4, bsz=144, num_updates=3890, lr=2.06065e-05, gnorm=1.107, clip=60, loss_scale=128, train_wall=25, gb_free=6.5, wall=9568
2023-08-01 19:01:05 - progress_bar.py[line:272] - INFO: epoch 002:   1161 / 2745 loss=1.672, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=1013.8, nsentences=144, sample_size=1013.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=412.8, ups=0.41, wpb=1013.8, bsz=144, num_updates=3900, lr=2.05774e-05, gnorm=1.07, clip=70, loss_scale=128, train_wall=25, gb_free=6.5, wall=9593
2023-08-01 19:01:29 - progress_bar.py[line:272] - INFO: epoch 002:   1171 / 2745 loss=1.692, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=1012.8, nsentences=144, sample_size=1012.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=412.9, ups=0.41, wpb=1012.8, bsz=144, num_updates=3910, lr=2.05483e-05, gnorm=1.031, clip=50, loss_scale=128, train_wall=24, gb_free=6.5, wall=9617
2023-08-01 19:01:54 - progress_bar.py[line:272] - INFO: epoch 002:   1181 / 2745 loss=1.697, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=1024.1, nsentences=144, sample_size=1024.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=418.5, ups=0.41, wpb=1024.1, bsz=144, num_updates=3920, lr=2.05193e-05, gnorm=1.152, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=9642
2023-08-01 19:02:18 - progress_bar.py[line:272] - INFO: epoch 002:   1191 / 2745 loss=1.69, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=1016.8, nsentences=144, sample_size=1016.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=415.7, ups=0.41, wpb=1016.8, bsz=144, num_updates=3930, lr=2.04902e-05, gnorm=1.049, clip=60, loss_scale=128, train_wall=24, gb_free=6.5, wall=9666
2023-08-01 19:02:43 - progress_bar.py[line:272] - INFO: epoch 002:   1201 / 2745 loss=1.687, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=1011.5, nsentences=144, sample_size=1011.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=411.8, ups=0.41, wpb=1011.5, bsz=144, num_updates=3940, lr=2.04612e-05, gnorm=1.031, clip=70, loss_scale=128, train_wall=25, gb_free=6.5, wall=9691
2023-08-01 19:03:07 - progress_bar.py[line:272] - INFO: epoch 002:   1211 / 2745 loss=1.685, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=1014.8, nsentences=144, sample_size=1014.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=413.5, ups=0.41, wpb=1014.8, bsz=144, num_updates=3950, lr=2.04321e-05, gnorm=1.073, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=9715
2023-08-01 19:03:32 - progress_bar.py[line:272] - INFO: epoch 002:   1221 / 2745 loss=1.688, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=1019.6, nsentences=144, sample_size=1019.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=417, ups=0.41, wpb=1019.6, bsz=144, num_updates=3960, lr=2.0403e-05, gnorm=1.086, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=9740
2023-08-01 19:03:56 - progress_bar.py[line:272] - INFO: epoch 002:   1231 / 2745 loss=1.695, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=1014.6, nsentences=144, sample_size=1014.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=414.5, ups=0.41, wpb=1014.6, bsz=144, num_updates=3970, lr=2.0374e-05, gnorm=1.153, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=9764
2023-08-01 19:04:21 - progress_bar.py[line:272] - INFO: epoch 002:   1241 / 2745 loss=1.683, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=1012.2, nsentences=144, sample_size=1012.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=412.1, ups=0.41, wpb=1012.2, bsz=144, num_updates=3980, lr=2.03449e-05, gnorm=1.075, clip=70, loss_scale=128, train_wall=25, gb_free=6.5, wall=9789
2023-08-01 19:04:46 - progress_bar.py[line:272] - INFO: epoch 002:   1251 / 2745 loss=1.683, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=1014.3, nsentences=144, sample_size=1014.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=413.5, ups=0.41, wpb=1014.3, bsz=144, num_updates=3990, lr=2.03158e-05, gnorm=1.057, clip=70, loss_scale=128, train_wall=25, gb_free=6.5, wall=9813
2023-08-01 19:05:10 - progress_bar.py[line:272] - INFO: epoch 002:   1261 / 2745 loss=1.698, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=1013, nsentences=144, sample_size=1013, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=413.4, ups=0.41, wpb=1013, bsz=144, num_updates=4000, lr=2.02868e-05, gnorm=1.115, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=9838
2023-08-01 19:05:35 - progress_bar.py[line:272] - INFO: epoch 002:   1271 / 2745 loss=1.692, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=1015.8, nsentences=144, sample_size=1015.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=410.9, ups=0.4, wpb=1015.8, bsz=144, num_updates=4010, lr=2.02577e-05, gnorm=1.112, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=9862
2023-08-01 19:05:59 - progress_bar.py[line:272] - INFO: epoch 002:   1281 / 2745 loss=1.699, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=1015.8, nsentences=144, sample_size=1015.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=413.9, ups=0.41, wpb=1015.8, bsz=144, num_updates=4020, lr=2.02286e-05, gnorm=1.133, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=9887
2023-08-01 19:06:24 - progress_bar.py[line:272] - INFO: epoch 002:   1291 / 2745 loss=1.686, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=1018.8, nsentences=144, sample_size=1018.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=414.1, ups=0.41, wpb=1018.8, bsz=144, num_updates=4030, lr=2.01996e-05, gnorm=1.049, clip=70, loss_scale=128, train_wall=25, gb_free=6.5, wall=9912
2023-08-01 19:06:49 - progress_bar.py[line:272] - INFO: epoch 002:   1301 / 2745 loss=1.696, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=1006.9, nsentences=144, sample_size=1006.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=409.2, ups=0.41, wpb=1006.9, bsz=144, num_updates=4040, lr=2.01705e-05, gnorm=1.106, clip=70, loss_scale=256, train_wall=25, gb_free=6.5, wall=9936
2023-08-01 19:07:13 - progress_bar.py[line:272] - INFO: epoch 002:   1311 / 2745 loss=1.689, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=1013.2, nsentences=144, sample_size=1013.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=413.6, ups=0.41, wpb=1013.2, bsz=144, num_updates=4050, lr=2.01414e-05, gnorm=1, clip=50, loss_scale=256, train_wall=24, gb_free=6.5, wall=9961
2023-08-01 19:07:38 - progress_bar.py[line:272] - INFO: epoch 002:   1321 / 2745 loss=1.676, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=1017.2, nsentences=144, sample_size=1017.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=414.8, ups=0.41, wpb=1017.2, bsz=144, num_updates=4060, lr=2.01124e-05, gnorm=1.071, clip=60, loss_scale=256, train_wall=24, gb_free=6.5, wall=9985
2023-08-01 19:08:02 - progress_bar.py[line:272] - INFO: epoch 002:   1331 / 2745 loss=1.686, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=1015, nsentences=144, sample_size=1015, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=413.7, ups=0.41, wpb=1015, bsz=144, num_updates=4070, lr=2.00833e-05, gnorm=1.188, clip=80, loss_scale=256, train_wall=25, gb_free=6.5, wall=10010
2023-08-01 19:08:27 - progress_bar.py[line:272] - INFO: epoch 002:   1341 / 2745 loss=1.681, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=1009.6, nsentences=144, sample_size=1009.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=409.6, ups=0.41, wpb=1009.6, bsz=144, num_updates=4080, lr=2.00543e-05, gnorm=1.061, clip=70, loss_scale=256, train_wall=25, gb_free=6.5, wall=10034
2023-08-01 19:08:51 - progress_bar.py[line:272] - INFO: epoch 002:   1351 / 2745 loss=1.689, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=1011.2, nsentences=144, sample_size=1011.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=413.3, ups=0.41, wpb=1011.2, bsz=144, num_updates=4090, lr=2.00252e-05, gnorm=1.178, clip=90, loss_scale=256, train_wall=24, gb_free=6.5, wall=10059
2023-08-01 19:09:16 - progress_bar.py[line:272] - INFO: epoch 002:   1361 / 2745 loss=1.677, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=1015.1, nsentences=144, sample_size=1015.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=414.6, ups=0.41, wpb=1015.1, bsz=144, num_updates=4100, lr=1.99961e-05, gnorm=1.106, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=10083
2023-08-01 19:09:40 - progress_bar.py[line:272] - INFO: epoch 002:   1371 / 2745 loss=1.689, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=1010.3, nsentences=144, sample_size=1010.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=411.9, ups=0.41, wpb=1010.3, bsz=144, num_updates=4110, lr=1.99671e-05, gnorm=1.159, clip=90, loss_scale=256, train_wall=24, gb_free=6.5, wall=10108
2023-08-01 19:10:05 - progress_bar.py[line:272] - INFO: epoch 002:   1381 / 2745 loss=1.685, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=1015.3, nsentences=144, sample_size=1015.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=414.7, ups=0.41, wpb=1015.3, bsz=144, num_updates=4120, lr=1.9938e-05, gnorm=1.036, clip=50, loss_scale=256, train_wall=24, gb_free=6.5, wall=10132
2023-08-01 19:10:29 - progress_bar.py[line:272] - INFO: epoch 002:   1391 / 2745 loss=1.687, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=1021.4, nsentences=144, sample_size=1021.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=418.3, ups=0.41, wpb=1021.4, bsz=144, num_updates=4130, lr=1.99089e-05, gnorm=1.25, clip=80, loss_scale=256, train_wall=24, gb_free=6.5, wall=10157
2023-08-01 19:10:54 - progress_bar.py[line:272] - INFO: epoch 002:   1401 / 2745 loss=1.688, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=1017.5, nsentences=144, sample_size=1017.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=415.7, ups=0.41, wpb=1017.5, bsz=144, num_updates=4140, lr=1.98799e-05, gnorm=1.022, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=10181
2023-08-01 19:11:18 - progress_bar.py[line:272] - INFO: epoch 002:   1411 / 2745 loss=1.692, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=1016.1, nsentences=144, sample_size=1016.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=414.6, ups=0.41, wpb=1016.1, bsz=144, num_updates=4150, lr=1.98508e-05, gnorm=1.137, clip=60, loss_scale=256, train_wall=24, gb_free=6.5, wall=10206
2023-08-01 19:11:43 - progress_bar.py[line:272] - INFO: epoch 002:   1421 / 2745 loss=1.691, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=1021, nsentences=144, sample_size=1021, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=417.1, ups=0.41, wpb=1021, bsz=144, num_updates=4160, lr=1.98217e-05, gnorm=1.134, clip=90, loss_scale=256, train_wall=24, gb_free=6.5, wall=10230
2023-08-01 19:12:07 - progress_bar.py[line:272] - INFO: epoch 002:   1431 / 2745 loss=1.683, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=1011, nsentences=144, sample_size=1011, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=410.8, ups=0.41, wpb=1011, bsz=144, num_updates=4170, lr=1.97927e-05, gnorm=1.078, clip=70, loss_scale=256, train_wall=25, gb_free=6.5, wall=10255
2023-08-01 19:12:32 - progress_bar.py[line:272] - INFO: epoch 002:   1441 / 2745 loss=1.684, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=1017.4, nsentences=144, sample_size=1017.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=413.2, ups=0.41, wpb=1017.4, bsz=144, num_updates=4180, lr=1.97636e-05, gnorm=1.051, clip=70, loss_scale=256, train_wall=25, gb_free=6.5, wall=10280
2023-08-01 19:12:57 - progress_bar.py[line:272] - INFO: epoch 002:   1451 / 2745 loss=1.679, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=1016.4, nsentences=144, sample_size=1016.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=410.9, ups=0.4, wpb=1016.4, bsz=144, num_updates=4190, lr=1.97345e-05, gnorm=1.187, clip=100, loss_scale=256, train_wall=25, gb_free=6.5, wall=10304
2023-08-01 19:13:21 - progress_bar.py[line:272] - INFO: epoch 002:   1461 / 2745 loss=1.683, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=1009.3, nsentences=144, sample_size=1009.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=406.8, ups=0.4, wpb=1009.3, bsz=144, num_updates=4200, lr=1.97055e-05, gnorm=1.076, clip=80, loss_scale=256, train_wall=25, gb_free=6.5, wall=10329
2023-08-01 19:13:46 - progress_bar.py[line:272] - INFO: epoch 002:   1471 / 2745 loss=1.687, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=1021.6, nsentences=144, sample_size=1021.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=417.1, ups=0.41, wpb=1021.6, bsz=144, num_updates=4210, lr=1.96764e-05, gnorm=1.026, clip=50, loss_scale=256, train_wall=24, gb_free=6.5, wall=10354
2023-08-01 19:14:10 - progress_bar.py[line:272] - INFO: epoch 002:   1481 / 2745 loss=1.678, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=1015.6, nsentences=144, sample_size=1015.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=413.5, ups=0.41, wpb=1015.6, bsz=144, num_updates=4220, lr=1.96474e-05, gnorm=1.106, clip=60, loss_scale=256, train_wall=25, gb_free=6.5, wall=10378
2023-08-01 19:14:35 - progress_bar.py[line:272] - INFO: epoch 002:   1491 / 2745 loss=1.685, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=1016.5, nsentences=144, sample_size=1016.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=414.3, ups=0.41, wpb=1016.5, bsz=144, num_updates=4230, lr=1.96183e-05, gnorm=1.098, clip=70, loss_scale=256, train_wall=25, gb_free=6.5, wall=10403
2023-08-01 19:15:00 - progress_bar.py[line:272] - INFO: epoch 002:   1501 / 2745 loss=1.684, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=1006.6, nsentences=144, sample_size=1006.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=408.5, ups=0.41, wpb=1006.6, bsz=144, num_updates=4240, lr=1.95892e-05, gnorm=1.075, clip=70, loss_scale=256, train_wall=25, gb_free=6.5, wall=10427
2023-08-01 19:15:24 - progress_bar.py[line:272] - INFO: epoch 002:   1511 / 2745 loss=1.689, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=1009.6, nsentences=144, sample_size=1009.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=411.4, ups=0.41, wpb=1009.6, bsz=144, num_updates=4250, lr=1.95602e-05, gnorm=1.095, clip=80, loss_scale=256, train_wall=25, gb_free=6.5, wall=10452
2023-08-01 19:15:49 - progress_bar.py[line:272] - INFO: epoch 002:   1521 / 2745 loss=1.688, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=1018.9, nsentences=144, sample_size=1018.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=416.2, ups=0.41, wpb=1018.9, bsz=144, num_updates=4260, lr=1.95311e-05, gnorm=1.001, clip=40, loss_scale=256, train_wall=24, gb_free=6.5, wall=10476
2023-08-01 19:16:13 - progress_bar.py[line:272] - INFO: epoch 002:   1531 / 2745 loss=1.689, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=1018.2, nsentences=144, sample_size=1018.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=416.3, ups=0.41, wpb=1018.2, bsz=144, num_updates=4270, lr=1.9502e-05, gnorm=1.041, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=10501
2023-08-01 19:16:38 - progress_bar.py[line:272] - INFO: epoch 002:   1541 / 2745 loss=1.684, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=1011.4, nsentences=144, sample_size=1011.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=412.6, ups=0.41, wpb=1011.4, bsz=144, num_updates=4280, lr=1.9473e-05, gnorm=1.012, clip=60, loss_scale=256, train_wall=24, gb_free=6.5, wall=10525
2023-08-01 19:17:02 - progress_bar.py[line:272] - INFO: epoch 002:   1551 / 2745 loss=1.681, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=1009.7, nsentences=144, sample_size=1009.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=411.8, ups=0.41, wpb=1009.7, bsz=144, num_updates=4290, lr=1.94439e-05, gnorm=1.074, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=10550
2023-08-01 19:17:27 - progress_bar.py[line:272] - INFO: epoch 002:   1561 / 2745 loss=1.674, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=1012.8, nsentences=144, sample_size=1012.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=413.2, ups=0.41, wpb=1012.8, bsz=144, num_updates=4300, lr=1.94148e-05, gnorm=1.059, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=10574
2023-08-01 19:17:51 - progress_bar.py[line:272] - INFO: epoch 002:   1571 / 2745 loss=1.684, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=1016.1, nsentences=144, sample_size=1016.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=414.9, ups=0.41, wpb=1016.1, bsz=144, num_updates=4310, lr=1.93858e-05, gnorm=1.084, clip=90, loss_scale=256, train_wall=24, gb_free=6.5, wall=10599
2023-08-01 19:18:16 - progress_bar.py[line:272] - INFO: epoch 002:   1581 / 2745 loss=1.678, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=1010.4, nsentences=144, sample_size=1010.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=413.6, ups=0.41, wpb=1010.4, bsz=144, num_updates=4320, lr=1.93567e-05, gnorm=1.131, clip=90, loss_scale=256, train_wall=24, gb_free=6.5, wall=10623
2023-08-01 19:18:40 - progress_bar.py[line:272] - INFO: epoch 002:   1591 / 2745 loss=1.68, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=1016.8, nsentences=144, sample_size=1016.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=415.5, ups=0.41, wpb=1016.8, bsz=144, num_updates=4330, lr=1.93276e-05, gnorm=1.117, clip=90, loss_scale=256, train_wall=24, gb_free=6.5, wall=10648
2023-08-01 19:19:05 - progress_bar.py[line:272] - INFO: epoch 002:   1601 / 2745 loss=1.68, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=1016.6, nsentences=144, sample_size=1016.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=415.7, ups=0.41, wpb=1016.6, bsz=144, num_updates=4340, lr=1.92986e-05, gnorm=1.069, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=10672
2023-08-01 19:19:29 - progress_bar.py[line:272] - INFO: epoch 002:   1611 / 2745 loss=1.678, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=1019.1, nsentences=144, sample_size=1019.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=416.8, ups=0.41, wpb=1019.1, bsz=144, num_updates=4350, lr=1.92695e-05, gnorm=1.145, clip=80, loss_scale=256, train_wall=24, gb_free=6.5, wall=10697
2023-08-01 19:19:54 - progress_bar.py[line:272] - INFO: epoch 002:   1621 / 2745 loss=1.685, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=1010.1, nsentences=144, sample_size=1010.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=410.7, ups=0.41, wpb=1010.1, bsz=144, num_updates=4360, lr=1.92405e-05, gnorm=1.128, clip=90, loss_scale=256, train_wall=25, gb_free=6.5, wall=10721
2023-08-01 19:20:18 - progress_bar.py[line:272] - INFO: epoch 002:   1631 / 2745 loss=1.686, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=1017, nsentences=144, sample_size=1017, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=415.2, ups=0.41, wpb=1017, bsz=144, num_updates=4370, lr=1.92114e-05, gnorm=1.114, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=10746
2023-08-01 19:20:43 - progress_bar.py[line:272] - INFO: epoch 002:   1641 / 2745 loss=1.688, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=1020.3, nsentences=144, sample_size=1020.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=416.6, ups=0.41, wpb=1020.3, bsz=144, num_updates=4380, lr=1.91823e-05, gnorm=1.082, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=10770
2023-08-01 19:21:07 - progress_bar.py[line:272] - INFO: epoch 002:   1651 / 2745 loss=1.68, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=1017.1, nsentences=144, sample_size=1017.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=415.8, ups=0.41, wpb=1017.1, bsz=144, num_updates=4390, lr=1.91533e-05, gnorm=1.11, clip=80, loss_scale=256, train_wall=24, gb_free=6.5, wall=10795
2023-08-01 19:21:31 - progress_bar.py[line:272] - INFO: epoch 002:   1661 / 2745 loss=1.68, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=1021.6, nsentences=144, sample_size=1021.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=417.7, ups=0.41, wpb=1021.6, bsz=144, num_updates=4400, lr=1.91242e-05, gnorm=1.13, clip=80, loss_scale=256, train_wall=24, gb_free=6.5, wall=10819
2023-08-01 19:21:56 - progress_bar.py[line:272] - INFO: epoch 002:   1671 / 2745 loss=1.68, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=1012.3, nsentences=144, sample_size=1012.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=412.9, ups=0.41, wpb=1012.3, bsz=144, num_updates=4410, lr=1.90951e-05, gnorm=1.074, clip=60, loss_scale=256, train_wall=24, gb_free=6.5, wall=10844
2023-08-01 19:22:21 - progress_bar.py[line:272] - INFO: epoch 002:   1681 / 2745 loss=1.673, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=1013.1, nsentences=144, sample_size=1013.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=412.1, ups=0.41, wpb=1013.1, bsz=144, num_updates=4420, lr=1.90661e-05, gnorm=1.086, clip=90, loss_scale=256, train_wall=25, gb_free=6.5, wall=10868
2023-08-01 19:22:45 - progress_bar.py[line:272] - INFO: epoch 002:   1691 / 2745 loss=1.676, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=1017.2, nsentences=144, sample_size=1017.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=414.3, ups=0.41, wpb=1017.2, bsz=144, num_updates=4430, lr=1.9037e-05, gnorm=1.072, clip=70, loss_scale=256, train_wall=25, gb_free=6.5, wall=10893
2023-08-01 19:23:10 - progress_bar.py[line:272] - INFO: epoch 002:   1701 / 2745 loss=1.68, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=1014.8, nsentences=144, sample_size=1014.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=414, ups=0.41, wpb=1014.8, bsz=144, num_updates=4440, lr=1.90079e-05, gnorm=1.144, clip=100, loss_scale=256, train_wall=24, gb_free=6.5, wall=10917
2023-08-01 19:23:34 - progress_bar.py[line:272] - INFO: epoch 002:   1711 / 2745 loss=1.673, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=1012.8, nsentences=144, sample_size=1012.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=411.7, ups=0.41, wpb=1012.8, bsz=144, num_updates=4450, lr=1.89789e-05, gnorm=1.071, clip=60, loss_scale=256, train_wall=25, gb_free=6.5, wall=10942
2023-08-01 19:23:59 - progress_bar.py[line:272] - INFO: epoch 002:   1721 / 2745 loss=1.683, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=1012.7, nsentences=144, sample_size=1012.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=410.9, ups=0.41, wpb=1012.7, bsz=144, num_updates=4460, lr=1.89498e-05, gnorm=1.093, clip=80, loss_scale=256, train_wall=25, gb_free=6.5, wall=10967
2023-08-01 19:24:24 - progress_bar.py[line:272] - INFO: epoch 002:   1731 / 2745 loss=1.685, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=1014, nsentences=144, sample_size=1014, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=412.4, ups=0.41, wpb=1014, bsz=144, num_updates=4470, lr=1.89208e-05, gnorm=1.175, clip=90, loss_scale=256, train_wall=25, gb_free=6.5, wall=10991
2023-08-01 19:24:48 - progress_bar.py[line:272] - INFO: epoch 002:   1741 / 2745 loss=1.68, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=1019.6, nsentences=144, sample_size=1019.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=417.1, ups=0.41, wpb=1019.6, bsz=144, num_updates=4480, lr=1.88917e-05, gnorm=1.035, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=11016
2023-08-01 19:25:12 - progress_bar.py[line:272] - INFO: epoch 002:   1751 / 2745 loss=1.673, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=1015.5, nsentences=144, sample_size=1015.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=414.9, ups=0.41, wpb=1015.5, bsz=144, num_updates=4490, lr=1.88626e-05, gnorm=1.067, clip=80, loss_scale=256, train_wall=24, gb_free=6.5, wall=11040
2023-08-01 19:25:37 - progress_bar.py[line:272] - INFO: epoch 002:   1761 / 2745 loss=1.686, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=1010.4, nsentences=144, sample_size=1010.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=411.3, ups=0.41, wpb=1010.4, bsz=144, num_updates=4500, lr=1.88336e-05, gnorm=1.144, clip=70, loss_scale=256, train_wall=25, gb_free=6.5, wall=11065
2023-08-01 19:25:39 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-08-01 19:26:04 - progress_bar.py[line:272] - INFO: epoch 002:   1772 / 2745 loss=1.677, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=1014.7, nsentences=144, sample_size=1014.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=377.4, ups=0.37, wpb=1014.7, bsz=144, num_updates=4510, lr=1.88045e-05, gnorm=1.152, clip=80, loss_scale=128, train_wall=27, gb_free=6.5, wall=11092
2023-08-01 19:26:28 - progress_bar.py[line:272] - INFO: epoch 002:   1782 / 2745 loss=1.668, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=1019, nsentences=144, sample_size=1019, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=416.3, ups=0.41, wpb=1019, bsz=144, num_updates=4520, lr=1.87754e-05, gnorm=1.088, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=11116
2023-08-01 19:26:53 - progress_bar.py[line:272] - INFO: epoch 002:   1792 / 2745 loss=1.678, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=1014.3, nsentences=144, sample_size=1014.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=413.7, ups=0.41, wpb=1014.3, bsz=144, num_updates=4530, lr=1.87464e-05, gnorm=1.119, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=11141
2023-08-01 19:27:17 - progress_bar.py[line:272] - INFO: epoch 002:   1802 / 2745 loss=1.686, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=1015.6, nsentences=144, sample_size=1015.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=414.5, ups=0.41, wpb=1015.6, bsz=144, num_updates=4540, lr=1.87173e-05, gnorm=1.113, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=11165
2023-08-01 19:27:42 - progress_bar.py[line:272] - INFO: epoch 002:   1812 / 2745 loss=1.677, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=1009.3, nsentences=144, sample_size=1009.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=409.7, ups=0.41, wpb=1009.3, bsz=144, num_updates=4550, lr=1.86882e-05, gnorm=1.147, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=11190
2023-08-01 19:28:07 - progress_bar.py[line:272] - INFO: epoch 002:   1822 / 2745 loss=1.687, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=1015.8, nsentences=144, sample_size=1015.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=413.3, ups=0.41, wpb=1015.8, bsz=144, num_updates=4560, lr=1.86592e-05, gnorm=1.084, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=11214
2023-08-01 19:28:31 - progress_bar.py[line:272] - INFO: epoch 002:   1832 / 2745 loss=1.682, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=1019.7, nsentences=144, sample_size=1019.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=416.4, ups=0.41, wpb=1019.7, bsz=144, num_updates=4570, lr=1.86301e-05, gnorm=1.17, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=11239
2023-08-01 19:28:56 - progress_bar.py[line:272] - INFO: epoch 002:   1842 / 2745 loss=1.678, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=1022.9, nsentences=144, sample_size=1022.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=417.5, ups=0.41, wpb=1022.9, bsz=144, num_updates=4580, lr=1.8601e-05, gnorm=1.006, clip=50, loss_scale=128, train_wall=24, gb_free=6.5, wall=11263
2023-08-01 19:29:20 - progress_bar.py[line:272] - INFO: epoch 002:   1852 / 2745 loss=1.681, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=1017, nsentences=144, sample_size=1017, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=413.9, ups=0.41, wpb=1017, bsz=144, num_updates=4590, lr=1.8572e-05, gnorm=1.126, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=11288
2023-08-01 19:29:45 - progress_bar.py[line:272] - INFO: epoch 002:   1862 / 2745 loss=1.669, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=1014.7, nsentences=144, sample_size=1014.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=413.2, ups=0.41, wpb=1014.7, bsz=144, num_updates=4600, lr=1.85429e-05, gnorm=1.14, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=11312
2023-08-01 19:30:09 - progress_bar.py[line:272] - INFO: epoch 002:   1872 / 2745 loss=1.681, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=1019.4, nsentences=144, sample_size=1019.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=415.4, ups=0.41, wpb=1019.4, bsz=144, num_updates=4610, lr=1.85139e-05, gnorm=1.125, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=11337
2023-08-01 19:30:34 - progress_bar.py[line:272] - INFO: epoch 002:   1882 / 2745 loss=1.688, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=1012.2, nsentences=144, sample_size=1012.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=411.7, ups=0.41, wpb=1012.2, bsz=144, num_updates=4620, lr=1.84848e-05, gnorm=1.127, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=11362
2023-08-01 19:30:58 - progress_bar.py[line:272] - INFO: epoch 002:   1892 / 2745 loss=1.674, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=1017.4, nsentences=144, sample_size=1017.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=414.9, ups=0.41, wpb=1017.4, bsz=144, num_updates=4630, lr=1.84557e-05, gnorm=1.053, clip=60, loss_scale=128, train_wall=24, gb_free=6.5, wall=11386
2023-08-01 19:31:23 - progress_bar.py[line:272] - INFO: epoch 002:   1902 / 2745 loss=1.684, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=1017, nsentences=144, sample_size=1017, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=415.1, ups=0.41, wpb=1017, bsz=144, num_updates=4640, lr=1.84267e-05, gnorm=1.1, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=11411
2023-08-01 19:31:47 - progress_bar.py[line:272] - INFO: epoch 002:   1912 / 2745 loss=1.684, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=1017.7, nsentences=144, sample_size=1017.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=414.9, ups=0.41, wpb=1017.7, bsz=144, num_updates=4650, lr=1.83976e-05, gnorm=1.108, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=11435
2023-08-01 19:32:12 - progress_bar.py[line:272] - INFO: epoch 002:   1922 / 2745 loss=1.682, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=1024.1, nsentences=144, sample_size=1024.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=418, ups=0.41, wpb=1024.1, bsz=144, num_updates=4660, lr=1.83685e-05, gnorm=1.145, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=11460
2023-08-01 19:32:36 - progress_bar.py[line:272] - INFO: epoch 002:   1932 / 2745 loss=1.673, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=1007.6, nsentences=144, sample_size=1007.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=410.7, ups=0.41, wpb=1007.6, bsz=144, num_updates=4670, lr=1.83395e-05, gnorm=1.077, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=11484
2023-08-01 19:33:01 - progress_bar.py[line:272] - INFO: epoch 002:   1942 / 2745 loss=1.678, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=1015.2, nsentences=144, sample_size=1015.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=413.8, ups=0.41, wpb=1015.2, bsz=144, num_updates=4680, lr=1.83104e-05, gnorm=1.114, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=11509
2023-08-01 19:33:26 - progress_bar.py[line:272] - INFO: epoch 002:   1952 / 2745 loss=1.676, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=1014.8, nsentences=144, sample_size=1014.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=414.1, ups=0.41, wpb=1014.8, bsz=144, num_updates=4690, lr=1.82813e-05, gnorm=1.038, clip=50, loss_scale=128, train_wall=24, gb_free=6.5, wall=11533
2023-08-01 19:33:50 - progress_bar.py[line:272] - INFO: epoch 002:   1962 / 2745 loss=1.682, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=1012.6, nsentences=144, sample_size=1012.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=412.1, ups=0.41, wpb=1012.6, bsz=144, num_updates=4700, lr=1.82523e-05, gnorm=1.126, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=11558
2023-08-01 19:34:15 - progress_bar.py[line:272] - INFO: epoch 002:   1972 / 2745 loss=1.68, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=1017.7, nsentences=144, sample_size=1017.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=414, ups=0.41, wpb=1017.7, bsz=144, num_updates=4710, lr=1.82232e-05, gnorm=1.054, clip=70, loss_scale=128, train_wall=25, gb_free=6.5, wall=11582
2023-08-01 19:34:39 - progress_bar.py[line:272] - INFO: epoch 002:   1982 / 2745 loss=1.685, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=1005.2, nsentences=144, sample_size=1005.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=409.7, ups=0.41, wpb=1005.2, bsz=144, num_updates=4720, lr=1.81941e-05, gnorm=1.119, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=11607
2023-08-01 19:35:04 - progress_bar.py[line:272] - INFO: epoch 002:   1992 / 2745 loss=1.681, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=1014.8, nsentences=144, sample_size=1014.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=412.6, ups=0.41, wpb=1014.8, bsz=144, num_updates=4730, lr=1.81651e-05, gnorm=1.081, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=11632
2023-08-01 19:35:28 - progress_bar.py[line:272] - INFO: epoch 002:   2002 / 2745 loss=1.689, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=1018.4, nsentences=144, sample_size=1018.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=415.6, ups=0.41, wpb=1018.4, bsz=144, num_updates=4740, lr=1.8136e-05, gnorm=1.131, clip=100, loss_scale=128, train_wall=24, gb_free=6.5, wall=11656
2023-08-01 19:35:53 - progress_bar.py[line:272] - INFO: epoch 002:   2012 / 2745 loss=1.662, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=1016, nsentences=144, sample_size=1016, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=412.8, ups=0.41, wpb=1016, bsz=144, num_updates=4750, lr=1.8107e-05, gnorm=1.063, clip=60, loss_scale=128, train_wall=25, gb_free=6.5, wall=11681
2023-08-01 19:36:18 - progress_bar.py[line:272] - INFO: epoch 002:   2022 / 2745 loss=1.679, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=1013.6, nsentences=144, sample_size=1013.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=413, ups=0.41, wpb=1013.6, bsz=144, num_updates=4760, lr=1.80779e-05, gnorm=1.126, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=11705
2023-08-01 19:36:42 - progress_bar.py[line:272] - INFO: epoch 002:   2032 / 2745 loss=1.675, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=1018.3, nsentences=144, sample_size=1018.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=415.9, ups=0.41, wpb=1018.3, bsz=144, num_updates=4770, lr=1.80488e-05, gnorm=1.117, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=11730
2023-08-01 19:37:06 - progress_bar.py[line:272] - INFO: epoch 002:   2042 / 2745 loss=1.665, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=1007.8, nsentences=144, sample_size=1007.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=411.5, ups=0.41, wpb=1007.8, bsz=144, num_updates=4780, lr=1.80198e-05, gnorm=1.151, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=11754
2023-08-01 19:37:31 - progress_bar.py[line:272] - INFO: epoch 002:   2052 / 2745 loss=1.674, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=1013.2, nsentences=144, sample_size=1013.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=414.4, ups=0.41, wpb=1013.2, bsz=144, num_updates=4790, lr=1.79907e-05, gnorm=1.103, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=11779
2023-08-01 19:37:55 - progress_bar.py[line:272] - INFO: epoch 002:   2062 / 2745 loss=1.668, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=1012.9, nsentences=144, sample_size=1012.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=413.6, ups=0.41, wpb=1012.9, bsz=144, num_updates=4800, lr=1.79616e-05, gnorm=1.091, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=11803
2023-08-01 19:38:20 - progress_bar.py[line:272] - INFO: epoch 002:   2072 / 2745 loss=1.686, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=1015.6, nsentences=144, sample_size=1015.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=414.6, ups=0.41, wpb=1015.6, bsz=144, num_updates=4810, lr=1.79326e-05, gnorm=1.143, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=11828
2023-08-01 19:38:44 - progress_bar.py[line:272] - INFO: epoch 002:   2082 / 2745 loss=1.674, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=1018.8, nsentences=144, sample_size=1018.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=416.5, ups=0.41, wpb=1018.8, bsz=144, num_updates=4820, lr=1.79035e-05, gnorm=1.102, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=11852
2023-08-01 19:39:09 - progress_bar.py[line:272] - INFO: epoch 002:   2092 / 2745 loss=1.676, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=1016, nsentences=144, sample_size=1016, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=415.5, ups=0.41, wpb=1016, bsz=144, num_updates=4830, lr=1.78744e-05, gnorm=1.115, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=11877
2023-08-01 19:39:33 - progress_bar.py[line:272] - INFO: epoch 002:   2102 / 2745 loss=1.677, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=1012.5, nsentences=144, sample_size=1012.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=413.1, ups=0.41, wpb=1012.5, bsz=144, num_updates=4840, lr=1.78454e-05, gnorm=1.108, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=11901
2023-08-01 19:39:58 - progress_bar.py[line:272] - INFO: epoch 002:   2112 / 2745 loss=1.675, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=1016.5, nsentences=144, sample_size=1016.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=415.1, ups=0.41, wpb=1016.5, bsz=144, num_updates=4850, lr=1.78163e-05, gnorm=1.109, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=11926
2023-08-01 19:40:22 - progress_bar.py[line:272] - INFO: epoch 002:   2122 / 2745 loss=1.676, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=1016.1, nsentences=144, sample_size=1016.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=414, ups=0.41, wpb=1016.1, bsz=144, num_updates=4860, lr=1.77873e-05, gnorm=1.107, clip=70, loss_scale=128, train_wall=25, gb_free=6.5, wall=11950
2023-08-01 19:40:47 - progress_bar.py[line:272] - INFO: epoch 002:   2132 / 2745 loss=1.675, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=1019.5, nsentences=144, sample_size=1019.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=416.2, ups=0.41, wpb=1019.5, bsz=144, num_updates=4870, lr=1.77582e-05, gnorm=1.122, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=11975
2023-08-01 19:41:11 - progress_bar.py[line:272] - INFO: epoch 002:   2142 / 2745 loss=1.669, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=1018.3, nsentences=144, sample_size=1018.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=416.4, ups=0.41, wpb=1018.3, bsz=144, num_updates=4880, lr=1.77291e-05, gnorm=1.096, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=11999
2023-08-01 19:41:36 - progress_bar.py[line:272] - INFO: epoch 002:   2152 / 2745 loss=1.666, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=1009.6, nsentences=144, sample_size=1009.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=411, ups=0.41, wpb=1009.6, bsz=144, num_updates=4890, lr=1.77001e-05, gnorm=1.089, clip=70, loss_scale=128, train_wall=25, gb_free=6.5, wall=12024
2023-08-01 19:42:00 - progress_bar.py[line:272] - INFO: epoch 002:   2162 / 2745 loss=1.681, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=1016.4, nsentences=144, sample_size=1016.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=415.1, ups=0.41, wpb=1016.4, bsz=144, num_updates=4900, lr=1.7671e-05, gnorm=1.19, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=12048
2023-08-01 19:42:25 - progress_bar.py[line:272] - INFO: epoch 002:   2172 / 2745 loss=1.678, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=1018.1, nsentences=144, sample_size=1018.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=416.2, ups=0.41, wpb=1018.1, bsz=144, num_updates=4910, lr=1.76419e-05, gnorm=1.17, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=12073
2023-08-01 19:42:49 - progress_bar.py[line:272] - INFO: epoch 002:   2182 / 2745 loss=1.668, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=1014.6, nsentences=144, sample_size=1014.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=414.8, ups=0.41, wpb=1014.6, bsz=144, num_updates=4920, lr=1.76129e-05, gnorm=1.146, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=12097
2023-08-01 19:43:14 - progress_bar.py[line:272] - INFO: epoch 002:   2192 / 2745 loss=1.678, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=1015.9, nsentences=144, sample_size=1015.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=415, ups=0.41, wpb=1015.9, bsz=144, num_updates=4930, lr=1.75838e-05, gnorm=1.136, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=12122
2023-08-01 19:43:38 - progress_bar.py[line:272] - INFO: epoch 002:   2202 / 2745 loss=1.685, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=1015.7, nsentences=144, sample_size=1015.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=414.6, ups=0.41, wpb=1015.7, bsz=144, num_updates=4940, lr=1.75547e-05, gnorm=1.13, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=12146
2023-08-01 19:44:03 - progress_bar.py[line:272] - INFO: epoch 002:   2212 / 2745 loss=1.668, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=1014.7, nsentences=144, sample_size=1014.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=413.5, ups=0.41, wpb=1014.7, bsz=144, num_updates=4950, lr=1.75257e-05, gnorm=1.147, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=12171
2023-08-01 19:44:27 - progress_bar.py[line:272] - INFO: epoch 002:   2222 / 2745 loss=1.676, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=1012.7, nsentences=144, sample_size=1012.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=412.2, ups=0.41, wpb=1012.7, bsz=144, num_updates=4960, lr=1.74966e-05, gnorm=1.202, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=12195
2023-08-01 19:44:52 - progress_bar.py[line:272] - INFO: epoch 002:   2232 / 2745 loss=1.675, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=1019.7, nsentences=144, sample_size=1019.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=417.4, ups=0.41, wpb=1019.7, bsz=144, num_updates=4970, lr=1.74675e-05, gnorm=1.131, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=12220
2023-08-01 19:45:16 - progress_bar.py[line:272] - INFO: epoch 002:   2242 / 2745 loss=1.672, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=1017.2, nsentences=144, sample_size=1017.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=415.4, ups=0.41, wpb=1017.2, bsz=144, num_updates=4980, lr=1.74385e-05, gnorm=1.059, clip=60, loss_scale=128, train_wall=24, gb_free=6.5, wall=12244
2023-08-01 19:45:41 - progress_bar.py[line:272] - INFO: epoch 002:   2252 / 2745 loss=1.675, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=1013.4, nsentences=144, sample_size=1013.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=414.8, ups=0.41, wpb=1013.4, bsz=144, num_updates=4990, lr=1.74094e-05, gnorm=1.062, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=12268
2023-08-01 19:46:05 - progress_bar.py[line:272] - INFO: epoch 002:   2262 / 2745 loss=1.687, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=1012.2, nsentences=144, sample_size=1012.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=414.6, ups=0.41, wpb=1012.2, bsz=144, num_updates=5000, lr=1.73804e-05, gnorm=1.155, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=12293
2023-08-01 19:46:30 - progress_bar.py[line:272] - INFO: epoch 002:   2272 / 2745 loss=1.668, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=1015.3, nsentences=144, sample_size=1015.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=413.8, ups=0.41, wpb=1015.3, bsz=144, num_updates=5010, lr=1.73513e-05, gnorm=1.056, clip=60, loss_scale=128, train_wall=25, gb_free=6.5, wall=12317
2023-08-01 19:46:54 - progress_bar.py[line:272] - INFO: epoch 002:   2282 / 2745 loss=1.663, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=1019.1, nsentences=144, sample_size=1019.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=415.5, ups=0.41, wpb=1019.1, bsz=144, num_updates=5020, lr=1.73222e-05, gnorm=1.117, clip=80, loss_scale=256, train_wall=24, gb_free=6.5, wall=12342
2023-08-01 19:47:19 - progress_bar.py[line:272] - INFO: epoch 002:   2292 / 2745 loss=1.681, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=1010.6, nsentences=144, sample_size=1010.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=411.8, ups=0.41, wpb=1010.6, bsz=144, num_updates=5030, lr=1.72932e-05, gnorm=1.179, clip=90, loss_scale=256, train_wall=25, gb_free=6.5, wall=12367
2023-08-01 19:47:43 - progress_bar.py[line:272] - INFO: epoch 002:   2302 / 2745 loss=1.676, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=1015.8, nsentences=144, sample_size=1015.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=414.2, ups=0.41, wpb=1015.8, bsz=144, num_updates=5040, lr=1.72641e-05, gnorm=1.044, clip=50, loss_scale=256, train_wall=24, gb_free=6.5, wall=12391
2023-08-01 19:48:08 - progress_bar.py[line:272] - INFO: epoch 002:   2312 / 2745 loss=1.678, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=1018.3, nsentences=144, sample_size=1018.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=415.4, ups=0.41, wpb=1018.3, bsz=144, num_updates=5050, lr=1.7235e-05, gnorm=1.009, clip=50, loss_scale=256, train_wall=24, gb_free=6.5, wall=12416
2023-08-01 19:48:32 - progress_bar.py[line:272] - INFO: epoch 002:   2322 / 2745 loss=1.672, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=1015.4, nsentences=144, sample_size=1015.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=414.3, ups=0.41, wpb=1015.4, bsz=144, num_updates=5060, lr=1.7206e-05, gnorm=1.087, clip=80, loss_scale=256, train_wall=24, gb_free=6.5, wall=12440
2023-08-01 19:48:57 - progress_bar.py[line:272] - INFO: epoch 002:   2332 / 2745 loss=1.666, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=1015.2, nsentences=144, sample_size=1015.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=413.6, ups=0.41, wpb=1015.2, bsz=144, num_updates=5070, lr=1.71769e-05, gnorm=1.087, clip=80, loss_scale=256, train_wall=25, gb_free=6.5, wall=12465
2023-08-01 19:49:21 - progress_bar.py[line:272] - INFO: epoch 002:   2342 / 2745 loss=1.682, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=1016.8, nsentences=144, sample_size=1016.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=415.3, ups=0.41, wpb=1016.8, bsz=144, num_updates=5080, lr=1.71478e-05, gnorm=1.159, clip=90, loss_scale=256, train_wall=24, gb_free=6.5, wall=12489
2023-08-01 19:49:46 - progress_bar.py[line:272] - INFO: epoch 002:   2352 / 2745 loss=1.677, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=1009, nsentences=144, sample_size=1009, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=411.3, ups=0.41, wpb=1009, bsz=144, num_updates=5090, lr=1.71188e-05, gnorm=1.147, clip=100, loss_scale=256, train_wall=25, gb_free=6.5, wall=12514
2023-08-01 19:50:10 - progress_bar.py[line:272] - INFO: epoch 002:   2362 / 2745 loss=1.668, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=992.1, nsentences=140.7, sample_size=992.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=413.7, ups=0.42, wpb=992.1, bsz=140.7, num_updates=5100, lr=1.70897e-05, gnorm=1.097, clip=80, loss_scale=256, train_wall=24, gb_free=6.5, wall=12538
2023-08-01 19:50:34 - progress_bar.py[line:272] - INFO: epoch 002:   2372 / 2745 loss=1.683, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=1008.3, nsentences=144, sample_size=1008.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=411.4, ups=0.41, wpb=1008.3, bsz=144, num_updates=5110, lr=1.70606e-05, gnorm=1.19, clip=80, loss_scale=256, train_wall=24, gb_free=6.5, wall=12562
2023-08-01 19:50:59 - progress_bar.py[line:272] - INFO: epoch 002:   2382 / 2745 loss=1.671, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=1017.7, nsentences=144, sample_size=1017.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=415.6, ups=0.41, wpb=1017.7, bsz=144, num_updates=5120, lr=1.70316e-05, gnorm=1.115, clip=80, loss_scale=256, train_wall=24, gb_free=6.5, wall=12587
2023-08-01 19:51:23 - progress_bar.py[line:272] - INFO: epoch 002:   2392 / 2745 loss=1.671, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=1020.4, nsentences=144, sample_size=1020.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=416.8, ups=0.41, wpb=1020.4, bsz=144, num_updates=5130, lr=1.70025e-05, gnorm=1.093, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=12611
2023-08-01 19:51:48 - progress_bar.py[line:272] - INFO: epoch 002:   2402 / 2745 loss=1.672, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=1012.6, nsentences=144, sample_size=1012.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=412.8, ups=0.41, wpb=1012.6, bsz=144, num_updates=5140, lr=1.69735e-05, gnorm=1.117, clip=70, loss_scale=256, train_wall=25, gb_free=6.5, wall=12636
2023-08-01 19:52:13 - progress_bar.py[line:272] - INFO: epoch 002:   2412 / 2745 loss=1.673, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=1014.3, nsentences=144, sample_size=1014.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=412.7, ups=0.41, wpb=1014.3, bsz=144, num_updates=5150, lr=1.69444e-05, gnorm=1.081, clip=60, loss_scale=256, train_wall=25, gb_free=6.5, wall=12660
2023-08-01 19:52:37 - progress_bar.py[line:272] - INFO: epoch 002:   2422 / 2745 loss=1.675, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=1015.8, nsentences=144, sample_size=1015.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=414.4, ups=0.41, wpb=1015.8, bsz=144, num_updates=5160, lr=1.69153e-05, gnorm=1.14, clip=80, loss_scale=256, train_wall=24, gb_free=6.5, wall=12685
2023-08-01 19:53:02 - progress_bar.py[line:272] - INFO: epoch 002:   2432 / 2745 loss=1.667, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=1013.1, nsentences=144, sample_size=1013.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=413, ups=0.41, wpb=1013.1, bsz=144, num_updates=5170, lr=1.68863e-05, gnorm=1.091, clip=90, loss_scale=256, train_wall=25, gb_free=6.5, wall=12709
2023-08-01 19:53:26 - progress_bar.py[line:272] - INFO: epoch 002:   2442 / 2745 loss=1.669, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=1012.8, nsentences=144, sample_size=1012.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=413.2, ups=0.41, wpb=1012.8, bsz=144, num_updates=5180, lr=1.68572e-05, gnorm=1.206, clip=80, loss_scale=256, train_wall=24, gb_free=6.5, wall=12734
2023-08-01 19:53:51 - progress_bar.py[line:272] - INFO: epoch 002:   2452 / 2745 loss=1.669, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=1006.6, nsentences=144, sample_size=1006.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=409.6, ups=0.41, wpb=1006.6, bsz=144, num_updates=5190, lr=1.68281e-05, gnorm=1.108, clip=70, loss_scale=256, train_wall=25, gb_free=6.5, wall=12758
2023-08-01 19:54:15 - progress_bar.py[line:272] - INFO: epoch 002:   2462 / 2745 loss=1.673, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=1017, nsentences=144, sample_size=1017, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=414.9, ups=0.41, wpb=1017, bsz=144, num_updates=5200, lr=1.67991e-05, gnorm=1.089, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=12783
2023-08-01 19:54:40 - progress_bar.py[line:272] - INFO: epoch 002:   2472 / 2745 loss=1.673, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=1014.3, nsentences=144, sample_size=1014.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=414.2, ups=0.41, wpb=1014.3, bsz=144, num_updates=5210, lr=1.677e-05, gnorm=1.141, clip=90, loss_scale=256, train_wall=24, gb_free=6.5, wall=12807
2023-08-01 19:54:54 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-08-01 19:55:07 - progress_bar.py[line:272] - INFO: epoch 002:   2483 / 2745 loss=1.686, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=1015.6, nsentences=144, sample_size=1015.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=376.7, ups=0.37, wpb=1015.6, bsz=144, num_updates=5220, lr=1.67409e-05, gnorm=1.146, clip=90, loss_scale=128, train_wall=27, gb_free=6.5, wall=12834
2023-08-01 19:55:31 - progress_bar.py[line:272] - INFO: epoch 002:   2493 / 2745 loss=1.662, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=1018.5, nsentences=144, sample_size=1018.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=415.9, ups=0.41, wpb=1018.5, bsz=144, num_updates=5230, lr=1.67119e-05, gnorm=1.042, clip=50, loss_scale=128, train_wall=24, gb_free=6.5, wall=12859
2023-08-01 19:55:56 - progress_bar.py[line:272] - INFO: epoch 002:   2503 / 2745 loss=1.665, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=1015.3, nsentences=144, sample_size=1015.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=413.7, ups=0.41, wpb=1015.3, bsz=144, num_updates=5240, lr=1.66828e-05, gnorm=1.101, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=12883
2023-08-01 19:56:20 - progress_bar.py[line:272] - INFO: epoch 002:   2513 / 2745 loss=1.669, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=1013.7, nsentences=144, sample_size=1013.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=413.6, ups=0.41, wpb=1013.7, bsz=144, num_updates=5250, lr=1.66537e-05, gnorm=1.076, clip=60, loss_scale=128, train_wall=24, gb_free=6.5, wall=12908
2023-08-01 19:56:45 - progress_bar.py[line:272] - INFO: epoch 002:   2523 / 2745 loss=1.68, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=1002.8, nsentences=144, sample_size=1002.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=409, ups=0.41, wpb=1002.8, bsz=144, num_updates=5260, lr=1.66247e-05, gnorm=1.242, clip=100, loss_scale=128, train_wall=24, gb_free=6.5, wall=12932
2023-08-01 19:57:09 - progress_bar.py[line:272] - INFO: epoch 002:   2533 / 2745 loss=1.678, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=1014.7, nsentences=144, sample_size=1014.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=413.5, ups=0.41, wpb=1014.7, bsz=144, num_updates=5270, lr=1.65956e-05, gnorm=1.076, clip=60, loss_scale=128, train_wall=25, gb_free=6.5, wall=12957
2023-08-01 19:57:34 - progress_bar.py[line:272] - INFO: epoch 002:   2543 / 2745 loss=1.685, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=1017.4, nsentences=144, sample_size=1017.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=415.6, ups=0.41, wpb=1017.4, bsz=144, num_updates=5280, lr=1.65666e-05, gnorm=1.086, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=12981
2023-08-01 19:57:58 - progress_bar.py[line:272] - INFO: epoch 002:   2553 / 2745 loss=1.672, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=1015.9, nsentences=144, sample_size=1015.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=414.7, ups=0.41, wpb=1015.9, bsz=144, num_updates=5290, lr=1.65375e-05, gnorm=1.101, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=13006
2023-08-01 19:58:23 - progress_bar.py[line:272] - INFO: epoch 002:   2563 / 2745 loss=1.674, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=1012.4, nsentences=144, sample_size=1012.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=412.9, ups=0.41, wpb=1012.4, bsz=144, num_updates=5300, lr=1.65084e-05, gnorm=1.144, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=13030
2023-08-01 19:58:47 - progress_bar.py[line:272] - INFO: epoch 002:   2573 / 2745 loss=1.675, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=1012.8, nsentences=144, sample_size=1012.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=413.3, ups=0.41, wpb=1012.8, bsz=144, num_updates=5310, lr=1.64794e-05, gnorm=1.105, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=13055
2023-08-01 19:59:12 - progress_bar.py[line:272] - INFO: epoch 002:   2583 / 2745 loss=1.676, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=1014.6, nsentences=144, sample_size=1014.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=414.3, ups=0.41, wpb=1014.6, bsz=144, num_updates=5320, lr=1.64503e-05, gnorm=1.197, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=13079
2023-08-01 19:59:36 - progress_bar.py[line:272] - INFO: epoch 002:   2593 / 2745 loss=1.672, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=1010.4, nsentences=144, sample_size=1010.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=412.7, ups=0.41, wpb=1010.4, bsz=144, num_updates=5330, lr=1.64212e-05, gnorm=1.106, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=13104
2023-08-01 20:00:01 - progress_bar.py[line:272] - INFO: epoch 002:   2603 / 2745 loss=1.668, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=1013.2, nsentences=144, sample_size=1013.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=413.9, ups=0.41, wpb=1013.2, bsz=144, num_updates=5340, lr=1.63922e-05, gnorm=1.047, clip=60, loss_scale=128, train_wall=24, gb_free=6.5, wall=13128
2023-08-01 20:00:25 - progress_bar.py[line:272] - INFO: epoch 002:   2613 / 2745 loss=1.678, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=1011.5, nsentences=144, sample_size=1011.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=412.2, ups=0.41, wpb=1011.5, bsz=144, num_updates=5350, lr=1.63631e-05, gnorm=1.178, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=13153
2023-08-01 20:00:50 - progress_bar.py[line:272] - INFO: epoch 002:   2623 / 2745 loss=1.674, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=1018.2, nsentences=144, sample_size=1018.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=415.6, ups=0.41, wpb=1018.2, bsz=144, num_updates=5360, lr=1.6334e-05, gnorm=1.136, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=13177
2023-08-01 20:01:14 - progress_bar.py[line:272] - INFO: epoch 002:   2633 / 2745 loss=1.663, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=1013.6, nsentences=144, sample_size=1013.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=413.2, ups=0.41, wpb=1013.6, bsz=144, num_updates=5370, lr=1.6305e-05, gnorm=1.118, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=13202
2023-08-01 20:01:39 - progress_bar.py[line:272] - INFO: epoch 002:   2643 / 2745 loss=1.671, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=1017.3, nsentences=144, sample_size=1017.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=415.4, ups=0.41, wpb=1017.3, bsz=144, num_updates=5380, lr=1.62759e-05, gnorm=1.154, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=13227
2023-08-01 20:02:03 - progress_bar.py[line:272] - INFO: epoch 002:   2653 / 2745 loss=1.668, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=1013.3, nsentences=144, sample_size=1013.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=413.2, ups=0.41, wpb=1013.3, bsz=144, num_updates=5390, lr=1.62469e-05, gnorm=1.105, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=13251
2023-08-01 20:02:28 - progress_bar.py[line:272] - INFO: epoch 002:   2663 / 2745 loss=1.65, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=1014.5, nsentences=144, sample_size=1014.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=414, ups=0.41, wpb=1014.5, bsz=144, num_updates=5400, lr=1.62178e-05, gnorm=0.979, clip=50, loss_scale=128, train_wall=24, gb_free=6.5, wall=13276
2023-08-01 20:02:52 - progress_bar.py[line:272] - INFO: epoch 002:   2673 / 2745 loss=1.672, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=1018.1, nsentences=144, sample_size=1018.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=415.5, ups=0.41, wpb=1018.1, bsz=144, num_updates=5410, lr=1.61887e-05, gnorm=1.068, clip=60, loss_scale=128, train_wall=24, gb_free=6.5, wall=13300
2023-08-01 20:03:17 - progress_bar.py[line:272] - INFO: epoch 002:   2683 / 2745 loss=1.675, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=1015.5, nsentences=144, sample_size=1015.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=413.9, ups=0.41, wpb=1015.5, bsz=144, num_updates=5420, lr=1.61597e-05, gnorm=1.203, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=13325
2023-08-01 20:03:41 - progress_bar.py[line:272] - INFO: epoch 002:   2693 / 2745 loss=1.664, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=1013.4, nsentences=144, sample_size=1013.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=413.5, ups=0.41, wpb=1013.4, bsz=144, num_updates=5430, lr=1.61306e-05, gnorm=1.212, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=13349
2023-08-01 20:04:06 - progress_bar.py[line:272] - INFO: epoch 002:   2703 / 2745 loss=1.667, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=1012, nsentences=144, sample_size=1012, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=411.5, ups=0.41, wpb=1012, bsz=144, num_updates=5440, lr=1.61015e-05, gnorm=1.073, clip=60, loss_scale=128, train_wall=25, gb_free=6.5, wall=13374
2023-08-01 20:04:31 - progress_bar.py[line:272] - INFO: epoch 002:   2713 / 2745 loss=1.676, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=1013.7, nsentences=144, sample_size=1013.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=413.2, ups=0.41, wpb=1013.7, bsz=144, num_updates=5450, lr=1.60725e-05, gnorm=1.077, clip=60, loss_scale=128, train_wall=25, gb_free=6.5, wall=13398
2023-08-01 20:04:55 - progress_bar.py[line:272] - INFO: epoch 002:   2723 / 2745 loss=1.667, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=1009.9, nsentences=144, sample_size=1009.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=411.1, ups=0.41, wpb=1009.9, bsz=144, num_updates=5460, lr=1.60434e-05, gnorm=1.078, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=13423
2023-08-01 20:05:20 - progress_bar.py[line:272] - INFO: epoch 002:   2733 / 2745 loss=1.67, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=1018, nsentences=144, sample_size=1018, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=415.3, ups=0.41, wpb=1018, bsz=144, num_updates=5470, lr=1.60143e-05, gnorm=1.13, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=13447
2023-08-01 20:05:44 - progress_bar.py[line:272] - INFO: epoch 002:   2743 / 2745 loss=1.667, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=1018.7, nsentences=144, sample_size=1018.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=417, ups=0.41, wpb=1018.7, bsz=144, num_updates=5480, lr=1.59853e-05, gnorm=1.175, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=13472
2023-08-01 20:05:48 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 2 @ 5482 updates
2023-08-01 20:05:48 - trainer.py[line:431] - INFO: Saving checkpoint to ../../checkpoints/OFA/vrd2_checkpoints/_4_3e-5_512_mix_wearing/checkpoint2.pt
local datafile ../../dataset/OFA_data/vrd_mix/vg_train_wearing.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/vrd_mix/vg_train_wearing.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/vrd_mix/vg_train_wearing.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/vrd_mix/vg_train_wearing.tsv slice_id 1 row count 131725 total row count 395175
local datafile ../../dataset/OFA_data/vrd_mix/vg_train_wearing.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/vrd_mix/vg_train_wearing.tsv slice_id 2 row count 131725 total row count 395175
slice_id 2 seek offset 263450
slice_id 1 seek offset 131725
2023-08-01 20:05:51 - trainer.py[line:441] - INFO: Finished saving checkpoint to ../../checkpoints/OFA/vrd2_checkpoints/_4_3e-5_512_mix_wearing/checkpoint2.pt
2023-08-01 20:05:52 - checkpoint_utils.py[line:133] - INFO: Saved checkpoint ../../checkpoints/OFA/vrd2_checkpoints/_4_3e-5_512_mix_wearing/checkpoint2.pt (epoch 2 @ 5482 updates, score None) (writing took 4.720682767001563 seconds)
2023-08-01 20:05:52 - train.py[line:332] - INFO: end of epoch 2 (average epoch stats below)
2023-08-01 20:05:52 - progress_bar.py[line:282] - INFO: epoch 002 | loss 1.69 | loss_v1 0 | loss_v2 0 | nll_loss 0.385 | ntokens 1014.55 | nsentences 143.962 | sample_size 1014.55 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.31 | wps 412.9 | ups 0.41 | wpb 1014.5 | bsz 144 | num_updates 5482 | lr 1.59795e-05 | gnorm 1.104 | clip 75.1 | loss_scale 128 | train_wall 6723 | gb_free 6.5 | wall 13480
2023-08-01 20:05:52 - trainer.py[line:639] - INFO: loading train data for epoch 3
local datafile ../../dataset/OFA_data/vrd_mix/vg_train_wearing.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../dataset/OFA_data/vrd_mix/vg_train_wearing.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../dataset/OFA_data/vrd_mix/vg_train_wearing.tsv slice_id 0 row count 131725 total row count 395175
slice_id 0 seek offset 0
2023-08-01 20:05:53 - trainer.py[line:703] - INFO: begin training epoch 3
2023-08-01 20:05:53 - train.py[line:305] - INFO: Start iterating over samples
2023-08-01 20:06:13 - progress_bar.py[line:272] - INFO: epoch 003:      8 / 2745 loss=1.663, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=959.8, nsentences=136.8, sample_size=959.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=333, ups=0.35, wpb=959.8, bsz=136.8, num_updates=5490, lr=1.59562e-05, gnorm=1.12, clip=80, loss_scale=128, train_wall=23, gb_free=6.5, wall=13501
2023-08-01 20:06:37 - progress_bar.py[line:272] - INFO: epoch 003:     18 / 2745 loss=1.673, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=1018.3, nsentences=144, sample_size=1018.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=415.9, ups=0.41, wpb=1018.3, bsz=144, num_updates=5500, lr=1.59271e-05, gnorm=1.109, clip=60, loss_scale=128, train_wall=24, gb_free=6.5, wall=13525
2023-08-01 20:07:02 - progress_bar.py[line:272] - INFO: epoch 003:     28 / 2745 loss=1.669, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=1017, nsentences=144, sample_size=1017, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=415.4, ups=0.41, wpb=1017, bsz=144, num_updates=5510, lr=1.58981e-05, gnorm=1.054, clip=60, loss_scale=128, train_wall=24, gb_free=6.5, wall=13550
2023-08-01 20:07:26 - progress_bar.py[line:272] - INFO: epoch 003:     38 / 2745 loss=1.675, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=1014.7, nsentences=144, sample_size=1014.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=414, ups=0.41, wpb=1014.7, bsz=144, num_updates=5520, lr=1.5869e-05, gnorm=1.126, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=13574
2023-08-01 20:07:51 - progress_bar.py[line:272] - INFO: epoch 003:     48 / 2745 loss=1.661, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=1014.2, nsentences=144, sample_size=1014.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=413, ups=0.41, wpb=1014.2, bsz=144, num_updates=5530, lr=1.584e-05, gnorm=1.028, clip=70, loss_scale=128, train_wall=25, gb_free=6.5, wall=13599
2023-08-01 20:08:15 - progress_bar.py[line:272] - INFO: epoch 003:     58 / 2745 loss=1.659, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=1007.2, nsentences=144, sample_size=1007.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=410.8, ups=0.41, wpb=1007.2, bsz=144, num_updates=5540, lr=1.58109e-05, gnorm=1.13, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=13623
2023-08-01 20:08:40 - progress_bar.py[line:272] - INFO: epoch 003:     68 / 2745 loss=1.662, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=1014.4, nsentences=144, sample_size=1014.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=414.4, ups=0.41, wpb=1014.4, bsz=144, num_updates=5550, lr=1.57818e-05, gnorm=1.12, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=13648
2023-08-01 20:09:04 - progress_bar.py[line:272] - INFO: epoch 003:     78 / 2745 loss=1.675, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=1012.6, nsentences=144, sample_size=1012.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=413.4, ups=0.41, wpb=1012.6, bsz=144, num_updates=5560, lr=1.57528e-05, gnorm=1.169, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=13672
2023-08-01 20:09:29 - progress_bar.py[line:272] - INFO: epoch 003:     88 / 2745 loss=1.662, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=1014.7, nsentences=144, sample_size=1014.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=413.5, ups=0.41, wpb=1014.7, bsz=144, num_updates=5570, lr=1.57237e-05, gnorm=1.017, clip=70, loss_scale=128, train_wall=25, gb_free=6.5, wall=13697
2023-08-01 20:09:54 - progress_bar.py[line:272] - INFO: epoch 003:     98 / 2745 loss=1.668, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=1012.7, nsentences=144, sample_size=1012.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=412.3, ups=0.41, wpb=1012.7, bsz=144, num_updates=5580, lr=1.56946e-05, gnorm=1.124, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=13721
2023-08-01 20:10:18 - progress_bar.py[line:272] - INFO: epoch 003:    108 / 2745 loss=1.671, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=1012.5, nsentences=144, sample_size=1012.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=412.9, ups=0.41, wpb=1012.5, bsz=144, num_updates=5590, lr=1.56656e-05, gnorm=1.164, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=13746
2023-08-01 20:10:43 - progress_bar.py[line:272] - INFO: epoch 003:    118 / 2745 loss=1.652, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=1013.4, nsentences=144, sample_size=1013.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=413.4, ups=0.41, wpb=1013.4, bsz=144, num_updates=5600, lr=1.56365e-05, gnorm=1.058, clip=40, loss_scale=128, train_wall=24, gb_free=6.5, wall=13770
2023-08-01 20:11:07 - progress_bar.py[line:272] - INFO: epoch 003:    128 / 2745 loss=1.663, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=1011.8, nsentences=144, sample_size=1011.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=412.8, ups=0.41, wpb=1011.8, bsz=144, num_updates=5610, lr=1.56074e-05, gnorm=1.09, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=13795
2023-08-01 20:11:32 - progress_bar.py[line:272] - INFO: epoch 003:    138 / 2745 loss=1.671, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=1018.4, nsentences=144, sample_size=1018.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=416.7, ups=0.41, wpb=1018.4, bsz=144, num_updates=5620, lr=1.55784e-05, gnorm=1.101, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=13819
2023-08-01 20:11:56 - progress_bar.py[line:272] - INFO: epoch 003:    148 / 2745 loss=1.654, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=1014.2, nsentences=144, sample_size=1014.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=414.8, ups=0.41, wpb=1014.2, bsz=144, num_updates=5630, lr=1.55493e-05, gnorm=1.179, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=13844
2023-08-01 20:12:21 - progress_bar.py[line:272] - INFO: epoch 003:    158 / 2745 loss=1.677, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=1009.2, nsentences=144, sample_size=1009.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=411.3, ups=0.41, wpb=1009.2, bsz=144, num_updates=5640, lr=1.55202e-05, gnorm=1.138, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=13868
2023-08-01 20:12:45 - progress_bar.py[line:272] - INFO: epoch 003:    168 / 2745 loss=1.667, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=1013.9, nsentences=144, sample_size=1013.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=413.8, ups=0.41, wpb=1013.9, bsz=144, num_updates=5650, lr=1.54912e-05, gnorm=1.126, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=13893
2023-08-01 20:13:10 - progress_bar.py[line:272] - INFO: epoch 003:    178 / 2745 loss=1.672, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=1011.9, nsentences=144, sample_size=1011.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=411.5, ups=0.41, wpb=1011.9, bsz=144, num_updates=5660, lr=1.54621e-05, gnorm=1.22, clip=100, loss_scale=128, train_wall=25, gb_free=6.5, wall=13917
2023-08-01 20:13:34 - progress_bar.py[line:272] - INFO: epoch 003:    188 / 2745 loss=1.671, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=1013, nsentences=144, sample_size=1013, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=413.3, ups=0.41, wpb=1013, bsz=144, num_updates=5670, lr=1.54331e-05, gnorm=1.156, clip=100, loss_scale=128, train_wall=24, gb_free=6.5, wall=13942
2023-08-01 20:13:59 - progress_bar.py[line:272] - INFO: epoch 003:    198 / 2745 loss=1.665, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=1020.6, nsentences=144, sample_size=1020.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=417.3, ups=0.41, wpb=1020.6, bsz=144, num_updates=5680, lr=1.5404e-05, gnorm=1.133, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=13966
2023-08-01 20:14:23 - progress_bar.py[line:272] - INFO: epoch 003:    208 / 2745 loss=1.67, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=1015.4, nsentences=144, sample_size=1015.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=414.1, ups=0.41, wpb=1015.4, bsz=144, num_updates=5690, lr=1.53749e-05, gnorm=1.053, clip=60, loss_scale=128, train_wall=24, gb_free=6.5, wall=13991
2023-08-01 20:14:48 - progress_bar.py[line:272] - INFO: epoch 003:    218 / 2745 loss=1.658, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=1010.5, nsentences=144, sample_size=1010.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=411, ups=0.41, wpb=1010.5, bsz=144, num_updates=5700, lr=1.53459e-05, gnorm=1.012, clip=40, loss_scale=128, train_wall=25, gb_free=6.5, wall=14015
2023-08-01 20:15:12 - progress_bar.py[line:272] - INFO: epoch 003:    228 / 2745 loss=1.668, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=1016.4, nsentences=144, sample_size=1016.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=414.8, ups=0.41, wpb=1016.4, bsz=144, num_updates=5710, lr=1.53168e-05, gnorm=1.136, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=14040
2023-08-01 20:15:37 - progress_bar.py[line:272] - INFO: epoch 003:    238 / 2745 loss=1.667, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=1012.3, nsentences=144, sample_size=1012.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=413, ups=0.41, wpb=1012.3, bsz=144, num_updates=5720, lr=1.52877e-05, gnorm=1.14, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=14064
2023-08-01 20:16:01 - progress_bar.py[line:272] - INFO: epoch 003:    248 / 2745 loss=1.674, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=1013.6, nsentences=144, sample_size=1013.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=413.3, ups=0.41, wpb=1013.6, bsz=144, num_updates=5730, lr=1.52587e-05, gnorm=1.373, clip=80, loss_scale=256, train_wall=24, gb_free=6.5, wall=14089
2023-08-01 20:16:26 - progress_bar.py[line:272] - INFO: epoch 003:    258 / 2745 loss=1.661, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=1018.3, nsentences=144, sample_size=1018.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=415.7, ups=0.41, wpb=1018.3, bsz=144, num_updates=5740, lr=1.52296e-05, gnorm=1.087, clip=60, loss_scale=256, train_wall=24, gb_free=6.5, wall=14113
2023-08-01 20:16:50 - progress_bar.py[line:272] - INFO: epoch 003:    268 / 2745 loss=1.672, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=1010.2, nsentences=144, sample_size=1010.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=411.1, ups=0.41, wpb=1010.2, bsz=144, num_updates=5750, lr=1.52005e-05, gnorm=1.066, clip=70, loss_scale=256, train_wall=25, gb_free=6.5, wall=14138
2023-08-01 20:17:15 - progress_bar.py[line:272] - INFO: epoch 003:    278 / 2745 loss=1.677, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=1016.3, nsentences=144, sample_size=1016.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=412.9, ups=0.41, wpb=1016.3, bsz=144, num_updates=5760, lr=1.51715e-05, gnorm=1.129, clip=80, loss_scale=256, train_wall=25, gb_free=6.5, wall=14163
2023-08-01 20:17:40 - progress_bar.py[line:272] - INFO: epoch 003:    288 / 2745 loss=1.665, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=1010.9, nsentences=144, sample_size=1010.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=410.4, ups=0.41, wpb=1010.9, bsz=144, num_updates=5770, lr=1.51424e-05, gnorm=1.118, clip=80, loss_scale=256, train_wall=25, gb_free=6.5, wall=14187
2023-08-01 20:18:04 - progress_bar.py[line:272] - INFO: epoch 003:    298 / 2745 loss=1.666, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=1019.3, nsentences=144, sample_size=1019.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=415.9, ups=0.41, wpb=1019.3, bsz=144, num_updates=5780, lr=1.51134e-05, gnorm=1.106, clip=80, loss_scale=256, train_wall=24, gb_free=6.5, wall=14212
2023-08-01 20:18:29 - progress_bar.py[line:272] - INFO: epoch 003:    308 / 2745 loss=1.66, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=1010.2, nsentences=144, sample_size=1010.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=410.6, ups=0.41, wpb=1010.2, bsz=144, num_updates=5790, lr=1.50843e-05, gnorm=1.076, clip=60, loss_scale=256, train_wall=25, gb_free=6.5, wall=14236
2023-08-01 20:18:53 - progress_bar.py[line:272] - INFO: epoch 003:    318 / 2745 loss=1.661, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=1017.6, nsentences=144, sample_size=1017.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=413.9, ups=0.41, wpb=1017.6, bsz=144, num_updates=5800, lr=1.50552e-05, gnorm=1.081, clip=70, loss_scale=256, train_wall=25, gb_free=6.5, wall=14261
2023-08-01 20:19:18 - progress_bar.py[line:272] - INFO: epoch 003:    328 / 2745 loss=1.662, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=1007.1, nsentences=144, sample_size=1007.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=408.9, ups=0.41, wpb=1007.1, bsz=144, num_updates=5810, lr=1.50262e-05, gnorm=1.198, clip=100, loss_scale=256, train_wall=25, gb_free=6.5, wall=14286
2023-08-01 20:19:43 - progress_bar.py[line:272] - INFO: epoch 003:    338 / 2745 loss=1.653, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=1012.5, nsentences=144, sample_size=1012.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=411.8, ups=0.41, wpb=1012.5, bsz=144, num_updates=5820, lr=1.49971e-05, gnorm=1.071, clip=80, loss_scale=256, train_wall=25, gb_free=6.5, wall=14310
2023-08-01 20:20:07 - progress_bar.py[line:272] - INFO: epoch 003:    348 / 2745 loss=1.665, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=1016.8, nsentences=144, sample_size=1016.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=414.3, ups=0.41, wpb=1016.8, bsz=144, num_updates=5830, lr=1.4968e-05, gnorm=1.092, clip=80, loss_scale=256, train_wall=25, gb_free=6.5, wall=14335
2023-08-01 20:20:32 - progress_bar.py[line:272] - INFO: epoch 003:    358 / 2745 loss=1.666, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=1019.5, nsentences=144, sample_size=1019.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=414.9, ups=0.41, wpb=1019.5, bsz=144, num_updates=5840, lr=1.4939e-05, gnorm=1.164, clip=80, loss_scale=256, train_wall=25, gb_free=6.5, wall=14359
2023-08-01 20:20:56 - progress_bar.py[line:272] - INFO: epoch 003:    368 / 2745 loss=1.671, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=1015.6, nsentences=144, sample_size=1015.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=413.4, ups=0.41, wpb=1015.6, bsz=144, num_updates=5850, lr=1.49099e-05, gnorm=1.046, clip=70, loss_scale=256, train_wall=25, gb_free=6.5, wall=14384
2023-08-01 20:21:21 - progress_bar.py[line:272] - INFO: epoch 003:    378 / 2745 loss=1.667, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=1018.1, nsentences=144, sample_size=1018.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=414.5, ups=0.41, wpb=1018.1, bsz=144, num_updates=5860, lr=1.48808e-05, gnorm=1.137, clip=90, loss_scale=256, train_wall=25, gb_free=6.5, wall=14408
2023-08-01 20:21:45 - progress_bar.py[line:272] - INFO: epoch 003:    388 / 2745 loss=1.68, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=1015.8, nsentences=144, sample_size=1015.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=413.9, ups=0.41, wpb=1015.8, bsz=144, num_updates=5870, lr=1.48518e-05, gnorm=1.115, clip=90, loss_scale=256, train_wall=25, gb_free=6.5, wall=14433
2023-08-01 20:22:10 - progress_bar.py[line:272] - INFO: epoch 003:    398 / 2745 loss=1.661, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=1010.6, nsentences=144, sample_size=1010.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=410.6, ups=0.41, wpb=1010.6, bsz=144, num_updates=5880, lr=1.48227e-05, gnorm=1.105, clip=80, loss_scale=256, train_wall=25, gb_free=6.5, wall=14458
2023-08-01 20:22:35 - progress_bar.py[line:272] - INFO: epoch 003:    408 / 2745 loss=1.648, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=1012.9, nsentences=144, sample_size=1012.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=411.8, ups=0.41, wpb=1012.9, bsz=144, num_updates=5890, lr=1.47936e-05, gnorm=0.98, clip=60, loss_scale=256, train_wall=25, gb_free=6.5, wall=14482
2023-08-01 20:22:59 - progress_bar.py[line:272] - INFO: epoch 003:    418 / 2745 loss=1.655, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=1013.8, nsentences=144, sample_size=1013.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=412.9, ups=0.41, wpb=1013.8, bsz=144, num_updates=5900, lr=1.47646e-05, gnorm=1.12, clip=80, loss_scale=256, train_wall=25, gb_free=6.5, wall=14507
2023-08-01 20:23:24 - progress_bar.py[line:272] - INFO: epoch 003:    428 / 2745 loss=1.673, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=1012.2, nsentences=144, sample_size=1012.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=412.5, ups=0.41, wpb=1012.2, bsz=144, num_updates=5910, lr=1.47355e-05, gnorm=1.052, clip=70, loss_scale=256, train_wall=25, gb_free=6.5, wall=14531
2023-08-01 20:23:48 - progress_bar.py[line:272] - INFO: epoch 003:    438 / 2745 loss=1.658, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=1014.8, nsentences=144, sample_size=1014.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=415.1, ups=0.41, wpb=1014.8, bsz=144, num_updates=5920, lr=1.47065e-05, gnorm=1.083, clip=60, loss_scale=256, train_wall=24, gb_free=6.5, wall=14556
2023-08-01 20:24:13 - progress_bar.py[line:272] - INFO: epoch 003:    448 / 2745 loss=1.663, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=1010.6, nsentences=144, sample_size=1010.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=411.6, ups=0.41, wpb=1010.6, bsz=144, num_updates=5930, lr=1.46774e-05, gnorm=1.028, clip=40, loss_scale=256, train_wall=25, gb_free=6.5, wall=14580
2023-08-01 20:24:37 - progress_bar.py[line:272] - INFO: epoch 003:    458 / 2745 loss=1.654, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=1013.9, nsentences=144, sample_size=1013.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=413.2, ups=0.41, wpb=1013.9, bsz=144, num_updates=5940, lr=1.46483e-05, gnorm=1.151, clip=90, loss_scale=256, train_wall=25, gb_free=6.5, wall=14605
2023-08-01 20:25:02 - progress_bar.py[line:272] - INFO: epoch 003:    468 / 2745 loss=1.651, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=1017.2, nsentences=144, sample_size=1017.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=414.7, ups=0.41, wpb=1017.2, bsz=144, num_updates=5950, lr=1.46193e-05, gnorm=1.077, clip=50, loss_scale=256, train_wall=25, gb_free=6.5, wall=14629
2023-08-01 20:25:26 - progress_bar.py[line:272] - INFO: epoch 003:    478 / 2745 loss=1.656, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=1015, nsentences=144, sample_size=1015, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=415.1, ups=0.41, wpb=1015, bsz=144, num_updates=5960, lr=1.45902e-05, gnorm=1.188, clip=80, loss_scale=256, train_wall=24, gb_free=6.5, wall=14654
2023-08-01 20:25:51 - progress_bar.py[line:272] - INFO: epoch 003:    488 / 2745 loss=1.659, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=1015.1, nsentences=144, sample_size=1015.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=414.1, ups=0.41, wpb=1015.1, bsz=144, num_updates=5970, lr=1.45611e-05, gnorm=1.141, clip=80, loss_scale=256, train_wall=24, gb_free=6.5, wall=14678
2023-08-01 20:26:15 - progress_bar.py[line:272] - INFO: epoch 003:    498 / 2745 loss=1.662, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=1012.9, nsentences=144, sample_size=1012.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=412.6, ups=0.41, wpb=1012.9, bsz=144, num_updates=5980, lr=1.45321e-05, gnorm=1.175, clip=90, loss_scale=256, train_wall=25, gb_free=6.5, wall=14703
2023-08-01 20:26:40 - progress_bar.py[line:272] - INFO: epoch 003:    508 / 2745 loss=1.651, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=1024, nsentences=144, sample_size=1024, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=419.3, ups=0.41, wpb=1024, bsz=144, num_updates=5990, lr=1.4503e-05, gnorm=1.077, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=14727
2023-08-01 20:27:04 - progress_bar.py[line:272] - INFO: epoch 003:    518 / 2745 loss=1.645, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=1014.9, nsentences=144, sample_size=1014.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=414.7, ups=0.41, wpb=1014.9, bsz=144, num_updates=6000, lr=1.44739e-05, gnorm=1.12, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=14752
2023-08-01 20:27:29 - progress_bar.py[line:272] - INFO: epoch 003:    528 / 2745 loss=1.666, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=1010.4, nsentences=144, sample_size=1010.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=411.5, ups=0.41, wpb=1010.4, bsz=144, num_updates=6010, lr=1.44449e-05, gnorm=1.123, clip=80, loss_scale=256, train_wall=25, gb_free=6.5, wall=14776
2023-08-01 20:27:53 - progress_bar.py[line:272] - INFO: epoch 003:    538 / 2745 loss=1.661, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=1019, nsentences=144, sample_size=1019, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=415.9, ups=0.41, wpb=1019, bsz=144, num_updates=6020, lr=1.44158e-05, gnorm=1.298, clip=100, loss_scale=256, train_wall=24, gb_free=6.5, wall=14801
2023-08-01 20:28:18 - progress_bar.py[line:272] - INFO: epoch 003:    548 / 2745 loss=1.648, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=1019.1, nsentences=144, sample_size=1019.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=415.9, ups=0.41, wpb=1019.1, bsz=144, num_updates=6030, lr=1.43867e-05, gnorm=1.07, clip=60, loss_scale=256, train_wall=24, gb_free=6.5, wall=14825
2023-08-01 20:28:42 - progress_bar.py[line:272] - INFO: epoch 003:    558 / 2745 loss=1.654, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=1012, nsentences=144, sample_size=1012, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=412.6, ups=0.41, wpb=1012, bsz=144, num_updates=6040, lr=1.43577e-05, gnorm=1.113, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=14850
2023-08-01 20:29:07 - progress_bar.py[line:272] - INFO: epoch 003:    568 / 2745 loss=1.649, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=1016.6, nsentences=144, sample_size=1016.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=415.4, ups=0.41, wpb=1016.6, bsz=144, num_updates=6050, lr=1.43286e-05, gnorm=1.16, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=14874
2023-08-01 20:29:31 - progress_bar.py[line:272] - INFO: epoch 003:    578 / 2745 loss=1.653, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=1014.6, nsentences=144, sample_size=1014.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=414.7, ups=0.41, wpb=1014.6, bsz=144, num_updates=6060, lr=1.42996e-05, gnorm=1.152, clip=80, loss_scale=256, train_wall=24, gb_free=6.5, wall=14899
2023-08-01 20:29:56 - progress_bar.py[line:272] - INFO: epoch 003:    588 / 2745 loss=1.67, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=1015, nsentences=144, sample_size=1015, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=414.9, ups=0.41, wpb=1015, bsz=144, num_updates=6070, lr=1.42705e-05, gnorm=1.207, clip=90, loss_scale=256, train_wall=24, gb_free=6.5, wall=14923
2023-08-01 20:30:20 - progress_bar.py[line:272] - INFO: epoch 003:    598 / 2745 loss=1.644, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=1010.1, nsentences=144, sample_size=1010.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=411, ups=0.41, wpb=1010.1, bsz=144, num_updates=6080, lr=1.42414e-05, gnorm=1.049, clip=80, loss_scale=256, train_wall=25, gb_free=6.5, wall=14948
2023-08-01 20:30:45 - progress_bar.py[line:272] - INFO: epoch 003:    608 / 2745 loss=1.65, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=1017.5, nsentences=144, sample_size=1017.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=414.9, ups=0.41, wpb=1017.5, bsz=144, num_updates=6090, lr=1.42124e-05, gnorm=1.077, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=14972
2023-08-01 20:31:09 - progress_bar.py[line:272] - INFO: epoch 003:    618 / 2745 loss=1.661, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=1019.8, nsentences=144, sample_size=1019.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=416.3, ups=0.41, wpb=1019.8, bsz=144, num_updates=6100, lr=1.41833e-05, gnorm=1.069, clip=50, loss_scale=256, train_wall=24, gb_free=6.5, wall=14997
2023-08-01 20:31:34 - progress_bar.py[line:272] - INFO: epoch 003:    628 / 2745 loss=1.657, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=1019.1, nsentences=144, sample_size=1019.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=415.9, ups=0.41, wpb=1019.1, bsz=144, num_updates=6110, lr=1.41542e-05, gnorm=1.131, clip=80, loss_scale=256, train_wall=24, gb_free=6.5, wall=15021
2023-08-01 20:31:58 - progress_bar.py[line:272] - INFO: epoch 003:    638 / 2745 loss=1.647, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=1015.2, nsentences=144, sample_size=1015.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=413.3, ups=0.41, wpb=1015.2, bsz=144, num_updates=6120, lr=1.41252e-05, gnorm=1.089, clip=80, loss_scale=256, train_wall=25, gb_free=6.5, wall=15046
2023-08-01 20:32:23 - progress_bar.py[line:272] - INFO: epoch 003:    648 / 2745 loss=1.646, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=1019.6, nsentences=144, sample_size=1019.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=416.5, ups=0.41, wpb=1019.6, bsz=144, num_updates=6130, lr=1.40961e-05, gnorm=1.009, clip=50, loss_scale=256, train_wall=24, gb_free=6.5, wall=15070
2023-08-01 20:32:47 - progress_bar.py[line:272] - INFO: epoch 003:    658 / 2745 loss=1.645, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=1012.3, nsentences=144, sample_size=1012.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=413.1, ups=0.41, wpb=1012.3, bsz=144, num_updates=6140, lr=1.4067e-05, gnorm=1.097, clip=60, loss_scale=256, train_wall=24, gb_free=6.5, wall=15095
2023-08-01 20:33:12 - progress_bar.py[line:272] - INFO: epoch 003:    668 / 2745 loss=1.653, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=1018.4, nsentences=144, sample_size=1018.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=414.6, ups=0.41, wpb=1018.4, bsz=144, num_updates=6150, lr=1.4038e-05, gnorm=1.05, clip=50, loss_scale=256, train_wall=25, gb_free=6.5, wall=15120
2023-08-01 20:33:36 - progress_bar.py[line:272] - INFO: epoch 003:    678 / 2745 loss=1.664, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=1017, nsentences=144, sample_size=1017, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=414.7, ups=0.41, wpb=1017, bsz=144, num_updates=6160, lr=1.40089e-05, gnorm=1.128, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=15144
2023-08-01 20:34:01 - progress_bar.py[line:272] - INFO: epoch 003:    688 / 2745 loss=1.666, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=1021.5, nsentences=144, sample_size=1021.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=416.2, ups=0.41, wpb=1021.5, bsz=144, num_updates=6170, lr=1.39798e-05, gnorm=1.214, clip=90, loss_scale=256, train_wall=25, gb_free=6.5, wall=15169
2023-08-01 20:34:25 - progress_bar.py[line:272] - INFO: epoch 003:    698 / 2745 loss=1.656, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=1009.9, nsentences=144, sample_size=1009.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=411.5, ups=0.41, wpb=1009.9, bsz=144, num_updates=6180, lr=1.39508e-05, gnorm=1.116, clip=80, loss_scale=256, train_wall=25, gb_free=6.5, wall=15193
2023-08-01 20:34:50 - progress_bar.py[line:272] - INFO: epoch 003:    708 / 2745 loss=1.643, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=1008.5, nsentences=144, sample_size=1008.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=409.9, ups=0.41, wpb=1008.5, bsz=144, num_updates=6190, lr=1.39217e-05, gnorm=1.058, clip=70, loss_scale=256, train_wall=25, gb_free=6.5, wall=15218
2023-08-01 20:35:15 - progress_bar.py[line:272] - INFO: epoch 003:    718 / 2745 loss=1.652, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=1016.9, nsentences=144, sample_size=1016.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=416.1, ups=0.41, wpb=1016.9, bsz=144, num_updates=6200, lr=1.38927e-05, gnorm=1.061, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=15242
2023-08-01 20:35:39 - progress_bar.py[line:272] - INFO: epoch 003:    728 / 2745 loss=1.66, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=1017.7, nsentences=144, sample_size=1017.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=415.7, ups=0.41, wpb=1017.7, bsz=144, num_updates=6210, lr=1.38636e-05, gnorm=1.28, clip=100, loss_scale=256, train_wall=24, gb_free=6.5, wall=15267
2023-08-01 20:36:04 - progress_bar.py[line:272] - INFO: epoch 003:    738 / 2745 loss=1.655, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=1014.7, nsentences=144, sample_size=1014.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=413.8, ups=0.41, wpb=1014.7, bsz=144, num_updates=6220, lr=1.38345e-05, gnorm=1.08, clip=80, loss_scale=256, train_wall=24, gb_free=6.5, wall=15291
2023-08-01 20:36:28 - progress_bar.py[line:272] - INFO: epoch 003:    748 / 2745 loss=1.647, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=1016.4, nsentences=144, sample_size=1016.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=413.9, ups=0.41, wpb=1016.4, bsz=144, num_updates=6230, lr=1.38055e-05, gnorm=1.13, clip=80, loss_scale=256, train_wall=25, gb_free=6.5, wall=15316
2023-08-01 20:36:53 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-08-01 20:36:55 - progress_bar.py[line:272] - INFO: epoch 003:    759 / 2745 loss=1.654, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=1021.6, nsentences=144, sample_size=1021.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=378.6, ups=0.37, wpb=1021.6, bsz=144, num_updates=6240, lr=1.37764e-05, gnorm=1.084, clip=70, loss_scale=256, train_wall=27, gb_free=6.5, wall=15343
2023-08-01 20:37:20 - progress_bar.py[line:272] - INFO: epoch 003:    769 / 2745 loss=1.653, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=1015.6, nsentences=144, sample_size=1015.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=414.6, ups=0.41, wpb=1015.6, bsz=144, num_updates=6250, lr=1.37473e-05, gnorm=1.126, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=15367
2023-08-01 20:37:44 - progress_bar.py[line:272] - INFO: epoch 003:    779 / 2745 loss=1.65, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=1018.6, nsentences=144, sample_size=1018.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=414.2, ups=0.41, wpb=1018.6, bsz=144, num_updates=6260, lr=1.37183e-05, gnorm=1.022, clip=50, loss_scale=256, train_wall=25, gb_free=6.5, wall=15392
2023-08-01 20:38:09 - progress_bar.py[line:272] - INFO: epoch 003:    789 / 2745 loss=1.648, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=1013.1, nsentences=144, sample_size=1013.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=412, ups=0.41, wpb=1013.1, bsz=144, num_updates=6270, lr=1.36892e-05, gnorm=1.051, clip=60, loss_scale=256, train_wall=25, gb_free=6.5, wall=15416
2023-08-01 20:38:33 - progress_bar.py[line:272] - INFO: epoch 003:    799 / 2745 loss=1.657, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=1015.7, nsentences=144, sample_size=1015.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=413.8, ups=0.41, wpb=1015.7, bsz=144, num_updates=6280, lr=1.36601e-05, gnorm=1.065, clip=60, loss_scale=256, train_wall=25, gb_free=6.5, wall=15441
2023-08-01 20:38:58 - progress_bar.py[line:272] - INFO: epoch 003:    809 / 2745 loss=1.663, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=1013.8, nsentences=144, sample_size=1013.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=411.6, ups=0.41, wpb=1013.8, bsz=144, num_updates=6290, lr=1.36311e-05, gnorm=1.102, clip=80, loss_scale=256, train_wall=25, gb_free=6.5, wall=15466
2023-08-01 20:39:23 - progress_bar.py[line:272] - INFO: epoch 003:    819 / 2745 loss=1.651, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=1017.4, nsentences=144, sample_size=1017.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=414.3, ups=0.41, wpb=1017.4, bsz=144, num_updates=6300, lr=1.3602e-05, gnorm=1.095, clip=70, loss_scale=256, train_wall=25, gb_free=6.5, wall=15490
2023-08-01 20:39:47 - progress_bar.py[line:272] - INFO: epoch 003:    829 / 2745 loss=1.646, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=1011.1, nsentences=144, sample_size=1011.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=411.2, ups=0.41, wpb=1011.1, bsz=144, num_updates=6310, lr=1.3573e-05, gnorm=1.101, clip=70, loss_scale=256, train_wall=25, gb_free=6.5, wall=15515
2023-08-01 20:40:12 - progress_bar.py[line:272] - INFO: epoch 003:    839 / 2745 loss=1.654, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=1005.2, nsentences=144, sample_size=1005.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=408.3, ups=0.41, wpb=1005.2, bsz=144, num_updates=6320, lr=1.35439e-05, gnorm=1.17, clip=90, loss_scale=256, train_wall=25, gb_free=6.5, wall=15539
2023-08-01 20:40:36 - progress_bar.py[line:272] - INFO: epoch 003:    849 / 2745 loss=1.659, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=1012.2, nsentences=144, sample_size=1012.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=411.6, ups=0.41, wpb=1012.2, bsz=144, num_updates=6330, lr=1.35148e-05, gnorm=1.159, clip=100, loss_scale=256, train_wall=25, gb_free=6.5, wall=15564
2023-08-01 20:41:01 - progress_bar.py[line:272] - INFO: epoch 003:    859 / 2745 loss=1.659, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=1017.5, nsentences=144, sample_size=1017.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=415.5, ups=0.41, wpb=1017.5, bsz=144, num_updates=6340, lr=1.34858e-05, gnorm=1.175, clip=80, loss_scale=256, train_wall=24, gb_free=6.5, wall=15589
2023-08-01 20:41:25 - progress_bar.py[line:272] - INFO: epoch 003:    869 / 2745 loss=1.651, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=1019.4, nsentences=144, sample_size=1019.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=416.7, ups=0.41, wpb=1019.4, bsz=144, num_updates=6350, lr=1.34567e-05, gnorm=1.118, clip=90, loss_scale=256, train_wall=24, gb_free=6.5, wall=15613
2023-08-01 20:41:50 - progress_bar.py[line:272] - INFO: epoch 003:    879 / 2745 loss=1.655, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=1017.2, nsentences=144, sample_size=1017.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=415.1, ups=0.41, wpb=1017.2, bsz=144, num_updates=6360, lr=1.34276e-05, gnorm=1.12, clip=90, loss_scale=256, train_wall=24, gb_free=6.5, wall=15637
2023-08-01 20:42:14 - progress_bar.py[line:272] - INFO: epoch 003:    889 / 2745 loss=1.651, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=1017.2, nsentences=144, sample_size=1017.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=415.2, ups=0.41, wpb=1017.2, bsz=144, num_updates=6370, lr=1.33986e-05, gnorm=1.184, clip=80, loss_scale=256, train_wall=24, gb_free=6.5, wall=15662
2023-08-01 20:42:39 - progress_bar.py[line:272] - INFO: epoch 003:    899 / 2745 loss=1.654, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=1007.5, nsentences=144, sample_size=1007.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=410.8, ups=0.41, wpb=1007.5, bsz=144, num_updates=6380, lr=1.33695e-05, gnorm=1.3, clip=80, loss_scale=256, train_wall=24, gb_free=6.5, wall=15687
2023-08-01 20:43:03 - progress_bar.py[line:272] - INFO: epoch 003:    909 / 2745 loss=1.649, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=1018.4, nsentences=144, sample_size=1018.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=416.1, ups=0.41, wpb=1018.4, bsz=144, num_updates=6390, lr=1.33404e-05, gnorm=1.146, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=15711
2023-08-01 20:43:28 - progress_bar.py[line:272] - INFO: epoch 003:    919 / 2745 loss=1.657, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=1014.6, nsentences=144, sample_size=1014.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=414.2, ups=0.41, wpb=1014.6, bsz=144, num_updates=6400, lr=1.33114e-05, gnorm=1.168, clip=90, loss_scale=256, train_wall=24, gb_free=6.5, wall=15735
2023-08-01 20:43:52 - progress_bar.py[line:272] - INFO: epoch 003:    929 / 2745 loss=1.634, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=1010.9, nsentences=144, sample_size=1010.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=412.5, ups=0.41, wpb=1010.9, bsz=144, num_updates=6410, lr=1.32823e-05, gnorm=1.141, clip=90, loss_scale=256, train_wall=24, gb_free=6.5, wall=15760
2023-08-01 20:44:17 - progress_bar.py[line:272] - INFO: epoch 003:    939 / 2745 loss=1.642, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=1012, nsentences=144, sample_size=1012, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=411.7, ups=0.41, wpb=1012, bsz=144, num_updates=6420, lr=1.32532e-05, gnorm=1.176, clip=90, loss_scale=256, train_wall=25, gb_free=6.5, wall=15785
2023-08-01 20:44:41 - progress_bar.py[line:272] - INFO: epoch 003:    949 / 2745 loss=1.649, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=1016.4, nsentences=144, sample_size=1016.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=414.6, ups=0.41, wpb=1016.4, bsz=144, num_updates=6430, lr=1.32242e-05, gnorm=1.118, clip=80, loss_scale=256, train_wall=24, gb_free=6.5, wall=15809
2023-08-01 20:45:06 - progress_bar.py[line:272] - INFO: epoch 003:    959 / 2745 loss=1.646, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=1015.3, nsentences=144, sample_size=1015.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=414, ups=0.41, wpb=1015.3, bsz=144, num_updates=6440, lr=1.31951e-05, gnorm=1.217, clip=90, loss_scale=256, train_wall=24, gb_free=6.5, wall=15834
2023-08-01 20:45:31 - progress_bar.py[line:272] - INFO: epoch 003:    969 / 2745 loss=1.657, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=1013.8, nsentences=144, sample_size=1013.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=412.6, ups=0.41, wpb=1013.8, bsz=144, num_updates=6450, lr=1.31661e-05, gnorm=1.065, clip=50, loss_scale=256, train_wall=25, gb_free=6.5, wall=15858
2023-08-01 20:45:55 - progress_bar.py[line:272] - INFO: epoch 003:    979 / 2745 loss=1.642, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=1018.2, nsentences=144, sample_size=1018.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=416, ups=0.41, wpb=1018.2, bsz=144, num_updates=6460, lr=1.3137e-05, gnorm=1.138, clip=90, loss_scale=256, train_wall=24, gb_free=6.5, wall=15883
2023-08-01 20:46:20 - progress_bar.py[line:272] - INFO: epoch 003:    989 / 2745 loss=1.653, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=1013.9, nsentences=144, sample_size=1013.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=413.3, ups=0.41, wpb=1013.9, bsz=144, num_updates=6470, lr=1.31079e-05, gnorm=1.11, clip=70, loss_scale=256, train_wall=25, gb_free=6.5, wall=15907
2023-08-01 20:46:44 - progress_bar.py[line:272] - INFO: epoch 003:    999 / 2745 loss=1.664, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=1012.1, nsentences=144, sample_size=1012.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=411, ups=0.41, wpb=1012.1, bsz=144, num_updates=6480, lr=1.30789e-05, gnorm=1.18, clip=100, loss_scale=256, train_wall=25, gb_free=6.5, wall=15932
2023-08-01 20:47:09 - progress_bar.py[line:272] - INFO: epoch 003:   1009 / 2745 loss=1.644, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=1017, nsentences=144, sample_size=1017, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=415.1, ups=0.41, wpb=1017, bsz=144, num_updates=6490, lr=1.30498e-05, gnorm=1.077, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=15956
2023-08-01 20:47:33 - progress_bar.py[line:272] - INFO: epoch 003:   1019 / 2745 loss=1.643, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=1020.8, nsentences=144, sample_size=1020.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=417.9, ups=0.41, wpb=1020.8, bsz=144, num_updates=6500, lr=1.30207e-05, gnorm=1.07, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=15981
2023-08-01 20:47:58 - progress_bar.py[line:272] - INFO: epoch 003:   1029 / 2745 loss=1.653, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=1015.4, nsentences=144, sample_size=1015.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=414.5, ups=0.41, wpb=1015.4, bsz=144, num_updates=6510, lr=1.29917e-05, gnorm=1.128, clip=50, loss_scale=256, train_wall=24, gb_free=6.5, wall=16005
2023-08-01 20:48:22 - progress_bar.py[line:272] - INFO: epoch 003:   1039 / 2745 loss=1.642, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=1017.2, nsentences=144, sample_size=1017.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=414.7, ups=0.41, wpb=1017.2, bsz=144, num_updates=6520, lr=1.29626e-05, gnorm=1.057, clip=60, loss_scale=256, train_wall=24, gb_free=6.5, wall=16030
2023-08-01 20:48:47 - progress_bar.py[line:272] - INFO: epoch 003:   1049 / 2745 loss=1.661, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=1016.1, nsentences=144, sample_size=1016.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=415.7, ups=0.41, wpb=1016.1, bsz=144, num_updates=6530, lr=1.29335e-05, gnorm=1.231, clip=90, loss_scale=256, train_wall=24, gb_free=6.5, wall=16054
2023-08-01 20:49:11 - progress_bar.py[line:272] - INFO: epoch 003:   1059 / 2745 loss=1.65, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=1017.5, nsentences=144, sample_size=1017.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=416, ups=0.41, wpb=1017.5, bsz=144, num_updates=6540, lr=1.29045e-05, gnorm=1.159, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=16079
2023-08-01 20:49:36 - progress_bar.py[line:272] - INFO: epoch 003:   1069 / 2745 loss=1.648, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=1014.4, nsentences=144, sample_size=1014.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=413.5, ups=0.41, wpb=1014.4, bsz=144, num_updates=6550, lr=1.28754e-05, gnorm=1.082, clip=80, loss_scale=256, train_wall=25, gb_free=6.5, wall=16103
2023-08-01 20:50:00 - progress_bar.py[line:272] - INFO: epoch 003:   1079 / 2745 loss=1.652, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=1014.7, nsentences=144, sample_size=1014.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=414.2, ups=0.41, wpb=1014.7, bsz=144, num_updates=6560, lr=1.28463e-05, gnorm=1.073, clip=60, loss_scale=256, train_wall=24, gb_free=6.5, wall=16128
2023-08-01 20:50:25 - progress_bar.py[line:272] - INFO: epoch 003:   1089 / 2745 loss=1.654, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=1014.9, nsentences=144, sample_size=1014.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=413.6, ups=0.41, wpb=1014.9, bsz=144, num_updates=6570, lr=1.28173e-05, gnorm=1.176, clip=100, loss_scale=256, train_wall=25, gb_free=6.5, wall=16152
2023-08-01 20:50:49 - progress_bar.py[line:272] - INFO: epoch 003:   1099 / 2745 loss=1.643, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=1012.6, nsentences=144, sample_size=1012.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=411, ups=0.41, wpb=1012.6, bsz=144, num_updates=6580, lr=1.27882e-05, gnorm=1.126, clip=80, loss_scale=256, train_wall=25, gb_free=6.5, wall=16177
2023-08-01 20:51:14 - progress_bar.py[line:272] - INFO: epoch 003:   1109 / 2745 loss=1.649, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=1017, nsentences=144, sample_size=1017, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=415.5, ups=0.41, wpb=1017, bsz=144, num_updates=6590, lr=1.27592e-05, gnorm=1.148, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=16201
2023-08-01 20:51:38 - progress_bar.py[line:272] - INFO: epoch 003:   1119 / 2745 loss=1.646, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=1009.1, nsentences=144, sample_size=1009.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=409.2, ups=0.41, wpb=1009.1, bsz=144, num_updates=6600, lr=1.27301e-05, gnorm=1.114, clip=70, loss_scale=256, train_wall=25, gb_free=6.5, wall=16226
2023-08-01 20:52:03 - progress_bar.py[line:272] - INFO: epoch 003:   1129 / 2745 loss=1.647, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=1007.5, nsentences=144, sample_size=1007.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=409.8, ups=0.41, wpb=1007.5, bsz=144, num_updates=6610, lr=1.2701e-05, gnorm=1.097, clip=80, loss_scale=256, train_wall=25, gb_free=6.5, wall=16251
2023-08-01 20:52:28 - progress_bar.py[line:272] - INFO: epoch 003:   1139 / 2745 loss=1.641, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=1011, nsentences=144, sample_size=1011, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=411.2, ups=0.41, wpb=1011, bsz=144, num_updates=6620, lr=1.2672e-05, gnorm=1.045, clip=60, loss_scale=256, train_wall=25, gb_free=6.5, wall=16275
2023-08-01 20:52:52 - progress_bar.py[line:272] - INFO: epoch 003:   1149 / 2745 loss=1.649, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=1015.2, nsentences=144, sample_size=1015.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=413.3, ups=0.41, wpb=1015.2, bsz=144, num_updates=6630, lr=1.26429e-05, gnorm=1.134, clip=90, loss_scale=256, train_wall=25, gb_free=6.5, wall=16300
2023-08-01 20:53:17 - progress_bar.py[line:272] - INFO: epoch 003:   1159 / 2745 loss=1.635, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=1017.5, nsentences=144, sample_size=1017.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=414.5, ups=0.41, wpb=1017.5, bsz=144, num_updates=6640, lr=1.26138e-05, gnorm=1.086, clip=70, loss_scale=256, train_wall=25, gb_free=6.5, wall=16324
2023-08-01 20:53:41 - progress_bar.py[line:272] - INFO: epoch 003:   1169 / 2745 loss=1.642, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=1010.4, nsentences=144, sample_size=1010.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=410.8, ups=0.41, wpb=1010.4, bsz=144, num_updates=6650, lr=1.25848e-05, gnorm=1.094, clip=80, loss_scale=256, train_wall=25, gb_free=6.5, wall=16349
2023-08-01 20:54:06 - progress_bar.py[line:272] - INFO: epoch 003:   1179 / 2745 loss=1.66, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=1023.6, nsentences=144, sample_size=1023.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=418.8, ups=0.41, wpb=1023.6, bsz=144, num_updates=6660, lr=1.25557e-05, gnorm=1.175, clip=100, loss_scale=256, train_wall=24, gb_free=6.5, wall=16373
2023-08-01 20:54:30 - progress_bar.py[line:272] - INFO: epoch 003:   1189 / 2745 loss=1.656, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=1018.4, nsentences=144, sample_size=1018.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=416.4, ups=0.41, wpb=1018.4, bsz=144, num_updates=6670, lr=1.25266e-05, gnorm=1.171, clip=100, loss_scale=256, train_wall=24, gb_free=6.5, wall=16398
2023-08-01 20:54:55 - progress_bar.py[line:272] - INFO: epoch 003:   1199 / 2745 loss=1.64, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=1011.6, nsentences=144, sample_size=1011.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=411.9, ups=0.41, wpb=1011.6, bsz=144, num_updates=6680, lr=1.24976e-05, gnorm=1.018, clip=50, loss_scale=256, train_wall=25, gb_free=6.5, wall=16422
2023-08-01 20:55:19 - progress_bar.py[line:272] - INFO: epoch 003:   1209 / 2745 loss=1.647, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=1012.8, nsentences=144, sample_size=1012.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=412.2, ups=0.41, wpb=1012.8, bsz=144, num_updates=6690, lr=1.24685e-05, gnorm=1.089, clip=80, loss_scale=256, train_wall=25, gb_free=6.5, wall=16447
2023-08-01 20:55:44 - progress_bar.py[line:272] - INFO: epoch 003:   1219 / 2745 loss=1.649, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=1019.6, nsentences=144, sample_size=1019.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=416.4, ups=0.41, wpb=1019.6, bsz=144, num_updates=6700, lr=1.24394e-05, gnorm=1.154, clip=60, loss_scale=256, train_wall=24, gb_free=6.5, wall=16472
2023-08-01 20:56:08 - progress_bar.py[line:272] - INFO: epoch 003:   1229 / 2745 loss=1.644, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=1013.4, nsentences=144, sample_size=1013.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=413.4, ups=0.41, wpb=1013.4, bsz=144, num_updates=6710, lr=1.24104e-05, gnorm=1.16, clip=90, loss_scale=256, train_wall=24, gb_free=6.5, wall=16496
2023-08-01 20:56:33 - progress_bar.py[line:272] - INFO: epoch 003:   1239 / 2745 loss=1.64, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=1013.5, nsentences=144, sample_size=1013.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=413.4, ups=0.41, wpb=1013.5, bsz=144, num_updates=6720, lr=1.23813e-05, gnorm=1.119, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=16521
2023-08-01 20:56:57 - progress_bar.py[line:272] - INFO: epoch 003:   1249 / 2745 loss=1.648, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=1015.3, nsentences=144, sample_size=1015.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=413, ups=0.41, wpb=1015.3, bsz=144, num_updates=6730, lr=1.23523e-05, gnorm=1.079, clip=60, loss_scale=256, train_wall=25, gb_free=6.5, wall=16545
2023-08-01 20:57:22 - progress_bar.py[line:272] - INFO: epoch 003:   1259 / 2745 loss=1.65, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=1012.9, nsentences=144, sample_size=1012.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=412.5, ups=0.41, wpb=1012.9, bsz=144, num_updates=6740, lr=1.23232e-05, gnorm=1.1, clip=100, loss_scale=256, train_wall=25, gb_free=6.5, wall=16570
2023-08-01 20:57:47 - progress_bar.py[line:272] - INFO: epoch 003:   1269 / 2745 loss=1.655, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=1017, nsentences=144, sample_size=1017, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=414.5, ups=0.41, wpb=1017, bsz=144, num_updates=6750, lr=1.22941e-05, gnorm=1.199, clip=90, loss_scale=256, train_wall=25, gb_free=6.5, wall=16594
2023-08-01 20:57:51 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-08-01 20:58:14 - progress_bar.py[line:272] - INFO: epoch 003:   1280 / 2745 loss=1.654, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=1014.3, nsentences=144, sample_size=1014.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=374.7, ups=0.37, wpb=1014.3, bsz=144, num_updates=6760, lr=1.22651e-05, gnorm=1.214, clip=100, loss_scale=256, train_wall=27, gb_free=6.5, wall=16621
2023-08-01 20:58:38 - progress_bar.py[line:272] - INFO: epoch 003:   1290 / 2745 loss=1.646, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=1017.3, nsentences=144, sample_size=1017.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=412.9, ups=0.41, wpb=1017.3, bsz=144, num_updates=6770, lr=1.2236e-05, gnorm=1.022, clip=50, loss_scale=256, train_wall=25, gb_free=6.5, wall=16646
2023-08-01 20:59:03 - progress_bar.py[line:272] - INFO: epoch 003:   1300 / 2745 loss=1.652, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=1007.7, nsentences=144, sample_size=1007.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=408.4, ups=0.41, wpb=1007.7, bsz=144, num_updates=6780, lr=1.22069e-05, gnorm=1.142, clip=90, loss_scale=256, train_wall=25, gb_free=6.5, wall=16671
2023-08-01 20:59:27 - progress_bar.py[line:272] - INFO: epoch 003:   1310 / 2745 loss=1.65, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=1013.7, nsentences=144, sample_size=1013.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=413.4, ups=0.41, wpb=1013.7, bsz=144, num_updates=6790, lr=1.21779e-05, gnorm=1.11, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=16695
2023-08-01 20:59:52 - progress_bar.py[line:272] - INFO: epoch 003:   1320 / 2745 loss=1.639, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=1016.4, nsentences=144, sample_size=1016.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=414.4, ups=0.41, wpb=1016.4, bsz=144, num_updates=6800, lr=1.21488e-05, gnorm=1.142, clip=90, loss_scale=256, train_wall=24, gb_free=6.5, wall=16720
2023-08-01 21:00:17 - progress_bar.py[line:272] - INFO: epoch 003:   1330 / 2745 loss=1.645, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=1016.4, nsentences=144, sample_size=1016.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=413.7, ups=0.41, wpb=1016.4, bsz=144, num_updates=6810, lr=1.21197e-05, gnorm=1.092, clip=70, loss_scale=256, train_wall=25, gb_free=6.5, wall=16744
2023-08-01 21:00:41 - progress_bar.py[line:272] - INFO: epoch 003:   1340 / 2745 loss=1.644, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=1009, nsentences=144, sample_size=1009, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=409, ups=0.41, wpb=1009, bsz=144, num_updates=6820, lr=1.20907e-05, gnorm=1.032, clip=70, loss_scale=256, train_wall=25, gb_free=6.5, wall=16769
2023-08-01 21:01:06 - progress_bar.py[line:272] - INFO: epoch 003:   1350 / 2745 loss=1.649, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=1011, nsentences=144, sample_size=1011, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=412.7, ups=0.41, wpb=1011, bsz=144, num_updates=6830, lr=1.20616e-05, gnorm=1.13, clip=100, loss_scale=256, train_wall=24, gb_free=6.5, wall=16793
2023-08-01 21:01:30 - progress_bar.py[line:272] - INFO: epoch 003:   1360 / 2745 loss=1.64, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=1016.8, nsentences=144, sample_size=1016.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=415.5, ups=0.41, wpb=1016.8, bsz=144, num_updates=6840, lr=1.20326e-05, gnorm=1.099, clip=80, loss_scale=256, train_wall=24, gb_free=6.5, wall=16818
2023-08-01 21:01:55 - progress_bar.py[line:272] - INFO: epoch 003:   1370 / 2745 loss=1.646, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=1008.9, nsentences=144, sample_size=1008.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=411.1, ups=0.41, wpb=1008.9, bsz=144, num_updates=6850, lr=1.20035e-05, gnorm=1.142, clip=100, loss_scale=256, train_wall=25, gb_free=6.5, wall=16842
2023-08-01 21:02:19 - progress_bar.py[line:272] - INFO: epoch 003:   1380 / 2745 loss=1.646, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=1016.5, nsentences=144, sample_size=1016.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=414.9, ups=0.41, wpb=1016.5, bsz=144, num_updates=6860, lr=1.19744e-05, gnorm=1.042, clip=60, loss_scale=256, train_wall=24, gb_free=6.5, wall=16867
2023-08-01 21:02:44 - progress_bar.py[line:272] - INFO: epoch 003:   1390 / 2745 loss=1.652, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=1018.6, nsentences=144, sample_size=1018.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=417.4, ups=0.41, wpb=1018.6, bsz=144, num_updates=6870, lr=1.19454e-05, gnorm=1.24, clip=90, loss_scale=256, train_wall=24, gb_free=6.5, wall=16891
2023-08-01 21:03:08 - progress_bar.py[line:272] - INFO: epoch 003:   1400 / 2745 loss=1.655, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=1018.6, nsentences=144, sample_size=1018.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=416.1, ups=0.41, wpb=1018.6, bsz=144, num_updates=6880, lr=1.19163e-05, gnorm=1.069, clip=80, loss_scale=256, train_wall=24, gb_free=6.5, wall=16916
2023-08-01 21:03:33 - progress_bar.py[line:272] - INFO: epoch 003:   1410 / 2745 loss=1.651, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=1015.5, nsentences=144, sample_size=1015.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=414.5, ups=0.41, wpb=1015.5, bsz=144, num_updates=6890, lr=1.18872e-05, gnorm=1.081, clip=50, loss_scale=256, train_wall=24, gb_free=6.5, wall=16940
2023-08-01 21:03:57 - progress_bar.py[line:272] - INFO: epoch 003:   1420 / 2745 loss=1.653, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=1022, nsentences=144, sample_size=1022, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=418.4, ups=0.41, wpb=1022, bsz=144, num_updates=6900, lr=1.18582e-05, gnorm=1.053, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=16965
2023-08-01 21:04:22 - progress_bar.py[line:272] - INFO: epoch 003:   1430 / 2745 loss=1.647, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=1009.8, nsentences=144, sample_size=1009.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=410.4, ups=0.41, wpb=1009.8, bsz=144, num_updates=6910, lr=1.18291e-05, gnorm=1.13, clip=90, loss_scale=256, train_wall=25, gb_free=6.5, wall=16989
2023-08-01 21:04:46 - progress_bar.py[line:272] - INFO: epoch 003:   1440 / 2745 loss=1.641, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=1017.3, nsentences=144, sample_size=1017.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=414.1, ups=0.41, wpb=1017.3, bsz=144, num_updates=6920, lr=1.18e-05, gnorm=1.093, clip=70, loss_scale=256, train_wall=25, gb_free=6.5, wall=17014
2023-08-01 21:05:11 - progress_bar.py[line:272] - INFO: epoch 003:   1450 / 2745 loss=1.648, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=1016.7, nsentences=144, sample_size=1016.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=414.3, ups=0.41, wpb=1016.7, bsz=144, num_updates=6930, lr=1.1771e-05, gnorm=1.196, clip=70, loss_scale=256, train_wall=25, gb_free=6.5, wall=17038
2023-08-01 21:05:35 - progress_bar.py[line:272] - INFO: epoch 003:   1460 / 2745 loss=1.651, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=1010.1, nsentences=144, sample_size=1010.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=411, ups=0.41, wpb=1010.1, bsz=144, num_updates=6940, lr=1.17419e-05, gnorm=1.064, clip=60, loss_scale=256, train_wall=25, gb_free=6.5, wall=17063
2023-08-01 21:06:00 - progress_bar.py[line:272] - INFO: epoch 003:   1470 / 2745 loss=1.652, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=1020.3, nsentences=144, sample_size=1020.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=414.4, ups=0.41, wpb=1020.3, bsz=144, num_updates=6950, lr=1.17128e-05, gnorm=1.145, clip=100, loss_scale=256, train_wall=25, gb_free=6.5, wall=17088
2023-08-01 21:06:24 - progress_bar.py[line:272] - INFO: epoch 003:   1480 / 2745 loss=1.646, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=1016, nsentences=144, sample_size=1016, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=415.2, ups=0.41, wpb=1016, bsz=144, num_updates=6960, lr=1.16838e-05, gnorm=1.138, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=17112
2023-08-01 21:06:49 - progress_bar.py[line:272] - INFO: epoch 003:   1490 / 2745 loss=1.648, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=1015.3, nsentences=144, sample_size=1015.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=414.5, ups=0.41, wpb=1015.3, bsz=144, num_updates=6970, lr=1.16547e-05, gnorm=1.116, clip=70, loss_scale=256, train_wall=24, gb_free=6.5, wall=17137
2023-08-01 21:07:14 - progress_bar.py[line:272] - INFO: epoch 003:   1500 / 2745 loss=1.645, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=1006.9, nsentences=144, sample_size=1006.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=408.8, ups=0.41, wpb=1006.9, bsz=144, num_updates=6980, lr=1.16257e-05, gnorm=1.082, clip=60, loss_scale=256, train_wall=25, gb_free=6.5, wall=17161
2023-08-01 21:07:38 - progress_bar.py[line:272] - INFO: epoch 003:   1510 / 2745 loss=1.651, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=1010.4, nsentences=144, sample_size=1010.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=410.4, ups=0.41, wpb=1010.4, bsz=144, num_updates=6990, lr=1.15966e-05, gnorm=1.066, clip=70, loss_scale=256, train_wall=25, gb_free=6.5, wall=17186
2023-08-01 21:08:03 - progress_bar.py[line:272] - INFO: epoch 003:   1520 / 2745 loss=1.656, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=1020, nsentences=144, sample_size=1020, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=416.2, ups=0.41, wpb=1020, bsz=144, num_updates=7000, lr=1.15675e-05, gnorm=1.126, clip=90, loss_scale=256, train_wall=24, gb_free=6.5, wall=17210
2023-08-01 21:08:22 - trainer.py[line:921] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-08-01 21:08:30 - progress_bar.py[line:272] - INFO: epoch 003:   1531 / 2745 loss=1.646, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=1017.6, nsentences=144, sample_size=1017.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=377.4, ups=0.37, wpb=1017.6, bsz=144, num_updates=7010, lr=1.15385e-05, gnorm=1.079, clip=80, loss_scale=128, train_wall=27, gb_free=6.5, wall=17237
2023-08-01 21:08:54 - progress_bar.py[line:272] - INFO: epoch 003:   1541 / 2745 loss=1.642, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=1011.4, nsentences=144, sample_size=1011.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=411.7, ups=0.41, wpb=1011.4, bsz=144, num_updates=7020, lr=1.15094e-05, gnorm=1.06, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=17262
2023-08-01 21:09:19 - progress_bar.py[line:272] - INFO: epoch 003:   1551 / 2745 loss=1.643, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=1009.7, nsentences=144, sample_size=1009.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=411.3, ups=0.41, wpb=1009.7, bsz=144, num_updates=7030, lr=1.14803e-05, gnorm=1.088, clip=70, loss_scale=128, train_wall=25, gb_free=6.5, wall=17287
2023-08-01 21:09:43 - progress_bar.py[line:272] - INFO: epoch 003:   1561 / 2745 loss=1.641, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=1012.8, nsentences=144, sample_size=1012.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=412.4, ups=0.41, wpb=1012.8, bsz=144, num_updates=7040, lr=1.14513e-05, gnorm=1.088, clip=60, loss_scale=128, train_wall=25, gb_free=6.5, wall=17311
2023-08-01 21:10:08 - progress_bar.py[line:272] - INFO: epoch 003:   1571 / 2745 loss=1.647, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=1016.1, nsentences=144, sample_size=1016.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=414.1, ups=0.41, wpb=1016.1, bsz=144, num_updates=7050, lr=1.14222e-05, gnorm=1.087, clip=60, loss_scale=128, train_wall=25, gb_free=6.5, wall=17336
2023-08-01 21:10:32 - progress_bar.py[line:272] - INFO: epoch 003:   1581 / 2745 loss=1.642, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=1010.4, nsentences=144, sample_size=1010.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=412.3, ups=0.41, wpb=1010.4, bsz=144, num_updates=7060, lr=1.13931e-05, gnorm=1.108, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=17360
2023-08-01 21:10:57 - progress_bar.py[line:272] - INFO: epoch 003:   1591 / 2745 loss=1.639, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=1016.8, nsentences=144, sample_size=1016.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=415.2, ups=0.41, wpb=1016.8, bsz=144, num_updates=7070, lr=1.13641e-05, gnorm=1.074, clip=50, loss_scale=128, train_wall=24, gb_free=6.5, wall=17385
2023-08-01 21:11:21 - progress_bar.py[line:272] - INFO: epoch 003:   1601 / 2745 loss=1.641, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=1016.6, nsentences=144, sample_size=1016.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=414.2, ups=0.41, wpb=1016.6, bsz=144, num_updates=7080, lr=1.1335e-05, gnorm=1.074, clip=60, loss_scale=128, train_wall=25, gb_free=6.5, wall=17409
2023-08-01 21:11:46 - progress_bar.py[line:272] - INFO: epoch 003:   1611 / 2745 loss=1.642, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=1019.1, nsentences=144, sample_size=1019.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=417, ups=0.41, wpb=1019.1, bsz=144, num_updates=7090, lr=1.13059e-05, gnorm=1.184, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=17434
2023-08-01 21:12:11 - progress_bar.py[line:272] - INFO: epoch 003:   1621 / 2745 loss=1.642, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=1010.1, nsentences=144, sample_size=1010.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=410.7, ups=0.41, wpb=1010.1, bsz=144, num_updates=7100, lr=1.12769e-05, gnorm=1.148, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=17458
2023-08-01 21:12:35 - progress_bar.py[line:272] - INFO: epoch 003:   1631 / 2745 loss=1.646, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=1017, nsentences=144, sample_size=1017, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=415.3, ups=0.41, wpb=1017, bsz=144, num_updates=7110, lr=1.12478e-05, gnorm=1.118, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=17483
2023-08-01 21:13:00 - progress_bar.py[line:272] - INFO: epoch 003:   1641 / 2745 loss=1.65, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=1020.3, nsentences=144, sample_size=1020.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=416.6, ups=0.41, wpb=1020.3, bsz=144, num_updates=7120, lr=1.12188e-05, gnorm=1.154, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=17507
2023-08-01 21:13:24 - progress_bar.py[line:272] - INFO: epoch 003:   1651 / 2745 loss=1.637, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=1017.1, nsentences=144, sample_size=1017.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=415.3, ups=0.41, wpb=1017.1, bsz=144, num_updates=7130, lr=1.11897e-05, gnorm=1.101, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=17532
2023-08-01 21:13:49 - progress_bar.py[line:272] - INFO: epoch 003:   1661 / 2745 loss=1.642, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=1021.6, nsentences=144, sample_size=1021.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=416.4, ups=0.41, wpb=1021.6, bsz=144, num_updates=7140, lr=1.11606e-05, gnorm=1.087, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=17556
2023-08-01 21:14:13 - progress_bar.py[line:272] - INFO: epoch 003:   1671 / 2745 loss=1.646, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=1012.3, nsentences=144, sample_size=1012.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=412.5, ups=0.41, wpb=1012.3, bsz=144, num_updates=7150, lr=1.11316e-05, gnorm=1.089, clip=70, loss_scale=128, train_wall=25, gb_free=6.5, wall=17581
2023-08-01 21:14:38 - progress_bar.py[line:272] - INFO: epoch 003:   1681 / 2745 loss=1.639, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=1013.1, nsentences=144, sample_size=1013.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=411.8, ups=0.41, wpb=1013.1, bsz=144, num_updates=7160, lr=1.11025e-05, gnorm=1.049, clip=60, loss_scale=128, train_wall=25, gb_free=6.5, wall=17605
2023-08-01 21:15:02 - progress_bar.py[line:272] - INFO: epoch 003:   1691 / 2745 loss=1.643, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=1017.2, nsentences=144, sample_size=1017.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=413.5, ups=0.41, wpb=1017.2, bsz=144, num_updates=7170, lr=1.10734e-05, gnorm=1.171, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=17630
2023-08-01 21:15:27 - progress_bar.py[line:272] - INFO: epoch 003:   1701 / 2745 loss=1.643, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=1014.8, nsentences=144, sample_size=1014.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=413.1, ups=0.41, wpb=1014.8, bsz=144, num_updates=7180, lr=1.10444e-05, gnorm=1.106, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=17655
2023-08-01 21:15:52 - progress_bar.py[line:272] - INFO: epoch 003:   1711 / 2745 loss=1.637, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=1012.8, nsentences=144, sample_size=1012.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=411.2, ups=0.41, wpb=1012.8, bsz=144, num_updates=7190, lr=1.10153e-05, gnorm=1.049, clip=70, loss_scale=128, train_wall=25, gb_free=6.5, wall=17679
2023-08-01 21:16:16 - progress_bar.py[line:272] - INFO: epoch 003:   1721 / 2745 loss=1.646, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=1012.7, nsentences=144, sample_size=1012.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=412.8, ups=0.41, wpb=1012.7, bsz=144, num_updates=7200, lr=1.09862e-05, gnorm=1.102, clip=100, loss_scale=128, train_wall=25, gb_free=6.5, wall=17704
2023-08-01 21:16:41 - progress_bar.py[line:272] - INFO: epoch 003:   1731 / 2745 loss=1.65, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=1014, nsentences=144, sample_size=1014, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=412.6, ups=0.41, wpb=1014, bsz=144, num_updates=7210, lr=1.09572e-05, gnorm=1.225, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=17728
2023-08-01 21:17:05 - progress_bar.py[line:272] - INFO: epoch 003:   1741 / 2745 loss=1.642, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=1019.6, nsentences=144, sample_size=1019.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=416.9, ups=0.41, wpb=1019.6, bsz=144, num_updates=7220, lr=1.09281e-05, gnorm=1.074, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=17753
2023-08-01 21:17:30 - progress_bar.py[line:272] - INFO: epoch 003:   1751 / 2745 loss=1.635, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=1015.5, nsentences=144, sample_size=1015.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=411.9, ups=0.41, wpb=1015.5, bsz=144, num_updates=7230, lr=1.08991e-05, gnorm=1.101, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=17777
2023-08-01 21:17:55 - progress_bar.py[line:272] - INFO: epoch 003:   1761 / 2745 loss=1.645, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=1010.4, nsentences=144, sample_size=1010.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=407.8, ups=0.4, wpb=1010.4, bsz=144, num_updates=7240, lr=1.087e-05, gnorm=1.203, clip=100, loss_scale=128, train_wall=25, gb_free=6.5, wall=17802
2023-08-01 21:18:19 - progress_bar.py[line:272] - INFO: epoch 003:   1771 / 2745 loss=1.644, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=989.1, nsentences=140.7, sample_size=989.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=412.3, ups=0.42, wpb=989.1, bsz=140.7, num_updates=7250, lr=1.08409e-05, gnorm=1.16, clip=100, loss_scale=128, train_wall=24, gb_free=6.5, wall=17826
2023-08-01 21:18:43 - progress_bar.py[line:272] - INFO: epoch 003:   1781 / 2745 loss=1.641, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=1022, nsentences=144, sample_size=1022, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=418.4, ups=0.41, wpb=1022, bsz=144, num_updates=7260, lr=1.08119e-05, gnorm=1.101, clip=60, loss_scale=128, train_wall=24, gb_free=6.5, wall=17851
2023-08-01 21:19:07 - progress_bar.py[line:272] - INFO: epoch 003:   1791 / 2745 loss=1.637, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=1013.4, nsentences=144, sample_size=1013.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=414.6, ups=0.41, wpb=1013.4, bsz=144, num_updates=7270, lr=1.07828e-05, gnorm=1.129, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=17875
2023-08-01 21:19:32 - progress_bar.py[line:272] - INFO: epoch 003:   1801 / 2745 loss=1.644, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=1015.1, nsentences=144, sample_size=1015.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=415, ups=0.41, wpb=1015.1, bsz=144, num_updates=7280, lr=1.07537e-05, gnorm=1.165, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=17900
2023-08-01 21:19:56 - progress_bar.py[line:272] - INFO: epoch 003:   1811 / 2745 loss=1.644, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=1007.5, nsentences=144, sample_size=1007.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=409.5, ups=0.41, wpb=1007.5, bsz=144, num_updates=7290, lr=1.07247e-05, gnorm=1.177, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=17924
2023-08-01 21:20:21 - progress_bar.py[line:272] - INFO: epoch 003:   1821 / 2745 loss=1.649, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=1016.5, nsentences=144, sample_size=1016.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=414, ups=0.41, wpb=1016.5, bsz=144, num_updates=7300, lr=1.06956e-05, gnorm=1.228, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=17949
2023-08-01 21:20:45 - progress_bar.py[line:272] - INFO: epoch 003:   1831 / 2745 loss=1.644, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=1020.5, nsentences=144, sample_size=1020.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=418, ups=0.41, wpb=1020.5, bsz=144, num_updates=7310, lr=1.06665e-05, gnorm=1.086, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=17973
2023-08-01 21:21:10 - progress_bar.py[line:272] - INFO: epoch 003:   1841 / 2745 loss=1.639, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=1022.1, nsentences=144, sample_size=1022.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=417.7, ups=0.41, wpb=1022.1, bsz=144, num_updates=7320, lr=1.06375e-05, gnorm=1.184, clip=90, loss_scale=128, train_wall=24, gb_free=6.5, wall=17998
2023-08-01 21:21:34 - progress_bar.py[line:272] - INFO: epoch 003:   1851 / 2745 loss=1.645, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=1016.2, nsentences=144, sample_size=1016.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=413.7, ups=0.41, wpb=1016.2, bsz=144, num_updates=7330, lr=1.06084e-05, gnorm=1.115, clip=60, loss_scale=128, train_wall=25, gb_free=6.5, wall=18022
2023-08-01 21:21:59 - progress_bar.py[line:272] - INFO: epoch 003:   1861 / 2745 loss=1.636, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=1017, nsentences=144, sample_size=1017, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=415.1, ups=0.41, wpb=1017, bsz=144, num_updates=7340, lr=1.05793e-05, gnorm=1.131, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=18047
2023-08-01 21:22:23 - progress_bar.py[line:272] - INFO: epoch 003:   1871 / 2745 loss=1.645, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=1017.9, nsentences=144, sample_size=1017.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=415.3, ups=0.41, wpb=1017.9, bsz=144, num_updates=7350, lr=1.05503e-05, gnorm=1.159, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=18071
2023-08-01 21:22:48 - progress_bar.py[line:272] - INFO: epoch 003:   1881 / 2745 loss=1.643, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=1014.3, nsentences=144, sample_size=1014.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=409.7, ups=0.4, wpb=1014.3, bsz=144, num_updates=7360, lr=1.05212e-05, gnorm=1.133, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=18096
2023-08-01 21:23:13 - progress_bar.py[line:272] - INFO: epoch 003:   1891 / 2745 loss=1.649, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=1015.5, nsentences=144, sample_size=1015.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=412.9, ups=0.41, wpb=1015.5, bsz=144, num_updates=7370, lr=1.04922e-05, gnorm=1.166, clip=90, loss_scale=128, train_wall=25, gb_free=6.5, wall=18121
2023-08-01 21:23:37 - progress_bar.py[line:272] - INFO: epoch 003:   1901 / 2745 loss=1.64, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=1016.9, nsentences=144, sample_size=1016.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=415.3, ups=0.41, wpb=1016.9, bsz=144, num_updates=7380, lr=1.04631e-05, gnorm=1.117, clip=70, loss_scale=128, train_wall=24, gb_free=6.5, wall=18145
2023-08-01 21:24:02 - progress_bar.py[line:272] - INFO: epoch 003:   1911 / 2745 loss=1.654, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=1018.3, nsentences=144, sample_size=1018.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=415.4, ups=0.41, wpb=1018.3, bsz=144, num_updates=7390, lr=1.0434e-05, gnorm=1.191, clip=80, loss_scale=128, train_wall=24, gb_free=6.5, wall=18170
2023-08-01 21:24:26 - progress_bar.py[line:272] - INFO: epoch 003:   1921 / 2745 loss=1.641, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=1022.4, nsentences=144, sample_size=1022.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=416.6, ups=0.41, wpb=1022.4, bsz=144, num_updates=7400, lr=1.0405e-05, gnorm=1.14, clip=80, loss_scale=128, train_wall=25, gb_free=6.5, wall=18194
WARNING:torch.distributed.elastic.agent.server.api:Received 2 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 338916 closing signal SIGINT
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 338917 closing signal SIGINT
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 338918 closing signal SIGINT
Traceback (most recent call last):
  File "../../train.py", line 539, in <module>
    cli_main()
  File "../../train.py", line 532, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/zcai75/Github/OFA/fairseq/fairseq/distributed/utils.py", line 374, in call_main
    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
  File "/home/zcai75/Github/OFA/fairseq/fairseq/distributed/utils.py", line 348, in distributed_main
    main(cfg, **kwargs)
  File "../../train.py", line 199, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "../../train.py", line 310, in train
    log_output = trainer.train_step(samples)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zcai75/Github/OFA_forked/trainer.py", line 773, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/home/zcai75/Github/OFA_forked/tasks/ofa_task.py", line 334, in train_step
    loss, sample_size, logging_output = criterion(model, sample, update_num=update_num)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zcai75/Github/OFA_forked/criterions/label_smoothed_cross_entropy.py", line 199, in forward
    net_output = model(**sample["net_input"])
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
Traceback (most recent call last):
  File "../../train.py", line 539, in <module>
    return forward_call(*input, **kwargs)
  File "/home/zcai75/Github/OFA/fairseq/fairseq/distributed/module_proxy_wrapper.py", line 55, in forward
    cli_main()
  File "../../train.py", line 532, in cli_main
    return self.module(*args, **kwargs)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    distributed_utils.call_main(cfg, main)
  File "/home/zcai75/Github/OFA/fairseq/fairseq/distributed/utils.py", line 374, in call_main
    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
  File "/home/zcai75/Github/OFA/fairseq/fairseq/distributed/utils.py", line 348, in distributed_main
    return forward_call(*input, **kwargs)    
main(cfg, **kwargs)  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1008, in forward

  File "../../train.py", line 199, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/contextlib.py", line 75, in inner
Traceback (most recent call last):
    return func(*args, **kwds)
  File "../../train.py", line 310, in train
    log_output = trainer.train_step(samples)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/contextlib.py", line 75, in inner
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 969, in _run_ddp_forward
    return func(*args, **kwds)
  File "/home/zcai75/Github/OFA_forked/trainer.py", line 773, in train_step
  File "../../train.py", line 539, in <module>
    cli_main()
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/home/zcai75/Github/OFA_forked/tasks/ofa_task.py", line 334, in train_step
    return module_to_run(*inputs[0], **kwargs[0])
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
      File "../../train.py", line 532, in cli_main
    distributed_utils.call_main(cfg, main)
loss, sample_size, logging_output = criterion(model, sample, update_num=update_num)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
  File "/home/zcai75/Github/OFA/fairseq/fairseq/distributed/utils.py", line 374, in call_main
    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
  File "/home/zcai75/Github/OFA/fairseq/fairseq/distributed/utils.py", line 348, in distributed_main
    main(cfg, **kwargs)
    return forward_call(*input, **kwargs)
  File "/home/zcai75/Github/OFA_forked/criterions/label_smoothed_cross_entropy.py", line 199, in forward
      File "../../train.py", line 199, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
return forward_call(*input, **kwargs)
      File "/home/zcai75/Github/OFA_forked/models/ofa/ofa.py", line 89, in forward
net_output = model(**sample["net_input"])
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
    encoder_out = self.encoder(
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
  File "../../train.py", line 310, in train
    log_output = trainer.train_step(samples)
    return forward_call(*input, **kwargs)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zcai75/Github/OFA/fairseq/fairseq/distributed/module_proxy_wrapper.py", line 55, in forward
    return self.module(*args, **kwargs)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
  File "/home/zcai75/Github/OFA_forked/trainer.py", line 773, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
    return forward_call(*input, **kwargs)
  File "/home/zcai75/Github/OFA_forked/models/ofa/unify_transformer.py", line 907, in forward
  File "/home/zcai75/Github/OFA_forked/tasks/ofa_task.py", line 334, in train_step
    loss, sample_size, logging_output = criterion(model, sample, update_num=update_num)
    return forward_call(*input, **kwargs)  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)

  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1008, in forward
  File "/home/zcai75/Github/OFA_forked/criterions/label_smoothed_cross_entropy.py", line 199, in forward
    net_output = model(**sample["net_input"])
      File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
return self.forward_scriptable(src_tokens,
  File "/home/zcai75/Github/OFA_forked/models/ofa/unify_transformer.py", line 1056, in forward_scriptable
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 969, in _run_ddp_forward
  File "/home/zcai75/Github/OFA/fairseq/fairseq/distributed/module_proxy_wrapper.py", line 55, in forward
    return self.module(*args, **kwargs)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
    return module_to_run(*inputs[0], **kwargs[0])
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1008, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
    encoder_padding_mask=encoder_padding_mask if has_pads else None,
KeyboardInterrupt
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 969, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
    return forward_call(*input, **kwargs)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zcai75/Github/OFA_forked/models/ofa/ofa.py", line 89, in forward
    encoder_out = self.encoder(
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
  File "/home/zcai75/Github/OFA_forked/models/ofa/ofa.py", line 89, in forward
    encoder_out = self.encoder(
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zcai75/Github/OFA_forked/models/ofa/unify_transformer.py", line 907, in forward
    return self.forward_scriptable(src_tokens,
    return forward_call(*input, **kwargs)
  File "/home/zcai75/Github/OFA_forked/models/ofa/unify_transformer.py", line 907, in forward
  File "/home/zcai75/Github/OFA_forked/models/ofa/unify_transformer.py", line 1056, in forward_scriptable
    encoder_padding_mask=encoder_padding_mask if has_pads else None,
KeyboardInterrupt
    return self.forward_scriptable(src_tokens,
  File "/home/zcai75/Github/OFA_forked/models/ofa/unify_transformer.py", line 1056, in forward_scriptable
    encoder_padding_mask=encoder_padding_mask if has_pads else None,
KeyboardInterrupt
wandb: Waiting for W&B process to finish... (failed 255). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:                  train/bsz ▁▁
wandb:                 train/clip █▁
wandb:              train/gb_free ▁▁
wandb:                train/gnorm █▁
wandb:                 train/loss █▁
wandb:           train/loss_scale ▁▁
wandb:              train/loss_v1 ▁▁
wandb:              train/loss_v2 ▁▁
wandb:                   train/lr █▁
wandb:             train/nll_loss █▁
wandb:           train/nsentences ▁▁
wandb:              train/ntokens █▁
wandb:                  train/ppl █▁
wandb:          train/sample_size █▁
wandb:       train/sample_size_v1 ▁▁
wandb:       train/sample_size_v2 ▁▁
wandb:           train/train_wall █▁
wandb:                  train/ups ▁▁
wandb:                 train/wall ▁█
wandb:                  train/wpb ▁▁
wandb:                  train/wps █▁
wandb:            train_inner/bsz ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:           train_inner/clip ██████████▇▆█▅▆▅▅▄▄▆▁▇▇▇▇▆▅▆▅▄▆▆▅▅▇▄▇▅▆▆
wandb:        train_inner/gb_free ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train_inner/gnorm █▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:           train_inner/loss █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_inner/loss_scale ▁▁▁▁▁▁▂▂▂▃▃▄▄▄█▄▄██▄▄▄██▄▄▄█▄▄▄███████▄▄
wandb:        train_inner/loss_v1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        train_inner/loss_v2 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:             train_inner/lr ▁▃▅█████▇▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃
wandb:       train_inner/nll_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_inner/nsentences ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        train_inner/ntokens ▂▃▂▆▆▅▃▃▆▃█▆▅▅▅▅▅▅▂▆▆▄▇▃▅▁▆▆▆▄▇▆▅▇▅▅▅▅▇▄
wandb:            train_inner/ppl █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    train_inner/sample_size ▂▃▂▆▆▅▃▃▆▃█▆▅▅▅▅▅▅▂▆▆▄▇▃▅▁▆▆▆▄▇▆▅▇▅▅▅▅▇▄
wandb: train_inner/sample_size_v1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_inner/sample_size_v2 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train_inner/train_wall ▃▁▃▁▁▃▃▃▁▃▁▁▁▃▃▁▁▃▃▁▁▁▁▁▃▃▁▁▁▃▁▃▁█▁▁▁▁▁▃
wandb:            train_inner/ups █████████████████████████████████▁█████▆
wandb:           train_inner/wall ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            train_inner/wpb ▂▃▂▆▆▅▃▃▆▃█▆▅▅▅▅▅▅▂▆▆▄▇▃▅▁▆▆▆▄▇▆▅▇▅▅▅▅▇▄
wandb:            train_inner/wps ▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇█▇▇▆█▇▇▇█▇▇▁▇▇▇▇█▆
wandb: 
wandb: Run summary:
wandb:                  train/bsz 144.0
wandb:                 train/clip 75.1
wandb:              train/gb_free 6.5
wandb:                train/gnorm 1.104
wandb:                 train/loss 1.69
wandb:           train/loss_scale 128.0
wandb:              train/loss_v1 0.0
wandb:              train/loss_v2 0.0
wandb:                   train/lr 2e-05
wandb:             train/nll_loss 0.385
wandb:           train/nsentences 143.962
wandb:              train/ntokens 1014.547
wandb:                  train/ppl 1.31
wandb:          train/sample_size 1014.547
wandb:       train/sample_size_v1 0.0
wandb:       train/sample_size_v2 0.0
wandb:           train/train_wall 6723.0
wandb:                  train/ups 0.41
wandb:                 train/wall 13480.0
wandb:                  train/wpb 1014.5
wandb:                  train/wps 412.9
wandb:            train_inner/bsz 144.0
wandb:           train_inner/clip 80.0
wandb:        train_inner/gb_free 6.5
wandb:          train_inner/gnorm 1.14
wandb:           train_inner/loss 1.641
wandb:     train_inner/loss_scale 128.0
wandb:        train_inner/loss_v1 0.0
wandb:        train_inner/loss_v2 0.0
wandb:             train_inner/lr 1e-05
wandb:       train_inner/nll_loss 0.335
wandb:     train_inner/nsentences 144.0
wandb:        train_inner/ntokens 1022.4
wandb:            train_inner/ppl 1.26
wandb:    train_inner/sample_size 1022.4
wandb: train_inner/sample_size_v1 0.0
wandb: train_inner/sample_size_v2 0.0
wandb:     train_inner/train_wall 25.0
wandb:            train_inner/ups 0.41
wandb:           train_inner/wall 18194.0
wandb:            train_inner/wpb 1022.4
wandb:            train_inner/wps 416.6
wandb: 
wandb: 🚀 View run _4_3e-5_512_mix_wearing at: https://wandb.ai/jackcai1206/OFA-VG/runs/t4mrlmdu
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230801_162116-t4mrlmdu/logs
Traceback (most recent call last):
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 850, in _invoke_run
    time.sleep(monitor_interval)
  File "/data/hulab/zcai75/anaconda3/envs/vilt/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 60, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 338881 got signal: 2
